[
  {
    "id": "CVPR-2022-101-rubric-0",
    "token_usage": {
      "prompt_tokens": 11877,
      "completion_tokens": 125,
      "total_tokens": 12002
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets",
          "Reasoning": "The paper introduces four video instance segmentation datasets for evaluating the proposed AirObject method: YouTube Video Instance Segmentation (YT-VIS), Unidentified Video Objects (UVO), Occluded Video Instance Segmentation (OVIS), and Tracking Any Object with Video Object Segmentation (TAO-VOS). These datasets are used for training and evaluation but are pre-existing datasets collected and annotated by humans, not newly introduced in this paper."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-101-rubric-1",
    "token_usage": {
      "prompt_tokens": 12677,
      "completion_tokens": 263,
      "total_tokens": 12940
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.1 Datasets",
            "reasoning": "Section 4.1 describes the use of four video instance segmentation datasets (YT-VIS, UVO, OVIS, TAO-VOS) with high-quality instance masks and annotations of objects across multiple frames, which were prepared by human annotators as indicated by their standard use in the community. These datasets are newly introduced (for the purpose of evaluation) and contain video object sequences with ground-truth instance masks labeled by multiple human experts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1 Datasets",
            "reasoning": "The paper does not provide any annotation instructions or guidelines regarding how the annotations were created for these datasets within the paper content."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1 Datasets",
            "reasoning": "The paper does not mention scoring rubrics or detailed criteria for annotation quality evaluation for these datasets or describe annotation rubrics in any section."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.1 Datasets (none)",
            "reasoning": "No examples or exemplar annotations are provided or referenced in the paper related to annotation guidelines for these datasets."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-101-rubric-2",
    "token_usage": {
      "prompt_tokens": 13867,
      "completion_tokens": 370,
      "total_tokens": 14237
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the introduced datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide information about multiple human experts conducting quality assurance on the introduced datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of a single human non-expert performing quality assurance on the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No details are given about multiple human non-experts conducting quality assurance on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of an AI model for quality assurance on the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information suggesting that an automatic or algorithmic quality assurance process was applied to the datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses four existing video instance segmentation datasets (YT-VIS, UVO, OVIS, TAO-VOS) for training and evaluation purposes but does not introduce any new dataset of its own. Consequently, it does not document or describe any quality assurance process for new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-101-rubric-3",
    "token_usage": {
      "prompt_tokens": 13477,
      "completion_tokens": 290,
      "total_tokens": 13767
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper uses four pre-existing, publicly available video instance segmentation datasets (YT-VIS, UVO, OVIS, TAO-VOS) as the data for training and evaluation. These datasets were not created by the authors but are leveraged as sources without significant modification, indicating the data is collated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source is specified and documented as existing benchmark datasets, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-101-rubric-4",
    "token_usage": {
      "prompt_tokens": 13995,
      "completion_tokens": 316,
      "total_tokens": 14311
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.2 Implementation Details",
          "reasoning": "The authors fine-tune the Graph Attention Encoder on the YT-VIS training split using supervised losses including sparse location loss, dense feature loss, and matching loss, and then train the Temporal Encoder supervised for object identification, indicating the dataset is used for supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets and Section 4.3 Evaluation Criteria",
          "reasoning": "The paper evaluates the proposed AirObject method on four large-scale video instance segmentation datasets (YT-VIS, UVO, OVIS, TAO-VOS) to benchmark performance for video object identification."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.5 Ablation Studies",
          "reasoning": "The datasets are used in ablation studies to analyze the impact of topological graph representations, sequence length, multi-frame features, and temporal encoder architecture, which provide insights into trends and characteristics of the model's performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-101-rubric-5",
    "token_usage": {
      "prompt_tokens": 14718,
      "completion_tokens": 600,
      "total_tokens": 15318
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces new datasets for video object identification tasks, but all datasets comprise videos and annotations of objects; there is no indication of entries containing textual data in multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described contain object video sequences and masks but no mention of exactly two human languages or bilingual textual content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are video instance segmentation datasets with video frames and annotations but are not described as containing only English language textual content; the nature of the datasets is visual/object-based, not linguistic."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets contain textual content in a non-English language exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces new datasets of video object sequences; it does not mention that the dataset entries include any programming language or code snippets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Section 3.1 (Topological Graph Representations)",
          "reasoning": "Mathematical notation and formal expressions (e.g., equations for graph construction and loss functions) appear in the paper describing the methods, but these are not included in the dataset entries themselves. The datasets consist of video sequences and annotations, not mathematical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are video datasets containing human-world objects; no biological sequences or non-human communication data are involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of fictional or artificially created languages in the dataset entries."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The nature of the dataset entries (video data of objects with masks) is clearly described; no language is ambiguously specified."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The introduced datasets consist of video sequences and image annotations of objects; they do not contain any language (human or code) as entries. The dataset entries are visual and structural data, thus no linguistic content exists."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-101-rubric-6",
    "token_usage": {
      "prompt_tokens": 11936,
      "completion_tokens": 225,
      "total_tokens": 12161
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper provides a link to the source code repository for AirObject (https://github.com/Nik-V9/AirObject), but this is related to the method for temporal 3D object encoding, not for constructing or generating any new dataset. The datasets used (YT-VIS, UVO, OVIS, TAO-VOS) are all publicly available, pre-existing video instance segmentation datasets introduced by other authors. The paper itself does not introduce any new dataset, nor does it provide code for dataset construction, preprocessing, or generation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper describes the four datasets used for evaluation (YT-VIS, UVO, OVIS, TAO-VOS) in Section 4.1. However, these datasets are existing publicly available datasets and not newly created by the authors. There is no documentation or description of dataset construction or creation processes from the authors in the paper, only usage and evaluation setup details."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1018-rubric-0",
    "token_usage": {
      "prompt_tokens": 11492,
      "completion_tokens": 137,
      "total_tokens": 11629
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Method, Figure 2 Overview; Section 4 Experiments Implementation Details",
          "Reasoning": "The new dataset is introduced as monocular video sequences of human subjects performing various motions including dances and custom motion sequences. These videos are human recorded data used to estimate 3D body fits and learn motion-dependent appearance. The paper specifically mentions using videos of about 10,000 frames from custom captures and publicly sourced dance videos for training their model. Hence, the new dataset modality is video, human generated, and not model generated or unknown."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1018-rubric-1",
    "token_usage": {
      "prompt_tokens": 12292,
      "completion_tokens": 161,
      "total_tokens": 12453
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3, Supplementary materials",
            "reasoning": "The paper describes that 3D body fits estimates are obtained using a new model-based tracking optimization. This is an automatic process (model-based tracking) rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not mention or provide any human annotation instructions for annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "No scoring rubrics are described in the paper for annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "No annotation examples are described or provided in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1018-rubric-2",
    "token_usage": {
      "prompt_tokens": 13482,
      "completion_tokens": 311,
      "total_tokens": 13793
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on any dataset or annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper describing quality assurance performed by multiple expert human annotators on datasets or annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description of quality assurance involving multiple non-expert human annotators is provided."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance conducted by an AI model as a judge or quality assessor of any dataset or annotation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly describe any automatic verification or algorithmic/rule-based quality assurance processes applied to validate datasets or annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces new datasets (i.e., monocular videos and corresponding 3D body fit estimates obtained from model-based tracking), but does not document or describe any quality assurance process applied to validate the dataset annotations or content. There is no mention of manual or automatic verification or curation steps for data quality; thus, no QA process is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1018-rubric-3",
    "token_usage": {
      "prompt_tokens": 13092,
      "completion_tokens": 507,
      "total_tokens": 13599
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4: Experiments and Implementation Details",
          "reasoning": "The paper presents multiple new video sequences collected by the authors, including custom sequences showing male and female subjects performing assorted motions and dance videos selected from YouTube as described in Section 4. These videos are original human-captured content, not generated or derived from existing datasets. The dataset consists of monocular videos capturing dressed humans in motion, acquired by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset or data exclusively generated by AI or machine learning models without reference or transformation of existing data. The training data is sourced from human-recorded videos."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any dataset created by human translation from another language in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation or similar processes were used to generate any data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4: Experiments",
          "reasoning": "The authors utilize several existing publicly available video sequences from YouTube and prior work [25]. They select these existing datasets as part of their evaluation and training data, which means these are collected and aggregated from existing sources with minor or no modifications."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.3 and Section 4: Experiments",
          "reasoning": "The data used for training includes 3D body fit estimates obtained from monocular videos via a model-based tracking optimization described in Section 3.3 and supplementary materials. Thus, the available original videos are processed and transformed to derive 3D body mesh sequences and UV maps, representing derived data based on existing sources with transformation applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper sufficiently documents the origins and processing of the datasets used, so this category is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1018-rubric-4",
    "token_usage": {
      "prompt_tokens": 13610,
      "completion_tokens": 454,
      "total_tokens": 14064
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using any new datasets exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a new dataset introduced by the authors is used to train models from scratch with randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors do not describe using any newly introduced dataset for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss reinforcement learning post-training techniques or use of new datasets for such methods."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used in the paper for evaluation are pre-existing or publicly available videos; no novel datasets are introduced solely for evaluation purposes."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper analyzes performance, there is no mention of a newly introduced dataset used mainly for analysis rather than training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is described as serving as a knowledge base to augment model capabilities, such as through retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not explicitly introduce or describe any new dataset created by the authors. Instead, the authors utilize existing monocular video sequences (some captured by themselves, some from public sources) for training and evaluation. No practical usage of a novel dataset is documented or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1018-rubric-5",
    "token_usage": {
      "prompt_tokens": 14333,
      "completion_tokens": 551,
      "total_tokens": 14884
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention or introduce any dataset involving multiple human languages. The focus is on visual data of human motion and appearance."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset with exactly two human languages is described or introduced in the paper."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Overall paper content, e.g., Abstract, Introduction, Experiments",
          "reasoning": "The paper and its introduced datasets (videos of human motion for appearance modeling) are primarily described and presented in English. All textual descriptions, experiments, and explanations are in English. No mention of other human languages for dataset entries is found."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention of data in any non-English language in the proposed datasets."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the approach involves programming and model implementations (e.g., PyTorch), the datasets introduced do not contain entries of programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper contains mathematical formulas for the method, these are part of the methodology and not the dataset itself. The datasets introduced do not contain mathematical or logical notation entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human motion video data and 3D body fits, not biological sequences or non-human communication content."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificial languages are present in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset is clearly English for descriptions; there is no ambiguity or unspecified language."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human video data with English textual annotations and descriptions, so it does contain language entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1018-rubric-6",
    "token_usage": {
      "prompt_tokens": 11551,
      "completion_tokens": 147,
      "total_tokens": 11698
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit reference to code availability in the paper",
          "reasoning": "The paper does not mention providing public access to code repositories or links related to the dataset creation, data collection, or preprocessing; thus, no indication of code availability is present."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No dedicated section or detailed description of dataset creation or data collection process",
          "reasoning": "The paper describes the use of existing monocular videos and 3D body fit estimates obtained via model-based tracking (details in supplementary materials), but does not document a new dataset creation process, nor does it provide comprehensive documentation of data collection or dataset construction steps within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1023-rubric-0",
    "token_usage": {
      "prompt_tokens": 12541,
      "completion_tokens": 123,
      "total_tokens": 12664
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and Section 4.1",
          "Reasoning": "The paper explicitly describes training on base classes and evaluation on novel classes using datasets PASCAL-5i and COCO-20i. These datasets consist of images with object segmentation masks, which are manually annotated. The authors do not introduce a new dataset, but use existing standard few-shot semantic segmentation datasets for training and testing. Therefore, no new dataset with data source is introduced by the authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1023-rubric-1",
    "token_usage": {
      "prompt_tokens": 13341,
      "completion_tokens": 325,
      "total_tokens": 13666
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3.1 Background Mining Module; Section 3.5 Distracting Objects Eliminating",
            "reasoning": "The paper describes modules such as the Background Mining Module (BGMM), Background Eliminating Module (BGEM), Distracting Object Eliminating Module (DOEM), and the use of prototypes learned and applied during network training and inference. These represent algorithmic processing steps rather than human annotation of data. The training data (PASCAL-5i and COCO-20i) are pre-existing datasets with provided annotations; the paper proposes methods that learn BG prototypes and do not require ground truth background or distracting object masks. Thus, the annotation of BG and DO regions is done via learned model components and automatic modules rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly provided",
            "reasoning": "The paper does not provide explicit detailed annotation instructions for human annotators related to the new dataset. The BG and DO regions are mined automatically by the model without human annotation instructions or manual annotation guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly provided",
            "reasoning": "There is no mention or description of scoring rubrics for annotation guidelines; since no human annotation of new labels was done, rubrics are not applicable."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly provided",
            "reasoning": "The paper does not provide annotation examples or guideline examples for human annotators since the BG and DO masks are mined automatically without human labeling."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1023-rubric-2",
    "token_usage": {
      "prompt_tokens": 14531,
      "completion_tokens": 284,
      "total_tokens": 14815
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on any newly introduced datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts performed quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any single non-expert human quality assurance process for the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of quality assurance conducted by multiple non-expert annotators is present in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes AI models used for segmentation and contrastive learning, but not for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Automatic verification or rule-based quality assurance of dataset annotations is not described."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses existing benchmark datasets (PASCAL-5i and COCO-20i) for experiments and does not introduce any new datasets, nor does it describe any quality assurance process related to dataset annotations. Therefore, no quality assurance process is applied or documented for new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1023-rubric-3",
    "token_usage": {
      "prompt_tokens": 14141,
      "completion_tokens": 359,
      "total_tokens": 14500
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset Description",
          "reasoning": "The paper uses two benchmark datasets, PASCAL-5i and COCO-20i, which are established public datasets (PASCAL VOC and MSCOCO) partitioned into folds for few-shot setting. This indicates that the data is collected from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset Description",
          "reasoning": "The paper forms the few-shot dataset splits by partitioning existing datasets (PASCAL VOC and MSCOCO) into folds, thus deriving new subsets for evaluation. This constitutes derived data with adaptations for few-shot learning. Additionally, the authors learn a general background (BG) prototype from training data, implicitly utilizing data derived from the original datasets with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset origins and preparation are clearly documented in Section 4.1."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1023-rubric-4",
    "token_usage": {
      "prompt_tokens": 14659,
      "completion_tokens": 211,
      "total_tokens": 14870
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; instead, it uses existing datasets (PASCAL-5i and COCO-20i) for training, evaluation, and benchmarking. There is no description of a newly proposed dataset in the paper, nor is there discussion of their practical use in the machine learning pipeline."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1023-rubric-5",
    "token_usage": {
      "prompt_tokens": 15382,
      "completion_tokens": 552,
      "total_tokens": 15934
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses common image datasets (PASCAL-5i and COCO-20i) for semantic segmentation focused on visual object categories with no mention of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of dataset entries containing exactly two human languages in the paper."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset Description and Introduction (e.g., PASCAL-5i and COCO-20i datasets)",
          "reasoning": "The datasets employed and discussed in the paper (PASCAL-5i and COCO-20i) are image datasets with English annotations for object categories typical in computer vision research, and the paper is written in English describing English-labeled datasets."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of datasets in a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are image segmentation datasets; no code or programming languages are part of the dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes mathematical notation for model description and loss functions, the datasets themselves do not contain mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets discussed contain natural images and annotations of object categories, not biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets contain fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The linguistic nature of the datasets is clearly specified and is English-labeled object segmentation datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain language in the form of English annotations for object classes and do contain English content."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1023-rubric-6",
    "token_usage": {
      "prompt_tokens": 12600,
      "completion_tokens": 248,
      "total_tokens": 12848
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and no explicit section",
          "reasoning": "The paper provides a URL to the code repository (https://github.com/LIUYUANWEI98/NERTNet) related to the proposed method's implementation, but this repository appears to be for the model code. There is no indication that the code used for constructing or processing the datasets (such as PASCAL-5i or COCO-20i) or generating the new datasets is provided or that new datasets constructed by the authors are introduced. The datasets used are existing benchmarks (PASCAL-5i and COCO-20i). Thus, as there is no newly introduced dataset construction code made publicly available, the answer is false."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Throughout the paper",
          "reasoning": "The paper uses existing datasets (PASCAL-5i and COCO-20i), which are publicly available and well-documented elsewhere. The authors do not introduce any new dataset or provide any documentation on dataset creation or construction. They specifically mention the use of standard splits and benchmarks. Therefore, no documentation about new dataset construction is present in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1026-rubric-0",
    "token_usage": {
      "prompt_tokens": 15101,
      "completion_tokens": 118,
      "total_tokens": 15219
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 In-vivo Image Datasets",
          "Reasoning": "The paper introduces a new dataset used in experiments, which consists of in-vivo MRI k-space data from brain and knee images acquired with different MRI sequences and fully sampled using multi-channel coils at 1.5T and 3.0T MR scanners. This data originates from human subjects scanned by MRI machines, so it is human generated signal/sensor data."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1026-rubric-1",
    "token_usage": {
      "prompt_tokens": 15901,
      "completion_tokens": 229,
      "total_tokens": 16130
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 3 (Methodology) and Section 4 (Experiments)",
            "reasoning": "The paper describes a deep learning framework involving neural ODEs to optimize k-space sampling trajectories and a reconstruction network, trained end-to-end on MRI data. This indicates that the annotation process (learning the optimal trajectories) is performed by the AI model rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not mention any detailed annotation instructions provided for the annotation process, as the optimization and learning occur through the model training rather than manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No mention of scoring rubrics or manual evaluation criteria for annotations is provided, since the learning and optimization are automated model processes."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No example annotations or labeling examples are described because the data annotation corresponds to model-learned sampling trajectories, not human-labeled data."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1026-rubric-2",
    "token_usage": {
      "prompt_tokens": 17091,
      "completion_tokens": 328,
      "total_tokens": 17419
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the new datasets. The data used are publicly available datasets (fastMRI), and no expert annotation or validation is described."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on the new datasets introduced. The study uses existing public datasets without described human expert validation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single non-expert human annotator for the data."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No descriptions indicate multiple non-expert human annotators performed quality assurance on the data."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention or describe any AI model used as a quality assurance judge for the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper involves automated processing for trajectory optimization and image reconstruction, there is no mention of automated quality assurance or verification of dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets by the authors. Instead, it uses publicly available datasets (fastMRI) for experiments. There is no description or evidence of quality assurance applied to validate dataset annotations or content. Therefore, no quality assurance process is applied or documented for new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1026-rubric-3",
    "token_usage": {
      "prompt_tokens": 16701,
      "completion_tokens": 444,
      "total_tokens": 17145
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that new datasets of MRI scans were manually collected or created by human contributors specifically for this study. Instead, they used existing in-vivo image datasets acquired with MRI scanners from a publicly available source."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used in this study are not described as generated or synthesized entirely by any AI or machine learning model. The study involves generating optimized k-space trajectories and reconstructing images, but these are outputs of models rather than datasets themselves."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication in the paper that the datasets involve human translations from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No content or dataset involved machine translation processes in this study."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 In-vivo Image Datasets",
          "reasoning": "The in-vivo image datasets used in this paper are from the fastMRI database, which is a publicly available MRI database. The authors selected subsets of knee and brain MR images from this existing source without indication of significant modification, constituting collated data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors derive optimized k-space trajectories and reconstruct images, these are outputs from their models and methods rather than new datasets derived from existing datasets. The input data themselves are not described as derived or heavily transformed versions of existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data source, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1026-rubric-4",
    "token_usage": {
      "prompt_tokens": 17219,
      "completion_tokens": 536,
      "total_tokens": 17755
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 4.2",
          "reasoning": "The paper describes training the neural ODE and the U-Net reconstruction network from scratch using the selected MRI datasets from fastMRI. In Section 4.1, the authors specify training, validation, and testing splits and training details indicating training from scratch. The Adam optimizer and learning rates are defined for both neural ODE and U-Net components, implying training of models from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or indication that the datasets were used to fine-tune pre-trained models. The networks appear to be trained from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss reinforcement learning or RL-based post-training methods such as RLHF using the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The paper evaluates reconstruction quality on held-out test sets using metrics like PSNR and SSIM, comparing fixed versus learned trajectories. This constitutes evaluation and performance measurement of the method using the dataset."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2.1 and Section 5",
          "reasoning": "The paper analyzes the learned k-space trajectories, comparing them with fixed trajectories, visualizing point-spread functions (PSF), and discussing the physical constraints and anatomical differences in learned trajectories. This constitutes analysis of dataset characteristics and trends."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base to augment models via retrieval or other means."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is practically used for training, evaluation and analysis; hence N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1026-rubric-5",
    "token_usage": {
      "prompt_tokens": 17942,
      "completion_tokens": 526,
      "total_tokens": 18468
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes datasets used for MRI image reconstruction but only in the context of medical imaging data; there is no mention of multiple human languages in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that the datasets contain exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Entire paper, implicit from context",
          "reasoning": "The paper is written in English and the dataset descriptions and annotations (e.g., MRI parameters, imaging protocols) are presented in English language terms. The dataset consists of medical images with accompanying metadata, and all documentation is in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English language dataset or data entries are mentioned in the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes using deep learning frameworks and neural networks, but does not propose or release any dataset consisting of code or programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notation and formulas are present in the paper for method description, but the dataset entries themselves do not contain mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human MRI images; no biological sequences or non-human communication data are involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are present in the dataset described."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is clearly English as per the paper's descriptions."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries consist of images with metadata documented in English; hence language is applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1026-rubric-6",
    "token_usage": {
      "prompt_tokens": 15160,
      "completion_tokens": 207,
      "total_tokens": 15367
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention of code availability in the paper",
          "reasoning": "The paper does not include any links or references to publicly available code repositories for the dataset construction, preprocessing, or generation. Despite mentioning the use of PyTorch and TorchDiffeq for their models and solvers, there is no indication that the code used for constructing or processing the datasets has been made publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 (In-vivo Image Datasets)",
          "reasoning": "The paper describes the data sources and preprocessing in Section 4.1, specifying that the experiments were conducted using subsets of the publicly available fastMRI database including brain and knee MR images. The datasets are described in terms of acquisition parameters, sample sizes, train/validation/test splits, data preprocessing (such as cropping to uniform size), and types of imaging sequences used. Thus, the dataset creation and preparation process is documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1055-rubric-0",
    "token_usage": {
      "prompt_tokens": 10807,
      "completion_tokens": 218,
      "total_tokens": 11025
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.3 and 5.1",
          "Reasoning": "The authors introduce new training data sets for their feature decoupling process by collecting \"action samples\" and their corresponding \"coupling samples\" from annotated untrimmed videos, e.g., from THUMOS14 and ActivityNet v1.3 datasets. These samples are derived from video snippets representing portions of human actions or co-occurring contexts, all originally captured in real-world videos, thus human-generated data. The novel aspect lies in how the authors select and use these samples to train their RefactorNet, constituting a new data construction or sampling approach rather than a standard dataset. While the raw videos are existing datasets, the paper's new dataset is specifically these curated collections of action and coupling samples derived from the annotated videos for decoupling. Hence, the data modality is video snippets, originally human-generated from recordings, not model- or algorithm-generated nor unknown origin."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1055-rubric-1",
    "token_usage": {
      "prompt_tokens": 11607,
      "completion_tokens": 280,
      "total_tokens": 11887
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.3",
            "reasoning": "The paper describes the collection of action samples and coupling samples from annotated training videos using the standard TAL annotations. This annotation involves marking action instances with start and end times and action categories, which requires expert knowledge typical in temporal action localization tasks. There is no indication that multiple annotators were involved, nor that the process is automatic or AI-generated; thus, the annotator is best categorized as a single human expert."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "The paper does not mention any detailed annotation instructions provided for annotators regarding the collection or labeling of action samples or coupling samples. The process uses standard TAL annotations without specifying instructions given to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "There is no mention of scoring rubrics or formal evaluation criteria provided as part of the annotation guidelines for creating the action samples or coupling samples."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "The paper does not provide annotation examples or sample annotated data to guide the annotation process; it only provides a visualization of samples in Figure 3 but does not indicate that these serve as annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1055-rubric-2",
    "token_usage": {
      "prompt_tokens": 12797,
      "completion_tokens": 307,
      "total_tokens": 13104
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human annotator who is a subject matter expert or member of the target demographic for dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by multiple human experts for dataset annotation or validation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence or mention of QA conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that multiple non-expert humans performed quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the method uses AI models for feature decoupling and detection, it does not perform quality assurance of the dataset via an AI model judge."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any automated verification process applied for dataset quality assurance."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The authors use existing datasets (THUMOS14 and ActivityNet v1.3) for training and evaluation, but do not introduce any new datasets themselves, nor do they report any quality assurance process related to dataset annotation or validation. Therefore, no dataset quality assurance process is applicable to this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1055-rubric-3",
    "token_usage": {
      "prompt_tokens": 12407,
      "completion_tokens": 332,
      "total_tokens": 12739
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset generated entirely by AI or machine learning models as original data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention translating data from other languages using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention translating data from other languages via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is introduced by collecting or aggregating from existing sources in this paper."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not create any new dataset by modifying or transforming existing datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; it uses existing datasets THUMOS14 and ActivityNet v1.3 for evaluation and does not document any new data source or dataset creation."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1055-rubric-4",
    "token_usage": {
      "prompt_tokens": 12925,
      "completion_tokens": 212,
      "total_tokens": 13137
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. It uses existing public datasets THUMOS14 and ActivityNet v1.3 for evaluation and demonstration of the method. No new dataset is proposed, collected, or utilized for pre-training, training, fine-tuning, evaluation, or analysis, as per the paper's content."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1055-rubric-5",
    "token_usage": {
      "prompt_tokens": 13648,
      "completion_tokens": 530,
      "total_tokens": 14178
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses standard video datasets (THUMOS14 and ActivityNet v1.3) that contain videos presumably with English annotations, but no mention is made of multiple human languages in the dataset content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper suggests that the new dataset(s) contain exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5.1 Datasets and Metrics",
          "reasoning": "The datasets used (THUMOS14 and ActivityNet v1.3) are standard benchmarks in temporal action localization with annotations and content in English. The paper does not introduce a new language or dataset with other languages, so the new dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets with exactly one non-English language content are introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of video features and annotations of actions, not programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical notation to describe their model and loss functions, these do not relate to the dataset content itself."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are human action videos, no biological sequences or non-human communication data are involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or constructed languages are included or mentioned in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the datasets is known and identified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain English language annotations; therefore, this metric is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1055-rubric-6",
    "token_usage": {
      "prompt_tokens": 10866,
      "completion_tokens": 157,
      "total_tokens": 11023
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not mention or provide any link to code related to data collection, preprocessing, or dataset generation. The datasets used are pre-existing benchmarks (THUMOS14 and ActivityNet v1.3) and the paper focuses on the method RefactorNet rather than creating a new dataset or releasing code for dataset construction."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section or details in the paper",
          "reasoning": "The paper does not introduce any new dataset that it constructs or documents. It uses existing datasets for evaluation and does not provide details about data collection or dataset creation procedures. Therefore, no documentation on dataset creation is presented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-107-rubric-0",
    "token_usage": {
      "prompt_tokens": 14698,
      "completion_tokens": 109,
      "total_tokens": 14807
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 (Settings) and throughout the paper",
          "Reasoning": "The paper introduces training on the Kinetics-700 dataset, which consists of YouTube videos of human actions. The authors train their end-to-end model on this dataset. Kinetics-700 videos are human-generated data, recorded and annotated by humans as they are videos of real-world human actions collected from YouTube."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-107-rubric-1",
    "token_usage": {
      "prompt_tokens": 15498,
      "completion_tokens": 325,
      "total_tokens": 15823
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Datasets and Training Protocol",
            "reasoning": "The paper trains the model end-to-end on the Kinetics-700 dataset which comprises 555,774 videos with 700 human action categories. These videos come with annotations (class labels) already provided by the dataset. The annotation process is thus performed by the original Kinetics-700 dataset creators, and this paper uses the dataset annotations directly as ground truth for training without describing any new manual annotation work or human annotators involved by the authors. Hence, the \u2018annotations\u2019 come from an automatic pre-existing dataset annotation process rather than new human annotations by the authors."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No mention found in the paper",
            "reasoning": "The paper does not provide or refer to any specific annotation instructions or guidelines developed or used by the authors for annotating the new dataset Kinetics-700, as the data comes from the original dataset itself."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention found in the paper",
            "reasoning": "There is no indication in the paper about any scoring rubrics or formal evaluation criteria used during annotation of the dataset, consistent with using pre-annotated data from Kinetics-700."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No mention found in the paper",
            "reasoning": "The paper does not provide any annotation examples or guidelines since the dataset annotations are from Kinetics-700, pre-existing prior to this work."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-107-rubric-2",
    "token_usage": {
      "prompt_tokens": 16688,
      "completion_tokens": 295,
      "total_tokens": 16983
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information indicating that quality assurance of dataset annotations was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of multiple human experts performing quality assurance on datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process by a single non-expert annotator for dataset annotations."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that multiple non-expert annotators performed quality assurance on datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using AI models to perform quality assurance on dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses automated training and evaluation procedures, it does not describe any automated verification process for dataset annotation quality assurance."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The datasets used (Kinetics-700, UCF101, HMDB51) are existing datasets for which the authors do not describe any quality assurance processes, nor do they introduce any new annotated datasets. Therefore, no quality assurance process is applied or documented for new datasets by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-107-rubric-3",
    "token_usage": {
      "prompt_tokens": 16298,
      "completion_tokens": 447,
      "total_tokens": 16745
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the authors collected or created any new dataset from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper proposes a class generator that synthesizes features of unseen classes by interpolating and extrapolating features of seen classes, this pertains only to generated feature representations used during model training, not a new dataset composed of raw samples. Thus, no new dataset generated entirely by AI or models is introduced."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that any dataset was human-translated from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that any dataset was machine-translated from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 (Settings)",
          "reasoning": "The authors use the Kinetics-700 dataset for training and UCF101 and HMDB51 datasets for evaluation. These datasets consist of videos collected from existing sources (e.g., YouTube) and are publicly available. The paper specifies collecting 555,774 videos by downloading from links provided in the Kinetics-700 dataset. Therefore, the data is collected or aggregated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors do not create derived datasets based on existing datasets with modifications; rather, they use pre-existing datasets for training and testing."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents clearly the sources of their datasets, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-107-rubric-4",
    "token_usage": {
      "prompt_tokens": 16816,
      "completion_tokens": 467,
      "total_tokens": 17283
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe introducing a new dataset that is used exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While training is described, no new dataset introduced by the authors is used for training from randomly initialized parameters. The authors use existing datasets (Kinetics-700, UCF101, HMDB51) for training and evaluation."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any new dataset introduced by the authors that is used for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or RL-based post-training methods utilizing any new dataset introduced by the authors."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing datasets (UCF101, HMDB51) for evaluation, but does not introduce any new datasets for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No newly introduced dataset is primarily used for analysis of trends, patterns, or characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce a new dataset serving as a knowledge base for augmenting models."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; it uses only existing datasets (Kinetics-700, UCF101, HMDB51) for training and evaluation. Therefore, no practical usage of newly proposed datasets is described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-107-rubric-5",
    "token_usage": {
      "prompt_tokens": 17539,
      "completion_tokens": 544,
      "total_tokens": 18083
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Architecture; Section 4.1 Settings (Datasets description: Kinetics-700, UCF101, HMDB51 class names in English)",
          "reasoning": "The proposed new dataset used for training and evaluation involves class names and semantic embeddings based on Word2vec applied to these class names, which are in English. There is no mention or evidence of multiple or other languages used in the dataset, and the textual semantic data is clearly in English. The video data corresponds to human action categories with English labels (e.g., 'playing basketball', 'skipping rope'). Hence, the dataset is monolingual with English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses algorithms, loss functions, and implementation details in PyTorch code, these do not constitute dataset entries containing programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains mathematical formulas and symbolic representations, but these are part of the methodology description, not part of the dataset entries themselves."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of video samples of human actions and related textual semantics; there is no indication of biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset includes any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is explicitly English with no ambiguity."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes natural language class names in English and hence contains language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-107-rubric-6",
    "token_usage": {
      "prompt_tokens": 14757,
      "completion_tokens": 155,
      "total_tokens": 14912
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract, Section 4.1",
          "reasoning": "The paper mentions that code is available in the abstract but does not provide explicit links or detailed information about code related specifically to dataset construction. The code availability appears to pertain to the method implementation rather than dataset collection or creation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 4.1",
          "reasoning": "The paper uses existing datasets (Kinetics-700, UCF101, HMDB51) and describes the process of filtering and selecting classes and videos for training and testing, but does not introduce or create any new datasets. Therefore, there is no documentation on the creation of any new dataset within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1089-rubric-0",
    "token_usage": {
      "prompt_tokens": 9151,
      "completion_tokens": 156,
      "total_tokens": 9307
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 (Datasets)",
          "Reasoning": "The paper introduces the 'LSUN combined' dataset as a new custom dataset composed by combining LSUN kitchen, LSUN dining room, and LSUN living room datasets. This is described in Section 5.1 where the authors specify that 'LSUN combined is a custom dataset that is a combination of LSUN kitchen, LSUN dining room, and LSUN living room' containing 4.1 million images. Since the dataset is composed by combining existing human-generated image datasets and is used directly for training generative models, its modality is image with human-generated origin."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1089-rubric-1",
    "token_usage": {
      "prompt_tokens": 9951,
      "completion_tokens": 206,
      "total_tokens": 10157
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 5.1 Datasets",
            "reasoning": "The datasets used for evaluation, including COCO-stuff, LSUN bedroom, and LSUN combined, are publicly available benchmark datasets. These datasets are pre-existing and not introduced in the paper. The work introduces a module (LAP) applied to GANs but does not introduce a new dataset."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There is no mention in the paper of annotation instructions provided for any new dataset, as no new dataset is introduced."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics for annotation are discussed because the paper does not introduce or annotate any new dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples are provided, as there is no new dataset annotation discussed in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1089-rubric-2",
    "token_usage": {
      "prompt_tokens": 11141,
      "completion_tokens": 357,
      "total_tokens": 11498
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on the introduced datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information or mention about multiple human experts validating or assuring the dataset quality."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no evidence of quality assurance by a single non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description exists of multiple non-expert human annotators conducting quality assurance on the datasets."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 5.3 Evaluation Metrics",
          "reasoning": "The authors use pretrained segmentation models (DeepLab-v2 and UperNet101) as AI models to segment images for computing class-wise FID and Frechet Segmentation Distance (FSD). These segmentation models serve as automated approaches to assess dataset content and model output quality, constituting a form of AI model-based quality assurance of the dataset annotations and synthetic images."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of quality assurance through automated verification of code or formulas or rule-based algorithmic techniques mentioned for the datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces 'LSUN combined' dataset by combining existing LSUN kitchen, dining room, and living room datasets and mentions class counts obtained via pretrained segmentation models, but does not describe any human or machine quality assurance performed specifically for this introduced dataset. Therefore, no documented quality assurance process is present for new dataset construction itself."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1089-rubric-3",
    "token_usage": {
      "prompt_tokens": 10751,
      "completion_tokens": 446,
      "total_tokens": 11197
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the creation of any original datasets or content created entirely from scratch by humans. Instead, it uses existing scene image datasets for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper focuses on generating images using GAN models, these generated images are outputs of the models, not datasets introduced by the authors. The paper does not claim or introduce any new datasets created solely by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any dataset being produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any dataset being produced by machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing datasets such as COCO-stuff, LSUN bedroom, and LSUN combined (a combination of existing LSUN subsets). These datasets are used for evaluation but the authors did not collect or aggregate new datasets without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 5.1 Datasets",
          "reasoning": "The paper introduces a custom 'LSUN combined' dataset, which is a combination of existing LSUN kitchen, LSUN dining room, and LSUN living room datasets. The combined dataset contains 4.1 million images identified using a pretrained segmentation model. This constitutes a derived dataset based on existing sources with modifications (aggregation and labeling) applied by the authors."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents the origin of the datasets used, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1089-rubric-4",
    "token_usage": {
      "prompt_tokens": 11269,
      "completion_tokens": 256,
      "total_tokens": 11525
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1 and 5.3",
          "reasoning": "The datasets COCO-stuff, LSUN bedroom, and LSUN combined are used as benchmarks to evaluate and compare the performance of the proposed Local Attention Pyramid (LAP) module and baseline GAN models via metrics such as FID, FSD, and class-wise FID."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 and Table 3",
          "reasoning": "The paper performs an empirical analysis of the class-wise visual quality imbalance problem using the datasets by measuring class-wise FID scores and segmenting object classes to understand generation quality distribution."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1089-rubric-5",
    "token_usage": {
      "prompt_tokens": 11992,
      "completion_tokens": 578,
      "total_tokens": 12570
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset containing entries with more than two human languages. The datasets mentioned are COCO-stuff, LSUN bedroom, and LSUN combined, which are image datasets and do not contain human language data."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given of datasets containing exactly two human languages in the entries. The datasets introduced are for visual scene images without language annotations."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce a new dataset consisting of only English textual content. The discussed datasets are image datasets for scene generation tasks and do not include English textual entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset with exactly one non-English language is introduced. The paper focuses on image synthesis datasets without linguistic content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses implementation details and algorithms with mathematical notation, but does not introduce a dataset comprised of code or programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses mathematical notation to describe the methodology, it does not introduce a new dataset containing entries with mathematical or logical formal expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset involving biological sequences or non-human communication is introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not contain or introduce datasets with constructed languages such as Klingon or Esperanto."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the proposed datasets is known (they are image datasets without linguistic entries), hence language is not unknown."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced (e.g., LSUN combined dataset) are visual scene image datasets that do not contain any human language entries or textual data. The datasets are composed of images with multiple semantic classes for scene generation but no linguistic content."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1089-rubric-6",
    "token_usage": {
      "prompt_tokens": 9210,
      "completion_tokens": 144,
      "total_tokens": 9354
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not mention or provide a link to any publicly available code repository for data collection, preprocessing, or generation. The focus is on a proposed attention module for GANs and experiments on existing datasets, but no indication is given that dataset construction code is shared."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section describing dataset creation",
          "reasoning": "The paper employs existing datasets (COCO-stuff, LSUN Bedroom, LSUN Combined) and does not introduce or construct any new dataset. Consequently, there is no documentation provided on dataset creation within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1122-rubric-0",
    "token_usage": {
      "prompt_tokens": 12405,
      "completion_tokens": 96,
      "total_tokens": 12501
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 (Experimental Protocols)",
          "Reasoning": "The paper introduces a new photometric stereo dataset named WPS, consisting of 400 different objects captured under 18 predefined lighting conditions. These are real images captured by camera sensors as indicated by the descriptions of objects and lighting setup, thus image data modality is human generated."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1122-rubric-1",
    "token_usage": {
      "prompt_tokens": 13205,
      "completion_tokens": 210,
      "total_tokens": 13415
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1 (Experimental Protocols)",
            "reasoning": "The newly introduced WPS dataset was established by the authors as part of their research work, implying that annotation was performed or overseen by the expert researchers involved in this study."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No specific section describing annotation instructions for the new dataset.",
            "reasoning": "The paper does not describe any detailed instructions provided to annotators for annotation of the WPS dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention is made of scoring rubrics or criteria for annotation in the dataset construction.",
            "reasoning": "There is no indication in the paper of rubrics used for annotation of the WPS dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No appendix or supplementary material describes annotation examples for the WPS dataset.",
            "reasoning": "The paper does not provide example annotations or guideline examples related to dataset annotation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1122-rubric-2",
    "token_usage": {
      "prompt_tokens": 14395,
      "completion_tokens": 332,
      "total_tokens": 14727
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human annotator who is a subject matter expert or target demographic member for the WPS dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper about quality assurance involving multiple human annotators with subject matter expertise for the newly created WPS dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any quality assurance or validation process performed by a single non-expert human annotator on the WPS dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains no information about quality assurance being conducted by multiple human non-expert annotators for the WPS dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made in the paper about quality assurance of the dataset being performed by AI models acting as judges."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification process or algorithmic quality assurance for the annotations or content of the WPS dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new photometric stereo dataset (WPS) consisting of 400 objects but provides no details or documentation regarding any quality assurance process for the dataset generation, annotation validation, or correctness verification. Therefore, it appears that no quality assurance process is applied or at least none is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1122-rubric-3",
    "token_usage": {
      "prompt_tokens": 14005,
      "completion_tokens": 465,
      "total_tokens": 14470
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1, Dataset descriptions",
          "reasoning": "The authors established a new photometric stereo dataset named WPS consisting of 400 different objects. This dataset was captured by the authors under 18 predefined lighting conditions. Therefore, this is original data created by human contributors from scratch, not translated, adapted, or derived from pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper indicates that any dataset was generated entirely by AI or machine learning models without reference or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the authors use existing public datasets like DiLiGenT and Gourd & Apple, the newly introduced WPS dataset was newly captured and not collected from existing sources. Hence, the new dataset is not collated."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2, Multimodal Pre-processing Stage (MPS)",
          "reasoning": "The paper describes deriving bridge normal maps (texture normal map and shape normal map) from the original normal maps using average filtering and other operations to establish correlation between modalities. These derived maps are modifications or adaptations based on existing sources (original normal images). This indicates derived data created through transformations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset origin and method of generation for the new dataset WPS is specified and documented; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1122-rubric-4",
    "token_usage": {
      "prompt_tokens": 14523,
      "completion_tokens": 296,
      "total_tokens": 14819
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 (Experimental Protocols)",
          "reasoning": "The newly constructed photometric stereo dataset called WPS, consisting of 400 objects, is used to train the proposed MNSRNet model from scratch as described in Section 4.1. The paper explicitly mentions using WPS for training, including data division (9 training : 1 validation) and training details."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 (Experimental Protocols) and Section 4.2 (Performance Comparisons)",
          "reasoning": "The WPS dataset is used together with public datasets (DiLiGenT and Gourd & Apple) to form the testing dataset for benchmarking and performance evaluation of the proposed method and baselines, as detailed in Section 4.1 and 4.2."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1122-rubric-5",
    "token_usage": {
      "prompt_tokens": 15246,
      "completion_tokens": 518,
      "total_tokens": 15764
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The newly constructed photometric stereo dataset (WPS) described in Section 4.1 contains photographic data of 3D objects and their modalities (texture, depth, normal images). There is no mention or implication of entries in multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state or suggest that the new dataset contains exactly two human languages or bilingual content anywhere."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper is written in English, the dataset itself consists of images and 3D surface data, not language content. There is no indication that the dataset entries contain any English textual content as data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries (images and 3D data) do not contain any non-English language content as per the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises photographic and 3D surface modalities, not programming or code entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Section 4.1; Dataset descriptions",
          "reasoning": "The dataset itself is described as photographic and geometric data, not containing mathematical or logical expressions as content entries. Mathematical notation appears in the methodology and equations describing the model, but not as part of dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of 3D object surfaces (butterfly wings, leaves, oil paintings, handicrafts), which are visual data. There is no indication that biological or non-human communication data (like DNA sequences or signals) are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that the dataset contains constructed languages or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset entries is clearly not applicable as entries are images and 3D modality data, not linguistic data with unspecified languages."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The newly introduced dataset (WPS) consists solely of multimodal visual and geometric data (photometric stereo images, depth, normals) without containing any human language text or linguistic data. Therefore, the dataset does not contain entries with any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1122-rubric-6",
    "token_usage": {
      "prompt_tokens": 12464,
      "completion_tokens": 189,
      "total_tokens": 12653
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4.1 Experimental Protocols",
          "reasoning": "The paper describes the establishment of a new photometric stereo dataset called WPS with 400 objects and mentions its use for training and testing. However, there is no indication or link provided in the paper to any publicly available code or repository for data collection, preprocessing, or generation related to this new dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Experimental Protocols",
          "reasoning": "The paper provides a description of the newly constructed photometric stereo dataset WPS, including the number of objects (400), the types of objects (butterfly wings, leaves, oil paintings, handicrafts), the lighting conditions (18 predefined lighting conditions), and the split between training, validation, and testing subsets. This information constitutes documentation of the dataset creation process relevant for reproducibility."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1144-rubric-0",
    "token_usage": {
      "prompt_tokens": 16011,
      "completion_tokens": 192,
      "total_tokens": 16203
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 (Method), Section 3 (Study: Target Features for MaskFeat), and Section 4 (Experiments: Video Recognition)",
          "Reasoning": "The paper introduces a new pre-training dataset comprised of unlabeled videos from the Kinetics-400 and Kinetics-600 datasets, which are captured and provided by human-operated video recording. The authors explicitly state they pre-train their models on the Kinetics-400 and Kinetics-600 datasets without using labels, indicating the videos are human-generated. There is no indication that these videos are model/simulated generated or of unknown origin. This dataset is the one used for their new masked feature prediction pre-training method (MaskFeat) on video data. The focus is on pre-training on unlabeled human-captured videos."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1144-rubric-1",
    "token_usage": {
      "prompt_tokens": 16811,
      "completion_tokens": 267,
      "total_tokens": 17078
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.1, 2.2, and 3",
            "reasoning": "The new datasets used for pre-training are unlabeled video datasets Kinetics-400 and Kinetics-600 which are publicly available and used without labels as unlabeled data for self-supervised learning. The MaskFeat approach performs prediction of features (e.g., HOG) extracted automatically through algorithmic feature extraction methods rather than manual annotation. There is no indication of human annotators performing data annotation for these datasets in this paper as the method is self-supervised and based on unlabeled videos."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There are no manual annotation instructions discussed for the unlabeled Kinetics datasets as no human annotation was performed; the pre-training uses automatic feature computation and self-supervised learning."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics are provided because there is no human annotation or grading involved in the unsupervised feature prediction process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples exist or are needed for the automatic feature extraction and regression process described."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1144-rubric-2",
    "token_usage": {
      "prompt_tokens": 18001,
      "completion_tokens": 318,
      "total_tokens": 18319
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the introduced datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description of multiple human experts performing quality assurance on any dataset in this paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or evidence in the paper indicates multiple non-expert human annotators performed quality assurance on any dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss or suggest that AI models were used as judges for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses automatic feature extraction (e.g., HOG) and model-driven pre-training tasks, it does not describe any automated quality assurance processes validating dataset content or annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets, nor does it describe any annotation or quality assurance process. It mainly uses existing datasets (e.g., Kinetics-400, ImageNet) and focuses on a self-supervised learning approach without dataset annotation or QA. Therefore, no quality assurance process is applied or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1144-rubric-3",
    "token_usage": {
      "prompt_tokens": 17611,
      "completion_tokens": 411,
      "total_tokens": 18022
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the authors created any new dataset entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper focuses on pre-training models on existing video datasets and does not introduce datasets generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data created by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any data generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The paper uses existing large-scale video datasets such as Kinetics-400, Kinetics-600, and Kinetics-700 for pre-training and evaluation. These datasets are publicly available and collected from existing sources without significant modification by the authors. Thus, the data used are aggregated or collated from existing datasets."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper describes using features such as HOG extracted from video frames as prediction targets, these are transformations of the input data rather than new datasets. The paper does not introduce new datasets derived by modification or adaptation of existing ones."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and sources of the data used are specified clearly; existing datasets are named and described."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1144-rubric-4",
    "token_usage": {
      "prompt_tokens": 18129,
      "completion_tokens": 575,
      "total_tokens": 18704
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Sections 2.1, 3, 4, 5",
          "reasoning": "The paper introduces MaskFeat, a method that uses unlabeled video and image datasets (e.g., Kinetics-400, Kinetics-600, Kinetics-700, and ImageNet-1K without labels) exclusively for self-supervised pre-training of video and image models by predicting features of masked regions (e.g., HOG features). This pre-training stage learns general visual and spatiotemporal representations prior to any supervised fine-tuning."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's experiments contrast the effectiveness of MaskFeat pre-training against training from scratch baselines, which are standard or prior models, but do not introduce new datasets used specifically for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 3, 4, 5",
          "reasoning": "After self-supervised pre-training on unlabeled datasets, the models are fine-tuned in a supervised manner on labeled datasets for various downstream tasks such as Kinetics classification, AVA action detection, and Something-Something v2. This supervised fine-tuning on standard labeled datasets is part of the transfer learning evaluation pipeline."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of reinforcement learning or RL-based post-training techniques such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 3, 4, 5",
          "reasoning": "The same datasets (Kinetics-400, Kinetics-600, Kinetics-700, AVA, SSv2, ImageNet-1K) are also used for evaluation of downstream task performance to benchmark the effectiveness of the pre-training method. This is standard and described throughout the experiments sections."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3, 5.2",
          "reasoning": "The authors analyze the impact of different feature targets (pixel colors, HOG, dVAE tokens, deep features, pseudo-labels) on model performance and efficiency using these datasets during pre-training. They also provide qualitative analyses and ablation studies on design choices such as masking strategy and HOG implementation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced and used in the paper are not described as knowledge bases for retrieval-augmented generation or similar purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes the utilization of the new datasets for pre-training, fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1144-rubric-5",
    "token_usage": {
      "prompt_tokens": 18852,
      "completion_tokens": 486,
      "total_tokens": 19338
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing entries with multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention of any dataset with exactly two human languages in the entries."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used (Kinetics-400, Kinetics-600, ImageNet-1K, etc.) are visual/video/image datasets and do not contain entries of English language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset with non-English linguistic content is proposed or analyzed in the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset contains programming or code-related content; the datasets are visual/video/image datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced or used do not contain any entries with mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or animal communication datasets are presented or introduced in this paper."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data involving fictional or artificial languages is proposed or used."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Language content of datasets is clear or is not applicable as the datasets are vision-based."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The datasets proposed or used in the paper are visual and video datasets consisting of images or videos, which do not contain entries with language content."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1144-rubric-6",
    "token_usage": {
      "prompt_tokens": 16070,
      "completion_tokens": 163,
      "total_tokens": 16233
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 1 Introduction, near the end",
          "reasoning": "The paper states that their code will be available in PyTorchVideo with a GitHub link (https://github.com/facebookresearch/pytorchvideo). Since PyTorchVideo is a publicly accessible repository, and their method is integrated there, it indicates that code related to the dataset preprocessing and model training will be available."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "N/A",
          "reasoning": "The paper does not introduce any new datasets; it uses existing datasets like Kinetics-400, Kinetics-600, Kinetics-700, AVA, SSv2, and ImageNet. Therefore, there is no dataset creation process documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1148-rubric-0",
    "token_usage": {
      "prompt_tokens": 11789,
      "completion_tokens": 144,
      "total_tokens": 11933
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets and Evaluation",
          "Reasoning": "The paper introduces and evaluates on the Distinctions-646 dataset, which the authors mention consists of 646 distinct foreground images divided into train and test samples. The images are synthetic composites generated by compositing foreground images with background images following the composition rule from prior work. Foreground images are human-created photographs (human generated), and the compositing process to create the dataset is model generated or algorithmic, hence the dataset contains both human generated (original photos) and model generated (combined composite images) image data."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1148-rubric-1",
    "token_usage": {
      "prompt_tokens": 12589,
      "completion_tokens": 358,
      "total_tokens": 12947
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 (Datasets and Evaluation)",
            "reasoning": "The new dataset introduced in this paper is Distinctions-646. It is described as comprised of 646 distinct foreground images split into train and test sets. The paper states that trimaps are not officially released; the authors generate trimaps from ground truth alpha mattes by binarization and random dilation, indicating that the annotation (trimap creation) process is automated rather than manually performed. There is no indication in the text that human annotators created the alpha mattes or trimaps for this dataset within this paper; Distinctions-646 is presented as a new dataset introduced by [34], cited in the paper. Hence, the annotation is handled via an automatic process within the training/testing preparation in this paper."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "No instructions for manual annotation are described in the paper for the Distinctions-646 dataset. The dataset usage involves automatic trimap generation from the ground truth alpha mattes, without human guide or instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "The paper does not mention any scoring rubrics or quality assessment guidelines for annotators in generating the dataset. The metrics used (SAD, MSE, etc.) are evaluation metrics for model performance, not annotation rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "The paper does not provide or mention any annotation examples or exemplar guidance related to annotation for this dataset. The dataset is used directly for training/evaluation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1148-rubric-2",
    "token_usage": {
      "prompt_tokens": 13779,
      "completion_tokens": 271,
      "total_tokens": 14050
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance processes for dataset annotations conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of quality assurance involving multiple human experts for dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide information about any QA process involving a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided regarding quality assurance by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss using AI models to perform quality assurance on datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification or rule-based techniques being used for quality assurance of dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses existing public datasets (Composition-1k and Distinctions-646) for training and evaluation but does not introduce any new datasets. Consequently, no quality assurance process for new dataset annotation or content validation is described or performed."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1148-rubric-3",
    "token_usage": {
      "prompt_tokens": 13389,
      "completion_tokens": 446,
      "total_tokens": 13835
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the authors created any new dataset from entirely new human-contributed content. Instead, they use existing public datasets (Composition-1k and Distinctions-646) for evaluation and training."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any new dataset solely by AI or machine learning models. The datasets used are existing ones."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any dataset produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any machine translation applied to produce dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets and Evaluation",
          "reasoning": "The paper evaluates on two publicly available datasets, Composition-1k and Distinctions-646, which are composed by combining foreground and background images from existing datasets such as PASCAL VOC2012 and MS COCO. These datasets are collected or aggregated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets and Evaluation",
          "reasoning": "Distinctions-646 dataset is derived by synthesizing foreground images with background images following a composition rule (as per [51]). Also, the authors generate trimaps from ground truth alpha mattes by binarizing and dilating, which is a derived form of existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of generation of datasets used are specified and documented, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1148-rubric-4",
    "token_usage": {
      "prompt_tokens": 13907,
      "completion_tokens": 230,
      "total_tokens": 14137
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new dataset is introduced by the authors in the paper. The authors utilize existing public datasets, namely Composition-1k and Distinctions-646, for evaluation and training purposes. The paper mentions that they train and evaluate their model on these publicly available datasets but does not present or contribute any novel, newly created dataset. Therefore, no practical usage of a new dataset introduced by the paper is described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1148-rubric-5",
    "token_usage": {
      "prompt_tokens": 14630,
      "completion_tokens": 533,
      "total_tokens": 15163
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the introduction of a new dataset containing entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset containing exactly two human languages is introduced or described in the paper."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce a new dataset containing only English language content; it uses existing image matting datasets composed of images and annotations."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset with exactly one non-English language content is introduced in the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses code related to model implementation and architecture but does not introduce a new dataset containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical expressions to describe the image matting problem, it does not introduce a new dataset containing mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or introduction of datasets involving biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce datasets containing fictional or artificially constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) of the datasets are known or not applicable because the paper does not propose any new linguistic dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces no new datasets. It only evaluates the proposed method on existing publicly available image matting datasets (Composition-1k and Distinctions-646), which are image datasets and do not contain any human language content. Therefore, the linguistic scope does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1148-rubric-6",
    "token_usage": {
      "prompt_tokens": 11848,
      "completion_tokens": 163,
      "total_tokens": 12011
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section",
          "reasoning": "The paper mentions that the authors' code for their method is publicly available at https://github.com/webtoon/matteformer, but there is no indication that the code for constructing any dataset is provided. Further, the datasets used (Composition-1k and Distinctions-646) are pre-existing public datasets, and no new dataset created by the authors is introduced in the paper."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No specific section",
          "reasoning": "The paper does not introduce or create any new dataset; it uses publicly available existing datasets (Composition-1k and Distinctions-646). Therefore, there is no documentation of any new dataset creation process in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1158-rubric-0",
    "token_usage": {
      "prompt_tokens": 11818,
      "completion_tokens": 99,
      "total_tokens": 11917
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset and Evaluation",
          "Reasoning": "The paper introduces a new adversarial attack method but applies it on pre-existing datasets, including Kinetics-400, Epic-Kitchens, and IITMD-WFP, which are human-recorded datasets of videos. The paper does not introduce any new dataset but evaluates only on these benchmark datasets."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1158-rubric-1",
    "token_usage": {
      "prompt_tokens": 12618,
      "completion_tokens": 252,
      "total_tokens": 12870
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1, Dataset and Evaluation",
            "reasoning": "The new dataset introduced is IITMD-WFP (or IITMD-FPR); it consists of egocentric video data for wearer recognition. There is no explicit mention that human annotators performed labeling for the data annotation. The dataset is cited as [38], indicating it is a previously introduced dataset, so no new dataset introduced by this paper strictly. The paper does not mention collecting new annotation data or new labeling efforts; it uses existing datasets. Thus, any annotation (labels) is likely from existing sources or automatic assignments tied to the known dataset properties."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not describe providing annotation instructions specific to IITMD-WFP or any new dataset for annotations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics for data annotation tasks in any new dataset introduced are described in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples related to any new dataset introduced are provided in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1158-rubric-2",
    "token_usage": {
      "prompt_tokens": 13808,
      "completion_tokens": 277,
      "total_tokens": 14085
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the introduced datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by multiple human experts for the datasets used."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance by a single non-expert annotator in the paper for the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance performed by multiple human non-expert annotators on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance by AI models as judges for dataset annotations is described in the paper."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance for the dataset annotations was done through automated verification or algorithmic techniques as described."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets nor does it describe any quality assurance process for dataset annotations or content. All datasets used are pre-existing, and no QA processes related to dataset annotation quality are documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1158-rubric-3",
    "token_usage": {
      "prompt_tokens": 13418,
      "completion_tokens": 446,
      "total_tokens": 13864
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the creation of any new dataset collected or created entirely by humans from scratch; rather, it uses existing datasets for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not generate new datasets entirely by AI or machine learning models; it uses adversarial perturbations on existing videos but does not create new data from scratch by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of any data created by human translation of content from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report or mention the use of machine translation to generate data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used for experiments (Kinetics-400, Epic-Kitchens, IITMD-WFP) are existing datasets and have not been aggregated together to form a new dataset; no new dataset is presented as collated data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper applies adversarial perturbations to existing videos, but does not present a new dataset that is a transformation or adaptation of the original datasets for public release. The transformed adversarial videos are for attack evaluation but the paper does not present these as new datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. It only uses existing public datasets (Kinetics-400, Epic-Kitchens, IITMD-WFP) for evaluation of their adversarial attack method. There is no description or documentation of any newly introduced dataset."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1158-rubric-4",
    "token_usage": {
      "prompt_tokens": 13936,
      "completion_tokens": 364,
      "total_tokens": 14300
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 (Dataset and Evaluation) and Section 4.2 (Quantitative Comparison)",
          "reasoning": "The introduced evaluation setup is based on using existing benchmark datasets (Kinetics-400, Epic-Kitchens, IITMD-FPR) to measure the success rate and average number of queries for adversarial attacks. Although these datasets are pre-existing, the paper's contribution lies primarily in proposing an adversarial attack method evaluated on these datasets, not a new dataset. There is no indication that new datasets are introduced by the authors for evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.4 (Ablation Study)",
          "reasoning": "The authors analyze the effects of different perturbation types (intensity-based, rotation-based, and their combination) and the distribution of the composition parameter gamma. They also analyze the relationship between gamma and video content, providing insights into their method rather than creating or utilizing a new dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. Instead, it employs pre-existing datasets for evaluation and analysis of their proposed adversarial attack method on DNN video analysis models. Therefore, no new dataset utility applies."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1158-rubric-5",
    "token_usage": {
      "prompt_tokens": 14659,
      "completion_tokens": 526,
      "total_tokens": 15185
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset with more than two human languages. All datasets mentioned are video datasets focusing on visual content without linguistic diversity."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that any of the new datasets contain exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets mentioned (Kinetics-400, Epic-Kitchens, IITMD-FPR) are video datasets related to video actions and activities, and do not specify contents in English language or any spoken language."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of datasets containing non-English language content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are video datasets and do not contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical expressions to describe the methodology, these are not part of any dataset introduced by the authors."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets containing biological sequences or non-human communication systems are introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There are no datasets involving constructed or artificial languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets' language content is not specified because they are video datasets with no linguistic indexing or textual content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The datasets introduced and used in this paper are video datasets (Kinetics-400, Epic-Kitchens, IITMD-FPR) containing visual content related to actions and activities, without any language content. Therefore, the datasets do not contain any language entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1158-rubric-6",
    "token_usage": {
      "prompt_tokens": 11877,
      "completion_tokens": 140,
      "total_tokens": 12017
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not provide any link or mention to code repositories associated with datasets or dataset construction. The datasets referenced are existing public datasets (Kinetics-400, Epic-Kitchens, IITMD-WFP) rather than new datasets introduced by the authors requiring dataset code publication."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "None",
          "reasoning": "Although the paper uses multiple datasets, it does not introduce any new dataset. Hence, there is no documentation about dataset creation process. The details provided about datasets are only brief descriptions of existing datasets and their domains, not of new dataset construction."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1182-rubric-0",
    "token_usage": {
      "prompt_tokens": 8970,
      "completion_tokens": 219,
      "total_tokens": 9189
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5: Experiments",
          "Reasoning": "The paper uses the SIDD dataset as the data source for experiments. Although SIDD is a pre-existing dataset, the authors introduce a new noisy image dataset by extracting approximately 3,000 patches of size 32x32 from each sRGB image in SIDD-Medium split and process these patches (randomly distributed for training and validation). The data used for training their model includes these processed sRGB image patches and corresponding clean patches. These images were captured from smartphones under various ISO and lighting conditions, which indicates human-generated photographic data acquired by real cameras. The authors specifically use the sRGB images processed with a software ISP pipeline simulating camera ISP, suggesting these image patches are derived from real camera captures and processed image data, not synthetic or model-generated images. Therefore, the modality is image, origin is human-generated (captured by cameras operated by humans), and not model-generated or unknown origin."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1182-rubric-1",
    "token_usage": {
      "prompt_tokens": 9770,
      "completion_tokens": 309,
      "total_tokens": 10079
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 5 (Experiments) and Section 1 (Introduction)",
            "reasoning": "The new dataset used for modeling and evaluation is the SIDD-Medium split, which contains noisy-clean image pairs and is explicitly stated as a dataset used for training and validation in the experiments. The annotation of clean and noisy images is considered an automatic or data collection process rather than manual subjective annotation by humans. There is no indication they were annotated by humans or experts for noise labels but rather captured directly by the camera sensors and processed via a simplified software ISP."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Sections 5 and 1",
            "reasoning": "No mention or description of explicit annotation guidelines or instructions provided for annotators is given in the paper for the SIDD dataset or any new data acquired. The dataset is used as-is for training and evaluation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Sections 5 and 1",
            "reasoning": "The paper does not describe any use of rubrics, scoring guidelines, or evaluation criteria for annotation of the data. The SIDD dataset is a collection of noisy and clean image pairs without subjective scoring or label rubric."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Sections 5 and 1",
            "reasoning": "There is no mention of providing annotation examples or exemplar data labeling instructions in the paper. The paper describes the dataset as collected images rather than annotated via examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1182-rubric-2",
    "token_usage": {
      "prompt_tokens": 10960,
      "completion_tokens": 301,
      "total_tokens": 11261
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on the dataset. The SIDD dataset is used but no manual QA by experts is described."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts performed quality assurance on the dataset annotations or content within this paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any quality assurance performed by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human non-experts conducted any quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report use of any AI models as judges or for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description or evidence of automated verification or algorithmic quality assurance processes performed on the dataset is provided in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses the publicly available SIDD dataset and does not describe performing any additional data annotation or a quality assurance process on the dataset. All evaluation and experiments are based on the dataset as provided; thus, no quality assurance process by the authors is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1182-rubric-3",
    "token_usage": {
      "prompt_tokens": 10570,
      "completion_tokens": 475,
      "total_tokens": 11045
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the creation of any new dataset originating entirely from scratch by human contributors. Instead, it uses existing datasets for training and evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper proposes a noise model able to generate synthetic noisy images, it does not introduce any new dataset generated entirely by AI models without reference to existing data. The focus is on modeling noise distributions and synthesizing noise samples, not creating a new dataset as a resource."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of any data translated from another language by humans is present in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data generated by machine translation systems is discussed or introduced by the authors."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5 (Experiments)",
          "reasoning": "The paper exclusively uses data extracted from the existing SIDD dataset for training and validating the proposed noise model. The authors extract approximately 3,000 patches from SIDD images but do not create a new dataset. The data is collected and aggregated from this existing source without fundamental modification, fitting the 'Collated' category."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper applies a software ISP on RAW-rgb images of the SIDD dataset, this is standard processing present in SIDD data, not a novel derivation or modification to create a new dataset. The authors do not release or propose a newly derived dataset based on the original data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and method of generation of data used and referenced in the paper is well documented (SIDD dataset), hence N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1182-rubric-4",
    "token_usage": {
      "prompt_tokens": 11088,
      "completion_tokens": 531,
      "total_tokens": 11619
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5",
          "reasoning": "The paper uses the SIDD dataset sRGB images to train their proposed noise model from scratch by minimizing the negative log likelihood loss (Section 5). The dataset is directly used to train the normalizing flows model to capture sRGB noise characteristics."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.3 and Table 3",
          "reasoning": "The synthesized noisy images generated from their noise model trained on the dataset are used to train a denoiser model (DnCNN) in a supervised fine-tuning manner, which is then evaluated on the SIDD Benchmark test set. This indicates usage of the dataset for supervised fine-tuning downstream models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss or demonstrate any reinforcement learning or RL-based post-training techniques using the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1 and 5.3, Table 1 and Table 3",
          "reasoning": "The SIDD dataset sRGB images are used for testing and benchmarking the quality of noise modeling through negative log likelihood and KL divergence metrics. The dataset is also used for evaluating denoiser performance on the SIDD Benchmark test set."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 and Figure 2, Figure 4",
          "reasoning": "The paper analyzes trends and characteristics of sRGB noise in the SIDD dataset, studying noise variance behavior across ISO levels and camera types, which informs the design of their noise model."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for retrieval or augmentation in this work."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is clearly used for model training, evaluation, analysis, and supervised fine-tuning purposes as detailed above."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1182-rubric-5",
    "token_usage": {
      "prompt_tokens": 11811,
      "completion_tokens": 553,
      "total_tokens": 12364
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset used is the Smartphone Image Denoising Dataset (SIDD), containing images captured by various smartphone cameras; however, the paper does not mention any human language data or multilingual content within the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper indicates the dataset contains exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset entries with human language content, English or otherwise, are described. The dataset consists of images from smartphone cameras; linguistic content is not present."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain entries in any human language, non-English or otherwise."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises image patches extracted from sRGB images; no programming or structured code-related content is described as part of the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notation appears extensively in the paper text to describe models and methods, but the dataset itself does not include entries with mathematical or logical expression content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data is present in the dataset; it consists only of sRGB images from smartphone cameras."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed languages or fictional languages appear in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset\u2019s nature and contents are clearly described (image patches from sRGB smartphone images); no ambiguity exists regarding language content because images are non-linguistic by nature."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new dataset introduced by the authors consists of image patches extracted from sRGB images captured by smartphones; the data do not contain any language, human or otherwise."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1182-rubric-6",
    "token_usage": {
      "prompt_tokens": 9029,
      "completion_tokens": 188,
      "total_tokens": 9217
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Conclusion and throughout the paper",
          "reasoning": "The paper provides source code for the noise model at https://yorkucvil.github.io/sRGBNoise/ but does not mention providing code for the dataset construction or preprocessing. The dataset used, SIDD, is a pre-existing dataset. The paper does not introduce any new dataset or the code to construct a new dataset. Therefore, no code for dataset construction is available."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 5 Experiments",
          "reasoning": "The paper uses the SIDD dataset which is a publicly available, pre-existing dataset; no new dataset was created by the authors. The paper does not describe or document any new dataset creation process, only referencing the use of SIDD and extraction of patches for training. Hence, there is no documentation related to dataset construction provided in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1196-rubric-0",
    "token_usage": {
      "prompt_tokens": 14251,
      "completion_tokens": 141,
      "total_tokens": 14392
    },
    "response": {
      "sources": [
        {
          "Modality": "time series",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 6 (Experiments) and Section 7 (Conclusion)",
          "Reasoning": "The new dataset introduced is the Archive of Motion Capture as Surface Shapes (AMASS), which is a unified collection of 18 motion capture datasets with 13944 motion-sequences from 460 subjects performing various actions. This dataset consists of human motion capture time series data, inherently requiring human involvement during data collection via motion capture systems. The paper uses AMASS as a new dataset for quantitative evaluation, confirming it is human generated time series data specifically for their motion prediction models."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1196-rubric-1",
    "token_usage": {
      "prompt_tokens": 15051,
      "completion_tokens": 267,
      "total_tokens": 15318
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 6, Datasets paragraph",
            "reasoning": "The new dataset introduced in the paper is the AMASS dataset, described as a unified collection of 18 motion capture datasets totaling 13944 motion-sequences from 460 subjects. The paper uses this dataset for evaluation, but no mention is made of any human annotators performing labeling or annotation manually. The data annotation is implicitly done by motion capture systems or automatic data collection processes."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit annotation guideline instructions mentioned for the AMASS dataset in the paper",
            "reasoning": "The paper does not describe any annotation guidelines or instructions for annotators for the AMASS dataset, nor does it suggest any manual annotation was involved."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No rubric or scoring system mentioned for annotating the AMASS dataset",
            "reasoning": "Because there is no evidence of manual annotation or evaluation within the paper related to the AMASS dataset annotation, no rubrics are described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of annotation provided in the paper for the AMASS dataset",
            "reasoning": "Since no detailed annotation process or guidelines are described, no example annotations are shown."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1196-rubric-2",
    "token_usage": {
      "prompt_tokens": 16241,
      "completion_tokens": 284,
      "total_tokens": 16525
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human annotator who is an expert or target demographic member for the new dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of quality assurance involving multiple human experts for the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is given about multiple non-expert humans performing quality assurance on dataset annotations."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of AI models to perform quality assurance on datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated verification or algorithmic rule-based quality assurance for dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses two pre-existing datasets (H3.6M and AMASS) without describing any new datasets introduced by the authors or any quality assurance process on dataset annotations. Therefore, no quality assurance process for new datasets is applied or documented in this paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1196-rubric-3",
    "token_usage": {
      "prompt_tokens": 15851,
      "completion_tokens": 415,
      "total_tokens": 16266
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses established motion capture datasets (H3.6M and AMASS) which are collected from human subjects but the authors do not introduce any new datasets collected from human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's model generates predicted human motions, but these generated outputs are not introduced as new datasets. The paper evaluates on existing datasets rather than creating new model-generated datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of data produced via machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 6 Datasets (page 6)",
          "reasoning": "The authors utilize existing publicly available datasets (H3.6M and AMASS). AMASS itself is described as a unified collection of 18 motion capture datasets. They do not create new datasets but collate existing data sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors process existing datasets (e.g., through forward kinematics to convert joint configurations to positions), they do not introduce new datasets derived from existing data via transformations or adaptations as standalone datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origins are clearly specified as existing public datasets; thus, the origin is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1196-rubric-4",
    "token_usage": {
      "prompt_tokens": 16369,
      "completion_tokens": 282,
      "total_tokens": 16651
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 6 (Experiments) and Section 5 (Motron)",
          "reasoning": "The paper describes training the Motron model from scratch using the Human 3.6 Million (H3.6M) and AMASS datasets for human motion prediction tasks. Training details, model architecture, and performance evaluations are provided, indicating use of these datasets to train models from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 6 (Experiments)",
          "reasoning": "The paper uses the H3.6M and AMASS datasets extensively for evaluation, benchmarking, and performance measurement of the proposed Motron model against state-of-the-art methods, employing metrics like APD, ADE, FDE, NLL, and others."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1196-rubric-5",
    "token_usage": {
      "prompt_tokens": 17092,
      "completion_tokens": 607,
      "total_tokens": 17699
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses datasets related to human motion capture (H3.6M and AMASS), which contain motion sequences and 3D skeletal information rather than linguistic content. There is no indication that the datasets contain text or language data in English or any other natural language."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of datasets containing non-English linguistic content. The paper's datasets focus entirely on human motion capture data without language annotations."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper provides source code and models online, the datasets themselves do not contain programming code or structured code data. Only the source code for the model is mentioned, but this is not part of the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 5 (Motron), Section 6 (Experiments)",
          "reasoning": "The datasets consist of joint rotations represented by differential quaternions and other mathematical constructs. The paper extensively describes mathematical representations including distributions on SO(3), Gaussian mixtures, and probabilistic formulations, which are integral to the dataset representations. Thus, the dataset entries inherently contain mathematical and formal symbolic representations of human motion rather than natural language."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are human motion capture sequences, not biological sequences like DNA or animal communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are included in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content or representation is clearly described and linked to mathematical and motion data; there is no ambiguity or unknown language content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The datasets introduced (H3.6M and AMASS) are collections of human motion sequences represented by kinematic and pose data (joint angles, positions, rotations). They do not contain entries with any language data (natural or constructed), but rather numeric and mathematical representations of human motion."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1196-rubric-6",
    "token_usage": {
      "prompt_tokens": 14310,
      "completion_tokens": 213,
      "total_tokens": 14523
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 5 and Conclusion",
          "reasoning": "The paper mentions that the source code, trained models, and data can be found online at https://github.com/TUM-AAS/motron-cvpr22, but this repository appears to be for the Motron model code and trained models, not for code related to constructing or generating the datasets themselves. The datasets used (H3.6M and AMASS) are pre-existing and not newly constructed by the authors. There is no indication that new dataset construction code is provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Dataset description in Section 6 and 6.1",
          "reasoning": "The paper uses pre-existing datasets Human 3.6 Million (H3.6M) and AMASS for experimentation, which are introduced by other authors. The paper does not create or describe any new dataset constructions or collection processes. Hence, there is no documentation of a dataset creation process for new datasets in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-120-rubric-0",
    "token_usage": {
      "prompt_tokens": 11536,
      "completion_tokens": 116,
      "total_tokens": 11652
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset",
          "Reasoning": "The paper introduces the use of the MVTec dataset, which consists of real-world images of textures and objects collected for anomaly detection. Although the authors used this dataset for evaluation, they did not introduce a new dataset in the paper. No new dataset was introduced by the authors according to the content of the paper. Therefore, there are no new data sources introduced by the authors to list."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-120-rubric-1",
    "token_usage": {
      "prompt_tokens": 12336,
      "completion_tokens": 198,
      "total_tokens": 12534
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.1 Dataset",
            "reasoning": "The paper mentions that the MVTec dataset, which is a newly introduced benchmark dataset in the anomaly detection community, contains pixel-level annotations for anomaly localization, which implies human expert annotators performed the annotation. Precise labels like pixel-level anomaly masks typically require expert annotation for accuracy."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any information about detailed annotation instructions provided to annotators for the MVTec dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention in the paper about scoring rubrics or criteria used during the annotation of the MVTec dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not include annotation examples or guidelines illustrating how to label anomalies in the MVTec dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-120-rubric-2",
    "token_usage": {
      "prompt_tokens": 13526,
      "completion_tokens": 342,
      "total_tokens": 13868
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process involving a single human expert for validating dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process involving multiple human experts for validating dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single non-expert human annotator on dataset annotations or content."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by multiple non-expert human annotators on dataset annotations or content."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model was used as a judge for quality assurance regarding dataset annotations or content in the paper."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any automated verification process (such as algorithmic or rule-based techniques) used for quality assurance of dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper utilizes existing datasets such as MVTec, MNIST, FashionMNIST, CIFAR10 and Caltech-256 for evaluation but does not introduce any new datasets. Consequently, there is no discussion or documentation of a quality assurance process for dataset annotations or content for a new dataset. Therefore, no quality assurance process is applied or documented for new datasets, because no new datasets are introduced in this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-120-rubric-3",
    "token_usage": {
      "prompt_tokens": 13136,
      "completion_tokens": 379,
      "total_tokens": 13515
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce datasets created entirely from scratch by human contributors. It uses existing datasets for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any datasets generated completely by AI or machine learning models. It focuses on algorithmic methods and evaluates on public datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of any data produced by translating content by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any machine-translated datasets."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The evaluation datasets used, such as MVTec, MNIST, CIFAR10, and Caltech-256 are existing public datasets; however, the authors do not claim to have newly collated or aggregated data from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any new datasets are created by modifying or transforming existing datasets. It only uses existing datasets for evaluation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. All datasets used for evaluation are publicly available and pre-existing, and no new data collection or generation method is described."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-120-rubric-4",
    "token_usage": {
      "prompt_tokens": 13654,
      "completion_tokens": 293,
      "total_tokens": 13947
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, Section 4.2",
          "reasoning": "The datasets are used exclusively for performance evaluation and benchmarking of the proposed anomaly detection and novelty detection methods. The MVTec dataset is used in Section 4.1 for anomaly detection and localization experiments, and semantic datasets such as MNIST, FashionMNIST, CIFAR10, and Caltech-256 are used in Section 4.2 for one-class novelty detection evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new datasets are introduced by the authors in this paper. The datasets mentioned (MVTec, MNIST, FashionMNIST, CIFAR10, Caltech-256) are existing benchmark datasets used solely for evaluation purposes; thus, no practical usage of new datasets is described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-120-rubric-5",
    "token_usage": {
      "prompt_tokens": 14377,
      "completion_tokens": 501,
      "total_tokens": 14878
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, including Sections 1, 3, and experimental setups",
          "reasoning": "The datasets introduced or used in this paper (e.g., MVTec, MNIST, FashionMNIST, CIFAR10, Caltech-256) contain image data and annotations documented and described solely in English. The paper itself is written entirely in English, and all dataset-related textual content, annotations, and descriptions are presented in English only. There is no indication or documentation of other human languages in the datasets."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are image datasets with labels and annotations but do not contain entries of programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains mathematical formulas and notation describing the method, but the datasets themselves do not contain mathematical or formal logical expressions as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequence or non-human communication data is introduced as part of any new dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or inclusion of fictional or constructed languages in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "All datasets used or introduced are clearly documented and known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain image data and annotations in English language, so language is applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-120-rubric-6",
    "token_usage": {
      "prompt_tokens": 11595,
      "completion_tokens": 110,
      "total_tokens": 11705
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not provide any mention or link to code repositories related to the construction of any dataset, nor the code used for data collection, preprocessing, or generation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets created by the authors nor documents any dataset construction process. It only uses existing benchmark datasets such as MVTec and others, and focuses on anomaly detection methods rather than dataset creation."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1202-rubric-0",
    "token_usage": {
      "prompt_tokens": 10679,
      "completion_tokens": 301,
      "total_tokens": 10980
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 3.1, Section 4.1, Section 4.2",
          "Reasoning": "The authors introduced a new dataset consisting of annotations on 1,000 face images. These images were selected from the existing ExpW in-the-wild dataset, which contains face images captured from movies and in-the-wild sources, i.e., human-photographed images. The modality is image, as these are photographs of faces. The images themselves are human-generated because they are captured from real-world scenes by humans (or human-operated devices). The dataset introduced includes multidimensional modulated facial expression annotations collected via crowdsourcing on these images, but the source images are original human-generated photos. The annotations are human-generated perceptual intensity ratings, but the dataset as introduced focuses on the images with their new annotations. The paper explicitly states that the 1,000 images are from the public ExpW dataset (Section 4.1), but the new dataset is the collected expression annotations on these images. The images remain human-generated as they originate from human photography; the annotations are human-generated as obtained from human annotators via crowdsourcing. No model-generated images or synthetic data are mentioned for these images. Therefore, the new dataset introduced consists of human-generated images with human-generated annotations, clearly explained in Sections 3.1 and 4.1."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1202-rubric-1",
    "token_usage": {
      "prompt_tokens": 11479,
      "completion_tokens": 239,
      "total_tokens": 11718
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Sections 3.1, 4.1",
            "reasoning": "The annotation was collected via online crowdsourcing using Amazon Mechanical Turk and Amazon SageMaker GroundTruth, involving multiple annotators (9 annotators per image) who are not described as experts but typical crowd workers."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "The paper describes a custom annotation interface and outlines the rating scale used (5-point scale from 'Not at all' to 'Extremely') implying detailed annotation instructions were given to annotators to rate expression intensity."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "The annotation used a 5-point scale with predefined intensity levels mapped to numeric values (0.1 to 0.9), which serves as a rubric/scoring guideline for the annotators."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "The paper does not explicitly mention providing annotation examples or sample annotated images to annotators as part of the guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1202-rubric-2",
    "token_usage": {
      "prompt_tokens": 12669,
      "completion_tokens": 395,
      "total_tokens": 13064
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any quality assurance performed by a single human expert annotator. Annotators are crowdsourced workers without mention of expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that multiple human experts conducted quality assurance or validated the annotations. The annotators are crowdsourced non-experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotation process involved multiple annotators per image, not a single annotator, thus this category does not apply."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4.2",
          "reasoning": "The authors collected annotations using crowdsourcing on Amazon Mechanical Turk, recruiting multiple human annotators (typically 9 unique annotators per image) who are non-experts to label expression intensities. The paper explicitly discusses analyzing the consistency and reproducibility of such non-expert crowd annotations, indicating QA was done by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models were benchmarked against the dataset, there is no indication that AI models were used as quality assurance judges for dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 and 4.2",
          "reasoning": "The paper describes fitting Beta distributions to the collected annotation ratings to model expression intensity and ambiguity. This modeling serves as an automatic, statistical approach to process and validate annotation consistency and variability, representing a form of automated quality assurance through formula-based verification and modeling."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a detailed annotation and modeling process, including crowdsourcing with multiple annotators and statistical modeling to assess reproducibility and ambiguity. Hence, QA is present and documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1202-rubric-3",
    "token_usage": {
      "prompt_tokens": 12279,
      "completion_tokens": 476,
      "total_tokens": 12755
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Sections 3.1, 4.1 and 4.2",
          "reasoning": "The paper describes the collection of new annotations created entirely by human annotators via crowdsourcing on Amazon Mechanical Turk for 1,000 images selected from an existing dataset. These annotations represent original multi-dimensional modulated facial expression intensity ratings created by humans from scratch, not translations or derivations. The data reflects human perception collected explicitly for this study, thus it is original human-generated content."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was generated entirely by AI or machine learning models without reference to existing data. The datasets introduced are based on human annotations."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any data produced via human translations from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation for dataset generation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The 1,000 face images used for annotation were culled from the public ExpW in-the-wild dataset, which is an existing resource. The authors collected new annotations on these images, thus collating existing images with newly collected labels. Since the images themselves were not created anew, but assembled from an existing source, the image data can be considered collated."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the annotations are newly collected, they are not derived or transformed versions of existing annotation data. The paper does not describe datasets as derived through modifications or adaptations of existing labels."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation method is explicitly specified."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1202-rubric-4",
    "token_usage": {
      "prompt_tokens": 12797,
      "completion_tokens": 374,
      "total_tokens": 13171
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The paper states that the newly collected annotated dataset of 1,000 in-the-wild face images is used to benchmark four public domain facial expression prediction algorithms, comparing their predicted expression intensities against the human annotations using proposed metrics such as cross entropy and absolute distance. This constitutes direct and exclusive use of the dataset for evaluation and performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 4.2 and 4.4",
          "reasoning": "The dataset is also used to analyze annotator behavior, reproducibility of expression intensity measurements, intensity ambiguity among annotators, and to explore the sufficiency of six primary expression dimensions versus compound expressions in capturing facial expressions. These analyses are detailed in the experiments and analysis sections."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1202-rubric-5",
    "token_usage": {
      "prompt_tokens": 13520,
      "completion_tokens": 534,
      "total_tokens": 14054
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains facial images and annotations related to expressions without any mention of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is presented that the dataset contains exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Sections Abstract, 3.1, and 4.1",
          "reasoning": "The dataset consists of facial images annotated with English labels of expressions such as 'happy', 'angry', 'surprised', etc. All expression labels and annotation interfaces use English terms. There is no mention or indication of other languages being involved in the dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains exactly one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves facial image annotations and no entries containing programming or structured code are indicated."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses mathematical expressions for modeling the annotation distributions (e.g., Beta distributions), these are not part of the dataset entries themselves. The dataset entries are images and annotation intensity ratings, not mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human facial images and their expression annotations, not biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificially constructed languages are involved in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as English in the expression labels and annotation interface."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain linguistic content in the form of English expression labels and annotations."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1202-rubric-6",
    "token_usage": {
      "prompt_tokens": 10738,
      "completion_tokens": 183,
      "total_tokens": 10921
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper",
          "reasoning": "The paper does not provide any links or explicit statements about the public availability of the code used for data collection, preprocessing, or generation of the dataset. There is no indication that the code repository is accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3.1, 3.2, 4.1, 4.2",
          "reasoning": "The paper provides detailed descriptions of the dataset creation process, including the annotation interface used (Section 3.1), the modeling of annotations with Beta distributions (Section 3.2), the crowd-sourcing setup and experimental design (Section 4.1), and analyses of annotator behavior and rating consistency (Section 4.2). This comprehensive documentation clearly explains how the dataset was collected and modeled."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1209-rubric-0",
    "token_usage": {
      "prompt_tokens": 13963,
      "completion_tokens": 139,
      "total_tokens": 14102
    },
    "response": {
      "sources": [
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3. Method and Section 4 Experiments (Table 1)",
          "Reasoning": "The paper introduces the multi-view datasets used for evaluation, such as MNIST-USPS, BDGP, CCV, Fashion, and Caltech with different views. These datasets provide raw features represented as multi-view inputs with samples and views, generally in tabular form as vectors reshaped from original forms for the experiments. The datasets are standard or public datasets but are not introduced as new datasets in this paper; rather, they are used for evaluation."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1209-rubric-1",
    "token_usage": {
      "prompt_tokens": 14763,
      "completion_tokens": 258,
      "total_tokens": 15021
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2 and Algorithm 1",
            "reasoning": "The paper describes the proposed method as a framework involving neural network components including autoencoders and MLPs. The clustering labels are obtained via K-means (Equation 8-12), and the whole process is summarized as an optimization algorithm executed via mini-batch gradient descent (Algorithm 1). There is no indication that human annotators label the data; clustering is performed algorithmically."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No instructions for annotation are described in the paper.",
            "reasoning": "There are no sections or statements describing human annotators receiving instructions or annotation guidelines. The 'annotation' (cluster labeling) is automatic via algorithmic learning."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No rubric descriptions are provided.",
            "reasoning": "No mention of scoring rubrics or evaluation rubrics for annotations is present; clustering is unsupervised and learned via neural networks."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of annotation guidelines are provided.",
            "reasoning": "The paper does not provide sample annotation examples or annotation guideline documents; clustering is performed by the proposed model automatically."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1209-rubric-2",
    "token_usage": {
      "prompt_tokens": 15953,
      "completion_tokens": 278,
      "total_tokens": 16231
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for dataset annotation or validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper indicating that multiple human experts were involved in quality assurance of datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human non-expert for the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No details are provided about quality assurance by multiple human non-experts in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's quality assurance does not involve the use of AI models as judges for dataset validation or annotation QA."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe automated verification processes for dataset annotation or content quality assurance."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce new datasets, nor does it describe any quality assurance process related to datasets. All datasets used are public and pre-existing datasets. Therefore, no quality assurance process is applied or documented for datasets in this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1209-rubric-3",
    "token_usage": {
      "prompt_tokens": 15563,
      "completion_tokens": 347,
      "total_tokens": 15910
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not claim to have created any new dataset from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets are generated by the models proposed in the paper. The work focuses on learning from existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any datasets used or introduced were produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any machine translation process for its datasets."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets mentioned are publicly available standard datasets; the paper does not indicate that new datasets were collated or aggregated from multiple sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document the introduction of any new dataset derived by modifying or adapting existing datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. All datasets used are publicly available benchmark datasets, and no new data collection or creation process is documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1209-rubric-4",
    "token_usage": {
      "prompt_tokens": 16081,
      "completion_tokens": 367,
      "total_tokens": 16448
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 (Method) and Section 4 (Experiments)",
          "reasoning": "The datasets presented in the paper (MNIST-USPS, BDGP, CCV, Fashion, Caltech variants) are used to train the proposed multi-level feature learning model from scratch, including training autoencoders and multi-layer perceptrons (MLPs) without mention of pre-trained weights. The paper describes training via mini-batch gradient descent and Adam optimizer, indicating training from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 (Result Analysis) and Section 5 (Model Analysis)",
          "reasoning": "The datasets are used for evaluation of clustering performance of the proposed method by reporting quantitative metrics such as clustering accuracy (ACC), normalized mutual information (NMI), and purity (PUR). The datasets serve as benchmarks for measuring and comparing the effectiveness of the clustering method."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.1 (Understand the Multi-level Feature Learning)",
          "reasoning": "The authors analyze the model's behavior and feature learning on datasets like MNIST-USPS through visualizations, convergence analysis, and parameter sensitivity, demonstrating their dataset's use for analysis of feature characteristics and model insights."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1209-rubric-5",
    "token_usage": {
      "prompt_tokens": 16804,
      "completion_tokens": 581,
      "total_tokens": 17385
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset entries containing more than two human languages. The datasets used are multi-view datasets with features or modalities, not human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's datasets do not contain entries with exactly two human languages. They focus on visual or multi-modal features but do not mention two human languages explicitly."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described (e.g., MNIST-USPS, BDGP, CCV, Fashion, Caltech variants) consist mainly of visual features or image modalities and do not contain English text as primary content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset in the paper is described as containing content in exactly one non-English human language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are image and visual feature datasets; they do not contain programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes many mathematical expressions and equations to describe the model and losses, no new dataset introduced by the authors contains mathematical or logical notation as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced and used are not of biological sequences, animal signals, or other non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets including constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly documents the datasets used; there is no indication that the language or modality of the datasets is unknown."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The datasets introduced and used (MNIST-USPS, BDGP, CCV, Fashion, Caltech variants) are multi-view or multi-modal datasets consisting of visual or extracted features but do not contain entries with any human or programming language. Thus, the entries do not contain any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1209-rubric-6",
    "token_usage": {
      "prompt_tokens": 14022,
      "completion_tokens": 194,
      "total_tokens": 14216
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Implementation section, last paragraph of Section 4",
          "reasoning": "The paper provides a link to the code repository (https://github.com/SubmissionsIn/MFLVC) for the model implementation, but this code repository is for the method implementation, not for constructing any new dataset. Moreover, the datasets used are publicly available benchmark datasets; the authors did not introduce or construct any new datasets. Therefore, code related to dataset construction is not available because no new datasets were constructed."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No dedicated section or mention in the paper regarding dataset construction process",
          "reasoning": "The paper only uses publicly available datasets (MNIST-USPS, BDGP, CCV, Fashion, Caltech variants) for experiments. There is no description or documentation explaining the creation or construction of any new dataset by the authors. Hence, no dataset creation documentation is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1216-rubric-0",
    "token_usage": {
      "prompt_tokens": 15340,
      "completion_tokens": 120,
      "total_tokens": 15460
    },
    "response": {
      "sources": [
        {
          "Modality": "time series",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 6.1 Datasets",
          "Reasoning": "The paper evaluates the proposed model on two existing datasets: Human3.6M and HumanEva-I, which are human motion capture datasets providing sequential human joint coordinates over time. These datasets are human-generated as they consist of recorded Cartesian coordinates of human joints captured at high frame rates (50Hz and 60Hz respectively). The paper does not introduce any new dataset but employs these existing datasets for evaluation."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1216-rubric-1",
    "token_usage": {
      "prompt_tokens": 16140,
      "completion_tokens": 196,
      "total_tokens": 16336
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.2, Section 6.1",
            "reasoning": "The paper states that the short-term oracle is learned using a conditional variational autoencoder trained on augmented data generated by similarity search (k-DPP) techniques, which is an automated model-based data processing rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No explicit annotation instructions for human annotators are mentioned in the paper for the new dataset or oracle data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not describe any rubrics or scoring guidelines used for annotation of the new dataset or oracle samples."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No examples of annotations or detailed annotation examples are provided or discussed for the new dataset or oracle data."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1216-rubric-2",
    "token_usage": {
      "prompt_tokens": 17330,
      "completion_tokens": 290,
      "total_tokens": 17620
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert to validate the datasets used."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information provided about quality assurance performed by multiple human experts for the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of quality assurance by multiple non-expert human annotators is found in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper involves AI models for prediction, it does not describe any AI model acting as a quality assurance judge for dataset annotation or validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automatic verification or algorithmic quality assurance process applied to the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses two standard pre-existing datasets (Human3.6M and HumanEva-I) for experiments and does not introduce any new datasets. Moreover, no quality assurance process related to dataset annotation or validation is described or documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1216-rubric-3",
    "token_usage": {
      "prompt_tokens": 16940,
      "completion_tokens": 439,
      "total_tokens": 17379
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing datasets Human3.6M and HumanEva-I for training and evaluation; it does not state that new data was collected or created by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper introduces a model to generate diverse human motion predictions, it does not introduce a new dataset generated purely by a model; rather, the datasets used are existing ones."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention of translating data from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention of machine translating data from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 6.1 Datasets",
          "reasoning": "The paper uses existing datasets (Human3.6M and HumanEva-I) for experiments. These datasets are established and collected by previous work; authors use these datasets as provided without modifying or creating new data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.2 Short-term Oracle Design",
          "reasoning": "The paper describes learning a short-term oracle by augmenting the training data with multiple pseudo-future motions obtained through similarity clustering and k-determinantal point processes. This process modifies and transforms the original dataset to generate augmented samples to supervise the model, effectively deriving new data from existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly states the use of existing datasets and details the derived augmentation process; thus, the data origin is specified."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1216-rubric-4",
    "token_usage": {
      "prompt_tokens": 17458,
      "completion_tokens": 423,
      "total_tokens": 17881
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used (Human3.6M and HumanEva-I) are established public datasets, not newly introduced by the authors, so no new dataset is used for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any newly introduced dataset is used specifically for supervised fine-tuning of pretrained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention reinforcement learning post-training techniques or use of datasets for such methods."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets evaluated in the paper (Human3.6M and HumanEva-I) are existing standard datasets; no new dataset introduced by the authors is used exclusively for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is introduced or used primarily for analysis of trends or characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is introduced or used as a knowledge base for model augmentation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. The datasets referenced and used are pre-existing public datasets (Human3.6M and HumanEva-I). Therefore, no practical usage of a new dataset is described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1216-rubric-5",
    "token_usage": {
      "prompt_tokens": 18181,
      "completion_tokens": 596,
      "total_tokens": 18777
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, e.g., Abstract and Sections 1, 2, 3, 4, 6",
          "reasoning": "The paper presents datasets and descriptions entirely in English. The datasets used (Human3.6M and HumanEva-I) contain human motion data without any mention of other human languages in the entries. All textual content, descriptions, and metadata within the proposed dataset context are in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper includes algorithmic descriptions and pseudocode, these do not constitute or describe dataset entries but rather the methodology. The datasets themselves do not contain code or programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains mathematical formulas describing the model and methodology but the dataset entries (i.e., the human motion data) are not composed of mathematical or logical notation as data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets comprise human joint Cartesian coordinates representing human motion; these are motion capture data, not biological sequences or non-human communication systems like DNA or animal signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention that the datasets contain entries in any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described are well-known motion capture datasets with clearly documented data types; language of data entries is known (non-linguistic numerical human joint coordinates)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Although the dataset entries consist of numeric joint positions (which are not language per se), the entries are not language-free since the paper content and labels/descriptions are in English; the dataset describes human motions, which is not language content itself, but the evaluation metric concerns human language coverage of the dataset, which applies to datasets containing linguistic information."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1216-rubric-6",
    "token_usage": {
      "prompt_tokens": 15399,
      "completion_tokens": 159,
      "total_tokens": 15558
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or link provided in the paper.",
          "reasoning": "The paper does not mention any code repository, supplementary material, or link to publicly available code that relates to dataset construction, preprocessing, or generation. The paper only discusses evaluation on existing datasets (Human3.6M and HumanEva-I) and does not introduce any new dataset or provide code for dataset construction."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Not applicable as no new dataset creation is described.",
          "reasoning": "The paper does not introduce any new datasets. The datasets used in experiments are existing human motion datasets (Human3.6M and HumanEva-I). Therefore, there is no documentation on dataset creation process within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1238-rubric-0",
    "token_usage": {
      "prompt_tokens": 10759,
      "completion_tokens": 227,
      "total_tokens": 10986
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4: Experiments and Results",
          "Reasoning": "The authors pretrain and finetune their NLX-GPT model on existing large-scale human-annotated datasets such as COCO captions, Flickr30k, Visual Genome, and image paragraph captioning datasets, which are composed of images captured and annotated by humans. These datasets are utilized for pretraining, but are pre-existing datasets, not new ones introduced by the paper. The paper does not introduce a new dataset of images."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Pretraining and Section 4 Experiments and Results",
          "Reasoning": "The text data used for pretraining consists of image captions from existing datasets like COCO captions, Flickr30k captions, and Visual Genome region descriptions, which are all human-generated captions describing images. These are pre-existing datasets; thus, no new dataset of text was introduced in this paper."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1238-rubric-1",
    "token_usage": {
      "prompt_tokens": 11559,
      "completion_tokens": 297,
      "total_tokens": 11856
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 4 (Experiments and Results), 4.4 Human evaluation and related work mentions",
            "reasoning": "The paper mentions human evaluation similar to previous works [20, 36] and details it in the Appendix; previous datasets like VQA-X, ACT-X, and e-SNLI-VE involved human annotators providing natural language explanations. Given the nature and volume of data (thousands of explanations), multiple non-expert annotators likely provided explanations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper of annotation instructions for these datasets.",
            "reasoning": "The paper does not describe annotation instructions or guidelines provided to annotators for the datasets it employs (VQA-X, ACT-X, e-SNLI-VE). Since these datasets were introduced in previous works, and the paper only uses them, no new annotation instructions are noted."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention of rubrics for annotation in the paper.",
            "reasoning": "There is no discussion or citation indicating that scoring rubrics or guidelines for explanation quality or content were provided to annotators during dataset annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation guideline examples discussed in the paper.",
            "reasoning": "The paper does not describe providing exemplar annotations or example explanations as part of annotation guidelines for the datasets."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1238-rubric-2",
    "token_usage": {
      "prompt_tokens": 12749,
      "completion_tokens": 319,
      "total_tokens": 13068
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information that quality assurance for any dataset was performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by multiple human experts. It only references existing datasets but does not detail their annotation quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence or mention of quality assurance by a single non-expert human annotator is provided in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple non-expert human annotators performed quality assurance on the datasets discussed or introduced."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes evaluation procedures using AI models (like DistilBERT and Sentence-BERT) for explanation evaluation, but does not state that dataset annotations were quality assured by AI models."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of an automated verification or rule-based quality assurance process for dataset annotations is described in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; it only uses existing ones (VQA-X, ACT-X, e-SNLI-VE, VCR). Therefore, there is no documented quality assurance process described for new datasets within this paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1238-rubric-3",
    "token_usage": {
      "prompt_tokens": 12359,
      "completion_tokens": 404,
      "total_tokens": 12763
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the authors created any new dataset from scratch by human contributors. The datasets mentioned (VQA-X, ACT-X, e-SNLI-VE, VCR) are existing datasets introduced in prior works."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any new dataset was generated entirely by AI or machine learning models. It focuses on proposing a model and evaluating on existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data created by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of data generated by machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe creating new collated datasets from existing sources by the authors."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset derived or adapted from existing datasets is described as being created by the authors. The paper reuses pre-existing datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses established NLE datasets (VQA-X, ACT-X, e-SNLI-VE, VCR) and does not introduce any new dataset. Therefore, no new data source or method of generation is documented for new datasets by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1238-rubric-4",
    "token_usage": {
      "prompt_tokens": 12877,
      "completion_tokens": 329,
      "total_tokens": 13206
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.3 (Finetuning)",
          "reasoning": "The paper describes finetuning the NLX-GPT model on natural language explanation (NLE) tasks such as VQA-X, e-SNLI-VE and ACT-X datasets, which are used to fine-tune the model using supervised learning methods on these vision and vision-language NLE tasks."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses evaluation measures for NLE datasets, it does not introduce any new datasets used exclusively for evaluation. The datasets referred to (VQA-X, ACT-X, e-SNLI-VE) are pre-existing datasets."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper performs analysis on performance and ablations but does not introduce new datasets primarily for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; it uses existing datasets such as VQA-X, ACT-X, e-SNLI-VE for training and evaluation. Therefore, no practical usage of a new dataset is described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1238-rubric-5",
    "token_usage": {
      "prompt_tokens": 13600,
      "completion_tokens": 285,
      "total_tokens": 13885
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4 Experiments and Results",
          "reasoning": "The paper describes the datasets VQA-X, ACT-X, and e-SNLI-VE that are used in experiments and finetuning. These datasets consist of natural language explanations in English. The paper does not mention any use of other human languages in the datasets, nor does it suggest any multilingual or bilingual data. The explanations and answers are in English as commonly known for these datasets. Thus, the proposed datasets introduced or used by the authors contain English content only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1238-rubric-6",
    "token_usage": {
      "prompt_tokens": 10818,
      "completion_tokens": 160,
      "total_tokens": 10978
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention of dataset construction code availability in the paper.",
          "reasoning": "The paper states that code for the NLX-GPT model is available but does not mention any new dataset construction code being released. The datasets used are pre-existing ones (VQA-X, ACT-X, e-SNLI-VE, VCR), and the paper does not introduce any new datasets or provide code for creating or processing them."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section or detailed description specific to dataset creation in the paper.",
          "reasoning": "The paper uses existing datasets for evaluation and does not introduce or create any new datasets. Therefore, there is no documentation describing new dataset creation process within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1273-rubric-0",
    "token_usage": {
      "prompt_tokens": 12352,
      "completion_tokens": 143,
      "total_tokens": 12495
    },
    "response": {
      "sources": [
        {
          "Modality": "other",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 5 The Geometric Shape Mating Dataset",
          "Reasoning": "The new dataset introduced is generated by a self-supervised data collection pipeline that generates pairwise shape mating data with ground truth by randomly cutting object meshes into two parts using algorithmic mesh cutting functions. The object meshes themselves are sourced from existing datasets (Thingi10K, Google Scanned Objects, ShapeNet), but the new mating dataset with 200K shape mating pairs is procedurally generated by the authors' method, hence model generated and computationally produced, not human generated or of unknown origin."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1273-rubric-1",
    "token_usage": {
      "prompt_tokens": 13152,
      "completion_tokens": 216,
      "total_tokens": 13368
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 5, The Geometric Shape Mating Dataset",
            "reasoning": "The dataset is generated by a self-supervised data collection pipeline that randomly cuts object meshes from existing datasets into two parts using defined cut functions, producing pairwise shape mating data with ground truth automatically without human intervention."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 5",
            "reasoning": "The paper describes an automated mesh cutting and point cloud sampling procedure for dataset generation but does not mention providing detailed annotation instructions to human annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 5",
            "reasoning": "There is no mention of scoring rubrics or criteria for annotation since the dataset is generated automatically via mesh cutting and algorithmic procedures."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 5",
            "reasoning": "While the paper provides visual examples of dataset samples in Figure 3, it does not provide annotation guideline examples, as no manual annotation is involved."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1273-rubric-2",
    "token_usage": {
      "prompt_tokens": 14342,
      "completion_tokens": 337,
      "total_tokens": 14679
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving a single human expert for validating the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple experts performing quality assurance or annotation validation of the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any single non-expert human involvement in quality assurance of the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about multiple non-expert humans performing quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report the use of AI models as judges or validators for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 5, The Geometric Shape Mating Dataset",
          "reasoning": "The dataset is self-supervised and generated procedurally by applying automated mesh cutting using algorithmic mesh boolean operations and parametric cut functions. The ground truth data such as signed distance functions are computed using automated numerical methods (Fast Winding Numbers). There is no mention of human annotation or manual quality control; hence, quality assurance is conducted through automated, rule-based, and algorithmic verification inherent to the data generation pipeline."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes a self-supervised automated data generation pipeline which acts as an automatic quality control mechanism, thus some form of quality assurance is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1273-rubric-3",
    "token_usage": {
      "prompt_tokens": 13952,
      "completion_tokens": 322,
      "total_tokens": 14274
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 5: The Geometric Shape Mating Dataset",
          "reasoning": "The dataset is created by applying random cuts (using five types of parameterized mesh cutting functions) to existing 3D object meshes collected from three pre-existing datasets: Thingi10K, Google Scanned Objects, and ShapeNet. The authors transform these existing meshes by performing mesh boolean operations (cutting) and generate point cloud samples and signed distance function ground truths, resulting in a large-scale set of paired shape parts for mating. Thus, the dataset is derived from existing sources with significant modifications that produce new paired parts suitable for the shape mating task."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1273-rubric-4",
    "token_usage": {
      "prompt_tokens": 14470,
      "completion_tokens": 421,
      "total_tokens": 14891
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.4, Section 5, Section 6.1",
          "reasoning": "The paper presents a self-supervised data collection pipeline to generate a large-scale geometric shape mating dataset and trains the Neural Shape Mating (NSM) model from scratch on this dataset. Section 4 describes the training method including losses and architecture, and Section 6 details experimental setup where NSM is trained on the dataset. This indicates the dataset is utilized for training the model from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the dataset for fine-tuning pre-trained models with supervised methods; it focuses on training NSM from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning-based post-training techniques such as RLHF using the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 6.2.1, Section 6.2.4, Table 1, Table 3",
          "reasoning": "The dataset is clearly used as a benchmark for evaluating NSM\u2019s performance against other methods, with quantitative metrics presented in Section 6.2 and tables showing evaluation results."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 6.2.4, Section 6.2.2, Section 6.3",
          "reasoning": "The paper uses the dataset for ablation studies, generalization tests across unseen categories and cut types, and noise robustness analysis, indicating use of the dataset for analysis beyond just training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base to augment models or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1273-rubric-5",
    "token_usage": {
      "prompt_tokens": 15193,
      "completion_tokens": 534,
      "total_tokens": 15727
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of 3D shape data and does not contain entries in multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include exactly two human languages; it is focused on 3D shapes and geometric data."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of 3D geometric shape data and associated point clouds, not text data in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of dataset containing non-English human language content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of 3D shape data and point clouds; no programming language data is included in the dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical notation in describing the methodology, the dataset itself contains 3D object meshes and point clouds, not mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of geometric shape mating pairs derived from 3D object meshes from datasets such as Thingi10K and ShapeNet. It does not contain biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset is clearly non-linguistic (shape data), so the language origin is not unknown but rather not applicable."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The proposed dataset consists exclusively of 3D object meshes and point cloud data representing geometric shapes. There is no natural language content of any kind in the dataset entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1273-rubric-6",
    "token_usage": {
      "prompt_tokens": 12411,
      "completion_tokens": 181,
      "total_tokens": 12592
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention of code availability in main paper sections 4 or 5 or elsewhere",
          "reasoning": "The paper mentions a self-supervised data collection pipeline to generate the geometric shape mating dataset, but does not provide any explicit indication, link, or statement that the code for dataset generation or preprocessing is publicly released or available. There is no repository URL or appendix section referencing dataset code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 5, The Geometric Shape Mating Dataset",
          "reasoning": "The paper provides a detailed documentation on the dataset creation process in Section 5, describing mesh cutting procedures, types of cut functions, point cloud sampling methods, and signed distance ground truth computation. Accompanying statistics and examples are also described. This is sufficient documentation about the dataset creation process within the paper itself."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1335-rubric-0",
    "token_usage": {
      "prompt_tokens": 13929,
      "completion_tokens": 125,
      "total_tokens": 14054
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets Used",
          "Reasoning": "The paper describes using CIFAR-10, NUS-WIDE, and COCO datasets, which are established, publicly available image datasets containing natural images collected from real-world sources and annotated with human-generated labels. The paper uses these datasets for evaluation and does not state that the authors have introduced any new dataset. The experiments and evaluation are conducted on these pre-existing datasets only, with no mention of new data collection or creation."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1335-rubric-1",
    "token_usage": {
      "prompt_tokens": 14729,
      "completion_tokens": 232,
      "total_tokens": 14961
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Datasets Used and Section 4 Experiments",
            "reasoning": "The dataset information provided in Section 4.1 describes three well-known benchmark datasets (CIFAR-10, NUS-WIDE, COCO) which are standard public datasets; thus, these datasets are not newly introduced in the paper. The paper does not indicate the introduction of any new dataset. The experiments utilize existing datasets only."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "Since the paper uses existing public datasets and does not introduce a new dataset, no explicit annotation instructions regarding dataset creation or labeling are provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not present or describe any rubrics for data annotation because no new dataset annotation process is performed."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There are no annotation examples provided related to any new dataset because the datasets used are existing standard datasets without new annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1335-rubric-2",
    "token_usage": {
      "prompt_tokens": 15919,
      "completion_tokens": 387,
      "total_tokens": 16306
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human annotator who is a subject matter expert or member of the target demographic for any new datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper about quality assurance performed by multiple human experts on any new datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by a single non-expert human annotator for new datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no documentation on QA processes involving multiple human non-expert annotators for the new datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using an AI model for quality assurance of new datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper of automated verification or algorithmic quality assurance applied to dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets, but uses publicly available datasets (CIFAR-10, NUS-WIDE, COCO) without describing any new annotation or quality assurance processes. Therefore, no quality assurance process is applied or documented for new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1335-rubric-3",
    "token_usage": {
      "prompt_tokens": 15529,
      "completion_tokens": 376,
      "total_tokens": 15905
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the creation of any original dataset from scratch by humans. It uses existing, well-known image datasets for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe generating any new datasets through AI or machine learning model outputs. The focus is on method development and evaluation on existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any dataset was generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets Used",
          "reasoning": "The authors use existing standard image datasets (CIFAR-10, NUS-WIDE, COCO) by selecting and partitioning images for training, query, and retrieval sets without significant modification, thus aggregating from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no statement indicating that existing datasets were modified, transformed, or adapted to create new datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented as existing image datasets, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1335-rubric-4",
    "token_usage": {
      "prompt_tokens": 16047,
      "completion_tokens": 378,
      "total_tokens": 16425
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets Used; Section 4.3 Comparison Methods; Section 4.5 Retrieval Performance",
          "reasoning": "The datasets CIFAR-10, NUS-WIDE, and COCO are employed as training datasets for learning deep supervised hashing models from scratch. The authors describe using subsets of these datasets for training, and evaluate retrieval performance of models trained on them, indicating these datasets are used for training models rather than pre-training or fine-tuning."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets Used; Section 4.5 Retrieval Performance; Section 4.7 Qualitative Analysis",
          "reasoning": "The datasets are also used for evaluation and benchmarking. The authors use query and retrieval subsets to measure mean average precision (mAP) and precision@R metrics, and visualize learned hash codes to assess retrieval quality. Hence, the datasets serve an evaluation role."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.7.2 Analysis of Effective Quantization and Performance Improvement; Section 3.2 Quantization and Code-balance",
          "reasoning": "The datasets are used for analysis of quantization error and bit entropy to demonstrate the relationship between effective quantization and retrieval performance. This reflects usage primarily for analyzing characteristics relevant to hashing."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1335-rubric-5",
    "token_usage": {
      "prompt_tokens": 16770,
      "completion_tokens": 399,
      "total_tokens": 17169
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3.1, Section 3.3",
          "reasoning": "The datasets used in the paper are image datasets (e.g., CIFAR-10, NUS-WIDE, COCO) and do not themselves contain languages. However, the paper's proposed dataset is based on learned continuous hash codes and discrete binary codes represented mathematically, with the paper containing multiple equations and symbolic mathematical notations to define the distributions, hash functions, and distances. The quantization objective and distributional matching formulations rely on formal mathematical constructs, indicating that the dataset entries are expressed with mathematical and symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The introduced datasets are existing image benchmark datasets (CIFAR-10, NUS-WIDE, COCO) pre-existing and not introduced by the authors. The paper does not introduce a new natural language or textual dataset. The data entries are image data which do not contain human language. The paper introduces a new approach to the quantization loss and hash function training but does not create a new dataset with language entries. Therefore, the new dataset introduced by the authors does not contain any language entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1335-rubric-6",
    "token_usage": {
      "prompt_tokens": 13988,
      "completion_tokens": 191,
      "total_tokens": 14179
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No location in paper",
          "reasoning": "The paper does not mention or provide any link to code related to dataset construction, data collection, preprocessing, or generation. The provided datasets (CIFAR10, NUS-WIDE, COCO) are well-known public datasets and the paper only uses them for evaluation; no new datasets are introduced and no code for dataset preparation or construction is referenced."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Datasets Used",
          "reasoning": "The paper documents the datasets used (CIFAR10, NUS-WIDE, COCO) in Section 4.1, describing their sizes, classes, splits for query, training, and retrieval sets, and specifics of the selection procedure. Although these are existing datasets, the paper provides clear documentation of the data splits and selection for reproducibility of evaluation."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1341-rubric-0",
    "token_usage": {
      "prompt_tokens": 12350,
      "completion_tokens": 176,
      "total_tokens": 12526
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.2 Implementation Details, Section 5 Experiments",
          "Reasoning": "The paper introduces a novel approach where an agent interacts with environments simulated using the Habitat Simulator based on the Matterport3D dataset. The images (RGB and RGB-D) are captured by the agent's sensors within the simulator and processed for segmentation and feature extraction. The dataset used involves both human-generated data (the Matterport3D RGB-D scans) and model-generated data from simulated agent perceptions in those environments. The dataset itself is not newly created by the authors, but the constructed Abstract Models representing knowledge learned incrementally in the new online learning approach, tied to these image data, constitute the new dataset introduced by the authors for storing agent experience and knowledge."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1341-rubric-1",
    "token_usage": {
      "prompt_tokens": 13150,
      "completion_tokens": 291,
      "total_tokens": 13441
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4. Approach, Specifically Section 4.1 Abstract Model Reuse and Section 4.2 Implementation Details",
            "reasoning": "The Abstract Models are built and updated by the agent automatically through its perception, segmentation, and mapping modules without human intervention. There is no indication of human annotations in the annotation or labeling of data for the new Abstract Models; instead, the annotation process is embedded in the perception and model-building pipeline described as automatic and algorithmic."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described in the paper",
            "reasoning": "The paper does not mention any instructions being provided to human annotators for labeling data related to the new Abstract Models or their component features. The entire process appears to be automatic and learned or computed by the agent."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described in the paper",
            "reasoning": "No scoring rubrics or evaluation criteria related to human annotation guidelines are reported. The evaluation metrics are for navigation task performance, not annotation quality."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No section or appendix provides annotation examples for the Abstract Models",
            "reasoning": "The paper does not describe or present annotation examples or guidelines for human annotators for the new dataset or Abstract Model annotations; the process is automatic and does not involve human annotators providing examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1341-rubric-2",
    "token_usage": {
      "prompt_tokens": 14340,
      "completion_tokens": 321,
      "total_tokens": 14661
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on the new dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance on the new dataset or annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information about any quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of quality assurance performed by multiple non-expert human annotators in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used explicitly to perform quality assurance on datasets or annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated verification processes for dataset quality assurance such as algorithmic or rule-based checks."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces an Abstract Model learned online by the agent and uses existing datasets (e.g., MatterPort3D) for evaluation, but does not describe any specific quality assurance processes on new dataset annotations or content. The Abstract Models are generated through the agent's online interaction and are incrementally learned without mention of QA steps. Therefore, no specific quality assurance process on dataset annotations or content is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1341-rubric-3",
    "token_usage": {
      "prompt_tokens": 13950,
      "completion_tokens": 461,
      "total_tokens": 14411
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4 and 5",
          "reasoning": "The paper introduces new Abstract Models learned incrementally by the agent as it interacts within the simulated environments. These models are constructed based on RGB-D input images and segmentation outputs, which are not translations or direct copies of existing datasets but rather novel data representations generated from scratch by human-designed mechanisms and environment interactions during training episodes."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data (Abstract Models) are learned from real sensory data with human-designed processing, but they are not stated to be originally generated purely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of data being translated from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of data being machine-translated from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the training data uses publicly available Matterport3D dataset, the new Abstract Models are not simply collected or aggregated from existing sources without modification; rather, they are incrementally learned and updated during interactions with the environment."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Sections 3 and 4",
          "reasoning": "The new Abstract Models are built based on existing sensory data from the Matterport3D dataset interpreted through segmentation and encoding models (e.g., Taskonomy features). They represent a derived form of data with transformations applied to raw sensor inputs to abstract environment states, thus qualifying as derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and generation of the Abstract Models are specified in detail throughout the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1341-rubric-4",
    "token_usage": {
      "prompt_tokens": 14468,
      "completion_tokens": 415,
      "total_tokens": 14883
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 (Experiments), Tables 1, 2, 3",
          "reasoning": "The introduced Abstract Models are used to evaluate agent performance on the Object Goal Navigation task by assessing improvements in Success Rate, SPL, SoftSPL, and Distance to Success (Sections 5.1 and 5.2). The evaluation benchmarks the effectiveness of the Abstract Models in enhancing navigation success."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.5 (Limitations and Failure analysis)",
          "reasoning": "The paper performs a failure analysis categorizing errors related to the learned Abstract Models, such as hallucination and exploration failures, thus using the dataset for detailed analysis of trends and error patterns."
        },
        "Knowledge Base": {
          "is_applicable": true,
          "reference": "Section 4.1 (Abstract Model Reuse), Section 5 (Experiments)",
          "reasoning": "The newly introduced Abstract Models serve as a knowledge base that the agent accumulates and reuses across episodes and environments, augmenting its capabilities for Object Goal Navigation by providing prior environmental knowledge."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1341-rubric-5",
    "token_usage": {
      "prompt_tokens": 15191,
      "completion_tokens": 535,
      "total_tokens": 15726
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only uses English language throughout and does not mention any dataset entries containing multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper suggests the dataset contains exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Entire paper, implicitly throughout",
          "reasoning": "The proposed dataset introduced by the authors involves agent perceptions from simulated 3D environments and object classes described in English (e.g., 'fridge', 'bed', 'sofa'). The dataset entries, including object classes and related annotations, utilize English language exclusively as the dataset is based on environment object classes and descriptions in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English language entries are used or mentioned for the datasets introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of sensory data (RGB-D images, poses) and associated abstract state representations, no programming or code content entries are indicated as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical expressions and symbolic representations appear in the paper to describe the model and evaluation metrics but are not part of the dataset entries themselves."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset concerns human environment perceptions and object classes; there is no inclusion of biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) of the dataset entries are clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes language entries in the form of English object class labels and descriptions."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1341-rubric-6",
    "token_usage": {
      "prompt_tokens": 12409,
      "completion_tokens": 248,
      "total_tokens": 12657
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No mention of code availability for data construction is found in the paper.",
          "reasoning": "The paper discusses the methodology, experiments, and baselines in detail, including references to public datasets such as Matterport3D and the Habitat simulator, but it does not provide any links or information about public availability of the code used to construct or preprocess any dataset or Abstract Models. There is no mention of a code repository or supplemental materials containing such code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 (Preliminaries) and 4 (Approach) provide detailed descriptions of the dataset construction process for the Abstract Models.",
          "reasoning": "The paper documents the dataset creation process extensively. It explains how the Abstract Models are built incrementally from sensory data, including specific details such as the use of RGB-D images from the Habitat simulator and the Matterport3D dataset, the use of semantic segmentation (RedNet), feature extraction (Taskonomy model bank), and SLAM for mapping. The process of building and updating the Abstract Models, merging them, and the strategies used are described in detail, supporting transparency and reproducibility of the dataset creation steps."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1417-rubric-0",
    "token_usage": {
      "prompt_tokens": 11245,
      "completion_tokens": 108,
      "total_tokens": 11353
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4 'Experimental Results', Datasets paragraph",
          "Reasoning": "The paper evaluates the proposed PDV model on the Waymo Open Dataset and KITTI datasets, which consist of LiDAR point cloud data collected by autonomous driving sensors. The data originates from human-operated vehicle sensors capturing real-world 3D point clouds, hence the sources are sensor-generated data with human involvement in data collection."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1417-rubric-1",
    "token_usage": {
      "prompt_tokens": 12045,
      "completion_tokens": 291,
      "total_tokens": 12336
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4 Experimental Results, Datasets description",
            "reasoning": "The new dataset introduced by the paper is the use of the Waymo Open Dataset, but the dataset itself is pre-existing. The paper does not introduce any new dataset for annotation, nor does it describe any manual annotation procedure. The dataset annotations are provided by the dataset creators, not by the authors, and no human annotation process is described. Therefore, the annotation is performed by an automatic or existing process (in this case, the original dataset creators)."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit instructions for annotation of the dataset are described in the paper.",
            "reasoning": "The paper does not describe any annotation instructions related to the dataset used or introduced, as no new dataset or annotation process is proposed or described by the authors."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention of annotation rubrics or scoring guidelines for dataset annotation in the paper.",
            "reasoning": "Since the paper does not present or describe any human annotation process or new dataset annotation, no rubrics for annotation are provided."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples are provided anywhere in the paper.",
            "reasoning": "The paper focuses on the 3D object detection method and its evaluation on existing datasets and does not provide annotation examples for any dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1417-rubric-2",
    "token_usage": {
      "prompt_tokens": 13235,
      "completion_tokens": 290,
      "total_tokens": 13525
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information regarding quality assurance performed by a single human expert on any introduced dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any quality assurance steps involving multiple human experts for any dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance conducted by a single non-expert annotator for any dataset in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by multiple non-expert annotators for the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance conducted by AI models as judges is mentioned in the context of dataset annotation or validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper involves automated processing methods for learning from data, it does not describe automated verification or quality assurance of dataset annotations or contents."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; instead, it utilizes existing datasets (Waymo Open Dataset, KITTI) without documenting any quality assurance process. Therefore, no quality assurance process is described or performed by the authors on datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1417-rubric-3",
    "token_usage": {
      "prompt_tokens": 12845,
      "completion_tokens": 367,
      "total_tokens": 13212
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the authors have created any new datasets consisting of original data collected from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset generated by AI or machine learning models from scratch."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data translation from another language performed by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation of data is discussed in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 (Experimental Results), throughout experimental evaluation",
          "reasoning": "The paper extensively uses the existing Waymo Open Dataset and KITTI dataset for experiments but does not introduce any new datasets. Since these are existing datasets, the data used for evaluation is collated from publicly available sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors do not create a derived dataset by modifying or transforming existing data; rather, they use existing datasets as is for evaluation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origins are explicitly stated as existing well-known datasets, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1417-rubric-4",
    "token_usage": {
      "prompt_tokens": 13363,
      "completion_tokens": 468,
      "total_tokens": 13831
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using any newly introduced datasets exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset with explicit description of using it to train models from scratch; it primarily uses established datasets like Waymo Open Dataset and KITTI."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any new dataset being used for supervised fine-tuning of models; the experimental results are obtained on existing datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the paper describes using any new dataset for reinforcement learning post-training methods such as RLHF."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses the Waymo Open Dataset and KITTI datasets for evaluation but does not introduce any new dataset for evaluation or benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper analyzes point density and its effects, these analyses are performed on existing datasets; no new dataset is introduced for the purpose of analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset introduced in the paper serves as a knowledge base or is used to augment models in a retrieval-based manner."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; all datasets referenced are pre-existing (e.g., Waymo Open Dataset and KITTI). Therefore, there is no task utility applicable for new datasets within the scope of this paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1417-rubric-5",
    "token_usage": {
      "prompt_tokens": 14086,
      "completion_tokens": 431,
      "total_tokens": 14517
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Entire paper, including Abstract and Sections 1-5",
          "reasoning": "The paper and dataset descriptions are written entirely in English, with all descriptions, explanations, and results presented in English only. The dataset pertains to LiDAR 3D point clouds without any content in non-English human languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes methods and equations related to LiDAR data processing but does not present or propose a new dataset containing programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3.2 (Voxel Point Centroid Localization), Section 3.3 (Density-aware RoI Grid Pooling), and Section 3.5 (Training Losses)",
          "reasoning": "The paper includes mathematical expressions and formal notation to describe the point cloud, voxel indices, kernel density estimation, and loss functions, indicating that the dataset contains mathematical representations related to 3D point cloud data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains LiDAR point cloud data for autonomous driving scenarios, with no biological sequences or non-human communication systems involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention of fictional or constructed languages in the dataset or paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language composition of the dataset is clearly specified as LiDAR 3D point cloud data with descriptions in English; thus, it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain human language descriptions and mathematical notation, so it is not devoid of language content."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1417-rubric-6",
    "token_usage": {
      "prompt_tokens": 11304,
      "completion_tokens": 115,
      "total_tokens": 11419
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Not mentioned",
          "reasoning": "The paper does not mention releasing any code related to dataset construction, preprocessing, or generation. The code discussed pertains to their model PDV, but not the dataset pipeline."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Not described",
          "reasoning": "The datasets used are existing public datasets (Waymo Open Dataset and KITTI). The paper describes their use and input parameter setups for these datasets but does not introduce or document any new dataset or dataset creation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1419-rubric-0",
    "token_usage": {
      "prompt_tokens": 11857,
      "completion_tokens": 135,
      "total_tokens": 11992
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Pre-Training Details",
          "Reasoning": "The authors pre-train their model using images from the ImageNet-1K training split (approximately 1.28 million images) and the COCO train2017 split (approximately 118k images). These datasets are used as the pre-training datasets. While ImageNet-1K and COCO are existing datasets, the paper does not introduce any new dataset. The authors propose a new self-supervised pre-training approach but do not introduce any new dataset themselves."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1419-rubric-1",
    "token_usage": {
      "prompt_tokens": 12657,
      "completion_tokens": 290,
      "total_tokens": 12947
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2 and 4.1",
            "reasoning": "The paper introduces a new dataset concept by dividing images into grid-based regions for self-supervised pre-training. The 'regions' are formed by uniformly dividing each image into a non-overlapping grid and then sampling points within these regions. This process is automatic and algorithmic, not involving human annotators for labeling. No mention of human annotation or manual labeling of these regions is made; they are generated automatically during pre-training."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not provide any details or describe instructions given to any human annotators related to the annotation in their newly introduced dataset. Since the regions and points are generated automatically, there is no evidence of instructions for human annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "There is no indication or description of any scoring rubrics or guidelines for evaluating annotation decisions in the newly introduced data. The pre-training samples points uniformly within grid regions automatically, so no rubric is applied."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "No annotation examples or labeling examples are provided in the paper for this newly introduced data. The process is automated; thus, examples are unnecessary and not mentioned."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1419-rubric-2",
    "token_usage": {
      "prompt_tokens": 13847,
      "completion_tokens": 287,
      "total_tokens": 14134
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for any new dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by multiple human experts for any new dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that quality assurance was carried out by a single non-expert human annotator for any dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving multiple non-expert human annotators for any dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report the use of AI models as judges or for quality assurance of any dataset content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any automated verification process or algorithmic QA applied to datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "This work does not introduce any new datasets; it uses standard, existing datasets such as ImageNet-1K and COCO for pre-training and evaluation. Therefore, no quality assurance process is described or performed by the authors for new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1419-rubric-3",
    "token_usage": {
      "prompt_tokens": 13457,
      "completion_tokens": 386,
      "total_tokens": 13843
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset created entirely by human contributors from scratch. Instead, it uses existing datasets for pre-training and downstream evaluations."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any new data generated entirely by models without using existing data sources."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine-translated data is described as part of the dataset creation or preparation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Pre-Training Details",
          "reasoning": "The datasets used for pre-training (ImageNet-1K and COCO) are existing datasets collected and aggregated from various sources without modification. The approach applies a novel pre-training method on these existing datasets."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the method applies modifications in the form of grid-based regions and point sampling, these are part of the learning approach rather than creation of a new dataset derived from existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the datasets used (ImageNet, COCO) is clearly specified, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1419-rubric-4",
    "token_usage": {
      "prompt_tokens": 13975,
      "completion_tokens": 377,
      "total_tokens": 14352
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4.1",
          "reasoning": "The authors introduce point-level region contrast as a self-supervised pre-training approach that operates on datasets such as ImageNet-1K and COCO in an unsupervised manner for pre-training representations before downstream tasks."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training of models from scratch using any new dataset; the focus is on pre-training representations using their approach."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is introduced for supervised fine-tuning. Evaluation on downstream tasks uses existing datasets (VOC, COCO, Cityscapes)."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using any new dataset for reinforcement learning or RL-based post-training techniques."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No newly introduced dataset is used exclusively for evaluation or benchmarking; all evaluations use standard existing datasets."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not present any new dataset used primarily for analysis or characterization."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce or use any new dataset as a knowledge base for augmenting models."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any novel datasets; all datasets used (ImageNet-1K, COCO, VOC, Cityscapes) are pre-existing. The work proposes a new pre-training approach rather than a new dataset, so no practical usage of new datasets is described."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1419-rubric-5",
    "token_usage": {
      "prompt_tokens": 14698,
      "completion_tokens": 570,
      "total_tokens": 15268
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset containing entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset with exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly introduce any new dataset containing entries with only English content. Instead, it uses existing datasets such as ImageNet-1K, COCO, VOC, and Cityscapes for pre-training and evaluation."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No newly introduced dataset with exactly one non-English language is described in the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or description of a new dataset containing programming or structured code content. The paper discusses algorithms and methods but no software or code-based dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper presents equations and formal descriptions (e.g., loss functions), it does not introduce any new dataset that contains entries with mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss or introduce any new datasets related to biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or artificial languages are involved or introduced in any new dataset in the paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "All datasets mentioned (ImageNet-1K, COCO, VOC, Cityscapes) are well documented and their languages (mostly English annotations) are known, but these are existing datasets, not newly introduced ones by the authors."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. It uses pre-existing computer vision datasets without new linguistic content. Therefore, no language is contained in any newly introduced dataset because no new dataset is introduced."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1419-rubric-6",
    "token_usage": {
      "prompt_tokens": 11916,
      "completion_tokens": 209,
      "total_tokens": 12125
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and main text",
          "reasoning": "The paper mentions that code will be made available but does not provide any link, supplementary materials, or explicit section containing code or detailed instructions on code access at the time of writing. There is no direct evidence or reference to a public repository containing the code for data collection or processing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1: Pre-Training Details",
          "reasoning": "The paper documents dataset creation in terms of describing the datasets used (ImageNet-1K and COCO train set) and details standard pre-training setups. It specifies regions as non-overlapping grids, sampling strategies, and hyperparameters for pre-training the model. While the datasets themselves are not newly collected, the paper explains in detail how the data regions are defined and sampled for their new method, thus providing documentation on the construction of the training data organization (region formation and point sampling) relevant for reproducibility of experiments."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1429-rubric-0",
    "token_usage": {
      "prompt_tokens": 13387,
      "completion_tokens": 156,
      "total_tokens": 13543
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4 - Experiments, Dataset Descriptions",
          "Reasoning": "The authors introduce two new multi-domain datasets composed from existing datasets but curated by their method for their experiments: the 'Cars' dataset (derived by splitting LSUN-Car dataset into five classes) and the 'Faces' dataset (composed of Flickr-Faces-HQ, MetFaces, and AFHQ animal classes). These datasets consist of images captured by humans and collected via human-operated sensors or manual labeling originally, hence are human generated images. These datasets are explicitly constructed by the authors for evaluation in their model, representing the new data source introduced and used in this paper."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1429-rubric-1",
    "token_usage": {
      "prompt_tokens": 14187,
      "completion_tokens": 312,
      "total_tokens": 14499
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.3 and 4.5",
            "reasoning": "The PMGAN model is trained on the newly constructed multi-domain datasets (Cars and Faces datasets) that are created by dividing existing datasets into domains. The annotations for semantic segmentation transfer (Section 4.3) use zero-shot transfer using morph maps on pretrained segmentation masks, and the training of PMGAN itself is a fully unsupervised generative model without human annotators explicitly labeling the data. Thus, the annotation here is effectively performed by the automatic PMGAN model processes (e.g., morph map learning, segmentation transfer via Morph operation) rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any mention or description of detailed annotation instructions provided to humans or annotators for these newly introduced datasets. The annotation process is model-driven and unsupervised."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No scoring rubrics or formal evaluation criteria for human annotation is described in the paper for the new multi-domain datasets. Evaluation metrics such as FID and classification accuracy are used, but these are for model evaluation, not annotation guidelines."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not present or refer to annotation examples for human annotators. The segmentation masks used for zero-shot transfer come from a pretrained network or pseudo-labels, not manual annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1429-rubric-2",
    "token_usage": {
      "prompt_tokens": 15377,
      "completion_tokens": 298,
      "total_tokens": 15675
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no documentation or mention in the paper of multiple human experts performing quality assurance on the introduced datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about multiple non-expert human annotators performing quality assurance on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that an AI model was used to perform quality assurance on datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains no indication that automated verification or algorithmic quality assurance processes were used to validate the introduced datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces two multi-domain datasets constructed from pre-existing datasets (LSUN-Car and AFHQ including Flickr-Faces-HQ, MetFaces, Cat, Dog, Wildlife) but does not document any quality assurance procedure for these datasets. Thus, no quality assurance process is applied or documented by the authors for these introduced datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1429-rubric-3",
    "token_usage": {
      "prompt_tokens": 14987,
      "completion_tokens": 382,
      "total_tokens": 15369
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any datasets generated entirely by AI models from scratch; rather, it uses existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data produced by human translation from another language is mentioned or introduced."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data generated by machine translation from another language is described."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments)",
          "reasoning": "The datasets used in the experiments (Cars and Faces datasets) are constructed by collating existing datasets such as LSUN-Car, Flickr-Faces-HQ, MetFaces, AFHQ dataset, along with fine-grained object classes divided by classifiers. These datasets are collected and aggregated from existing sources without the authors creating new content."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets are derived by transforming or adapting existing data; the paper uses existing datasets as-is or aggregated as described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origins are documented and specified as existing public datasets, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1429-rubric-4",
    "token_usage": {
      "prompt_tokens": 15505,
      "completion_tokens": 542,
      "total_tokens": 16047
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the introduced datasets as being used exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses pre-trained StyleGAN2 models on parent domains to initialize training; it does not describe training models completely from scratch on the introduced datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not described as being used to fine-tune a pre-trained model in a supervised manner; rather, they are used jointly during PMGAN training."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or description of reinforcement learning based post-training such as RLHF on the introduced datasets is present in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), specifically 4.1 Ablation Studies and Tables 1 and 2",
          "reasoning": "The datasets introduced (Cars composed of car classes and Faces composed of FFHQ, MetFaces, Cat, Dog, Wild life) are used to evaluate the proposed PMGAN model's ability to generate aligned samples, as well as to measure sample quality (via FID), classification accuracy, and image-to-image translation performance."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 Qualitative Analysis, Section 4.3 Zero-shot Segmentation Transfer, Section 4.4 Image-to-Image Translation",
          "reasoning": "The datasets are used to analyze and demonstrate semantic alignment, geometric morphing, editing across domains, segmentation mask transfer, and cross-domain image translation capabilities of PMGAN."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or indication of using the datasets as a knowledge base for augmenting models via retrieval or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The introduced datasets have demonstrated practical usage for evaluation and analysis of the proposed PMGAN model."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1429-rubric-5",
    "token_usage": {
      "prompt_tokens": 16228,
      "completion_tokens": 610,
      "total_tokens": 16838
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset entries containing more than two human languages. The introduced multi-domain image datasets consist of images such as faces and cars, but these do not involve multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset entries containing exactly two human languages. The datasets are image datasets and do not include language data."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper deals exclusively with image datasets and does not describe datasets composed of English text or entries with English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset entries with exactly one non-English human language are mentioned in the paper. The datasets are image datasets and not linguistic."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper presents algorithms and model architectures and describes code-related operations such as MorphNet and GANs, it does not introduce any dataset consisting of code or programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes mathematical expressions to describe the model and operations, but these do not pertain to the proposed dataset entries. The dataset consists of images and does not contain entries of mathematical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the dataset includes images of non-human animals (cats, dogs, wildlife), these are visual images and do not constitute biological sequences or non-human communication systems entries such as DNA or chemical signaling."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any constructed or fictional languages in its datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are clearly described as image datasets of cars and faces across multiple domains; there is no indication that the language or content type is unknown or unspecified."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced by the authors are image datasets consisting of pictures of cars and faces from multiple related domains. These datasets do not contain entries with any human or programming language, only images. Hence, the linguistic scope is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1429-rubric-6",
    "token_usage": {
      "prompt_tokens": 13446,
      "completion_tokens": 206,
      "total_tokens": 13652
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No mention in the paper",
          "reasoning": "The paper does not include any statement or link to publicly available code repositories for the code used in dataset creation or processing. There is no indication that the code for constructing the datasets or preprocessing used in the paper is publicly released."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 (Experiments) and subsections 4.1 and 4.2",
          "reasoning": "The paper documents the construction of its new multi-domain datasets in Section 4. It describes the Cars dataset, split into five classes using a pretrained classifier, and the Faces dataset, combining several existing datasets into multiple domains. It specifies data splits and sizes, the parent domains for StyleGAN2 pretraining, and details useful for reproduction. Although not exhaustive, the documentation provides a clear description of how these new multi-domain datasets are composed from existing datasets and additional classifier labeling, sufficient for understanding the dataset creation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1430-rubric-0",
    "token_usage": {
      "prompt_tokens": 14906,
      "completion_tokens": 149,
      "total_tokens": 15055
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Datasets and Section 5 Experiments",
          "Reasoning": "The authors introduced synthetic variants of the CIFAR-100 dataset including CIFAR-stretch, CIFAR-tile, and CIFAR-large as toy datasets by applying transformations such as vertical stretching, tiling, and upsampling/cropping. These datasets are newly constructed by the authors from CIFAR-100 images and used for evaluating their DynOPool method. Since CIFAR-100 is a human-generated natural image dataset, and the authors altered them with simple manipulations, these datasets retain their human-generated image modality."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1430-rubric-1",
    "token_usage": {
      "prompt_tokens": 15706,
      "completion_tokens": 216,
      "total_tokens": 15922
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 5.1 (Datasets) and Section 5 (Experiments)",
            "reasoning": "The paper describes experiments using multiple datasets including CIFAR-100, FGVC-Aircraft, ImageNet, and PascalVOC. These are established datasets, hence they are not newly introduced by the paper. The new contribution is the DynOPool method applied on these datasets. There is no mention of any newly introduced dataset or new annotated dataset created by the authors in the paper."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "Since no new dataset with human annotations is introduced, there are no annotation instructions provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation rubrics are described, as no new annotated dataset is introduced."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation guideline examples are provided, as no new annotated dataset is introduced."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1430-rubric-2",
    "token_usage": {
      "prompt_tokens": 16896,
      "completion_tokens": 381,
      "total_tokens": 17277
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset annotation or quality assurance process performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper about multiple human experts participating in quality assurance of datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of multiple non-expert annotators performing quality assurance on dataset annotations."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by an AI model judging the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe an automated verification or algorithmic quality assurance process applied to dataset content or annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets, nor does it describe any quality assurance procedures for datasets. The datasets used are pre-existing (e.g., CIFAR-100, ImageNet, FGVC-Aircraft, PascalVOC) and the paper focuses on proposing a learnable pooling operation rather than on dataset creation or annotation quality assurance."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1430-rubric-3",
    "token_usage": {
      "prompt_tokens": 16506,
      "completion_tokens": 435,
      "total_tokens": 16941
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets created entirely from scratch by human contributors. Instead, it uses existing datasets such as CIFAR-100, ImageNet, and FGVC-Aircraft for experimentation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets are generated by AI or machine learning models independently within this work. The paper focuses on proposing a learnable pooling operation using existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper of data produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data generated by machine translation systems is mentioned."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are existing benchmark datasets collected previously; the paper does not indicate collecting or aggregating new data from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3 (Motivation), Section 5.2 (Experiment Setup and Results)",
          "reasoning": "The paper uses existing datasets such as CIFAR-100 and creates synthetic variants like CIFAR-stretch, CIFAR-tile, and CIFAR-large by applying transformations such as vertical stretching, tiling, cropping, and upsampling. These variants are modifications of existing data and thus represent derived datasets. This is stated explicitly in Section 3 (Motivation) through toy experiments with transformed CIFAR-100 datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation for all datasets used is specified."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1430-rubric-4",
    "token_usage": {
      "prompt_tokens": 17024,
      "completion_tokens": 326,
      "total_tokens": 17350
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets of its own. It uses existing datasets such as CIFAR-100, ImageNet, FGVC-Aircraft, and PascalVOC to validate and demonstrate the effectiveness of the proposed DynOPool operation. The experiments and evaluations are conducted on these widely recognized pre-existing datasets. Therefore, there is no new dataset introduced by the authors, nor any described or demonstrated practical usage of a new dataset in the machine learning pipeline."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1430-rubric-5",
    "token_usage": {
      "prompt_tokens": 17747,
      "completion_tokens": 540,
      "total_tokens": 18287
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset that contains entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention or introduce a dataset containing exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset introduced by the authors contains only English language content. The datasets mentioned (CIFAR-100, ImageNet, FGVC-Aircraft, PascalVOC) are existing visual datasets and are image datasets without linguistic content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset introduced by the authors contains a single non-English language content. Also, the datasets are image datasets, not language-based."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes a method and experiments on image data but does not introduce datasets composed of programming or code data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses mathematical notations in the method description (Section 4) but does not introduce datasets with entries containing mathematical or logical symbolic data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication datasets are introduced in this paper."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not include datasets with any constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets mentioned are established image benchmarks; language type is either not applicable or well known, so there is no unknown language situation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset containing language entries. The datasets referenced are existing image datasets consisting of image data without any language content. Hence, linguistic scope does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1430-rubric-6",
    "token_usage": {
      "prompt_tokens": 14965,
      "completion_tokens": 152,
      "total_tokens": 15117
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not provide any indication or link to code repositories related to data collection, preprocessing, or dataset generation. The datasets used (e.g., CIFAR-100, ImageNet, FGVC-Aircraft, PascalVOC) are standard, publicly available datasets; no new dataset construction or code associated with new datasets is presented in the paper."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section or content referring to dataset creation",
          "reasoning": "The paper uses existing standard datasets and does not introduce or describe the creation of any new dataset; thus, there is no documentation related to any dataset creation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1435-rubric-0",
    "token_usage": {
      "prompt_tokens": 13145,
      "completion_tokens": 307,
      "total_tokens": 13452
    },
    "response": {
      "sources": [
        {
          "Modality": "time series",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1, 3. Methodology; Section 3.2 Module detail; Section 4. Experiments 4.1 Dataset",
          "Reasoning": "The paper introduces a new dataset of 3D human pose sequences generated via a self-supervised iterative framework called PoseTriplet. The initial 3D poses are synthesized with a zero-data generating strategy using a reinforcement learning based pose imitator controlling a physics simulator to produce physically plausible 3D pose sequences (Section 3.1). Then these sequences are diversefied and enhanced by a pose hallucinator (Section 3.2.3). The final enhanced and plausible 3D pose sequences are paired with projected 2D poses to form 2D-3D pose pairs used to train a pose estimator, thus forming a new dataset of paired 2D and 3D pose time series sequences entirely generated by model components without human annotation or recording of 3D motion capture data. The input 2D poses are human-generated, but the new dataset focuses on augmented 2D-3D pairs generated from the model components themselves. Therefore, the new dataset is composed of time series data modality, generated by model (model generated: true; human generated: false). The paper clearly distinguishes these generated sequences from existing human-generated datasets (e.g., Human3.6M)."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1435-rubric-1",
    "token_usage": {
      "prompt_tokens": 13945,
      "completion_tokens": 313,
      "total_tokens": 14258
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1, 3.2, Figure 2",
            "reasoning": "The newly introduced dataset comprises self-generated 2D-3D pose pairs created by the system's components: pose estimator, imitator (a physics-based reinforcement learning module), and hallucinator (a generative motion interpolation model). This pipeline produces annotation data (paired 2D and physically plausible 3D pose sequences) fully automatically without human annotators. The pose imitator incorporates a physics simulator and reinforcement learning; the hallucinator is a neural network model. Therefore, annotation is generated by an automatic AI-based process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not mention any manual annotation guidelines, instruction documents, or human labeling steps for the generated pose data. All data generation is automated with no described annotation instructions provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No mention of explicit rubrics or scoring criteria governing the annotation or generation process is provided in the paper. The reinforcement learning reward functions for the imitator are algorithmic and not human annotation rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not provide specific examples or templates for annotation guidelines as the data generation is fully automatic and algorithmic. Illustrations such as figures demonstrate generated pose sequences but are not annotation examples in a human labeling sense."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1435-rubric-2",
    "token_usage": {
      "prompt_tokens": 15135,
      "completion_tokens": 423,
      "total_tokens": 15558
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on any introduced datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts were involved in quality assurance for any introduced datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance carried out by a single non-expert annotator on dataset annotations or content."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance involving multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of any AI model used as a judge for quality assurance of dataset content or annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3. Methodology, Section 3.2.2 Pose imitator",
          "reasoning": "The pose imitator module uses a reinforcement learning based physics simulation and predefined reward metrics to ensure physical plausibility of generated pose sequences. This process acts as an automated verification of the data's physical soundness, effectively functioning as an automatic quality assurance mechanism on the generated 3D pose sequences used as training data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an automated process (the pose imitator) used to verify physical plausibility of the generated 3D pose sequences, indicating that some form of QA process is indeed applied."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1435-rubric-3",
    "token_usage": {
      "prompt_tokens": 14745,
      "completion_tokens": 594,
      "total_tokens": 15339
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset created entirely from scratch by human contributors. Instead, it uses existing 2D pose sequences as input for its framework."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.1 (PoseTriplet), Section 3.2.3 (Pose hallucinator), and Section 3.2.2 (Pose imitator)",
          "reasoning": "The paper introduces novel 3D pose sequences generated by their PoseTriplet framework components: the pose estimator transforms 2D inputs to 3D poses; the pose imitator refines these to be physically plausible using reinforcement learning in a physics simulator; and the pose hallucinator generates novel and diverse 3D motions through generative motion interpolation. These generated 3D pose sequences are model-created data used to augment supervision for pose estimation training, constituting new data generated entirely by their machine learning models and physics-based simulation without relying on pre-existing labeled 3D data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication or mention is given about data generated through human translation in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention data generated by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the method uses existing 2D pose sequences as inputs, it does not present these 2D pose datasets as new datasets introduced by the authors. Also, the newly generated 3D data is not simply collated but synthesized/generated by the model."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3 (Methodology), Section 3.1 (PoseTriplet)",
          "reasoning": "The new 3D pose data is derived from existing 2D pose inputs by multiple transformations: from 2D to low-fidelity 3D poses by pose estimator; refinement to physically plausible 3D poses by imitator; and diversification by hallucinator. These transformations modify and adapt existing data (2D poses) into new 3D pose data used for training, indicating the new data is derived rather than raw or entirely new human-created data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes the origin and generation process of the new datasets; thus, data origin is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1435-rubric-4",
    "token_usage": {
      "prompt_tokens": 15263,
      "completion_tokens": 435,
      "total_tokens": 15698
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 (Methodology), Section 3.1 (PoseTriplet), Section 4 (Experiments)",
          "reasoning": "The paper describes that their proposed self-supervised framework 'PoseTriplet' generates augmented 2D-3D pose pairs to train a pose estimator from scratch without relying on any external 3D ground truth data. They explicitly describe training the pose estimator from only 2D poses through iterative co-evolution with the imitator and hallucinator, starting from zero-data initializations (Section 3.1). Experimental results in Section 4 demonstrate training pose estimators with this self-generated data yielding improved results, confirming that the dataset produced by their framework is used for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the pose imitator module is trained with reinforcement learning, it uses physically simulated data generated internally and is not a separate dataset used for reinforcement learning post-training. The self-generated data is not explicitly used for RL post-training techniques like RLHF on existing models."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the generated data set serves only for evaluation purposes; instead, it explicitly supports training."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not primarily used just for analysis of trends or patterns, but to enable training of the pose estimator."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the dataset as serving as a knowledge base for model augmentation/retrieval."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the use of the new self-generated dataset to train the pose estimator from scratch as per the co-evolving framework; thus, it has practical usage."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1435-rubric-5",
    "token_usage": {
      "prompt_tokens": 15986,
      "completion_tokens": 517,
      "total_tokens": 16503
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Entire paper, especially Abstract and Methodology sections",
          "reasoning": "The newly introduced dataset consists of 2D-3D human pose pairs represented as geometric joint coordinates and associated temporal sequences. The textual content of the dataset and paper is entirely in English. There is no mention of any non-English or multilingual human language content within the dataset entries. The dataset entries are numerical pose data, with all descriptions and documentation in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper describes algorithms and uses mathematical formulae, the datasets themselves do not contain programming languages or code snippets as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notations appear in the paper text to describe methods, but the dataset entries themselves are not comprised of mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of human poses represented geometrically; no biological sequences or non-human communication data is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of any constructed or fictional languages in the dataset content."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language and format of dataset entries are clearly documented and described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries are numerical pose data and do not consist of natural language content; however, the dataset annotations and documentation are in English, so language coverage is considered monolingual English."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1435-rubric-6",
    "token_usage": {
      "prompt_tokens": 13204,
      "completion_tokens": 295,
      "total_tokens": 13499
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and various sections",
          "reasoning": "The paper provides a URL to a public GitHub repository (https://github.com/Garfield-kh/PoseTriplet) which hosts the code related to the project, including dataset construction aspects such as the self-generated motion data, pose estimation, imitation, and hallucination components. This indicates that the code used to generate the datasets within their framework is publicly available for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and 4, including 3.1 'PoseTriplet', 3.2 'Module detail', and 4.1 'Dataset'",
          "reasoning": "The paper contains detailed documentation of the dataset creation process through the self-supervised framework PoseTriplet. It describes the dual-loop architecture that produces physically plausible and diverse 3D pose sequences from given 2D pose sequences without requiring ground truth 3D data. The process involves a pose estimator, pose imitator, and pose hallucinator modules, and explains how initial synthetic data is generated to bootstrap training. The methodology includes information about projection strategies for training data, and experimental setup specifies the 2D input data used (MPI-INF-3DHP, H36M subsets). The descriptions are comprehensive and clarify how the new data is generated and utilized. Hence, the dataset construction process is well documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1481-rubric-0",
    "token_usage": {
      "prompt_tokens": 12160,
      "completion_tokens": 7,
      "total_tokens": 12167
    },
    "response": {
      "sources": []
    }
  },
  {
    "id": "CVPR-2022-1481-rubric-1",
    "token_usage": {
      "prompt_tokens": 12960,
      "completion_tokens": 226,
      "total_tokens": 13186
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4, Datasets",
            "reasoning": "The paper mentions that datasets are converted to multicut instances using existing methods or code, e.g., connectomics datasets are converted using [48] and [47], which indicates an automatic preprocessing step rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4, Datasets",
            "reasoning": "No annotation instructions for human annotators are provided because the dataset is either derived from existing datasets or automatically converted; no manual annotation procedure is described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4, Datasets",
            "reasoning": "No scoring rubrics or quality criteria for human annotations are mentioned; the dataset is algorithmically generated from raw data, so no rubric applies."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4, Datasets",
            "reasoning": "No annotation examples are provided because the dataset creation is automatic or uses existing datasets. The paper focuses on algorithmic solvers, not annotation manual guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1481-rubric-2",
    "token_usage": {
      "prompt_tokens": 14150,
      "completion_tokens": 300,
      "total_tokens": 14450
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about quality assurance by multiple human experts for the datasets described in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of multiple non-expert human annotators performing quality assurance on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report using AI models as judges or quality assurance for the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses algorithms and methods for solving optimization problems, it does not describe using an automatic process specifically for quality assurance of dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses three pre-existing datasets (Connectomics-SP, Connectomics-Raw, Cityscapes) for benchmark purposes but does not introduce any new datasets nor describe any quality assurance or validation processes for dataset annotations or content. There is no documentation or application of any quality assurance process related to datasets in this paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1481-rubric-3",
    "token_usage": {
      "prompt_tokens": 13760,
      "completion_tokens": 424,
      "total_tokens": 14184
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state creation of any original dataset entirely from scratch by human contributors. The experimental evaluation uses existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any new dataset purely from models or artificial data generation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of datasets being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of datasets generated by machine translation of content from other languages."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are referenced and publicly available but the paper does not indicate that the authors collated or aggregated these datasets into a new compiled dataset."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4 (Datasets)",
          "reasoning": "The authors use existing datasets such as Connectomics-SP, Connectomics-Raw, and Cityscapes which are publicly available and processed into multicut problem instances. For example, Connectomics-SP comes from CREMI challenge data converted by prior work [48]. Similarly, Cityscapes data uses affinities computed by another method [2]. Thus the datasets used are derived by applying transformations, conversions, and adaptations to existing data to create multicut problem instances for algorithm evaluation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides sufficient documentation about the datasets used and their origins, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1481-rubric-4",
    "token_usage": {
      "prompt_tokens": 14278,
      "completion_tokens": 420,
      "total_tokens": 14698
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the introduced datasets for pre-training any models."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that these datasets are used for training any model from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not described as being used for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of these datasets for reinforcement learning or RL-based post-training methods."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments)",
          "reasoning": "The introduced datasets, such as Connectomics-SP, Connectomics-Raw, and Cityscapes, are utilized for evaluation and benchmarking the performance and runtime of the proposed multicut algorithm variants against baseline methods, as described in Section 4 Experimental Evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not present these datasets being used primarily for analyzing trends or characteristics beyond their evaluation use."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of the datasets serving as a knowledge base or augmenting models through retrieval or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets are explicitly used in the paper for benchmarking and evaluation tasks."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1481-rubric-5",
    "token_usage": {
      "prompt_tokens": 15001,
      "completion_tokens": 552,
      "total_tokens": 15553
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4, Datasets",
          "reasoning": "The datasets introduced do not contain multilingual textual data but rather consist of graph-based representations derived from biological and image data. The documentation, descriptions, and experimental discussion in the paper are exclusively in English, indicating that all textual content associated with the datasets is solely in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains algorithms and code references, the datasets described are graph-based problem instances for multicut algorithms, not programming code or structured programming data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematics and logical expressions appear in the methodology sections describing the algorithms, but the new datasets introduced are graph decompositions or multicut problem instances without explicit inclusion of mathematical notation as part of their entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": true,
          "reference": "Section 4, Datasets - Connectomics-SP and Connectomics-Raw",
          "reasoning": "The Connectomics-SP and Connectomics-Raw datasets represent neuron segmentation problems from the fruit-fly brain derived from electron microscopy volumes, which are biological data reflecting the structure of neurons. These can be considered biological non-human communication systems data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or artificially created languages within the dataset descriptions."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets\u2019 content and nature are clearly described; no ambiguous or unspecified language content is included."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets do contain language in the form of English textual descriptions and labels and representations derived from biological sources, so the category N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1481-rubric-6",
    "token_usage": {
      "prompt_tokens": 12219,
      "completion_tokens": 218,
      "total_tokens": 12437
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4 (Datasets) and Conclusion",
          "reasoning": "The paper references external datasets (e.g., the CREMI challenge datasets and Cityscapes) that are pre-existing and accessible elsewhere. The authors do not introduce any new datasets themselves. Moreover, there is no mention or link to code that processes or constructs any new dataset by the authors. The code linked (https://github.com/pawelswoboda/RAMA) corresponds to the algorithm implementation, not to dataset creation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 4 (Datasets)",
          "reasoning": "The paper describes the existing datasets used in the experiments including how they were generated or converted into multicut problem instances by prior works or by referencing other methods (e.g., using superpixels or edge affinities from existing models). Since no new dataset is introduced in this work, and dataset processing steps are only briefly mentioned as inherited from previous works, there is no original dataset creation documentation provided in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1487-rubric-0",
    "token_usage": {
      "prompt_tokens": 12980,
      "completion_tokens": 108,
      "total_tokens": 13088
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4, Implementation Details",
          "Reasoning": "The new dataset used for training and testing is built based on a publicly available VIS/NIR Scene dataset. The images consist of visible (VIS) and near-infrared (NIR) image pairs manually collected by human-operated imaging devices capturing real scenes, as indicated by the use of VIS/NIR pairs and context in Section 4 Implementation Details."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1487-rubric-1",
    "token_usage": {
      "prompt_tokens": 13780,
      "completion_tokens": 261,
      "total_tokens": 14041
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4 and Section 3.1, 3.2",
            "reasoning": "The paper introduces a new dataset by cropping and flipping images from a publicly available VIS/NIR Scene dataset to build the training and test datasets. The annotation involved ground-truth landmark points for registration were built (5 pairs per image pair) for quantitative evaluation, which is a specialized data preparation process typically performed automatically or semi-automatically as described in Section 4. No mention of manual human annotation by experts or non-experts is indicated. Hence, the annotation process is best characterized as an Automatic Process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit instructions described",
            "reasoning": "The paper does not explicitly mention any annotation instructions or guideline documents provided for annotators of the new dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No scoring rubrics described",
            "reasoning": "The paper does not describe any scoring rubrics for the annotations or evaluations related to the new dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of annotations provided",
            "reasoning": "There are no annotation examples or samples provided or discussed in the paper for the new dataset's annotations."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1487-rubric-2",
    "token_usage": {
      "prompt_tokens": 14970,
      "completion_tokens": 318,
      "total_tokens": 15288
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the datasets used or introduced."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of multiple human experts performing quality assurance on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about quality assurance conducted by multiple non-expert humans for the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using an AI model as a judge for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the method involves automated processing and evaluation metrics (e.g., NCC, RMSE) for registration and fusion quality, there is no indication that an automatic quality assurance process was applied specifically for validating or verifying dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any process or procedure for quality assurance of the datasets used. There is no mention of manual annotation validation or automated verification of dataset content. The datasets are built based on a publicly available VIS/NIR Scene dataset, but no QA of these datasets is described or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1487-rubric-3",
    "token_usage": {
      "prompt_tokens": 14580,
      "completion_tokens": 278,
      "total_tokens": 14858
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4, Implementation Details",
          "reasoning": "The paper states that the training and test datasets are built based on a publicly available VIS/NIR Scene dataset. There is no indication that the authors created new datasets from scratch; rather, they collected and aggregated existing multi-modal images (VIS and NIR) for training and testing. The augmentation by cropping and flipping further supports collating existing data for their experiments."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1487-rubric-4",
    "token_usage": {
      "prompt_tokens": 15098,
      "completion_tokens": 520,
      "total_tokens": 15618
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the proposed new dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments) Implementation Details",
          "reasoning": "The training and test datasets are built based on a publicly available VIS/NIR Scene dataset, and the networks (AffineNet, TransNet, F2M) are trained from scratch on these datasets, as described in the implementation details of section 4. This is evident as the training epochs, batch size, optimizer, and learning rates are specified for training the networks on this dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention fine-tuning a pre-trained model using the new dataset with supervised learning methods."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or related post-training techniques using the proposed dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The paper uses the constructed dataset (training and testing splits) to evaluate registration and fusion performance of the proposed method and compares it with baselines. Quantitative and qualitative benchmarking results are presented in Section 4.1 and 4.2."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3 Ablation Study",
          "reasoning": "The dataset is used for ablation studies analyzing the impact of different design choices and components in the model, especially texture preservation strategies and coarse registration methods."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base to augment models or retrieval-based generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper demonstrates the practical usage of the introduced dataset for training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1487-rubric-5",
    "token_usage": {
      "prompt_tokens": 15821,
      "completion_tokens": 535,
      "total_tokens": 16356
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, including Abstract and Sections 1, 3, 4, and 5",
          "reasoning": "The paper and its descriptions, including dataset descriptions, experimental results, and methodological explanations, are entirely presented in English. The datasets introduced (VIS/NIR Scene dataset patches) are image data and associated annotations described in English. There is no mention or presence of data entries in any other human language."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper describes methods implemented in TensorFlow, it does not introduce or contain any dataset entries consisting of programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3 and throughout the methodology and loss function equations",
          "reasoning": "The dataset includes mathematical and logical notations as part of its formal description and loss function definitions, equations, and symbolic representations related to image transformations and optimization. However, these are part of the methodological descriptions not the dataset content itself."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets involve images from visible and near-infrared sensors, no biological sequences or non-human communication systems are involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or presence of any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of dataset entries are clearly documented as English for textual descriptions; images do not convey natural language."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains image and textual data in English, so cannot be labeled as having no language content."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1487-rubric-6",
    "token_usage": {
      "prompt_tokens": 13039,
      "completion_tokens": 142,
      "total_tokens": 13181
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not mention any publicly available code repository or provide any link to a codebase related to dataset construction or processing. No indication of code release for data generation or preprocessing is provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "",
          "reasoning": "While the paper describes that training and test datasets are built based on a publicly available VIS/NIR Scene dataset, and describes some processing steps such as cropping and flipping, it does not provide detailed documentation about dataset creation, annotation, or any custom dataset construction process. The dataset creation process is not the focus of the paper and lacks explicit documentation for reproducibility."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1496-rubric-0",
    "token_usage": {
      "prompt_tokens": 12875,
      "completion_tokens": 210,
      "total_tokens": 13085
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 (Dataset and Experimental Setup) and Experiments (Tables 1 and 5)",
          "Reasoning": "The authors introduce the use of three benchmark datasets, including SVHN, CIFAR-100, and ISIC 2018, for evaluation of their new federated semi-supervised learning (FSSL) method, RSCFed. They perform experiments on these datasets, resizing and preprocessing images as described. Although the datasets themselves are public and pre-existing, the paper describes partitioning these datasets into labeled and unlabeled clients (including partially labeled clients) for their novel federated semi-supervised learning experimental setup. Therefore, the data partitions and federated semi-supervised learning contexts with these datasets form the new data setting introduced by the authors, involving image data modality. The images are human generated since they originate from human-captured images of digits, natural scenes, and medical images."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1496-rubric-1",
    "token_usage": {
      "prompt_tokens": 13675,
      "completion_tokens": 289,
      "total_tokens": 13964
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2 and Section 3.3",
            "reasoning": "The annotations correspond to client model training steps which are conducted via training algorithms (supervised cross-entropy loss on labeled clients and mean-teacher based consistency loss on unlabeled clients) as described in Section 3.2. There is no mention of human annotators labeling data; datasets are standard benchmarks for which ground truth labels exist (e.g., SVHN, CIFAR-100, ISIC 2018). The process involves model training and aggregation automatically without reference to human annotations done as part of this work."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in paper",
            "reasoning": "The paper does not describe any instructions given to human annotators for labeling data since it uses existing benchmark datasets with existing labels or unlabeled sets. No new data annotation instructions were introduced."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in paper",
            "reasoning": "No scoring rubrics for annotation are discussed or described, as the annotation is automated model training rather than human labeling."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in paper",
            "reasoning": "No annotation examples for human annotators are provided in the paper. The dataset usage is standard with no new dataset creation or annotation examples required."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1496-rubric-2",
    "token_usage": {
      "prompt_tokens": 14865,
      "completion_tokens": 358,
      "total_tokens": 15223
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset annotation or quality assurance conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on any dataset introduced in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information about quality assurance conducted by a single non-expert human."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss quality assurance being performed by multiple non-expert humans."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No statement in the paper suggests that an AI model was used as a judge for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of any automated or algorithmic verification process for dataset annotation quality within the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses public benchmark datasets (SVHN, CIFAR-100, and ISIC 2018) and does not introduce any new datasets nor document any quality assurance process related to dataset annotation or content validation."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1496-rubric-3",
    "token_usage": {
      "prompt_tokens": 14475,
      "completion_tokens": 392,
      "total_tokens": 14867
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report the creation of any new datasets collected or created by human contributors from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any data generated entirely by AI or machine learning models as novel datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data is produced by human translations from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data in the paper is described as being generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset and Experimental Setup",
          "reasoning": "The paper evaluates the proposed method on three public benchmark datasets (SVHN, CIFAR-100, ISIC 2018) which are well known, publicly available datasets collected previously by other researchers. The paper uses these existing datasets without indication of modification, thus the data is collated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided that the data used is derived from existing sources with transformation or adaptation; the datasets are standard benchmark public datasets used without modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data is specified clearly as public benchmark datasets; thus, this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1496-rubric-4",
    "token_usage": {
      "prompt_tokens": 14993,
      "completion_tokens": 597,
      "total_tokens": 15590
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not described as being used exclusively for pre-training large models on general patterns. Instead, they are used within the federated semi-supervised learning framework for training and evaluation."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2 Local Training; Section 4.1 Dataset and Experimental Setup; Section 4.2 Results with labeled and unlabeled clients",
          "reasoning": "The datasets (SVHN, CIFAR-100, ISIC 2018) are partitioned into clients and used to train local models from the current global model in the federated semi-supervised learning setup. This training involves initializing from the global model, but the global model itself is trained from scratch starting with random initialization as described, and the datasets facilitate this process."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence that the datasets are used specifically for fine-tuning pre-trained models using supervised learning methods. Rather, they are used to train federated models jointly with unlabeled data."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning post-training methods such as RLHF being applied to these datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Experiments; Section 4.1 Dataset and Experimental Setup; Tables 1-7",
          "reasoning": "The datasets are explicitly used for experimental evaluation and benchmarking of the proposed RSCFed method and competing methods. Results are reported on these datasets to measure metrics such as accuracy and AUC."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.4 Ablation Studies; Section 4.2 and 4.3 Results discussions",
          "reasoning": "The datasets are used for detailed analysis including ablation studies on algorithm components and sensitivity to hyperparameters, communication costs, and client ratios. This analysis helps understand the behaviour of the proposed method."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as knowledge bases or for retrieval augmentation in the study."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents clear usage of the datasets for training, evaluation, and analyses, so N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1496-rubric-5",
    "token_usage": {
      "prompt_tokens": 15716,
      "completion_tokens": 551,
      "total_tokens": 16267
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the datasets contain entries with more than two human languages. The datasets (SVHN, CIFAR-100, ISIC 2018) are standard benchmarks using English or language-neutral image data."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets contain exactly two human languages. The datasets focus on image classification, with labels presumably in English only."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset and Experimental Setup; Entire paper describing datasets",
          "reasoning": "The datasets used (SVHN, CIFAR-100, ISIC 2018) involve labeled data annotations and information presented in English. No other languages are mentioned or implied in dataset content or labels."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not contain entries in non-English languages; no mention of such in paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the method and code are described, the datasets themselves are image datasets and do not contain code or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains mathematical formulations describing the method, but the datasets themselves do not contain mathematical or logical notation entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although one dataset is medical (ISIC 2018 skin lesion images), the dataset entries themselves are images and labels, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are included in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of datasets are specified and known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain entries with human language annotations (English)."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1496-rubric-6",
    "token_usage": {
      "prompt_tokens": 12934,
      "completion_tokens": 214,
      "total_tokens": 13148
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and main text",
          "reasoning": "The paper provides a link to code related to their method (https://github.com/XMed-Lab/RSCFed) but does not provide code related to dataset construction, preprocessing, or generation. The datasets used are publicly available benchmark datasets (SVHN, CIFAR-100, ISIC 2018), and the paper does not mention publishing code for dataset construction or preprocessing steps. Hence, code for dataset construction is not made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Dataset and Experimental Setup",
          "reasoning": "The paper documents the datasets used (SVHN, CIFAR-100, ISIC 2018), including data splits (80% training, 20% testing), image resizing, cropping, and normalization procedures, as well as details on data partitioning strategy (Non-IID partitioning via Dirichlet distribution). These descriptions provide transparency on dataset preparation, which aids reproducibility."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-151-rubric-0",
    "token_usage": {
      "prompt_tokens": 11795,
      "completion_tokens": 161,
      "total_tokens": 11956
    },
    "response": {
      "sources": [
        {
          "Modality": "time series",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets",
          "Reasoning": "The paper introduces new datasets in behavior analysis domains, specifically mouse, fly, and basketball datasets, all consisting of trajectory data representing agent movements over time. This type of data is inherently time series, as it captures sequences of poses and locations frame-by-frame or over sequence steps. The data is human generated as it originates from human-recorded videos and manually annotated behaviors as described by domain experts, e.g., frame-level annotations of videos of fruit fly and mouse interactions, and basketball sequences from Stats Perform. The paper explicitly states the use of datasets with frame-level pose and behavior annotations stemming from human observational data."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-151-rubric-1",
    "token_usage": {
      "prompt_tokens": 12595,
      "completion_tokens": 328,
      "total_tokens": 12923
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.1 Datasets; Section 4.3 Training Setup",
            "reasoning": "The paper states that domain-level labeling functions (LFs) were provided by domain experts for each domain (mouse, fly, basketball), and these originate from existing expert-defined behavioral features or annotations from related works [14,24]. These annotations are task-level frame-level behavior labels annotated by domain experts. The new dataset label annotations are thus performed by multiple human experts as indicated by the multiple expert-provided domain-level LFs and annotations."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.1 Datasets; Section 3.1 Background",
            "reasoning": "The datasets involve frame-level annotations of complex behaviors such as fly social actions and mouse behaviors by domain experts. While the paper does not give a direct verbatim copy of annotation instructions, it explicitly states that domain experts provide domain-level LFs based on annotated behavioral features and that these require expertise, implying instructions or guidelines were given to experts for consistent annotations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention found",
            "reasoning": "The paper does not mention explicit scoring rubrics or formalized scoring guidelines used for annotation by the human experts or for the annotation datasets introduced."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention found",
            "reasoning": "There is no mention of providing annotation examples or example annotations given to annotators in the paper for the new datasets introduced. Examples are not explicitly described in the annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-151-rubric-2",
    "token_usage": {
      "prompt_tokens": 13785,
      "completion_tokens": 314,
      "total_tokens": 14099
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process performed by a single human expert on the dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention in the paper of multiple human expert annotators performing quality assurance on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single non-expert performed quality assurance on the dataset annotations."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses AI models for generating labeling functions and downstream classifiers, it does not describe using AI models as judges for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper primarily discusses program synthesis for generating labeling functions, not an automated verification process for dataset annotation quality assurance. There is no description of automated quality checks or rule-based verification applied to dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not provide any explicit description of quality assurance processes employed on the dataset annotations. The datasets used are pre-existing and cited from prior works, with no mention of annotation validation procedures. Thus, there is no documented QA process for dataset content in this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-151-rubric-3",
    "token_usage": {
      "prompt_tokens": 13395,
      "completion_tokens": 458,
      "total_tokens": 13853
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any newly created datasets wholly authored and annotated from scratch by human contributors within this work."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used in the paper are not generated by AI or machine learning models originally; rather, the paper focuses on synthesizing labeling functions, not synthesizing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data produced by translation from other languages by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine-translated data from other languages; datasets used are not described as translated."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper uses existing datasets from behavioral neuroscience (mouse and fly behaviors) and sports analytics (basketball player trajectories). These datasets were collected from existing sources, such as Fly-Tracker [14], CalMS21 [27], and StatsPerform, without significant modification reported. The paper leverages these existing datasets for their experiments."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets and Section 3 Methods",
          "reasoning": "The authors apply transformations by synthesizing labeling functions based on existing domain-level labeling functions and features extracted from the original datasets. The labeling functions themselves are derived through program synthesis using a domain-specific language and a small labeled subset of existing data. This represents a derivation or transformation of the original dataset annotations rather than new raw data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources and their origins are clearly specified and documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-151-rubric-4",
    "token_usage": {
      "prompt_tokens": 13913,
      "completion_tokens": 366,
      "total_tokens": 14279
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2, Section 5.1",
          "reasoning": "The datasets introduced serve as benchmarks in behavior analysis domains to evaluate the effectiveness of the AutoSWAP synthesized labeling functions. The paper uses the datasets to measure model performance improvements in active learning and weak supervision settings (Sections 4.2, 5.1)."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.2",
          "reasoning": "The datasets are used to analyze performance trends such as the impact of the diversity cost on labeling function synthesis performance and the effectiveness on rare behaviors (Section 5.2), indicating usage for analyzing characteristics and patterns."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not described as knowledge bases to augment models or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-151-rubric-5",
    "token_usage": {
      "prompt_tokens": 14636,
      "completion_tokens": 685,
      "total_tokens": 15321
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any datasets containing more than two human languages. The datasets focus on behavior data of flies, mice, and basketball players without linguistic content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are related to behavioral trajectories and annotations, and there is no indication that exactly two human languages are contained in the dataset."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets; Throughout the paper",
          "reasoning": "The datasets (Fly, Mouse, Basketball) consist primarily of annotations and labels in English, with no indication of other human languages being involved. The paper's text and provided code snippets are in English, and domain-level labeling functions are described in English. Hence, the dataset annotations and metadata are in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of the dataset containing entries in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": true,
          "reference": "Section 3 Methods (3.1 and 3.2), Figures 2 and 3",
          "reasoning": "The dataset includes domain-level labeling functions and task-level labeling functions expressed as code snippets in Python. The paper describes the use of a domain-specific language (DSL) for program synthesis, including functional programs and code structures, indicating the dataset entries contain programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3.2 AutoSWAP, Equation (3), Figures 2 and 3",
          "reasoning": "The datasets and methods include mathematical expressions such as functions, tree edit distances, formal equations and logical conditions used within labeling functions and program synthesizer algorithms, hence the dataset entries include mathematical and logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the datasets are from biological domains involving animal behavior (flies, mice), the data consists of trajectory and behavior annotations rather than biological sequences or non-human communication signals like DNA or animal vocalizations. The paper does not mention inclusion of such data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the datasets include entries in fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages used in the dataset annotations and code are specified and documented (English and programming languages), so this category does not apply."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language content in the form of English annotations and code, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-151-rubric-6",
    "token_usage": {
      "prompt_tokens": 11854,
      "completion_tokens": 292,
      "total_tokens": 12146
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper provides a link to the implementation of AutoSWAP framework (https://github.com/autoswap/autoswap_cvpr_2022) which contains code related to the weak supervision and labeling function synthesis method, but there is no explicit indication or statement that the code for constructing, processing, or generating the new datasets used in the experiments (the mouse, fly, and basketball behavior datasets) is publicly available. The datasets themselves are pre-existing or derived from prior works ([14], [24], StatsPerform) but no new dataset creation code is provided."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper provides a clear description of the datasets used in their experiments, specifying that the datasets are based on agent trajectories from behavioral neuroscience and sports analytics domains, referencing the original works [14],[24], and StatsPerform for basketball data. It describes the train/validation/test splits, the types of behaviors annotated, and details domain-level labeling functions created from features in prior works. However, since the datasets are not newly introduced by the authors but rather taken from previous sources or providers, only documentation on their use is given, not their construction. Given that the paper documents the datasets used including statistics and data splits, this counts as documentation for the dataset usage, but not for new dataset construction."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1533-rubric-0",
    "token_usage": {
      "prompt_tokens": 11397,
      "completion_tokens": 156,
      "total_tokens": 11553
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Datasets and Implementation Details",
          "Reasoning": "The paper uses COCO-20 dataset as the source domain for training and conducts experiments with it as the source data. COCO-20 is a large-scale dataset of natural images with human-annotated pixel-level semantic segmentation labels. The paper states training on COCO-20 images with pixel annotations for categories. This dataset is human-generated in the sense that images are natural photographs captured by humans and annotated by human labelers. The paper does not introduce or propose a new dataset; rather, it uses COCO-20 as the source dataset provided by prior work."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1533-rubric-1",
    "token_usage": {
      "prompt_tokens": 12197,
      "completion_tokens": 709,
      "total_tokens": 12906
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 5.1 Datasets",
            "reasoning": "The new source training dataset used in this paper is COCO-20 which contains 80 categories with dense pixel-level annotations. The paper cites that COCO-20 is a large existing dataset extensively used and cites previous dataset annotation processes. Since COCO-20 is a known human-annotated dataset created by multiple human annotators (experts) for segmentation tasks, the paper assumes the availability of ground-truth pixel annotation. The paper does not specify new annotation procedures but uses COCO-20 as the source dataset, which was originally annotated by multiple expert annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit instructions mentioned for annotation in the paper",
            "reasoning": "The paper does not discuss or provide any annotation guidelines, instructions, or protocols regarding labeling for the COCO-20 dataset or any other dataset. It relies on the existing annotations in public datasets."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No rubric information provided",
            "reasoning": "No scoring rubrics or criteria for label quality assessment are mentioned in the paper for the dataset annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of annotations shown or described",
            "reasoning": "The paper does not provide any annotation examples or demonstrate the annotation process; rather, it uses existing datasets with established annotations."
          }
        },
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 5.1 Datasets - SUIM dataset description",
            "reasoning": "SUIM is used as a target domain dataset for testing in the paper, which is described as an exclusive underwater image segmentation dataset containing over 1500 images with pixel-level annotations. The original SUIM dataset is publicly available and was annotated by multiple human experts for underwater scenarios. The paper leverages SUIM as a target domain test set but does not describe annotation processes itself."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No annotation instructions or guidelines described in the paper for SUIM",
            "reasoning": "The paper does not describe any new annotation instructions or guidelines for SUIM or any new dataset; it uses SUIM as an existing dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No rubrics mentioned",
            "reasoning": "No mention of rubrics is provided for annotation quality assessment for SUIM."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples presented",
            "reasoning": "The paper does not provide examples of annotations for SUIM."
          }
        },
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 5.1 Datasets - FSS-1000 dataset description",
            "reasoning": "FSS-1000 is a publicly available few-shot segmentation dataset with 1000 categories and pixel-level annotations. The paper uses it as a target domain test dataset, with no new annotation generation described. The original dataset was annotated by multiple expert annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No instructions about data annotation in the paper",
            "reasoning": "The paper does not describe any instructions for annotation for FSS-1000."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No rubrics mentioned",
            "reasoning": "No rubric for annotation scores or quality mentioned."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples shown",
            "reasoning": "No annotation examples shown in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1533-rubric-2",
    "token_usage": {
      "prompt_tokens": 13387,
      "completion_tokens": 302,
      "total_tokens": 13689
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of quality assurance performed by multiple human experts for the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide information about quality assurance conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is given regarding multiple non-expert humans performing quality assurance on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used as a judge or for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated process used for quality assurance of the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper makes use of existing datasets (COCO-20, PASCAL-5, FSS-1000, SUIM) and does not introduce any new dataset, nor does it describe any quality assurance process applied to these datasets. Hence, no quality assurance process is documented or applied for new dataset annotations in this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1533-rubric-3",
    "token_usage": {
      "prompt_tokens": 12997,
      "completion_tokens": 414,
      "total_tokens": 13411
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creating any new datasets from scratch by human contributors. Instead, it uses existing datasets such as COCO-20, PASCAL-5, FSS-1000, and SUIM."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report generating any novel datasets entirely by AI or machine learning models. The focus is on method development and evaluation on existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data produced by translating content from another language through human translators is found in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data generated by machine translation systems is indicated in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.1 Datasets",
          "reasoning": "The paper employs existing datasets (COCO-20, PASCAL-5, FSS-1000, SUIM) as source and target domains. These datasets are collected and aggregated from previously established sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe modifying, transforming, or adapting existing datasets to create a new dataset. It uses standard splits of existing datasets as source and target domains."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies and references the origin of all datasets used; thus, this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1533-rubric-4",
    "token_usage": {
      "prompt_tokens": 13515,
      "completion_tokens": 302,
      "total_tokens": 13817
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.2 Implementation Details - Source training",
          "reasoning": "The COCO-20 dataset is used as the source domain training dataset where the model is trained with full supervision for the cross-domain few-shot semantic segmentation task. This demonstrates supervised fine-tuning usage of the dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1 Datasets and Section 5.3 Comparisons with State-of-the-art",
          "reasoning": "Datasets such as PASCAL-5, FSS-1000, and SUIM are used as target domains exclusively for evaluation and benchmarking the cross-domain few-shot semantic segmentation methods."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.4 Ablation Study and Section 5.5 Parameters Analysis",
          "reasoning": "The datasets are also used in analysis such as ablation studies and parameter influence evaluation to analyze the impact of the proposed methods on performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1533-rubric-5",
    "token_usage": {
      "prompt_tokens": 14238,
      "completion_tokens": 612,
      "total_tokens": 14850
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the proposed datasets contain entries with more than two human languages. The datasets mentioned (COCO-20, PASCAL-5, FSS-1000, SUIM) are image datasets with pixel-level annotations, and no multilingual textual content is described."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets contain exactly two human languages. The paper discusses datasets for semantic segmentation without mentioning any bilingual textual annotations."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5.1 Datasets",
          "reasoning": "The datasets (COCO-20, PASCAL-5, FSS-1000, SUIM) are common benchmark datasets for semantic segmentation with annotations in English. The paper describes usage of these datasets for training and testing, all with English language labels or annotations."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets involved are primarily English-labeled datasets; no dataset with non-English-only annotations is introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the methodology section includes mathematical equations and algorithm descriptions, the datasets introduced are image datasets with pixel-level annotations, not code or structured programming content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Section 4 Methodology",
          "reasoning": "The paper contains mathematical formulas and notation describing their meta-memory module and loss functions; however, these are descriptions of methods and not datasets containing mathematical or logical notation as dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are natural image datasets used for semantic segmentation containing human-made category annotations, not biological or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificially constructed languages are introduced or used in the proposed datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly specifies the datasets and their annotation language (English), so the language is specified and known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain human language annotations (English), so it is not applicable that they lack language content."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1533-rubric-6",
    "token_usage": {
      "prompt_tokens": 11456,
      "completion_tokens": 189,
      "total_tokens": 11645
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or part mentions code availability related to dataset construction.",
          "reasoning": "The paper does not include any information or links to code repositories related to dataset creation or data collection. It primarily uses existing public datasets such as COCO-20, PASCAL-5, FSS-1000, and SUIM for experiments, but does not introduce or provide code for creating new datasets."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 5.1 (Datasets)",
          "reasoning": "The paper describes the use and splitting of existing datasets for source and target domains, including COCO-20, PASCAL-5, FSS-1000, and SUIM, but does not introduce any new datasets created by the authors nor provides documentation about dataset construction processes. The datasets used are standard or publicly available datasets referenced by other works."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1545-rubric-0",
    "token_usage": {
      "prompt_tokens": 10894,
      "completion_tokens": 120,
      "total_tokens": 11014
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets, and Section 4 Experiments",
          "Reasoning": "The paper uses the KiTS19 dataset, which consists of labeled 3D computed tomography (CT) scans. These CT scans are volumetric medical images obtained from human subjects. Although the paper evaluates on three public datasets including KiTS19, it does not introduce any new dataset. The KiTS19 dataset is explicitly described as a public medical benchmark used for experiments."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1545-rubric-1",
    "token_usage": {
      "prompt_tokens": 11694,
      "completion_tokens": 240,
      "total_tokens": 11934
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2, Figure 1",
            "reasoning": "The paper describes a latent representation learning (LRL) architecture based on conditional variational autoencoder principles and a regular 3D-UNet with MC dropout to generate pseudo labels. The generated pseudo labels are produced by the model itself rather than human annotators, indicating the annotation process is performed automatically by the learning model architecture."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not provide any mention or description of detailed annotation instructions given to annotators for labeling data in the new dataset; instead, pseudo labels are generated by the model."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No scoring rubrics or evaluation criteria for human annotations or guidelines for manual annotations are described for the new dataset or for generating pseudo labels."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper lacks description or inclusion of annotation examples or guideline examples for human annotators, as the labels are generated algorithmically via the model pipeline."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1545-rubric-2",
    "token_usage": {
      "prompt_tokens": 12884,
      "completion_tokens": 322,
      "total_tokens": 13206
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that a single human expert conducted quality assurance for dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that multiple human experts performed quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper about quality assurance conducted by a single non-expert human annotator for the datasets used."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any involvement of multiple non-expert human annotators in the quality assurance process for the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper suggesting that AI models were used as judges for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or rule-based algorithms used to verify dataset annotations as part of quality assurance."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses three public medical datasets for evaluation but does not introduce any new datasets. It does not describe any quality assurance processes for dataset annotations, nor does it discuss annotation procedures or validation of annotation quality. Therefore, no quality assurance process is applied or documented for any new dataset, consistent with the instruction to assess only new datasets introduced by the authors, which are none."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1545-rubric-3",
    "token_usage": {
      "prompt_tokens": 12494,
      "completion_tokens": 389,
      "total_tokens": 12883
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses publicly available medical image datasets (KiTS19, Atrial Segmentation Challenge), which are not newly created by human contributors specifically for this work."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the proposed GBDL model generates pseudo labels during training, the underlying datasets are pre-existing public datasets. The newly generated pseudo labels are model outputs used internally during training, not a new dataset released by the authors."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by translation from another language by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 (Datasets)",
          "reasoning": "The paper uses three well-known public medical image datasets as benchmarks. These datasets were collected and publicly released previously; the authors aggregate and use them for evaluation."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state any creation of new datasets derived from existing ones through modifications, transformations, or adaptations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the datasets used is clearly specified as public medical datasets, so the data origin is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1545-rubric-4",
    "token_usage": {
      "prompt_tokens": 13012,
      "completion_tokens": 232,
      "total_tokens": 13244
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.3 Ablation Studies and Section 4.4 Comparison with State-of-the-Art Methods",
          "reasoning": "The paper discusses training the regular 3D-UNet with MC dropout from scratch using the inputs and pseudo-labels generated by the proposed latent representation learning architecture (LRL) component of GBDL. The training is done on medical volumes with partially labeled data, and the experimental results reflect training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1545-rubric-5",
    "token_usage": {
      "prompt_tokens": 13735,
      "completion_tokens": 637,
      "total_tokens": 14372
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing entries in more than two human languages. The datasets used are medical imaging datasets which typically contain images and annotations related to medical structures, not multi-language text data."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset introduced in the paper contains exactly two human languages. The datasets comprise volumetric medical images and their labels without indication of bilingual textual content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are medical image datasets, not text datasets; the paper is written in English but the datasets themselves do not contain English textual data as entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset introduced contains content in a single non-English human language. The datasets are medical images and segmentation maps rather than language data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "The paper mentions source code availability but does not introduce any dataset containing code or programming language entries.",
          "reasoning": "The datasets are volumetric medical images with labels and do not contain programming language or code content as part of the dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Although the paper contains mathematical formulas extensively in Section 3.1 and 3.2, these are part of the methodology description, not dataset content.",
          "reasoning": "The datasets introduced are medical image data and associated label volumes without embedded mathematical or logical symbolic representations. Mathematical notation is used in method descriptions rather than dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human medical imaging data (CT scans) and their segmentation labels, not biological sequences like DNA or animal communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or artificial languages are involved in the datasets introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset entries is clearly identified as volumetric medical images with segmentation labels; no uncertainty exists about what languages are present or their nature."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The datasets introduced consist of volumetric medical images (CT scans) and their corresponding segmentation labels; these are image data without any natural or artificial language content. Thus, the dataset entries do not contain any languages."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1545-rubric-6",
    "token_usage": {
      "prompt_tokens": 10953,
      "completion_tokens": 229,
      "total_tokens": 11182
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper provides a link to the source code for their method (https://github.com/Jianf-Wang/GBDL) but does not mention or provide any code related to dataset construction or preprocessing steps. The datasets used are all public medical datasets, which are obtained externally, and the paper utilizes them but does not introduce any new dataset or describe code used to construct datasets. Thus, there is no code provided for dataset creation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 4.1 and throughout the paper",
          "reasoning": "The paper mentions use of three public medical datasets (including KiTS19 and Atrial Segmentation Challenge dataset) with some settings and data splits described in Section 4.1. However, it does not document any dataset creation process since the datasets are public and pre-existing. No new datasets are introduced by the authors, and so there is no documentation about dataset construction. They only provide some details on data usage and experimental setup with these existing datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1562-rubric-0",
    "token_usage": {
      "prompt_tokens": 12487,
      "completion_tokens": 114,
      "total_tokens": 12601
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 7.1 Dataset and Implementation Details",
          "Reasoning": "The paper explicitly states that experiments are conducted on the MS-COCO 2017 dataset, which consists of images. This dataset is well-known to be composed of real-world images annotated by humans, implying human involvement in data generation. The authors do not introduce any new dataset but only use MS-COCO for evaluation, which is a human-generated image dataset."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1562-rubric-1",
    "token_usage": {
      "prompt_tokens": 13287,
      "completion_tokens": 222,
      "total_tokens": 13509
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 6 (Adaptive Ranking Pair Selection), Algorithm 1",
            "reasoning": "The paper proposes the Adaptive Ranking Pair Selection (ARPS) algorithm that dynamically selects ranking pairs based on adaptive thresholds and clustering; this process is described as an algorithmic procedure without mention of human annotators. Thus, the annotation corresponds to an automatic process, i.e., deterministic computation during model training rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any annotation instructions directed at human annotators for the ranking pair selection or any labeling task related to the new dataset or method."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No scoring rubrics or criteria for manual annotation are mentioned or provided in the paper regarding the ranking pair selection or the new method."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There are no example annotations or labeled examples given for any annotation task since the process is automatic and algorithmic."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1562-rubric-2",
    "token_usage": {
      "prompt_tokens": 14477,
      "completion_tokens": 302,
      "total_tokens": 14779
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance or annotation process involving a single human expert for dataset curation or validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description of multiple human experts performing quality assurance on any dataset introduced by the authors."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any manual quality assurance performed by a single non-expert annotator for the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description or evidence of quality assurance by multiple human non-expert annotators is present in the paper for any dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance process is not performed by any AI model judging dataset contents or annotations; the paper focuses on method development and evaluation on standard datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss automated verification of dataset annotations or includes any automated quality assurance process on datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset, nor does it detail any dataset quality assurance process. It uses the existing MS-COCO 2017 dataset for evaluation and does not mention any quality control steps related to data annotations."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1562-rubric-3",
    "token_usage": {
      "prompt_tokens": 14087,
      "completion_tokens": 417,
      "total_tokens": 14504
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creation of new datasets by human annotation or collection. The experiments rely solely on the existing MS-COCO dataset, which is a well-known pre-existing benchmark dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new data generated entirely by models without reference to existing data is mentioned. The method focuses on improved loss functions and pairwise error calculations on existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of machine translation used to generate any data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are existing benchmarks and are not collected or aggregated by the authors from multiple sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the authors propose a new loss and training strategy, no new datasets are derived or created by modifying or transforming existing data according to the paper. The paper focuses on training and evaluation methods applied to established datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a novel training loss and evaluation method but does not introduce any new dataset. The data source is exclusively the publicly available MS-COCO dataset as stated in Section 7.1 'Dataset and Implementation Details'. Hence, data origin is specified but no new dataset is created or introduced."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1562-rubric-4",
    "token_usage": {
      "prompt_tokens": 14605,
      "completion_tokens": 484,
      "total_tokens": 15089
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe use of any new dataset for pre-training large models. The MS-COCO dataset is used for training and evaluation, but it is not a new dataset introduced by the authors."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset used to train models from scratch. The experiments use the existing MS-COCO 2017 dataset for training the detection models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is described as being used specifically for supervised fine-tuning a pre-trained model. The paper focuses on loss functions and ranking pair selection methods applied during training on an existing dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention reinforcement learning or RL-based post-training techniques that use a new dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The evaluation is conducted using MS-COCO 2017 dataset, which is a standard benchmark dataset and not a newly introduced one by the authors."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors analyze existing datasets and losses, they do not introduce any new datasets for the sole purpose of analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce a dataset that serves as a knowledge base for retrieval or model augmentation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. All experiments and analysis are conducted on standard pre-existing datasets such as MS-COCO. Therefore, there is no practical usage of a new dataset described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1562-rubric-5",
    "token_usage": {
      "prompt_tokens": 15328,
      "completion_tokens": 425,
      "total_tokens": 15753
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce a new dataset with code or programming language content; it focuses on object detection loss functions and ranking losses using common datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Sections 3, 5, and 6",
          "reasoning": "The paper introduces new datasets specifically for the Adaptive Pairwise Error loss evaluation on MS-COCO images with significant mathematical notation representing loss functions, ranking pairs, and evaluation metrics. The dataset entries contain mathematical expressions related to object detection and ranking losses."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The dataset consists of images from the MS-COCO dataset, which does not contain linguistic content or language data. The focus is on visual images with bounding box annotations, not on human or natural language content."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1562-rubric-6",
    "token_usage": {
      "prompt_tokens": 12546,
      "completion_tokens": 219,
      "total_tokens": 12765
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 7.1 Dataset and Implementation Details",
          "reasoning": "The paper provides a URL for the implementation code of the proposed method (https://github.com/Xudangliatiger/APE-Loss). However, this code repository relates to the object detection method code and training, not to any new dataset construction or data collection code. The authors conduct experiments on an existing dataset (MS-COCO 2017) which is publicly available and standard. The paper does not describe any new dataset collected or constructed by the authors, nor does it mention code for creating any dataset."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 7.1 Dataset and Implementation Details",
          "reasoning": "The paper only uses the well-known MS-COCO 2017 dataset for experiments. It does not introduce or create any new dataset nor describe any data collection or preprocessing procedures distinct from using the existing MS-COCO dataset. Therefore, there is no dataset creation process to document in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-158-rubric-0",
    "token_usage": {
      "prompt_tokens": 9925,
      "completion_tokens": 340,
      "total_tokens": 10265
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4 (Experiments), especially 4. Experiments and 4.1 Evaluation Methods, and throughout the paper describing the use of the Stanford Extra Dog dataset (StanExt) extended by the authors",
          "Reasoning": "The authors introduce an extended version of the Stanford Extra Dog dataset (StanExt) for training BARC. The dataset consists of images of 120 dog breeds with manual annotations of 2D keypoints and silhouettes. The extension includes additional keypoints (eye, withers, throat). This data is collected from real images, which obviously involve human generation for acquisition (photography and annotation). The dataset is explicitly introduced and extended by the authors for this work to train their regression network."
        },
        {
          "Modality": "other",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Dog Model and 3.5 Novel Breed Losses, and throughout the description of training the breed-aware latent shape space",
          "Reasoning": "The authors introduce an extended 3D dog shape model derived from scanned toy figurines and 3D Unity models of dogs. They fit meshes to these 3D models and build a PCA shape space for the dog breeds, which is new and specific to their method. These 3D models are created via 3D scanning and manual modeling (human generated), and the authors integrate them into their model space to better represent dog shapes with breed-specific effects. Thus, the 3D shape models constitute a new dataset modality introduced and used by the authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-158-rubric-1",
    "token_usage": {
      "prompt_tokens": 10725,
      "completion_tokens": 582,
      "total_tokens": 11307
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4, 4.1 Evaluation Methods",
            "reasoning": "The paper states that the Stanford Extra (StanExt) dataset is used for training and evaluation, which is an existing dataset with breed labels, keypoints, and silhouettes, extended with new keypoints via training another network. Specifically, perceptual evaluations are conducted using Amazon Mechanical Turk with qualified workers assessing relative correctness of 3D shapes, indicating multiple human non-expert annotators for perceptual labels. However, the core annotation of breed labels, keypoints, and silhouettes in the new dataset are based on existing datasets and do not specify new manual annotation details by humans in this paper. Moreover, the paper newly creates a small dataset of 3D dogs for evaluation purposes, but it does not specify human annotation for these 3D models; these appear to be collected or adapted 3D models, not manually annotated data. Therefore, considering the new dataset introduced is the creation of 3D dog models for evaluation, which are collected/purchased 3D Unity models and scans, not manually annotated images. Overall, the new data pertains mainly to 3D models and the extension of keypoints via a trained model, and the perceptual study uses multiple human non-experts. As the perceptual evaluation involves multiple human non-expert annotators, but no explicit annotation guidelines for image annotation are described for the new dataset, the annotation in the new dataset corresponds best to Multiple Human Non-Experts for the perceptual study and Automatic Process or AI Model for keypoint extensions. The request asks only for annotation of a new dataset introduced in the paper. Since the main new dataset is the 3D dog models used for evaluation, which are obtained from Unity models, and the perceptual study is done with multiple non-expert humans on results, the annotators for the perceptual labels are multiple human non-experts. The paper does not specify human expert annotation. Therefore, the best choice for new dataset annotation is Multiple Human Non-Experts for the perceptual study dataset of 3D shape evaluation, which is newly created data. "
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly specified",
            "reasoning": "The paper does not provide any explicit mention of annotation instructions given to the annotators, especially for the newly created evaluation dataset or the perceptual studies' annotators. Therefore, instructions are not confirmed to be present."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly specified",
            "reasoning": "No detailed or scoring rubrics for annotation tasks are mentioned or described in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly specified",
            "reasoning": "The paper does not mention examples provided as part of annotation guidelines for the new dataset or perceptual evaluation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-158-rubric-2",
    "token_usage": {
      "prompt_tokens": 11915,
      "completion_tokens": 320,
      "total_tokens": 12235
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the datasets. The annotation process or verification by such an expert is not described."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of quality assurance involving multiple human experts verifying or annotating the data in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA process conducted by a single non-expert human annotator for dataset annotation or verification."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance conducted by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models are used for pose and shape regression, there is no indication that an AI model was used specifically for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any automated quality assurance or verification process of dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses the Stanford Extra Dog dataset (StanExt) and extends it with additional keypoints, but there is no documented quality assurance process for these new annotations or the dataset in general. The paper does not describe any QA procedure applied to dataset annotations or content; thus, no quality assurance is present or reported."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-158-rubric-3",
    "token_usage": {
      "prompt_tokens": 11525,
      "completion_tokens": 538,
      "total_tokens": 12063
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report the creation of any new dataset from scratch by human contributors; it relies on existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generation of any original data solely via AI or models without existing data reference."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of human translation of data from another language is present."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of machine translation of data from another language is present."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), specifically 4.1 and 4.2; also Introduction and Related Work discussing data sources.",
          "reasoning": "The authors assemble training data primarily from the Stanford Extra dog dataset (StanExt), RGBD-Dog dataset, and existing 3D model scans and Unity canine models. These are existing datasets that the authors combine and utilize without indication of significant modification to produce their training data. While the StanExt dataset is extended with additional keypoints, this represents minor annotation extension rather than collection of new primary data. Thus, the datasets used are collated from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Sections 3.1 (Dog Model) and 3.5 (Novel Breed Losses)",
          "reasoning": "The authors build a new parametric dog shape model by fitting 3D meshes (from scanned toy figurines and purchased Unity 3D models) to the SMAL model topology and recomputing the mean shape and PCA shape space, reweighting training data to emphasize dogs. They extend SMAL by adding limb scaling parameters and modifying the shape space with additional dog shapes. Also, breed prototype shapes are used in breed-specific 3D losses. These modifications constitute derivations based on existing 3D scans and models, transforming and adapting them into a new model and shape space for dogs."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of data used is clearly documented and referenced."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-158-rubric-4",
    "token_usage": {
      "prompt_tokens": 12043,
      "completion_tokens": 375,
      "total_tokens": 12418
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.3 'Training Procedure' and Section 4 'Experiments'",
          "reasoning": "The Stanford Extra Dog dataset (StanExt), extended by the authors with additional keypoints, is used to train the BARC network from scratch using 2D supervision (keypoints and silhouettes), as described in Section 3.3 and evaluated in Section 4. The model is trained end-to-end from scratch with no mention of pre-trained weights."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 'Evaluation Methods' and Section 4.2 'Comparison to Baselines'",
          "reasoning": "The authors evaluate their model using the Stanford Extra Dog dataset test split, conducting 2D reprojection error measurements (IoU, PCK), a perceptual shape evaluation study via Amazon Mechanical Turk, and novel breed prototype consistency metrics for benchmarking and performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3 'Ablation Study' and Figure 3",
          "reasoning": "The authors analyze the learned breed-aware latent shape space, including t-SNE visualization showing clustering of breeds consistent with breed clades and evaluate the impact of breed losses on shape accuracy and perceptual correctness. This reveals trends and patterns in breed-shape similarities."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-158-rubric-5",
    "token_usage": {
      "prompt_tokens": 12766,
      "completion_tokens": 531,
      "total_tokens": 13297
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the proposed dataset containing entries in multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The proposed dataset is not described as containing exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, including Abstract and Section 4. Experiments",
          "reasoning": "The proposed dataset, e.g., the Stanford Extra Dog dataset (StanExt), contains data annotated and labelled in English only. The paper uses English labels for dog breeds and annotations. No other languages are mentioned in association with the dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the dataset uses a non-English language exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses models, network architectures, and code available for their method, the dataset itself is composed of images and annotations, not programming or code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries themselves are not described as containing mathematical or logical notation; such notation appears only in the model and loss descriptions, not in the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset consists of dog images and breed labels, there are no biological sequences or non-human communication signals included as dataset entries."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or constructed languages are mentioned in relation to the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset annotations and labels is explicitly documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language in the form of breed labels and annotations in English; therefore, language is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-158-rubric-6",
    "token_usage": {
      "prompt_tokens": 9984,
      "completion_tokens": 150,
      "total_tokens": 10134
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Introduction",
          "reasoning": "The paper mentions in the Abstract and Introduction that their code is publicly available for research purposes at https://barc.is.tue.mpg.de/. This indicates that the code used for data processing and model training is accessible."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "N/A",
          "reasoning": "The paper describes use of existing datasets such as Stanford Extra Dog dataset (StanExt) extended by the authors with additional keypoints, but does not document creation of a new dataset by the authors themselves or provide detailed dataset construction processes. It focuses more on method development and training with available datasets and shape model reconstruction rather than dataset construction."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1592-rubric-0",
    "token_usage": {
      "prompt_tokens": 13945,
      "completion_tokens": 105,
      "total_tokens": 14050
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 (Datasets)",
          "Reasoning": "The paper evaluates the proposed method on two existing benchmark stereo image datasets: Cityscapes and InStereo2K. Both datasets are publicly known datasets of stereo image pairs captured by human-operated cameras in real environments. However, the paper does not introduce any new dataset; instead, it uses these pre-existing datasets for evaluation."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1592-rubric-1",
    "token_usage": {
      "prompt_tokens": 14745,
      "completion_tokens": 267,
      "total_tokens": 15012
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Dataset Description and Section 4.2 Implementation Details",
            "reasoning": "The paper describes the use of large-scale datasets Cityscapes and InStereo2K for evaluation and training respectively. The annotations are not performed manually but data are images collected and used as-is from the datasets, with no mention of human annotation or labeling for the compression task. Compression residuals and shifts are computed algorithmically during encoding and decoding, hence the 'annotation' here is performed automatically by the compression method and underlying algorithms."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not describe any manual annotation instructions because the data are stereo image pairs used for compression experiments, not subject to annotation by human annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No scoring rubrics are discussed because no human annotations are performed for ground truth or label assessment; the evaluation metrics are algorithmic (e.g., PSNR) rather than annotations scored by humans."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No examples of annotation guidelines or annotated data are described as the datasets are image collections used directly for compression evaluation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1592-rubric-2",
    "token_usage": {
      "prompt_tokens": 15935,
      "completion_tokens": 387,
      "total_tokens": 16322
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert on any new dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any involvement of multiple human experts performing quality assurance on any new dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of dataset QA by a single non-expert human annotator is provided in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of QA performed by multiple non-expert human annotators on any dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using any AI model as a judge or for quality assurance of datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated or algorithmic quality assurance process applied to datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses two existing datasets (Cityscapes and InStereo2K) for evaluation, without introducing any new datasets. The InStereo2K dataset is cited from previous work [6], and Cityscapes is a well-known dataset. No new datasets are introduced by the authors. Therefore, no QA process for new datasets is applicable or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1592-rubric-3",
    "token_usage": {
      "prompt_tokens": 15545,
      "completion_tokens": 389,
      "total_tokens": 15934
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the creation of any new dataset by human contributors entirely from scratch. Instead, it uses existing datasets for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses a learned compression model but does not generate or introduce any new dataset created entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication in the paper of datasets produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication in the paper of datasets produced by machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper evaluates the method on two existing public benchmark stereo image datasets, Cityscapes and InStereo2K, which are collected datasets from real scenes. The paper explicitly mentions these datasets as being used for evaluation and does not document creating new original data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset derived by modifying or transforming existing datasets; only existing datasets are used without indication of such adaptation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the datasets used is specified and documented; hence, N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1592-rubric-4",
    "token_usage": {
      "prompt_tokens": 16063,
      "completion_tokens": 320,
      "total_tokens": 16383
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2 Implementation Details",
          "reasoning": "The paper states that the models are trained from scratch on both Cityscapes and InStereo2K datasets using the training splits described in Section 4.1 with 300 and 400 epochs respectively. This training from scratch is used to optimize the compressed image representation and rate-distortion trade-off."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 4.4",
          "reasoning": "The two datasets, Cityscapes and InStereo2K, are used as benchmark datasets for evaluation and benchmarking the proposed stereo image compression method's performance. Section 4.4 presents rate-distortion curves and quality comparisons on the test sets of these datasets."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.5 Ablation Study",
          "reasoning": "The datasets are also used for analysis in the ablation studies to compare contributions of model components and their effect on performance metrics such as BD-rate and BD-PSNR."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1592-rubric-5",
    "token_usage": {
      "prompt_tokens": 16786,
      "completion_tokens": 532,
      "total_tokens": 17318
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing English linguistic content nor is English used as dataset content. The paper discusses image datasets, not linguistic or text data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset with linguistic content in a non-English language is introduced. The datasets are image datasets without language content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are stereo image datasets; although the paper includes descriptions of algorithms and code (e.g., GitHub repository), these are not datasets containing code or programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notation appears in the paper as part of the method description; however, the datasets themselves do not contain entries with mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described are stereo image pairs of urban scenes and indoor scenes, with no mention of biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or use of constructed or artificial languages in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are well specified (Cityscapes and InStereo2K) and their content is thoroughly described as images; there is no ambiguity regarding language content because the datasets do not contain language."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The proposed new dataset used for evaluation consists of stereo image pairs (Cityscapes and InStereo2K datasets), which are image data without any linguistic content. Therefore, the entries do not contain any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1592-rubric-6",
    "token_usage": {
      "prompt_tokens": 14004,
      "completion_tokens": 175,
      "total_tokens": 14179
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 5 Conclusion and end of paper",
          "reasoning": "The paper states that the model code is publicly available, but there is no indication that the code for constructing or preprocessing the datasets used (Cityscapes and InStereo2K) is provided or made publicly available by the authors. Cityscapes and InStereo2K are existing benchmark datasets, not new datasets introduced by the authors, and the authors use these existing datasets without providing code for their construction."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "",
          "reasoning": "The paper uses two existing standard benchmark datasets (Cityscapes and InStereo2K) for evaluation; there is no presentation or introduction of new datasets by the authors. Therefore, there is no documentation of dataset creation provided for any new dataset in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1621-rubric-0",
    "token_usage": {
      "prompt_tokens": 10893,
      "completion_tokens": 200,
      "total_tokens": 11093
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.3 Dataset generation; Section 4 Implementation notes; Section 5 Experiments",
          "Reasoning": "The paper introduces a new training and testing dataset generated from images in the Megadepth dataset, which consists of real photographic images taken from scenes with 3D reconstructions. These images are human-captured photographic data. They generate synthetic pairs on the fly by applying planar affine transformations for training, but also use pairs of real images with scale factors estimated from covisible 3D points to provide more realistic challenging training samples. The dataset is explicitly created by the authors for training and evaluating ScaleNet and hence is a new dataset introduced by them. The data modality is image since the dataset consists of image pairs, and the origins are labeled as human generated because the source images are captured photographs, and model generated for synthetic pairs created via affine transformations."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1621-rubric-1",
    "token_usage": {
      "prompt_tokens": 11693,
      "completion_tokens": 240,
      "total_tokens": 11933
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3 Dataset generation",
            "reasoning": "The paper describes generating the training data for ScaleNet using a method based on 3D reconstructions that automatically estimate scale factors from 3D point correspondences projected onto images. This process uses registered 3D points and image projections to compute scale ratios without human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.3 Dataset generation",
            "reasoning": "There is no evidence in the paper of any provided instructions for human annotators because the dataset is generated automatically from 3D models and image correspondences."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.3 Dataset generation",
            "reasoning": "The paper does not mention any scoring rubrics or evaluation criteria designed for annotators; the annotations are computed via the described automatic procedure."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.3 Dataset generation",
            "reasoning": "No examples of annotation guidelines or sample annotations provided for human annotators are mentioned, since the dataset annotation is generated automatically without human intervention."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1621-rubric-2",
    "token_usage": {
      "prompt_tokens": 12883,
      "completion_tokens": 312,
      "total_tokens": 13195
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the dataset annotation or validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance for the dataset or its annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that multiple non-expert humans conducted quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report use of AI models for dataset annotation or quality assurance."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3 Dataset generation",
          "reasoning": "The scale annotations for real image pairs are generated automatically by computing the average ratio of distances between corresponding keypoints extracted from 3D reconstructions and their projections, using fully automated processing of 3D point clouds and image correspondences. This procedure is algorithmic and does not involve manual verification or annotation, thus constituting an automatic quality assurance process through automated verification and generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an automated process for dataset annotation and quality assurance, thus N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1621-rubric-3",
    "token_usage": {
      "prompt_tokens": 12493,
      "completion_tokens": 546,
      "total_tokens": 13039
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.3 Dataset generation",
          "reasoning": "The authors generated a new training dataset by computing scale factors between pairs of real images from 3D reconstruction datasets, specifically using the Megadepth dataset. They used registered 3D points and their 2D projections to estimate scale factors between image pairs and created a custom dataset consisting of 250,000 training and 25,000 validation image pairs with well-represented scale factors. This dataset was created entirely from scratch by the authors using human-driven data processing and labeling procedures."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated entirely by AI or machine learning models without reference to existing data. The dataset is built from real images and synthetic transformations, but not generated from scratch by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of human translation of data from other languages is present in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of machine translation being used to generate dataset content is present in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.3 Dataset generation",
          "reasoning": "The dataset leverages existing public 3D reconstruction datasets such as Megadepth by selecting images and covisible 3D points, then computing scale factors. This involves collecting and aggregating data from existing sources without significant modification of the original images themselves, but using derived labels."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.3 Dataset generation",
          "reasoning": "The authors derived new ground-truth scale annotations by computing ratios of distances between matched 3D points projected into images from existing 3D reconstructions. Additionally, synthetic pairs are generated by applying planar affine transformations to real images from existing datasets to augment training data. Therefore, the dataset is based on existing sources with modifications and transformations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the origin and method of generation of the datasets used, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1621-rubric-4",
    "token_usage": {
      "prompt_tokens": 13011,
      "completion_tokens": 468,
      "total_tokens": 13479
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.3 Dataset generation",
          "reasoning": "The authors generate a custom training dataset for ScaleNet using both synthetic image pairs and real image pairs from 3D reconstruction datasets. The training dataset contains 250,000 training pairs with ground-truth scale factors derived from 3D points and affine transformations, and the network is trained from scratch on this data as described in Sections 3.2 and 3.3."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the dataset for fine-tuning pretrained models; training is described as from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning or RL-based post-training techniques are mentioned with respect to the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 Experiments",
          "reasoning": "The dataset generated from Megadepth is used not only for training but also for validation and testing of ScaleNet and its integration into tasks such as camera pose estimation and geometric matching, as evident from sections 5.1 to 5.5."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's use of the dataset focuses on training and evaluation rather than purely analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for retrieval-augmented generation or similar purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is clearly used for both training from scratch and evaluation as described in multiple sections."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1621-rubric-5",
    "token_usage": {
      "prompt_tokens": 13734,
      "completion_tokens": 509,
      "total_tokens": 14243
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset described in the paper consists of image pairs with scale annotations, but does not contain programming or structured code-related content as dataset entries. The code is mentioned as a tool/resource but is not part of the dataset itself."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3.3 Dataset generation",
          "reasoning": "The dataset generation process involves computation of scale factors derived from mathematical formulas using 2D distances between points and logarithmic transformations. These mathematical expressions (formulas 4, 5, etc.) are integral to how the dataset's ground-truth scale distributions are constructed, indicating the presence of mathematical notation associated with the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any constructed or fictional languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The linguistic content and annotations of the dataset are fully specified as scale factors computed from image keypoints and 3D correspondences, thus language is known (mathematical/logical notation)."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The dataset primarily consists of images and corresponding scale factor annotations derived from geometric relationships; the dataset entries do not contain human language text or spoken language content."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1621-rubric-6",
    "token_usage": {
      "prompt_tokens": 10952,
      "completion_tokens": 196,
      "total_tokens": 11148
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 6 (Reproducibility)",
          "reasoning": "The paper explicitly states that the code, evaluation protocols, and trained models are publicly available at https://github.com/axelBarroso/ScaleNet, and also mentions publishing training scripts and custom training dataset, indicating that the code related to dataset generation and evaluation is accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.3 (Dataset generation) and Section 6 (Reproducibility)",
          "reasoning": "The paper provides a detailed description of the dataset generation process in Section 3.3, explaining how the scale factors are computed using 3D points and image correspondences, and mentions using the Megadepth dataset for generating training and testing images. Section 6 reiterates that the custom training dataset and evaluation protocols are made public, indicating clear documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1637-rubric-0",
    "token_usage": {
      "prompt_tokens": 15643,
      "completion_tokens": 111,
      "total_tokens": 15754
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 6.1 APRICOT-Mask Dataset",
          "Reasoning": "The APRICOT-Mask dataset is a new dataset introduced by the authors. It consists of images from the real world containing physical adversarial patches. The pixel-level annotations (segmentation masks) are human-generated annotations of these adversarial patches derived from the APRICOT dataset's images, which were captured in a physical environment using cameras."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1637-rubric-1",
    "token_usage": {
      "prompt_tokens": 16443,
      "completion_tokens": 199,
      "total_tokens": 16642
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 6.1",
            "reasoning": "The APRICOT-Mask dataset annotations were done by three annotators using Labelbox and manually reviewed to ensure quality, indicating multiple human experts performed the annotations."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 6.1",
            "reasoning": "Annotations used Labelbox and manual review to ensure quality, which implies that detailed annotation instructions were provided to annotators during the creation of APRICOT-Mask."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 6.1",
            "reasoning": "The paper does not mention the use of scoring rubrics for annotation; only quality review is described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 6.1",
            "reasoning": "No mention of providing annotation examples or exemplars in the guidelines; the focus is on segmentation masks and quality control."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1637-rubric-2",
    "token_usage": {
      "prompt_tokens": 17633,
      "completion_tokens": 455,
      "total_tokens": 18088
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide information indicating that a single subject matter expert or member of the target demographic performed quality assurance on the APRICOT-Mask dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that multiple experts or members of the target demographic conducted the quality assurance for APRICOT-Mask annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert annotator was solely responsible for the quality assurance of the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 6.1 APRICOT-Mask Dataset",
          "reasoning": "The paper states that the segmentation masks in APRICOT-Mask were annotated by three annotators using Labelbox and manually reviewed to ensure annotation quality. However, it does not specify that these annotators have subject matter expertise, implying they are non-experts. Multiple annotators performed the annotations and reviews, indicating multiple human non-experts conducted quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information suggesting that an AI model was used as a judge for quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While bounding boxes are generated automatically from segmentation masks, this is a standard geometric computation and not described as a quality assurance process. No algorithmic or rule-based quality verification is detailed."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly states a quality assurance process involving multiple annotators and manual review, so it is not the case that no QA was performed or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1637-rubric-3",
    "token_usage": {
      "prompt_tokens": 17243,
      "completion_tokens": 448,
      "total_tokens": 17691
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 6.1 APRICOT-Mask Dataset",
          "reasoning": "The APRICOT-Mask dataset was created by annotators who provided pixel-level segmentation masks and more accurate bounding boxes for physical adversarial patches in the existing APRICOT dataset. The paper states three annotators manually annotated the masks using Labelbox and reviewed them to ensure quality. This indicates that the dataset was created from scratch by human contributors through manual annotation, not translated or derived automatically."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset involved human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of machine translation is made in relation to the datasets."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The APRICOT-Mask dataset is not simply collated from existing sources; it involved manual segmentation annotation, thus significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 6.1 APRICOT-Mask Dataset",
          "reasoning": "The APRICOT-Mask dataset is based on the existing APRICOT dataset which provides bounding box annotations only. The authors enhanced the APRICOT dataset by adding pixel-level segmentation masks and corrected bounding boxes, thus modifying and extending existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin and process of dataset creation (manual annotation of APRICOT dataset)."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1637-rubric-4",
    "token_usage": {
      "prompt_tokens": 17761,
      "completion_tokens": 377,
      "total_tokens": 18138
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are not used for training models from randomly initialized parameters, but rather for specific purposes such as fine-tuning or evaluation."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 6.2",
          "reasoning": "The APRICOT-Mask dataset is used to train the patch segmenter, which is a supervised learning process using pixel-level segmentation masks."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used for reinforcement learning or RL-based post-training methods."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 6.2",
          "reasoning": "The APRICOT-Mask dataset is used for evaluating SAC's effectiveness on physical adversarial patch attacks by providing accurate segmentation masks for benchmark evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not primarily used for analysis of trends or characteristics but for training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base augmentation for models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1637-rubric-5",
    "token_usage": {
      "prompt_tokens": 18484,
      "completion_tokens": 581,
      "total_tokens": 19065
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset introduced, APRICOT-Mask, is a segmentation mask dataset for adversarial patches; there is no indication that it contains entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the APRICOT-Mask dataset contains two human languages. No information about bilingual content is provided."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 6.1 APRICOT-Mask Dataset",
          "reasoning": "The APRICOT-Mask dataset provides pixel-level annotations for physical adversarial patches and bounding boxes, but the paper only references labeling and annotation tools (Labelbox) and dataset descriptions in English. There is no indication of multiple languages or non-English languages, so the dataset is considered monolingual (English) based on the paper content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The APRICOT-Mask dataset contains annotations in English; no information suggests it contains data in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of image segmentation masks and annotation data, not code or programming languages."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are pixel-level masks and bounding boxes; no mention of symbolic mathematical or logical notation is present in relation to the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is related to adversarial patches on images, no biological or non-human communication signals are involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset involves fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper explicitly states the annotations are done using Labelbox and manual review; language used is clearly English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language in the form of labeling and descriptions in English; therefore, it is not language free."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1637-rubric-6",
    "token_usage": {
      "prompt_tokens": 15702,
      "completion_tokens": 169,
      "total_tokens": 15871
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 6.1",
          "reasoning": "The paper mentions the creation of the APRICOT-Mask dataset with pixel-level annotations, but it does not provide a link or mention public availability of the code used for dataset creation or annotation. The only code availability statement refers to the defense method's implementation (https://github.com/joellliu/SegmentAndComplete), but not specifically to dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 6.1",
          "reasoning": "The paper documents the creation process of the APRICOT-Mask dataset in Section 6.1, describing that segmentation masks and bounding boxes were annotated manually by three annotators using Labelbox and reviewed for quality. It provides sufficient transparency about dataset annotation methodology."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1652-rubric-0",
    "token_usage": {
      "prompt_tokens": 10949,
      "completion_tokens": 136,
      "total_tokens": 11085
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1.1 Datasets: Jellyfish and Vegetation datasets",
          "Reasoning": "The Jellyfish dataset is described as an in-house video dataset containing 30k frames of recorded swimming jellyfish, and the Vegetation dataset is an in-house video dataset acquired using a drone to record swaying trees with recorded wind speeds. Both datasets are newly introduced by the authors for testing their model on new organisms and downstream physics-based modeling, indicating they are videos captured by humans (or human-operated equipment) and thus of human origin."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1652-rubric-1",
    "token_usage": {
      "prompt_tokens": 11749,
      "completion_tokens": 232,
      "total_tokens": 11981
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1.1 Datasets and Section 3 Method",
            "reasoning": "The paper describes that the keypoints are discovered using the B-KinD self-supervised model trained on unlabelled behavioral videos with no manual supervision required for the discovery process. The annotations in the new datasets (e.g., Jellyfish and Vegetation) are not manually labeled keypoints but are obtained by the model automatically through self-supervised learning."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described in the paper",
            "reasoning": "Since there are no human annotators performing manual annotation in the new datasets, annotation instructions for humans are not applicable."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described in the paper",
            "reasoning": "Without human annotators, there is no scoring or rubrics provided for annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described in the paper",
            "reasoning": "No annotation examples for human annotators are discussed, as annotation is performed automatically by the model."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1652-rubric-2",
    "token_usage": {
      "prompt_tokens": 12939,
      "completion_tokens": 338,
      "total_tokens": 13277
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any data quality assurance performed by a single human expert annotator for the new datasets introduced by the authors (Jellyfish and Vegetation)."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance on the new datasets (Jellyfish and Vegetation) introduced by the authors."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance conducted by a single non-expert annotator for the new datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that multiple non-expert annotators conducted quality assurance on the Jellyfish or Vegetation datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process performed by an AI model as a judge for the newly introduced datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic/rule-based quality assurance process applied to the dataset annotations or content for the new datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced by the authors (Jellyfish and Vegetation) are used for qualitative and physics-based model evaluation, respectively, but the paper does not describe any quality assurance procedure or annotation validation process for these datasets. Therefore, we conclude that no quality assurance process is applied or documented for these datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1652-rubric-3",
    "token_usage": {
      "prompt_tokens": 12549,
      "completion_tokens": 495,
      "total_tokens": 13044
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1.1 Datasets",
          "reasoning": "The authors introduce new in-house video datasets of jellyfish and vegetation (trees) captured for the purpose of this work. The jellyfish dataset contains 30k frames of recorded swimming jellyfish and the vegetation dataset contains 2.41M frames of a recorded oak tree with corresponding wind speed measurements. These datasets were created entirely from scratch by human contributors for the behavioral analysis tasks presented."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new dataset generated purely by AI or machine learning models. The keypoints discovered are representations produced by the model from the analyzed videos, but the underlying video data itself is not generated by a model."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1.1 Datasets",
          "reasoning": "The authors utilize several existing datasets such as CalMS21, MARS-Pose, Fly vs. Fly, and Human 3.6M for their experiments. These datasets have been collected by other groups previously and are used here without indication of significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses deriving keypoints and features from video data, the raw datasets themselves are either newly collected (jellyfish, vegetation) or existing datasets used as-is; there is no indication that the authors create derived datasets by modifying existing ones."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and acquisition method of the datasets used are clearly documented, including usage of existing datasets and newly collected datasets described in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1652-rubric-4",
    "token_usage": {
      "prompt_tokens": 13067,
      "completion_tokens": 470,
      "total_tokens": 13537
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4.1.2",
          "reasoning": "The datasets (e.g., CalMS21 training split without labels) are used to train the Behavioral Keypoint Discovery (B-KinD) model in a self-supervised manner, reconstructing spatiotemporal differences between video frames to discover keypoints without manual annotation."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1.2",
          "reasoning": "The B-KinD model is trained from randomly initialized parameters on the unlabelled training splits of the introduced datasets (e.g., CalMS21, Fly vs. Fly) to learn keypoint representations directly without pre-training."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 4.2 and 4.3",
          "reasoning": "The discovered keypoints are used as inputs to downstream supervised classifiers (behavior classification) and pose regression models, fine-tuning supervised models using the keypoints extracted from the datasets for improved performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe or mention the use of reinforcement learning or RL-based methods on the introduced datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 4.2, 4.3, and 4.4",
          "reasoning": "Introduced datasets like CalMS21 and Fly vs. Fly are used for evaluation and benchmarking of the B-KinD method's performance on behavior classification and keypoint regression tasks."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.4",
          "reasoning": "The in-house jellyfish and vegetation datasets are analyzed using discovered keypoints to study pulsing frequency and wind speed respectively, demonstrating the use of datasets for analyzing behavioral patterns and environmental phenomena."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as knowledge bases to augment models via retrieval or similar techniques."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes multiple practical uses of the datasets for training, evaluation, and analysis, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1652-rubric-5",
    "token_usage": {
      "prompt_tokens": 13790,
      "completion_tokens": 616,
      "total_tokens": 14406
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any datasets that include more than two human languages within the new datasets introduced. The datasets are video recordings of animals and humans with annotations related to behavior or pose, not linguistic content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets presented (e.g., CalMS21, MARS-Pose, Fly vs. Fly, Human 3.6M, Jellyfish, Vegetation) are behavioral video datasets without linguistic content, bilingual or otherwise."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper and dataset annotations are presumably in English as the primary language of scientific communication, the datasets themselves consist of video data of behavior and do not contain entries that are English language text per se."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset with exclusively non-English human language content is introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are behavioral video datasets and do not contain entries that are programming or structured code languages."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper includes mathematical formulas in the methodology description, the datasets themselves do not contain mathematical or logical expressions as dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": true,
          "reference": "Section 4.1.1 Datasets; Abstract; Introduction",
          "reasoning": "The new datasets introduced include videos of biological organisms such as mice, flies, jellyfish, humans, and trees. The data consists of behavioral videos capturing movements and signals from non-human organisms, which qualifies as biological and non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are part of any new dataset introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) involved in the datasets are clear since they are video data of organisms with no linguistic content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Although the datasets do not contain human language per se, they do contain communication signals and biological signals from organisms in video form, so they are not language-free datasets; thus N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1652-rubric-6",
    "token_usage": {
      "prompt_tokens": 11008,
      "completion_tokens": 198,
      "total_tokens": 11206
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and footnotes",
          "reasoning": "The paper's abstract and the header explicitly provide a link to a project website (https://sites.google.com/view/b-kind) which includes code and project resources related to the method and datasets used. This indicates that the authors have made the code publicly available for reproducibility of their dataset processing and model training."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1.1 Datasets and Section 4.1.2 Training and evaluation procedure",
          "reasoning": "The paper provides detailed descriptions of datasets used, including in-house datasets like Jellyfish and Vegetation with their characteristics (frame counts, resolution, and target organism size). It also describes clearly the training and evaluation procedures used for these datasets, including preprocessing (rescaling, training splits), and how the datasets are applied downstream. This constitutes documentation of dataset creation and usage process in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1685-rubric-0",
    "token_usage": {
      "prompt_tokens": 16159,
      "completion_tokens": 108,
      "total_tokens": 16267
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Experimental Setup (Datasets)",
          "Reasoning": "The paper uses the Breakfast and CrossTask datasets which consist of instructional task videos. These videos are human-recorded instructional cooking and procedural task videos, indicating human involvement in data generation. The paper does not introduce a new dataset; it utilizes existing datasets for experiments and mainly introduces a learning framework. No new dataset creation is described."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1685-rubric-1",
    "token_usage": {
      "prompt_tokens": 16959,
      "completion_tokens": 275,
      "total_tokens": 17234
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 5.1, Datasets",
            "reasoning": "The paper uses two new benchmark datasets, Breakfast and CrossTask, which are well-established datasets constructed with human annotations involving multiple annotators. These datasets provide transcripts (action sequences) and task labels as annotations, implying that multiple human experts or annotators created the annotations for weakly-labeled videos as referenced in the datasets description."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not provide detailed annotation guidelines, instructions, or protocols given to the annotators who produced the weak labels (transcripts and task labels) for the datasets. There is no mention of annotation instruction documentation for the new datasets introduced."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "There is no description or mention of scoring rubrics or formal scoring guidelines used for annotation in the new datasets. The paper focuses on the learning methods rather than annotation rubric details."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not include or reference example annotations or annotation examples provided as guidelines for annotators in the new datasets. The discussion is limited to usage of the datasets in the evaluation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1685-rubric-2",
    "token_usage": {
      "prompt_tokens": 18149,
      "completion_tokens": 320,
      "total_tokens": 18469
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving a single human expert for the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper about multiple human experts performing quality assurance on the datasets used."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by a single non-expert annotator for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert annotators conducting quality assurance on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used as a judge to quality assure dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper proposes algorithmic methods (such as the Soft Restricted Edit distance) and self-training strategies for learning from data, these are methods for training the action segmentation model and not described as quality assurance processes for data annotation or validation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses two existing benchmark datasets (Breakfast and CrossTask) without introducing new datasets. It also does not discuss any new dataset creation or perform quality assurance processes on data annotations. No quality assurance or annotation validation process is documented in the paper for any new datasets (as none are introduced)."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1685-rubric-3",
    "token_usage": {
      "prompt_tokens": 17759,
      "completion_tokens": 396,
      "total_tokens": 18155
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state the collection or creation of new data from human contributors. Instead, it uses existing benchmark datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not generate new datasets entirely from AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.1 Experimental Setup - Datasets",
          "reasoning": "The paper uses two benchmark datasets, Breakfast and CrossTask, which are publicly available and pre-existing. The authors do not create new datasets but use these datasets, splitting existing training sets into weakly-labeled and unlabeled subsets. Therefore, the data used is collected from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors split datasets into subsets for their experiments, this is a standard experimental procedure rather than creation of new datasets with modifications; no new data derived from existing datasets is indicated."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and source of datasets are specified in the paper as existing benchmark datasets, so origin is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1685-rubric-4",
    "token_usage": {
      "prompt_tokens": 18277,
      "completion_tokens": 447,
      "total_tokens": 18724
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using any new dataset for pre-training large models; the datasets used are existing benchmark datasets."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset that is used to train models from scratch. Instead, it uses existing datasets with weakly-labeled and unlabeled splits."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe fine-tuning pre-trained models on a newly proposed dataset; no new datasets introduced are used specifically for supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or description of reinforcement learning or RL-based methods applied on any new dataset introduced by the authors."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets mentioned in the paper (Breakfast and CrossTask) are existing benchmark datasets. No newly introduced datasets are described as being used exclusively for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper analyzes results and performance on existing datasets but does not introduce new datasets primarily for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not present any newly introduced dataset serving as a knowledge base for augmenting models."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; it uses existing datasets (Breakfast and CrossTask) for experimentation and evaluation, and thus no practical usage of new datasets is described."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1685-rubric-5",
    "token_usage": {
      "prompt_tokens": 19000,
      "completion_tokens": 401,
      "total_tokens": 19401
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the presence of more than two human languages in the datasets used or introduced."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets include exactly two human languages; no bilingual data is mentioned."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5.1 Experimental Setup - Datasets",
          "reasoning": "The datasets used, Breakfast and CrossTask, consist of instructional videos in English as typical for these datasets, and the paper discusses transcripts and actions in English. The paper does not mention any other languages in the dataset, implying the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English language datasets or entries are introduced or mentioned."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset entries with programming or code content are provided; the datasets contain video data with action transcripts, not code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the paper describes mathematical formulations for the loss and algorithm, the datasets themselves do not contain mathematical or logical notation as data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are instructional videos of human actions; no biological sequences or non-human communication data is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages appear in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of dataset entries is known and specified (English). No unknown languages or unspecified language entries are present."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain transcripts in English and thus contain language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1685-rubric-6",
    "token_usage": {
      "prompt_tokens": 16218,
      "completion_tokens": 149,
      "total_tokens": 16367
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not provide any links or references to code repositories related to dataset construction or preprocessing. The focus is on the methodology and utilization of existing benchmark datasets, with no indication that code for new dataset preparation or construction is made publicly available."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper uses existing benchmark datasets (Breakfast and CrossTask) and does not introduce new datasets created by the authors. Therefore, there is no documentation regarding dataset creation or construction processes. The paper documents experimental setups and methodology but does not provide documentation for dataset creation."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1726-rubric-0",
    "token_usage": {
      "prompt_tokens": 16048,
      "completion_tokens": 116,
      "total_tokens": 16164
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Annotated Image Collection Dataset",
          "Reasoning": "The paper introduces the AIC dataset, an annotated image collection dataset derived from 3402 human-generated natural images selected from DUTS and ImageNet datasets, which are composed of real-world images with manual category and theme labeling as well as saliency masks. Thus, the data modality is 'image' and the source originates from human-generated content collected and labeled by humans."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1726-rubric-1",
    "token_usage": {
      "prompt_tokens": 16848,
      "completion_tokens": 256,
      "total_tokens": 17104
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1 Annotated Image Collection Dataset",
            "reasoning": "The paper describes AIC as an annotated image collection dataset, collected and manually divided into themes and categories for evaluation purposes, implying expert human involvement in annotation and dataset curation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.1 Annotated Image Collection Dataset",
            "reasoning": "The dataset was constructed with specific requirements for sampling (e.g., each image collection includes images from at least two categories, uniform category distribution) indicating that instructions or guidelines were used to ensure data quality and consistency."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4 Experiments Metrics and Section 4.1 Annotated Image Collection Dataset",
            "reasoning": "The paper defines specific quantitative metrics for evaluating collages (e.g., correlation preservation M_n, saliency loss M_s) based on the annotated labels in the dataset, indicating rubric-like quantitative criteria provided along with annotations for evaluation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire Paper",
            "reasoning": "The paper does not explicitly mention providing annotation examples or detailed exemplars for the annotation process of AIC dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1726-rubric-2",
    "token_usage": {
      "prompt_tokens": 18038,
      "completion_tokens": 404,
      "total_tokens": 18442
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that a single human expert performed quality assurance on the AIC dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of multiple human experts performing quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance being performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description indicating that multiple non-expert human annotators conducted QA on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of an AI model as a judge or for quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the dataset is derived from the DUTS saliency detection dataset, the paper does not explicitly report using automated verification or algorithmic QA for the annotations in the newly developed AIC dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document any quality assurance process for validating the annotations or content of the proposed AIC dataset beyond describing its construction from existing datasets with known annotations. There is no explicit mention of any QA procedure applied to the dataset, nor any description of annotation verification or validation by humans, AI models, or automated methods."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1726-rubric-3",
    "token_usage": {
      "prompt_tokens": 17648,
      "completion_tokens": 516,
      "total_tokens": 18164
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Annotated Image Collection Dataset",
          "reasoning": "The paper explicitly states that the authors collected and annotated a new large-scale dataset named AIC for image collage evaluation. The dataset includes images selected from DUTS, organized manually into 72 categories divided into 11 themes, with annotation of category, theme, and saliency masks. The dataset is described as 'large-scale public-available annotated dataset' created by the authors to facilitate image collage research. This indicates original dataset creation by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention translation of data from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention machine translation of data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Annotated Image Collection Dataset",
          "reasoning": "The AIC dataset is constructed by selecting and aggregating images from the existing DUTS dataset, which in turn is partially collected from ImageNet. The authors organize, sample, and annotate these images into themes and categories suitable for their collage evaluation task. Thus, the dataset is collated from existing image sources with labeling and organization by the authors."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Annotated Image Collection Dataset",
          "reasoning": "Although the dataset is collated from existing sources, the authors created derived annotations such as category, theme labels and saliency masks, and developed a sampling framework to create image collections with specific properties to enable new evaluation metrics. These modifications constitute derivations and adaptations of existing data for the specific collage evaluation purpose."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides explicit description of data sources and annotation process, so the data origin is specified."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1726-rubric-4",
    "token_usage": {
      "prompt_tokens": 18166,
      "completion_tokens": 220,
      "total_tokens": 18386
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 'Annotated Image Collection Dataset' and Experiments section",
          "reasoning": "The paper introduces the AIC dataset as a large-scale publicly available annotated dataset specifically for image collage evaluation. Multiple quantitative metrics for collage results evaluation are defined using this dataset. The experiments and comparisons to baseline methods utilize the AIC dataset exclusively for benchmarking and performance measurement of image collage methods."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1726-rubric-5",
    "token_usage": {
      "prompt_tokens": 18889,
      "completion_tokens": 537,
      "total_tokens": 19426
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced, AIC, consists of image collections with labels such as category, theme, and saliency masks, not involving multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of dataset entries involving exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Annotated Image Collection Dataset",
          "reasoning": "The dataset AIC is described with labels such as category names, theme names, and other annotations presented in English as per the paper's textual content. All textual data supporting annotations and descriptions are in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset and paper do not indicate use of a non-English single language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are collections of images and annotations; there is no mention of structured code or programming language content within the dataset itself."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper presents mathematical expressions and notation for defining the model and methods, the dataset itself does not contain entries with mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains images annotated by human-labeled categories and themes; no biological or non-human communication system data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of annotations and labels is clearly described as English; thus the language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language in the form of English labels and category names, so it does contain language entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1726-rubric-6",
    "token_usage": {
      "prompt_tokens": 16107,
      "completion_tokens": 200,
      "total_tokens": 16307
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 4.1",
          "reasoning": "The paper states in the abstract that data and codes are available at https://github.com/ChineseYjh/SoftCollage. This repository is indicated to contain data and code relevant to the work, which includes the newly introduced dataset AIC. There is no explicit mention in the paper that only model code is available, so it can be reasonably concluded that code related to data collection and generation is also available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1",
          "reasoning": "Section 4.1 titled 'Annotated Image Collection Dataset' provides detailed documentation on how the new dataset AIC was constructed, including the source of images (DUTS), category and theme definitions, sampling methodology, train/test split, and dataset statistics such as image aspect ratios and category distributions. This detailed description constitutes comprehensive documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1727-rubric-0",
    "token_usage": {
      "prompt_tokens": 10109,
      "completion_tokens": 124,
      "total_tokens": 10233
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Experimental Settings, Datasets",
          "Reasoning": "The paper conducts experiments on the ScanNet v2 and S3DIS datasets which are standard benchmark datasets containing 3D scans captured by sensors, representing indoor scenes as point clouds. These datasets are explicitly mentioned as used in experiments, indicating human-captured sensor data as the input for the proposed method. Although these datasets are pre-existing, the paper evaluates the method on them without introducing a new dataset."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1727-rubric-1",
    "token_usage": {
      "prompt_tokens": 10909,
      "completion_tokens": 225,
      "total_tokens": 11134
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 4.1 Datasets; Section 4 Experiments",
            "reasoning": "The paper experiments on existing benchmark datasets ScanNet v2 and S3DIS, which are publicly available datasets with predefined human annotations. The paper does not introduce a new dataset; it only references these existing datasets for evaluation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not applicable as no new dataset is introduced",
            "reasoning": "Since the paper does not introduce a new dataset, it does not provide annotation instructions related to data annotation for a new dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not applicable as no new dataset is introduced",
            "reasoning": "The paper does not describe any scoring rubrics for annotation related to a newly introduced dataset since no new dataset is introduced."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not applicable as no new dataset is introduced",
            "reasoning": "The paper does not provide annotation examples associated with a newly introduced dataset because there is no new dataset introduced."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1727-rubric-2",
    "token_usage": {
      "prompt_tokens": 12099,
      "completion_tokens": 298,
      "total_tokens": 12397
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on the dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about quality assurance by multiple human experts on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that quality assurance was performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA performed by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any use of AI models as judges or for QA on dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated verification or algorithmic QA process applied to validate the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper leverages existing standard benchmark datasets (ScanNet v2 and S3DIS) for experiments and does not introduce any new datasets. Additionally, there is no documentation or description of any quality assurance process applied to dataset annotations. Therefore, no quality assurance process is applied or described for new dataset annotations in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1727-rubric-3",
    "token_usage": {
      "prompt_tokens": 11709,
      "completion_tokens": 273,
      "total_tokens": 11982
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper does not introduce any new datasets but uses existing benchmark datasets such as ScanNet v2 and S3DIS, which are publicly available datasets captured and annotated previously by other human contributors. Hence, the data is collected from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly states the datasets used, so the data source is specified."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1727-rubric-4",
    "token_usage": {
      "prompt_tokens": 12227,
      "completion_tokens": 188,
      "total_tokens": 12415
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses existing standard benchmark datasets ScanNet v2 and S3DIS for experiments and evaluation but does not introduce any new datasets. Therefore, no new dataset is introduced with documented usage in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1727-rubric-5",
    "token_usage": {
      "prompt_tokens": 12950,
      "completion_tokens": 495,
      "total_tokens": 13445
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any new dataset with English language textual entries; it focuses on 3D point cloud datasets like ScanNet v2 and S3DIS, which are collections of 3D geometric data rather than textual language data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides descriptions of methods and algorithms but does not introduce any new dataset containing programming or structured code-related content as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical expressions to describe losses and metrics, these are part of the methodology and not part of any dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or inclusion of biological sequences or non-human communication data in any proposed dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or inclusion of fictional or constructed languages within any proposed dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used or introduced in this paper are standard 3D point cloud datasets annotated for instance segmentation, with no unspecified or undocumented language contents."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets containing language data. The datasets are collections of 3D point cloud scans and annotations related to geometry and instance segmentation labels, which do not contain human language entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1727-rubric-6",
    "token_usage": {
      "prompt_tokens": 10168,
      "completion_tokens": 205,
      "total_tokens": 10373
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 4.1",
          "reasoning": "The paper provides a link to the source code and trained models of their method for ScanNet v2 and S3DIS datasets at https://github.com/thangvubk/SoftGroup.git. However, this code link is for their method implementation, not for code used to construct the datasets themselves. The datasets used (ScanNet v2 and S3DIS) are existing benchmark datasets, not new datasets introduced by the authors. No new datasets are introduced in this paper, so no dataset construction code is provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "N/A",
          "reasoning": "The paper does not introduce any new datasets nor provide documentation on dataset creation processes. It uses existing publicly available benchmark datasets (ScanNet v2 and S3DIS) and refers to their standard dataset splits and evaluation. Therefore, there is no documentation regarding dataset creation process in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1753-rubric-0",
    "token_usage": {
      "prompt_tokens": 9708,
      "completion_tokens": 181,
      "total_tokens": 9889
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.2, Table 1",
          "Reasoning": "The paper introduces the SHVC codec and evaluates it on multiple image datasets including CIFAR10, ImageNet32, and ImageNet64. The paper reports training SHVC models on these datasets and compares against other codecs. These datasets are composed of images captured or created by humans, not generated by a model or synthetic process, as per description and context in evaluations. The paper does not mention the introduction of any new datasets; however, the only new data used for training and testing the introduced method is standard image datasets. The SHVC codec is introduced as a method, not as a new image dataset. Therefore, the paper does not introduce any new datasets but rather uses existing human-generated image datasets."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1753-rubric-1",
    "token_usage": {
      "prompt_tokens": 10508,
      "completion_tokens": 237,
      "total_tokens": 10745
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Throughout the paper, especially Sections 3 and 4",
            "reasoning": "The paper introduces new datasets implicitly by training and evaluating their model SHVC and variants on existing benchmark datasets (CIFAR10, ImageNet32, ImageNet64) but does not explicitly introduce or collect any new datasets with human annotation or labeling tasks. Their method involves lossless compression models and experiments on existing image datasets without describing any manual annotation or labeling process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "No annotation instructions related to dataset creation or labeling are provided, as the datasets used are standard image datasets without human annotation beyond standard labels which are not relevant here."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "The paper does not provide any rubrics or scoring guidelines for annotation since no annotation process is described for any newly introduced dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "There are no annotation examples given in the paper because no annotation process is described for new datasets."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1753-rubric-2",
    "token_usage": {
      "prompt_tokens": 11698,
      "completion_tokens": 276,
      "total_tokens": 11974
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for any new dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no reference or indication that multiple human experts performed quality assurance on datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of a single non-expert conducting quality assurance on datasets is present in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no evidence that multiple non-expert annotators performed quality assurance on any new dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance process does not mention or involve AI models as judges for dataset quality assurance."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper involves automated coding for compression, it does not describe any automated verification or algorithmic quality assurance process applied to dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces no new datasets; it uses existing standard datasets such as CIFAR10 and ImageNet variants for evaluation. Therefore, no quality assurance process for new datasets is described or necessary."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1753-rubric-3",
    "token_usage": {
      "prompt_tokens": 11308,
      "completion_tokens": 384,
      "total_tokens": 11692
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the creation of any new datasets derived entirely from scratch by human contributors. Instead, it primarily discusses modeling techniques and compression algorithms applied to existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper focuses on model architectures and methods for lossless image compression; it does not introduce any datasets generated purely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about datasets produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets generated via machine translation are discussed or introduced."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors use existing public image datasets such as CIFAR10 and ImageNet variants for training and evaluation. There is no indication that they collate or aggregate new data collections to produce a dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that the authors introduce new datasets created as modifications or adaptations of existing data sources. They focus on methods and architectures applied to pre-existing datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not specify or document the creation of any new datasets; it exclusively uses standard, existing datasets for evaluation and experiments."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1753-rubric-4",
    "token_usage": {
      "prompt_tokens": 11826,
      "completion_tokens": 346,
      "total_tokens": 12172
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets used exclusively for pre-training large models. No such usage is described."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets used for training models from scratch. Instead, they use existing standard image datasets for training and evaluation."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any new dataset is introduced or used for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning based post-training techniques are described or utilized on any new dataset in the paper."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing standard datasets such as CIFAR10, ImageNet32, and ImageNet64 for evaluation and benchmarking, but these are pre-existing datasets, not newly introduced by the authors."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper analyzes model performance and components, no new dataset is introduced that serves purely for analysis or trend evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets serving as a knowledge base for augmentation in models."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. All referenced datasets are standard benchmarks used for training and evaluation, and no practical usage of a proposed new dataset is described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1753-rubric-5",
    "token_usage": {
      "prompt_tokens": 12549,
      "completion_tokens": 599,
      "total_tokens": 13148
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset containing entries with multiple human languages. The datasets mentioned (e.g., CIFAR10, ImageNet32, ImageNet64) are image datasets without any linguistic content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the datasets contain exactly two human languages. The datasets are image data unrelated to human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any text-based datasets containing only English content. The focus is on image compression datasets."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any datasets containing text in a single non-English language. The datasets are image-based without language specification."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce a dataset of programming code or structured code content. The focus is on image datasets and compression methods."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Throughout the paper (e.g., Sections 3, 4, and equations therein).",
          "reasoning": "While the paper contains extensive mathematical expressions and formal notation describing the methods, it does not introduce a new dataset consisting of these mathematical or logical notations as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets related to biological sequences or non-human communication systems are introduced or used in this work."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not include any datasets with constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the datasets is clearly non-linguistic images; thus, no uncertainty about language applies."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced in the paper are image datasets, such as CIFAR10, ImageNet32, and ImageNet64, containing image data with no linguistic content. Therefore, the entries do not contain any human or programming languages, constructed languages, or biological communication. The linguistic scope is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1753-rubric-6",
    "token_usage": {
      "prompt_tokens": 9767,
      "completion_tokens": 195,
      "total_tokens": 9962
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or location in the paper mentions dataset creation code or provides links to code repositories related to dataset construction.",
          "reasoning": "The paper focuses on proposing a novel VAE-based image compression method (SHVC) and conducts experiments on existing image datasets such as CIFAR10, ImageNet32, and ImageNet64, but does not introduce any new datasets or mention code for dataset creation, collection, or preprocessing. There are no explicit mentions or references to code availability for dataset construction or dataset-related code."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section in the paper describes the process of dataset creation or construction.",
          "reasoning": "The paper uses existing, publicly available image datasets for evaluation and does not create or introduce any new datasets. Therefore, there is no documentation describing dataset creation processes. Dataset construction or collection is not discussed, so no dataset documentation is provided in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1764-rubric-0",
    "token_usage": {
      "prompt_tokens": 11437,
      "completion_tokens": 102,
      "total_tokens": 11539
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Experimental Setup, Dataset",
          "Reasoning": "The paper uses an 'ImageNet-compatible dataset' comprised of 1,000 images widely used in recent FGSM-based attacks. This dataset consists of real images typically collected and curated by humans, hence human generated. The dataset is explicitly mentioned as used for experiments, indicating new dataset usage implied for evaluation."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1764-rubric-1",
    "token_usage": {
      "prompt_tokens": 12237,
      "completion_tokens": 233,
      "total_tokens": 12470
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Experimental Setup, Dataset paragraph",
            "reasoning": "The paper states that experiments are conducted on an ImageNet-compatible dataset comprised of 1,000 images and widely used in prior FGSM-based attacks, implying the dataset is pre-existing rather than newly introduced. No other new dataset is introduced in the paper."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No section describes annotation instructions for dataset construction (e.g., Section 3.1, 4.1)",
            "reasoning": "The paper does not provide any annotation instructions or guidelines related to dataset annotation as it does not introduce a new dataset or any data annotation process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention in the paper",
            "reasoning": "No scoring rubrics or evaluation rubrics for annotation are described, since no new annotations are introduced."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No mention in the paper",
            "reasoning": "No annotation examples or guidelines are provided, consistent with the lack of new dataset or annotation task."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1764-rubric-2",
    "token_usage": {
      "prompt_tokens": 13427,
      "completion_tokens": 320,
      "total_tokens": 13747
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance processes conducted by a single human expert on the dataset. There is no indication that dataset annotations or content were validated by a single expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple human experts performing quality assurance on the dataset annotations or content in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not refer to any single non-expert human annotator or validator involved in the quality assurance of the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of multiple non-expert human validators involved in the quality assurance of any dataset annotations or content is present in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance being conducted by an AI model as a judge for dataset annotation or validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper involves algorithmic and optimization processes, it does not describe any automated quality assurance or verification process applied to dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses an existing ImageNet-compatible dataset comprised of 1,000 images for experiments. No new datasets are introduced by the authors. Consequently, no quality assurance process related to dataset annotation or content validation is described or applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1764-rubric-3",
    "token_usage": {
      "prompt_tokens": 13037,
      "completion_tokens": 265,
      "total_tokens": 13302
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental Setup, Dataset",
          "reasoning": "The dataset used is an ImageNet-compatible dataset comprising 1,000 images widely used in prior FGSM-based attacks, indicating it is collected from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly states the dataset origin as an existing ImageNet-compatible dataset, thus the origin is specified."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1764-rubric-4",
    "token_usage": {
      "prompt_tokens": 13555,
      "completion_tokens": 281,
      "total_tokens": 13836
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental Setup; Section 4.2 Attack Normally Trained Models; Section 4.3 Attack Advanced Defense Models; Section 4.4 Comparison on Loss",
          "reasoning": "The paper uses the ImageNet-compatible dataset of 1,000 images primarily to evaluate the adversarial attack methods by measuring attack success rate and loss values on hold-out models and defense models, demonstrating transferability and effectiveness of the proposed SVRE attack."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new datasets are introduced by the authors in the paper; the dataset used for experiments is an existing ImageNet-compatible 1,000-image dataset widely used in prior adversarial attacks, thus no new dataset with practical usage is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1764-rubric-5",
    "token_usage": {
      "prompt_tokens": 14278,
      "completion_tokens": 587,
      "total_tokens": 14865
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset entries involving more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper of datasets containing exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 (Experimental Setup) - Dataset paragraph",
          "reasoning": "The new dataset used by the authors is an ImageNet-compatible dataset composed of 1,000 images widely used in recent FGSM-based attacks. ImageNet and its subsets are known and used with English annotations. The paper's text and references are in English, and no other languages or language versions are described for the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset described is not indicated to contain only one non-English language; rather, it is based on ImageNet which is English-based."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the paper contains programming-related content such as algorithm pseudocode and code references, the dataset itself is visual images from ImageNet. The paper does not introduce a new dataset containing code or structured programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes formulas and symbolic representations describing loss functions and algorithms, but these mathematical expressions are not part of a new dataset introduced by the authors. The dataset consists of images, not mathematical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the dataset contains biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are involved in the dataset or paper content."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is explicitly known and specified as English based on the paper's description."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of images with associated class labels presumably in English; thus it contains language descriptions and is not devoid of language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1764-rubric-6",
    "token_usage": {
      "prompt_tokens": 11496,
      "completion_tokens": 171,
      "total_tokens": 11667
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 4",
          "reasoning": "The paper provides a link to code for their attack method but does not provide or reference any code related to data collection or dataset construction. The dataset used is a standard ImageNet-compatible dataset consisting of 1,000 images widely used in previous works; no new dataset is constructed or introduced by the authors, hence no code for dataset construction is applicable or provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 4.1 (Dataset subsection)",
          "reasoning": "The paper uses an existing ImageNet-compatible dataset of 1,000 images widely used in previous FGSM-based attacks. No new dataset is introduced or constructed by the authors in this work, and hence no dataset creation process is documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1766-rubric-0",
    "token_usage": {
      "prompt_tokens": 11376,
      "completion_tokens": 108,
      "total_tokens": 11484
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets and Evaluation Metrics",
          "Reasoning": "The datasets discussed in Section 4.1 (Cityscapes, ADE20K, Pascal VOC 2012) are standard image datasets with raw images captured from real-world scenes, indicated by their description and properties. However, these datasets are used for training and evaluation but are not newly introduced by the authors in this paper."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1766-rubric-1",
    "token_usage": {
      "prompt_tokens": 12176,
      "completion_tokens": 312,
      "total_tokens": 12488
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Entire paper, especially Section 3 (Method) and Section 4 (Experiments)",
            "reasoning": "The paper introduces a semantic segmentation knowledge distillation method using teacher and student neural networks trained on existing datasets. The segmentation training/annotation data itself comes from public datasets (Cityscapes, ADE20K, Pascal VOC) with annotations already established. The paper proposes methods (e.g., Contourlet Decomposition Module, Denoised Texture Intensity Equalization Module) that automatically extract and utilize texture knowledge from these features for distillation. There is no mention of new human annotation efforts or human annotators labeling data; the approach uses existing datasets and automatically applies the knowledge distillation technique."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No mention in the paper",
            "reasoning": "The paper does not describe any detailed annotation instructions for human annotators because the datasets used (Cityscapes, ADE20K, Pascal VOC) are pre-existing datasets. The paper does not introduce any new dataset requiring manual annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention in the paper",
            "reasoning": "No scoring rubrics for annotation are described because no new dataset annotations were conducted in this paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No mention in the paper",
            "reasoning": "No annotation examples or guidelines for human annotators are provided as the paper does not create or annotate a new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1766-rubric-2",
    "token_usage": {
      "prompt_tokens": 13366,
      "completion_tokens": 272,
      "total_tokens": 13638
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about quality assurance of dataset annotation by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper of dataset quality assurance performed by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance of dataset annotation by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about quality assurance performed by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that an AI model was used to perform quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automatic verification or algorithmic QA process for dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses existing public benchmark datasets (Cityscapes, ADE20K, Pascal VOC 2012) and does not introduce any new dataset. Therefore, no quality assurance process for dataset annotation is described or performed in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1766-rubric-3",
    "token_usage": {
      "prompt_tokens": 12976,
      "completion_tokens": 336,
      "total_tokens": 13312
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not present any datasets generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data generated through machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper utilizes well-known existing datasets (Cityscapes, ADE20K, Pascal VOC 2012) but does not introduce new datasets that are collated or aggregated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets are introduced that are based on existing data with modifications or transformations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. All datasets used are existing benchmarks without newly created or derived data described in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1766-rubric-4",
    "token_usage": {
      "prompt_tokens": 13494,
      "completion_tokens": 215,
      "total_tokens": 13709
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. Instead, it uses existing benchmark datasets (Cityscapes, Pascal VOC 2012, ADE20K) exclusively. No new dataset is proposed, described, or utilized by the authors in the methodology or experiments. Therefore, no practical utility of a new dataset can be identified in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1766-rubric-5",
    "token_usage": {
      "prompt_tokens": 14217,
      "completion_tokens": 544,
      "total_tokens": 14761
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used for evaluation (Cityscapes, ADE20K, Pascal VOC 2012) are image segmentation datasets and do not contain entries with multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets discussed do not contain exactly two human languages in their entries; language content is not addressed."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing only English content; the datasets are visual semantic segmentation datasets with images and labels."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English monolingual dataset is introduced or described."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions methods and algorithms but does not introduce any new dataset containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical formulae and notations are presented to describe the methods, but these are not datasets or dataset entries; no new dataset entries contain such notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data is mentioned in the newly introduced datasets."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or artificial languages are mentioned in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used and proposed for evaluation are standard semantic segmentation datasets with well-documented content; no unspecified languages."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced or used in the paper (Cityscapes, Pascal VOC 2012, ADE20K) are image datasets for semantic segmentation containing images and annotation labels but do not contain any human language text entries. They contain pixel-wise labels for visual categories, but not language data."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1766-rubric-6",
    "token_usage": {
      "prompt_tokens": 11435,
      "completion_tokens": 188,
      "total_tokens": 11623
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or part in the paper mentions code availability for dataset construction",
          "reasoning": "The paper mentions three well-known existing datasets: Cityscapes, ADE20K, and Pascal VOC 2012. These are acknowledged standard datasets used for benchmarking semantic segmentation and are not created by the authors. The paper does not introduce any new dataset, nor does it provide or reference code related to dataset collection, preprocessing, or generation. Therefore, there is no code provided for dataset construction or generation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section or part in the paper describes dataset creation or construction",
          "reasoning": "The paper uses existing benchmark datasets (Cityscapes, ADE20K, Pascal VOC 2012) with no indication that the authors created or modified a dataset. Consequently, there is no documentation about dataset creation process within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1795-rubric-0",
    "token_usage": {
      "prompt_tokens": 12980,
      "completion_tokens": 82,
      "total_tokens": 13062
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2 Datasets",
          "Reasoning": "The paper introduces the new dataset VinText consisting of 1,200 training images and 500 testing images, which involves images of Vietnamese scene text collected for this dataset, indicating human generation through photography or manual collection."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1795-rubric-1",
    "token_usage": {
      "prompt_tokens": 13780,
      "completion_tokens": 196,
      "total_tokens": 13976
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.2 Datasets",
            "reasoning": "The paper lists several datasets used for training and evaluation, including Total-Text, SCUT-CTW1500, ReCTS, and VinText, which are public datasets and not new. However, the paper does not explicitly introduce or release any new dataset. Therefore, no new dataset is introduced in the paper to evaluate annotation information for."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "Since no new dataset is introduced in the paper, there is no mention of instructions for annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No new dataset means no rubric information provided or applicable."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No new dataset means no examples or annotation samples are provided."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1795-rubric-2",
    "token_usage": {
      "prompt_tokens": 14970,
      "completion_tokens": 287,
      "total_tokens": 15257
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on any dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any details or indication that multiple human experts performed quality assurance on the dataset(s)."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper about quality assurance being performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by multiple non-expert human annotators for the datasets used or introduced."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given in the paper that AI models were used as judges for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification or algorithmic quality assurance processes applied to dataset annotations or content in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses multiple pre-existing benchmark datasets for evaluation and does not introduce any new datasets. No new dataset is introduced by the authors and no quality assurance process is described or documented for datasets in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1795-rubric-3",
    "token_usage": {
      "prompt_tokens": 14580,
      "completion_tokens": 357,
      "total_tokens": 14937
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.2 (Datasets) and Section 4.1 (Implementation Details)",
          "reasoning": "The authors train and evaluate their model on multiple existing public datasets such as Curved SynthText, ICDAR 2013, ICDAR 2015, Total-Text, SCUT-CTW1500, ReCTS, VinText, and others. They do not introduce any new dataset created entirely from scratch, but rather rely on these established datasets, some of which are synthetic like Curved SynthText, and others are real-world datasets that originate from previous works. The usage involves training on a mixture and fine-tuning, implying that their data usage is based on existing datasets with designed data augmentation and training protocols, which constitutes a derived data usage rather than introduction of completely new data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1795-rubric-4",
    "token_usage": {
      "prompt_tokens": 15098,
      "completion_tokens": 194,
      "total_tokens": 15292
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets created by the authors. All datasets mentioned are pre-existing public benchmarks or synthesized datasets from prior works. Therefore, there is no practical usage of any newly proposed dataset described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1795-rubric-5",
    "token_usage": {
      "prompt_tokens": 15821,
      "completion_tokens": 614,
      "total_tokens": 16435
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 4.3 and Section 4.2 (Datasets)",
          "reasoning": "The paper evaluates the proposed SwinTextSpotter on multilingual datasets such as ReCTS (Chinese) and VinText (Vietnamese), and also mentions use of ICDAR-MLT which contains multiple languages. This shows that the dataset(s) involved contain more than two human languages, including English, Chinese, and Vietnamese, thus qualifying as multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset that is exclusively bilingual (exactly two languages). Datasets used are either monolingual or multilingual."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.3 and Section 4.2 (Datasets)",
          "reasoning": "The paper uses several monolingual English datasets, such as ICDAR 2013, ICDAR 2015, Total-Text, SCUT-CTW1500, and RoIC13. These datasets contain only English text entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 4.3 and Section 4.2 (Datasets)",
          "reasoning": "The paper evaluates on single-language non-English datasets such as ReCTS (Chinese) and VinText (Vietnamese), indicating monolingual datasets with languages other than English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce or mention any datasets containing programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets containing mathematical or logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset based on biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets contain any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "All datasets are specified and their languages documented. No unknown or unspecified language datasets are introduced."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets all contain human language text entries; none are language-free."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1795-rubric-6",
    "token_usage": {
      "prompt_tokens": 13039,
      "completion_tokens": 197,
      "total_tokens": 13236
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract, last line",
          "reasoning": "The paper provides a link to the code repository (https://github.com/mxin262/SwinTextSpotter) for the developed model SwinTextSpotter, but does not include or mention any code related to dataset construction, data collection, preprocessing, or dataset generation. The datasets used are publicly known and existing datasets, and the paper does not introduce any new dataset constructed by the authors. Therefore, there is no code relevant to dataset construction available."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 4.2 Datasets",
          "reasoning": "The paper lists and describes publicly available datasets used for training and evaluation, but it does not describe any dataset created or constructed by the authors. There is no documentation on dataset construction or data collection process since no new dataset is introduced by the authors. Therefore, there is no dataset creation documentation to assess."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-181-rubric-0",
    "token_usage": {
      "prompt_tokens": 16700,
      "completion_tokens": 162,
      "total_tokens": 16862
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 1 Introduction; Section 3 Methodology; Section 4 Experiments",
          "Reasoning": "The paper introduces the DensePASS dataset as a benchmark for outdoor panoramic semantic segmentation. DensePASS consists of 360\u00b0 surround-view panoramic images captured with panoramic cameras, which is new and used as a target domain in their experiments. The images are acquired through human-operated camera systems and labeled with semantic segmentation annotations. The paper also refers to Stanford2D3D as an existing indoor panoramic dataset, so it is not a new dataset introduced in this work. Thus, only DensePASS is a new dataset introduced, consisting of images captured via human involvement (cameras)."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-181-rubric-1",
    "token_usage": {
      "prompt_tokens": 17500,
      "completion_tokens": 540,
      "total_tokens": 18040
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 1 Introduction, Abstract, and Section 3 Methodology",
            "reasoning": "The paper introduces new panoramic semantic segmentation datasets and models. The annotations for the datasets, such as Stanford2D3D and DensePASS, are established datasets with pixel-wise annotations, but the paper itself builds on these and focuses on model architecture and domain adaptation rather than creating new annotated datasets. There is no explicit mention of new datasets created or annotated in this work; rather, they utilize existing benchmarks. However, since they mention circumventing expensive panorama labels, their work is more about adaptation than new annotation collection. Therefore, the annotation process is conducted by original dataset creators (assumed experts), not newly introduced here."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No relevant section describing new annotation instructions",
            "reasoning": "The paper does not describe any specific annotation instructions for new datasets; it only uses existing benchmarks for evaluation and does not introduce new labeled data with instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No relevant section describing annotation rubrics",
            "reasoning": "The paper does not mention any rubric or scoring criteria for annotations as no new annotation task or dataset is introduced."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of annotations or annotation guidelines provided in paper",
            "reasoning": "No annotation examples or guidelines are given because no new annotated dataset is introduced by the authors."
          }
        },
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4 Experiments and Appendix (dataset descriptions)",
            "reasoning": "The only datasets mentioned are Stanford2D3D and DensePASS, both are existing datasets with pixel-wise annotations. The paper does not state creating or annotating any new dataset. The annotations in these datasets are produced by expert annotators as per original dataset papers cited."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No section in the paper details annotation instructions",
            "reasoning": "The paper exclusively uses existing datasets and does not provide or discuss annotation instructions for any new dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No section in the paper discusses annotation rubrics",
            "reasoning": "No rubrics or scoring guidelines are detailed since there is no new data annotation presented."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples or guidelines in the paper",
            "reasoning": "No examples of annotations are given because the paper does not introduce new data annotations."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-181-rubric-2",
    "token_usage": {
      "prompt_tokens": 18690,
      "completion_tokens": 364,
      "total_tokens": 19054
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset annotations being quality assured by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper about multiple human experts performing quality assurance on dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any quality assurance conducted by a single human annotator without expertise."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about quality assurance being performed by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by AI models acting as a judge on dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automatic or algorithmic verification of dataset annotations or content as part of quality assurance."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets nor describe any quality assurance or annotation verification process for datasets. The datasets used are publicly available benchmarks (Stanford2D3D, DensePASS), and no new dataset or annotation quality assurance process is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-181-rubric-3",
    "token_usage": {
      "prompt_tokens": 18300,
      "completion_tokens": 405,
      "total_tokens": 18705
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the authors introduce any completely new dataset created entirely by human contributors from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generation of a new dataset purely generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no reference to datasets produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided that data was generated by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing datasets (Stanford2D3D and DensePASS) but does not create new data by simply collecting or aggregating from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Throughout the paper, e.g., Abstract and Section 3 (Methodology)",
          "reasoning": "The paper focuses on domain adaptation and transferring learning from existing label-rich pinhole image datasets to label-scarce panoramic datasets. They do not introduce new original datasets; instead, they adapt existing data modalities through model and architectural innovations. Thus, while no completely new dataset is introduced, the data used is derived by adaptation techniques applied to existing pinhole and panoramic domain datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents data sources clearly; existing datasets are used, so data origin is known."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-181-rubric-4",
    "token_usage": {
      "prompt_tokens": 18818,
      "completion_tokens": 408,
      "total_tokens": 19226
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe introducing or using any new dataset for training models from scratch. Instead, existing datasets are used."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly mention using a newly introduced dataset for supervised fine-tuning of pre-trained models. No new dataset introduced by the authors is described for this purpose."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss using a newly introduced dataset for reinforcement learning based post-training (e.g., RLHF)."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets are introduced in this paper. Evaluation is performed on existing benchmarks such as Stanford2D3D and DensePASS."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper performs analysis on existing datasets but does not introduce any new dataset for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of using any newly introduced dataset as a knowledge base in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; all datasets used are pre-existing and publicly available, such as Stanford2D3D and DensePASS."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-181-rubric-5",
    "token_usage": {
      "prompt_tokens": 19541,
      "completion_tokens": 626,
      "total_tokens": 20167
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset entries containing more than two human languages. The datasets used are related to image domains (pinhole and panoramic) for semantic segmentation, with no language content described."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries described in the paper relate to images and do not contain two human languages. The paper focuses on visual data rather than language data."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets introduced contain entries with English linguistic content. The datasets are image-based and pertain to visual semantic segmentation tasks without text or language data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset with non-English human language content. The datasets are composed of visual panorama and pinhole images with semantic pixel-wise annotations, not linguistic data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "Although the paper discusses algorithmic methods, architecture designs, and mathematical formulas, there is no new dataset introduced that contains entries with programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The paper contains mathematical and logical notations within the methodology sections to describe model components, but these do not pertain to new datasets containing mathematical or formal logical expressions as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced do not contain biological sequences or non-human communication systems; they are visual datasets for semantic segmentation."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or artificially created languages in any dataset introduced by the authors."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly describes the nature and origin of the datasets used and introduced; no dataset is described as having unspecified or undocumented language content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced and used in the paper are comprised entirely of visual data (panoramic and pinhole images) with pixel-wise semantic annotations. These datasets do not contain any entries with human language content; hence, the language coverage for these datasets is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-181-rubric-6",
    "token_usage": {
      "prompt_tokens": 16759,
      "completion_tokens": 178,
      "total_tokens": 16937
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract, Introduction",
          "reasoning": "The paper provides a link to code related to the model (https://github.com/jamycheung/Trans4PASS), however, no code is mentioned or linked regarding dataset construction, preprocessing, or generation. The paper uses existing datasets (Stanford2D3D, DensePASS) and domain adaptation but does not introduce any new datasets or provide code to construct datasets."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No specific section",
          "reasoning": "The paper does not describe the creation process of any new dataset. It uses existing datasets Stanford2D3D and DensePASS benchmarks, which are standard in the domain and cited. There is no documentation on dataset collection or construction, as the focus is on model architecture and adaptation methods rather than new dataset introduction."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1843-rubric-0",
    "token_usage": {
      "prompt_tokens": 13148,
      "completion_tokens": 139,
      "total_tokens": 13287
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4 (Experiment), Implementation details",
          "Reasoning": "The paper conducts extensive experiments on the ImageNet-1k dataset, which is a well-known image dataset consisting of images captured from the real world. The paper uses this dataset for training and evaluation but does not introduce any new dataset itself. As such, the sole data source referenced and used in experiments is the existing ImageNet dataset, which is composed of human-generated images collected from the internet, manually labeled and curated. Hence, the data modality used is 'image' with human-generated origin."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1843-rubric-1",
    "token_usage": {
      "prompt_tokens": 13948,
      "completion_tokens": 211,
      "total_tokens": 14159
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4 Experiment, implementation details, and evaluation on ImageNet",
            "reasoning": "The new dataset used for experiments is ImageNet-1k, a pre-existing dataset; no new datasets are introduced in this paper. The experiments are conducted on pre-existing datasets and publicly available pre-trained models. The paper focuses on methods applied to existing datasets rather than creating new, annotated datasets."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "As no new data annotation is performed on a new dataset introduced by the paper, there are no instructions regarding annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics are described for any new annotation task, as no new dataset with annotations is introduced."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples are provided because the paper does not introduce any new dataset or annotation procedure."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1843-rubric-2",
    "token_usage": {
      "prompt_tokens": 15138,
      "completion_tokens": 270,
      "total_tokens": 15408
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process involving a single human expert annotating or validating any dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper describing that multiple human experts conducted quality assurance on any dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance process involving multiple non-expert human annotators is described in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance performed by AI models as judges for dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses automated evaluation metrics for model redundancy and diversity, but does not describe an automated process as a quality assurance method for dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; thus, no quality assurance process related to dataset annotation or content is described or applied."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1843-rubric-3",
    "token_usage": {
      "prompt_tokens": 14748,
      "completion_tokens": 363,
      "total_tokens": 15111
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets created entirely from scratch by human contributors; it focuses on method development and experiments on existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not generate any datasets synthesized entirely by AI or machine learning models; it only uses existing datasets for evaluation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of producing data by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced through machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not collect or aggregate data from existing sources to create new datasets; it uses existing datasets like ImageNet for experiments."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not present datasets that are transformations or adaptations of existing data; it solely focuses on model training and evaluation on existing benchmarks."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset, nor does it specify or document any new data generation method; all data used are from existing well-known datasets such as ImageNet."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1843-rubric-4",
    "token_usage": {
      "prompt_tokens": 15266,
      "completion_tokens": 355,
      "total_tokens": 15621
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 (Experiment) and Implementation details",
          "reasoning": "The paper uses the ImageNet dataset to train various vision transformer models from scratch with their proposed diversity regularizers. Models such as ViT-Small/Base and DeiT are trained on ImageNet for 300 epochs, demonstrating the utility of the dataset for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 (Experiment), Implementation details for Swin transformer",
          "reasoning": "For Swin transformers, the paper starts from official pre-trained models and fine-tunes them on ImageNet for 30 epochs with proposed diversity regularization, demonstrating supervised fine-tuning usage of the dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "ImageNet dataset is used for training and fine-tuning of models, not exclusively for evaluation or benchmarking. The evaluation results reported are on the same training dataset splits, but the paper does not introduce a separate dataset solely for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The ImageNet dataset is used for training and fine-tuning models, not primarily for analysis of trends or characteristics separate from training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or for retrieval-augmented generation in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1843-rubric-5",
    "token_usage": {
      "prompt_tokens": 15989,
      "completion_tokens": 483,
      "total_tokens": 16472
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing multiple human languages. The focus is on vision transformers and ImageNet dataset, which is image data rather than multilingual textual data."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset with exactly two human languages is discussed or introduced in the paper."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, particularly in Section 4 (Experiment) and dataset description (ImageNet-1k) in Section 4.",
          "reasoning": "The authors use the ImageNet dataset, which is an image classification dataset with English labels and category names; the paper's written content and evaluation are in English with no mention of other human languages included in the dataset. Therefore, the dataset can be considered monolingual (English) for the purposes of this metric."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication that the dataset contains only non-English languages; the textual data used (ImageNet labels etc.) is in English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes some pseudocode-style mathematical formulae and descriptions of algorithms but does not introduce or describe a dataset comprising programming or code entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical formulations describing the models and regularizers, these are part of the scientific exposition rather than a dataset introduced in the paper. No new dataset with mathematical or logical notation entries is introduced."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets containing biological sequences or non-human communication are introduced or mentioned."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or introduction of datasets containing constructed, fictional, or artificial languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly states it uses ImageNet, a well-known, documented dataset; there is no indication that dataset language classification is unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset used contains language (English labels); thus, the metric N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1843-rubric-6",
    "token_usage": {
      "prompt_tokens": 13207,
      "completion_tokens": 174,
      "total_tokens": 13381
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract, Section 4 (Experiment)",
          "reasoning": "The paper provides a GitHub link (https://github.com/VITA-Group/Diverse-ViT) for the code related to training and evaluating vision transformers with diversity regularizers. However, the paper uses the ImageNet-1k dataset, which is a standard and pre-existing dataset, and does not introduce any new dataset. Therefore, there is no code for dataset construction or generation described or referenced in the paper."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No sections explicitly describe any new dataset creation process",
          "reasoning": "The paper exclusively uses existing datasets, specifically ImageNet-1k, for experiments. It does not introduce any new datasets, hence there is no dataset creation process documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1866-rubric-0",
    "token_usage": {
      "prompt_tokens": 12943,
      "completion_tokens": 126,
      "total_tokens": 13069
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4 Experiments and Dataset description in Section 4 (CIFAR-10, CIFAR-100, SVHN)",
          "Reasoning": "The paper uses image datasets CIFAR-10, CIFAR-100, and SVHN for experiments; these datasets contain images captured or collected by humans or human-operated processes as typical image classification benchmarks. The paper does not introduce any new dataset but uses these pre-existing datasets; therefore, no new dataset entries are introduced by the authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1866-rubric-1",
    "token_usage": {
      "prompt_tokens": 13743,
      "completion_tokens": 294,
      "total_tokens": 14037
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.3 and 3.4",
            "reasoning": "The paper describes the derivation and design of the Gradient Synchronization Filter (GSF) and Prototype Proximity Filter (PPF) as new filters within the newly introduced Taylor Expansion Inspired Filtration (TEIF) framework. These filters involve computing gradients and prototype features for selecting samples from the moderately confident unlabeled data. The process appears to be designed and carried out by the authors (experts), rather than being an automatic or AI-model-generated annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit section describing annotation instructions",
            "reasoning": "The paper does not provide or mention detailed annotation instructions given to annotators for labeling or selecting data samples; the filters are algorithmic methods incorporated into the training process, not manual annotations requiring instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention found regarding scoring rubrics",
            "reasoning": "The method is a computational filtering framework rather than a human annotation process with scoring rubrics; thus, no rubrics for scoring or labeling quality are discussed."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of annotation procedures or labeled data provided",
            "reasoning": "The paper does not include explicit examples illustrating annotation or manual labeling examples; the sample selection relies on computed gradients and features within the model training pipeline."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1866-rubric-2",
    "token_usage": {
      "prompt_tokens": 14933,
      "completion_tokens": 312,
      "total_tokens": 15245
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving a single human expert for validating dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by multiple human experts for dataset validation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about single non-expert human annotator involvement in quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance done by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used for classification and filtering processes, the paper does not describe any use of AI models as judges specifically for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper focuses on semi-supervised learning methods and sample selection filters but does not describe any automated verification process or algorithmic rule-based quality assurance of dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; it uses existing benchmark datasets (CIFAR-10, CIFAR-100, SVHN) and does not describe any quality assurance process for dataset annotations or content. Therefore, no quality assurance process is applied or documented for any new dataset."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1866-rubric-3",
    "token_usage": {
      "prompt_tokens": 14543,
      "completion_tokens": 417,
      "total_tokens": 14960
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets created by human contributors from scratch. Instead, it uses existing benchmark datasets such as CIFAR-10, CIFAR-100, and SVHN for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not create any new datasets generated purely by AI models. It develops filters to select samples within existing unlabeled datasets but does not generate new data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any data generated using machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used (CIFAR-10, CIFAR-100, SVHN) are pre-existing benchmark datasets, but the paper does not collect or aggregate any new data from existing sources for new dataset creation."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper proposes methods to select subsets of samples from existing unlabeled data for training, but it does not construct new derived datasets by transformation or adaptation of existing data. The modifications relate to training approaches rather than new dataset construction."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. It entirely uses established public benchmark datasets for evaluation without specifying any novel data collection or generation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1866-rubric-4",
    "token_usage": {
      "prompt_tokens": 15061,
      "completion_tokens": 325,
      "total_tokens": 15386
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments)",
          "reasoning": "The datasets CIFAR-10, CIFAR-100, and SVHN are used to train classification models from randomly initialized parameters, in the semi-supervised learning setting described in Section 3 and evaluated in Section 4. The models are trained using the proposed filters (GSF and PPF) on labeled and unlabeled data from these datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 (Results) and Table 1",
          "reasoning": "The datasets CIFAR-10, CIFAR-100, and SVHN serve as benchmarks for evaluating and comparing the accuracy of models trained with the proposed methods and baseline methods."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.1 (Ablation Study) and multiple figures",
          "reasoning": "The authors analyze the behavior and characteristics of the proposed methods on these datasets, examining parameters like sample selection ratios, mislabeled ratios, impact of confidence thresholds, and visualizations to understand model learning."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1866-rubric-5",
    "token_usage": {
      "prompt_tokens": 15784,
      "completion_tokens": 588,
      "total_tokens": 16372
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the introduction of any dataset containing entries with multiple human languages. The datasets used (CIFAR-10, CIFAR-100, SVHN) consist of image data without multiple human language contents."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets containing exactly two human languages in the paper. The datasets used are standard image classification benchmarks without language-specific content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset. The experiments are conducted on existing image classification benchmarks that do not contain textual or language content. Although the paper is written in English, this does not pertain to dataset language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset with only one non-English language is introduced. The datasets are image classification datasets with no inherent human language content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset containing code or programming language. The data involved are image datasets without any structured code entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical notation in describing their methods and algorithms, there is no new dataset introduced containing mathematical or logical notation as content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reference to biological sequences or non-human communication datasets is made, and no such dataset is introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any datasets with constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly identifies the datasets used (CIFAR-10, CIFAR-100, SVHN), and they are standard image classification datasets without language content. No unknown language dataset is introduced."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset with language content. The datasets employed and discussed consist of images without human or programming language content."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1866-rubric-6",
    "token_usage": {
      "prompt_tokens": 13002,
      "completion_tokens": 155,
      "total_tokens": 13157
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not mention or provide a link to any code repository related to dataset construction or data processing for the new methods involving sample filtration or dataset handling. The focus is on SSL methods applied to existing benchmark datasets without new dataset creation code."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper uses commonly known benchmark datasets (CIFAR-10, CIFAR-100, SVHN) and does not introduce a new dataset or document any new data collection or creation process. There is no description or documentation on dataset construction processes, only usage of existing datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1874-rubric-0",
    "token_usage": {
      "prompt_tokens": 11220,
      "completion_tokens": 355,
      "total_tokens": 11575
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2 Data, COCO-SCE and Task and Data description sections",
          "Reasoning": "The new COCO-SCE split introduced in the paper uses images from the COCO dataset, which are human-captured photographs. The paper introduces this split explicitly to test skill-concept generalization, so the image data involved is new in the sense of the split, but originates from human-generated images in COCO."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2 Data, COCO-SCE, Task and Data (4.1 and 4.2)",
          "Reasoning": "The new COCO-SCE split includes text annotations such as task descriptions, prompts, questions, answers, captions, and referring expressions derived from original COCO, VQA V2 and REFCOCO+ dataset annotations. These textual annotations are human-generated in the original datasets and the new split reorganizes them into held-out and training splits for evaluation. The paper introduces this new split of these human-generated text annotations."
        },
        {
          "Modality": "tabular",
          "Human Generated": false,
          "Model Generated": false,
          "Unknown Origin": true,
          "Reference": "Section 4.2 Data, COCO-SCE split discussion",
          "Reasoning": "The COCO-SCE split involves label assignments (category splits into held-out and seen sets). These splits and annotation grouping are not raw data but processed tabular data created by the authors. The origin is not explicitly stated as human or model generated, but rather constructed by authors for evaluation, so origin is unknown based on text."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1874-rubric-1",
    "token_usage": {
      "prompt_tokens": 12020,
      "completion_tokens": 297,
      "total_tokens": 12317
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 4.2 Data",
            "reasoning": "The paper indicates that the data for the tasks\u2014including VQA, captioning, localization, classification, and referring expressions\u2014are constructed from existing COCO, VQA V2, and REFCOCO+ datasets which involve human annotations. Since these datasets are collected via crowdsourcing platforms like AMT (common for such datasets) and no mention of expert-only annotators is made, it is reasonable to conclude that multiple human non-expert annotators performed the annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit description in the paper",
            "reasoning": "The paper does not provide any explicit description or details about annotation instructions given to human annotators for the new COCO-SCE split or any newly introduced annotation data construction. Therefore, no explicit instructions are documented in the paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit description in the paper",
            "reasoning": "The paper does not mention any scoring rubrics or formal grading criteria being given or used for annotation quality control or scoring; hence, no rubrics are documented."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit description in the paper",
            "reasoning": "The paper does not mention providing annotation examples or guides demonstrating how annotations should be performed, particularly for COCO-SCE or the tasks\u2019 annotations."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1874-rubric-2",
    "token_usage": {
      "prompt_tokens": 13210,
      "completion_tokens": 316,
      "total_tokens": 13526
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the new datasets introduced."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description of quality assurance done by multiple human experts in the paper for the new datasets introduced."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information regarding quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by multiple human non-experts on the new datasets introduced."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of AI models for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any automatic or rule-based verification process employed for quality assurance of new datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new data split called COCO-SCE created from existing COCO, VQA, and referring expressions datasets. The authors describe how the split is created by holding out certain concepts for each task but do not mention any quality assurance process, expert validation, or annotation verification specific to this new split. Therefore, no quality assurance process is documented or applied for the new dataset split introduced."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1874-rubric-3",
    "token_usage": {
      "prompt_tokens": 12820,
      "completion_tokens": 470,
      "total_tokens": 13290
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset where the data was created entirely from scratch by human contributors. Instead, all tasks and data samples are derived from or based on existing datasets such as COCO, VQA V2, and REFCOCO+."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any new data was generated entirely by AI or machine learning models without reference to existing data. All data used is based on existing human-annotated datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated through human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated through machine translation from other languages."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.2 Data",
          "reasoning": "The authors use and collate data from existing sources such as COCO, VQA V2, and REFCOCO+ datasets without significant modification. They reorganize these datasets into a new data split called COCO-SCE to evaluate skill-concept generalization, but this primarily aggregates and partitions existing data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.2 Data",
          "reasoning": "While the data is primarily from existing datasets, the authors derive new splits and create new task samples (e.g., different prompt templates for captioning, classification, localization) and a novel COCO-SCE split for evaluating generalization. This involves adaptations and transformations applied to the existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and provenance of the data used for training and evaluation are clearly documented and referenced."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1874-rubric-4",
    "token_usage": {
      "prompt_tokens": 13338,
      "completion_tokens": 649,
      "total_tokens": 13987
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the proposed COCO-SCE dataset for unsupervised or self-supervised pre-training. Instead, the dataset is used for supervised training and evaluation of multiple tasks."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions initialization of model components from pretrained weights (e.g., DETR pretrained on COCO detection), indicating that training is not from randomly initialized parameters. Thus, the new COCO-SCE dataset is not used for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.2, Section 5.3",
          "reasoning": "The COCO-SCE dataset is used for supervised fine-tuning of the GPV-1 model on multiple vision-language tasks such as VQA, captioning, classification, localization, and referring expressions. Section 4.2 defines the COCO-SCE splits used to train and evaluate the model, while Section 5.3 discusses fine-tuning and learning efficiency on new tasks like referring expressions using these splits."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning based post-training methods (such as RLHF) using the proposed dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2, Section 5.1, Section 5.2",
          "reasoning": "COCO-SCE is introduced specifically as a new split of COCO annotations designed for evaluation of skill-concept generalization across tasks. Sections 5.1 and 5.2 present evaluation results on COCO-SCE test splits to benchmark performance and generality of the GPV-1 model, indicating the dataset's primary utility in evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.2",
          "reasoning": "The COCO-SCE dataset is used to analyze the ability of the model to generalize knowledge of concepts across different tasks (skills), a form of trend and characteristic analysis. The paper uses this dataset to study skill-concept transfer which is an analysis beyond straightforward training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using COCO-SCE as a knowledge base to augment models via retrieval or similar mechanisms."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents multiple uses of the new COCO-SCE dataset including supervised fine-tuning, evaluation, and analysis, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1874-rubric-5",
    "token_usage": {
      "prompt_tokens": 14061,
      "completion_tokens": 264,
      "total_tokens": 14325
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4 Data and Tasks, Section 4.2 Data",
          "reasoning": "The new dataset COCO-SCE and the task annotations used (VQA V2, COCO, REFCOCO+) are all based on English language annotations. The prompts, questions, captions, and referring expressions used for tasks are all in English, and there is no mention of using any other language in the dataset or tasks."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1874-rubric-6",
    "token_usage": {
      "prompt_tokens": 11279,
      "completion_tokens": 193,
      "total_tokens": 11472
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention anywhere in the paper",
          "reasoning": "The paper does not provide any link or statement indicating that the code for constructing the COCO-SCE split or dataset preprocessing is publicly available. The only URL present (https://prior.allenai.org/projects/gpv) relates to the project site but there is no explicit mention of dataset code release within the paper text."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.2 Data",
          "reasoning": "Section 4.2 provides a detailed description of the new COCO-SCE split introduced by the authors, explaining how the 80 COCO classes are partitioned into held-out and shared subsets across tasks, specifying which annotations are excluded or included for training and validation for different tasks. The paper explains the methodology for constructing the dataset splits clearly enough to understand the composition and purpose for skill-concept evaluation."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1879-rubric-0",
    "token_usage": {
      "prompt_tokens": 11390,
      "completion_tokens": 152,
      "total_tokens": 11542
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1",
          "Reasoning": "The paper explicitly states that the datasets used are O-Haze, NH-Haze, and RESIDE indoor/outdoor, which are composed of paired hazy and haze-free images captured in real or synthetic scenarios. The images are human-generated in the sense that they originate from photography or controlled setups and are explicitly described as paired images. The paper does not indicate that these datasets were newly collected by the authors; rather, these are existing datasets. No mention of a new dataset introduced by the authors is found in the paper; the authors focus on methods and training on combined datasets."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1879-rubric-1",
    "token_usage": {
      "prompt_tokens": 12190,
      "completion_tokens": 338,
      "total_tokens": 12528
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.1 and Section 4",
            "reasoning": "The paper describes the introduction of a new helper network trained with supervised paired images of hazy and haze-free pairs from multiple domains (O-Haze, NH-Haze, and RESIDE), indicating that annotations (paired images) are collected or utilized under human expert supervision to construct these datasets for training. The process involves using paired images for supervised learning, consistent with expert annotation or dataset preparation. There is no mention of multiple non-expert annotators or automatic generation of these ground truth pairs beyond the described datasets."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No specific section in the paper describes detailed annotation instructions for annotators in dataset creation.",
            "reasoning": "The paper does not provide any explicit annotation guidelines or instructions given to annotators to generate or label data; instead, the new dataset relies on existing paired haze and haze-free images from namely O-Haze and NH-Haze, with no detailed instructions described for annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No specific section mentions scoring rubrics for annotation guidelines.",
            "reasoning": "The paper does not discuss any scoring or rating rubrics for evaluating annotations or labels, as the dataset consists of paired images rather than subjective labels requiring rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of annotation guidelines or annotation examples are presented in the paper.",
            "reasoning": "Since the dataset consists of paired hazy and haze-free images, no annotation examples (such as labeled textual or categorical examples) are necessary or shown."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1879-rubric-2",
    "token_usage": {
      "prompt_tokens": 13380,
      "completion_tokens": 422,
      "total_tokens": 13802
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information or description about quality assurance by multiple human experts for any newly introduced dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss quality assurance by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance by multiple non-expert human annotators in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper introduces a helper network (an AI model) used to evaluate dehazing outputs during test-time training, this helper network is not described as performing quality assurance on dataset annotations or content, but rather as a self-supervised loss estimator during model adaptation. Therefore, this does not qualify as dataset quality assurance."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification processes or algorithmic/rule-based techniques used specifically for quality assurance of dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper only uses existing publicly available datasets (O-Haze, NH-Haze, RESIDE) and does not introduce any new datasets. Furthermore, it does not document any quality assurance process for dataset annotations or content because no new datasets are created or annotated by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1879-rubric-3",
    "token_usage": {
      "prompt_tokens": 12990,
      "completion_tokens": 378,
      "total_tokens": 13368
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not claim to have created any new dataset of hazy and haze-free image pairs by human contributors from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce data generated purely by AI or machine learning models without reference or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of data created by translating content from another language using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of data generated by machine translation systems from other languages."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 (Datasets and Evaluation Metrics)",
          "reasoning": "The paper uses existing publicly available datasets (O-Haze, NH-Haze, RESIDE indoor/outdoor) combined into a joint dataset for training and evaluation. It collects and aggregates these existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the authors transformed or adapted existing datasets to create new derived data; rather, they combined existing datasets as-is."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly states the datasets used and their origins, so the data source is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1879-rubric-4",
    "token_usage": {
      "prompt_tokens": 13508,
      "completion_tokens": 439,
      "total_tokens": 13947
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any new dataset introduced by the authors being used for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing datasets combined for training dehazing models but does not introduce any new dataset used for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is introduced by the authors for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not involve reinforcement learning or RL-based post-training techniques with any new dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used for evaluation (O-Haze, NH-Haze, RESIDE) are pre-existing and not newly introduced by the authors."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The analysis in the paper is based on existing datasets; no new datasets are introduced solely for analyzing trends or characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is introduced to serve as a knowledge base for model augmentation or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; all datasets mentioned are existing publicly available datasets used for training, evaluation, and analysis. Therefore, no practical usage of a newly introduced dataset is described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1879-rubric-5",
    "token_usage": {
      "prompt_tokens": 14231,
      "completion_tokens": 546,
      "total_tokens": 14777
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset entries containing multiple human languages. It primarily discusses image datasets containing visual data."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets include entries with exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 4.1 Datasets and Evaluation Metrics",
          "reasoning": "The datasets used (O-Haze, NH-Haze, RESIDE) consist of images and do not contain textual entries in English or any particular language."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence exists in the paper to suggest the datasets involve any non-English human language text."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are collections of paired hazy and haze-free images; there are no code or programming language entries described as part of the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses mathematical notation to describe models and losses, the proposed datasets themselves do not contain mathematical or formal logical expressions or symbolic representations as dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain image data relating to haze and dehazing tasks, with no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or artificially created languages in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The linguistic nature of the dataset entries is clearly non-linguistic (images), so the concept of unknown language does not apply."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced and used in the paper consist purely of paired images (hazy and haze-free), which do not contain any human language or linguistic content."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1879-rubric-6",
    "token_usage": {
      "prompt_tokens": 11449,
      "completion_tokens": 161,
      "total_tokens": 11610
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or part in the paper mentions publicly available code for dataset construction.",
          "reasoning": "The paper describes using existing datasets such as O-Haze, NH-Haze, and RESIDE, which are publicly available, but does not introduce any new datasets nor provides code related to dataset construction or data preparation. No links or references to code repositories are present."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No explicit section or detailed description on dataset creation is provided in the paper.",
          "reasoning": "The paper only references known existing datasets and does not describe creation of any new dataset. The descriptions focus on usage of these datasets and their characteristics, but there is no documentation of dataset construction or creation process in the paper content."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1890-rubric-0",
    "token_usage": {
      "prompt_tokens": 11850,
      "completion_tokens": 195,
      "total_tokens": 12045
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5, 6.1",
          "Reasoning": "The paper uses the CIFAR10 and CIFAR100 datasets as the unlabeled pool and initial labeled data in their active learning experiments. CIFAR datasets consist of human-captured images. The paper does not state that these datasets were created by the authors; these are public, pre-existing datasets. Hence, these are not new datasets introduced by the authors."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5, 6.5",
          "Reasoning": "The paper also evaluates active learning methods on ImageNet dataset. ImageNet images are human-generated (captured and annotated). ImageNet is a well-known existing dataset. There is no indication that the authors created a new dataset with ImageNet images."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1890-rubric-1",
    "token_usage": {
      "prompt_tokens": 12650,
      "completion_tokens": 300,
      "total_tokens": 12950
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 5: Implementation Details and Section 9: Conclusion and Proposed Guidelines",
            "reasoning": "The paper states that annotation is performed by an oracle providing labels for selected samples during active learning iterations, but gives no indication that human annotators labeled the data manually. All experiments use existing public image classification datasets (CIFAR10, CIFAR100, ImageNet) that come with labels, and the annotation process is simulated via selecting samples from unlabeled pools and using the ground truth labels already available in the datasets. Therefore, the annotation is done by an automatic process referencing pre-labeled datasets rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No specific section describes instructions for annotators; annotation is simulated via oracle using existing dataset labels.",
            "reasoning": "Since the annotation is performed by oracle lookup on existing dataset labels and not via human manual annotation, no annotation instructions are provided or needed."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not described anywhere in the paper.",
            "reasoning": "No scoring rubrics or criteria are given because annotation is by pre-existing labels used as oracle labels, so no rubrics are applicable."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not described anywhere in the paper.",
            "reasoning": "The paper does not provide annotation examples or guidelines, as it uses existing labeled datasets and simulates annotation via oracle label lookup."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1890-rubric-2",
    "token_usage": {
      "prompt_tokens": 13840,
      "completion_tokens": 322,
      "total_tokens": 14162
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process for dataset annotations involving a single human expert. No information is provided about dataset annotation QA being performed by a single expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance of dataset annotations by multiple human experts. There is no information about such a process being conducted."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of QA by a single human non-expert in relation to dataset annotation or content validation."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA process involving multiple human non-experts for dataset annotation or content validation."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models are used for active learning and sampling, there is no description of an AI model performing quality assurance on dataset annotations or data content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss automatic or algorithmic verification processes applied to dataset annotations or content as a QA measure."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses publicly available datasets (CIFAR10, CIFAR100, ImageNet) without introducing any new dataset. Since no new dataset is introduced, there is no described quality assurance process for dataset annotations or content. Therefore, QA is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1890-rubric-3",
    "token_usage": {
      "prompt_tokens": 13450,
      "completion_tokens": 403,
      "total_tokens": 13853
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the authors created any new datasets from scratch by human contributors. The datasets used are standard benchmark datasets (CIFAR10, CIFAR100, ImageNet) which are pre-existing."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe generating any novel datasets purely from AI or ML model outputs. The study focuses on evaluating active learning methods using existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any datasets created via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used (e.g., CIFAR10, CIFAR100, ImageNet) are well-known publicly available datasets, but no new collation or aggregation of existing datasets is described."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets derived from modifications or transformations of existing data are introduced. The paper applies various active learning and regularization methods on existing datasets without creating new derived datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. All data used are established benchmark datasets. Therefore, data source or method of generation for new datasets is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1890-rubric-4",
    "token_usage": {
      "prompt_tokens": 13968,
      "completion_tokens": 473,
      "total_tokens": 14441
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using any new dataset for pre-training models. Instead, it focuses on experiments with standard datasets."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5 (Implementation Details), Section 6 (Experiments and Results)",
          "reasoning": "The authors use datasets such as CIFAR10, CIFAR100, and ImageNet (all standard datasets) and train models from random initialization during AL iterations (Section 5). This usage implies training from scratch on existing datasets; however, the paper does not introduce any new dataset but uses existing benchmarks."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of fine-tuning pre-trained models on any newly introduced dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no application of reinforcement learning post-training methods described using any dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper performs extensive evaluation using standard datasets, no new datasets are introduced specifically for evaluation purposes."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper analyzes trends and variance in active learning methods using existing datasets but does not introduce new datasets specifically for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No usage of a dataset as a knowledge base is described."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new datasets are introduced in this paper. All datasets used (CIFAR10, CIFAR100, ImageNet) are pre-existing and publicly available. Therefore, there is no practical usage to evaluate for any new dataset introduced by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1890-rubric-5",
    "token_usage": {
      "prompt_tokens": 14691,
      "completion_tokens": 331,
      "total_tokens": 15022
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": true,
          "reference": "Section 5 (Implementation Details) and throughout the paper",
          "reasoning": "The paper presents a PyTorch-based toolkit and code implementations of various active learning methods. The new dataset introduced involves experimental data processed and used within code environments. The paper contains programming code related to model training and active learning sample selection functions. Hence, there is programming code related content in the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 2 (Pool Based Active Learning Methods)",
          "reasoning": "The paper defines formal mathematical notations for labeled data sets, unlabeled data pools, models, sampling functions and acquisition functions using mathematical symbols and formulas. These formal expressions are part of the dataset description and experimental formalism."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset involves images and their labels and uses human language for annotation and explanation, so it is not without any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1890-rubric-6",
    "token_usage": {
      "prompt_tokens": 11909,
      "completion_tokens": 222,
      "total_tokens": 12131
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 5 Implementation Details and Supplementary Material",
          "reasoning": "The paper mentions that they provide a PyTorch-based toolkit implementing the active learning methods and that all codes were written in Python using PyTorch and other libraries, made available as part of the supplementary material. However, they do not introduce any new datasets or dataset construction code. The code focuses on the implementation and evaluation of active learning methods on existing datasets such as CIFAR10, CIFAR100, and ImageNet. There is no mention of code for dataset construction since no new dataset is introduced by the authors."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Not applicable",
          "reasoning": "Since the paper does not introduce any new datasets but instead conducts experiments on existing publicly available datasets (CIFAR10, CIFAR100, ImageNet), there is no dataset creation process to document. The paper instead details experimental settings, data splits, and data partitioning indices, but these relate to the usage of existing datasets, not new ones created by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1895-rubric-0",
    "token_usage": {
      "prompt_tokens": 10046,
      "completion_tokens": 304,
      "total_tokens": 10350
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.5 Annotation Cost Study and throughout the paper",
          "Reasoning": "The paper introduces TextTranSpotter trained using real-world images with text. The images are real data, gathered and annotated with text transcriptions and optionally bounding boxes or polygons, which require human involvement during capture and annotation. The images contain scene text for the text spotting task."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Text Hungarian Loss and Section 4.5 Annotation Cost Study",
          "Reasoning": "The new dataset includes annotations of text transcriptions corresponding to the text appearing in the images. These text annotations are human-generated transcriptions of scene text, which the model uses for training either weakly- or fully-supervised. They represent original human-labeled text content aligned with image data."
        },
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.3 Weakly-Supervised Results and Section 3.1 Architecture",
          "Reasoning": "The paper describes using synthetic data along with real data for training (TTS synthetic). This synthetic data consists of images generated algorithmically to simulate scene text scenarios. Such synthetic images are model generated (algorithmically created) rather than captured by humans, as explicitly mentioned related to training data types."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1895-rubric-1",
    "token_usage": {
      "prompt_tokens": 10846,
      "completion_tokens": 199,
      "total_tokens": 11045
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.5",
            "reasoning": "Section 4.5 describes a user study conducted with 9 annotators who were asked to annotate images using various annotation methods, indicating that multiple human annotators performed the new dataset annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "The paper does not explicitly mention any detailed annotation instructions provided to annotators for the new dataset in the study."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "There is no mention of scoring rubrics or formal evaluation criteria provided as guidelines for the annotators in the annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "The paper does not provide or mention any annotation examples given to annotators to guide the annotation on the new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1895-rubric-2",
    "token_usage": {
      "prompt_tokens": 12036,
      "completion_tokens": 331,
      "total_tokens": 12367
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was conducted by a single human expert annotator for any new dataset. No such QA process is documented."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that multiple human experts performed quality assurance for any dataset annotations or content. There is no explicit reporting of expert QA."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description of quality assurance by a single human non-expert annotator in the paper. No such process is documented for the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide evidence that multiple human non-expert annotators conducted quality assurance on datasets or annotations. There is no documentation of this in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance performed by an AI model as a judge is not described for any dataset or annotation in the paper."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or rule-based process for quality assuring the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets nor describe any quality assurance processes applied to data annotations or content. It uses existing datasets with known annotations, and no QA process is documented for any new dataset, implying no QA was performed or reported."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1895-rubric-3",
    "token_usage": {
      "prompt_tokens": 11646,
      "completion_tokens": 442,
      "total_tokens": 12088
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the authors created any original dataset from scratch by human contributors. Instead, they worked with existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating new datasets entirely by AI or models without referencing existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any machine translation applied to data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes usage of existing public datasets for evaluation and training; however, it does not clearly state that the authors collated or aggregated data themselves without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.3 (Weakly-Supervised Results), Section 4.5 (Annotation Cost Study)",
          "reasoning": "The paper uses existing synthetic datasets and real datasets (e.g., Total-Text, ICDAR 2015) and proposes a weakly-supervised method that learns from only partial annotations (text transcriptions) instead of full polygonal annotations. The authors also conducted a user study on annotation times and used these partial annotations to train their model, effectively deriving new training data labels or annotations based on existing datasets by modifying supervision levels and annotation types."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data is specified in the paper through references to existing datasets and annotation types; the data sources and methods are documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1895-rubric-4",
    "token_usage": {
      "prompt_tokens": 12164,
      "completion_tokens": 503,
      "total_tokens": 12667
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of a new dataset specifically for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of training models from scratch using a newly introduced dataset; the paper discusses training with standard datasets (with different supervision schemes), but no new dataset is introduced for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's focus is on training modalities (fully- and weakly-supervised) using existing public datasets; it does not mention fine-tuning pre-trained models with a newly introduced dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of reinforcement learning or RL-based post-training methods using any new datasets in the paper."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper evaluates its method on existing datasets such as ICDAR 2015, Total-Text, and Rotated ICDAR 2013, but does not introduce any new datasets used exclusively for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper analyzes annotation cost and ablation studies, these analyses are performed on existing datasets and do not involve any newly introduced datasets."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset serving as a knowledge base to augment models."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. All datasets used (e.g., ICDAR 2015, Total-Text, Rotated ICDAR 2013) are existing benchmark datasets. Therefore, there is no practical usage of a newly introduced dataset described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1895-rubric-5",
    "token_usage": {
      "prompt_tokens": 12887,
      "completion_tokens": 452,
      "total_tokens": 13339
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the new dataset introduced contains entries in more than two languages. There is no mention of multiple human languages being included in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the new dataset contains exactly two human languages. There is no evidence of bilingual dataset creation or usage."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, e.g. Section 4.2 and Table 1, and general descriptions",
          "reasoning": "The new dataset introduced by the authors uses real data with text annotations only, and the text shown in qualitative examples and benchmarks (ICDAR 2015, Total-Text, Rotated ICDAR 2013) is predominantly or exclusively English. There is no indication that other languages are present. Thus, the dataset can be considered monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication or mention of any non-English language datasets being introduced by the authors."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or evidence that the dataset contains programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical formulas in its methodology section, these belong to the paper's text description and loss functions, not to the dataset entries themselves."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or evidence that the dataset contains biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that constructed or fictional languages are part of the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is specified as English based on examples and references; thus language is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains textual data which is in English, so it is not devoid of language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1895-rubric-6",
    "token_usage": {
      "prompt_tokens": 10105,
      "completion_tokens": 115,
      "total_tokens": 10220
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not mention any code availability relating to dataset construction or dataset creation process; there is no link or indication that code for dataset creation or preprocessing is publicly available."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "",
          "reasoning": "The paper does not introduce or describe any new dataset creation process; it primarily uses existing datasets such as Total-Text, ICDAR 2015, etc. There is no documentation or detailed description of creating a new dataset within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-19-rubric-0",
    "token_usage": {
      "prompt_tokens": 14216,
      "completion_tokens": 210,
      "total_tokens": 14426
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 4.4, Section 6.2",
          "Reasoning": "The paper introduces a new few-shot image generation dataset consisting of small target domain datasets with 10-shot real images for adaptation, e.g., 10-shot Sketches, 10-shot Babies, and 10-shot Sunglasses. These datasets are used as target training data for GAN adaptation with very limited samples. The modality is image, as these are photographic and artistic images used for training the GAN. The data is human generated, as these are real images collected from human-created sources (like sketches, baby photos, sunglasses images) used as target domain few-shot data. This is explicitly stated as '10-shot samples from target domains that contain abundant data originally.' Hence, the new data introduced is image data, human generated by collection of real samples in the target domain for few-shot adaptation, not model generated or unknown origin."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-19-rubric-1",
    "token_usage": {
      "prompt_tokens": 15016,
      "completion_tokens": 216,
      "total_tokens": 15232
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.2 and 4.3",
            "reasoning": "The paper describes a binary classifier trained to assess the quality of generated images and mentions using LPIPS-based evaluation to assess diversity by clustering generated images relative to target samples. These assessments imply expert human involvement in designing and conducting these evaluations, as they involve training classifiers and defining metrics for adaptation analysis."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not explicitly provide annotation or evaluation instructions given to annotators or human evaluators for these metrics or classifier training."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "There are no details or mention of scoring rubrics provided for annotators or evaluators in the context of this dataset or evaluation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not include example annotations or outputs as part of annotation guidelines or evaluation instructions for the new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-19-rubric-2",
    "token_usage": {
      "prompt_tokens": 16206,
      "completion_tokens": 415,
      "total_tokens": 16621
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human annotator with subject matter expertise or membership in the target demographic. The dataset used are mainly publicly available few-shot samples, and no human annotation QA is reported."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple expert annotators performing quality assurance on dataset annotations or content. The datasets considered are pre-existing with small samples; no human expert QA process is mentioned."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance done by a single human without subject matter expertise. No such annotation or QA process is described."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe multiple non-expert human annotators performing quality assurance for any new dataset introduced."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 4.2 (Binary Classification for Quality Evaluation)",
          "reasoning": "The paper introduces a binary classifier trained to distinguish real images from source and target domains to assess the quality (realisticness) of generated images during GAN adaptation. This classifier is an AI model acting as a judge to provide quality assessment, functioning as an automated QA measure for generated image quality."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.3 (Intra-Cluster Diversity Evaluation)",
          "reasoning": "The paper uses an automated metric based on Learned Perceptual Image Patch Similarity (LPIPS) and intra-cluster LPIPS to quantitatively evaluate diversity degradation during adaptation. This metric is computed automatically over generated samples, serving as an automated quality assurance process to assess diversity quantitatively."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is a documented and described quality assurance process through automated AI model classifiers and automated diversity metrics. Hence, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-19-rubric-3",
    "token_usage": {
      "prompt_tokens": 15816,
      "completion_tokens": 575,
      "total_tokens": 16391
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.4 Experiment settings and various dataset descriptions (e.g., FFHQ, Sketches, Babies, Sunglasses)",
          "reasoning": "The paper uses several few-shot target domain datasets comprising images (e.g., Sketches, Babies, Sunglasses) which are originally collected and created by humans. Although the datasets like FFHQ and others are pre-existing, the few-shot subsets (e.g., 10-shot samples from these datasets) used for adaptation are selections of human-created original images. Hence, the target images represent original human-created content. The paper does not create new human-sourced images but uses a small set of images from existing human-created datasets as target domain data for few-shot GAN adaptation."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 4 & 5 (Analysis of adapted generators and proposed Dual Contrastive Learning approach)",
          "reasoning": "The paper produces new synthetic images generated by adapting pretrained GAN models to few-shot target domains via their proposed methods (e.g., Dual Contrastive Learning). These generated images are new data created entirely by AI models without direct reference or transformation of existing images at generation time. The synthetic images serve as novel data outputs of the adapted generative models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that any data was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data generated by machine translation in the context of this paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing datasets but does not simply collect or aggregate them as new datasets; rather, it utilizes them to adapt pretrained GAN models. There is no evidence that the paper introduces a new dataset collated from existing sources without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper derives synthetic images using adapted models, the paper does not present these as newly introduced datasets derived from existing sources with modification. The few-shot datasets are subsets of existing human-created datasets rather than derived versions."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources (existing human-created image datasets and synthetic images generated by models) are explicitly described or implied in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-19-rubric-4",
    "token_usage": {
      "prompt_tokens": 16334,
      "completion_tokens": 415,
      "total_tokens": 16749
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3 (Preliminary), Section 4 (Revisit Few-Shot GAN Adaptation)",
          "reasoning": "The few-shot target datasets introduced (e.g., 10-shot samples from Sketches, Babies, Sunglasses) are used to fine-tune pretrained GAN models via adaptation. The training involves fine-tuning the pretrained generator and discriminator on these small datasets to adapt to the target domain, which constitutes supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 (Binary Classification for Quality Evaluation), Section 4.3 (Intra-Cluster Diversity Evaluation), Section 6 (Experiments)",
          "reasoning": "The study uses the few-shot target domain datasets to evaluate the GAN adaptations in terms of quality and diversity measures. For example, a binary classifier trained with source and target domain images (excluding adaptation data) is used to assess generated image quality on the target domain. Quantitative metrics such as Fr\u00e9chet Inception Distance (FID) and intra-cluster LPIPS are applied to benchmark performance on these datasets."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Revisit Few-Shot GAN Adaptation), Section 4.5 (Empirical Analysis Results)",
          "reasoning": "The few-shot datasets are used to analyze trends and characteristics of GAN adaptation methods, such as the progression of image quality and diversity degradation during adaptation to the target domain. This analysis reveals insights into method performance and motivates the proposed approach."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-19-rubric-5",
    "token_usage": {
      "prompt_tokens": 17057,
      "completion_tokens": 538,
      "total_tokens": 17595
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not propose or introduce any dataset containing entries in more than two human languages. There is no mention of multiple human languages or multilingual content in the new datasets."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not propose or introduce any dataset containing exactly two human languages. No bilingual data is referenced or described in the new dataset."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper is written in English, the few-shot image generation task and datasets (e.g., FFHQ, Sketches, Babies, Sunglasses) are image datasets without linguistic content. Thus, no dataset entries contain English text content as part of the data samples."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset entries containing exactly one non-English human language. The few-shot target datasets relate to images and do not involve non-English language data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper describes algorithms, equations, and contrastive learning for GANs, it does not introduce any new dataset entries consisting of programming code or structured code-related content. The new dataset is image data for GAN adaptation, not code samples."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes mathematical expressions and loss functions as part of the methodology descriptions but does not describe new datasets containing mathematical or logical notation as dataset entries themselves. The new datasets are image data, not math notation entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper of any new dataset containing biological sequences or non-human communication systems. The datasets are image datasets for few-shot GAN adaptation."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed languages or fictional/artificial languages are included in the new datasets introduced in this paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language or communication mode of dataset entries is clearly non-linguistic image data; thus the language type is not unknown but not applicable."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new dataset introduced in the paper consists of images (FFHQ, Sketches, Babies, Sunglasses, and artistic paintings) without any language content. These are visual datasets with no linguistic entries, hence the dataset does not contain any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-19-rubric-6",
    "token_usage": {
      "prompt_tokens": 14275,
      "completion_tokens": 156,
      "total_tokens": 14431
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No mention in the paper",
          "reasoning": "The paper does not provide any link, repository, or explicit mention of code related to dataset construction or the data generation process. It describes evaluation metrics and experimental settings but does not mention releasing code or scripts for data handling or dataset construction."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No mention in the paper",
          "reasoning": "The paper does not describe creating any new datasets by the authors. Instead, it uses existing public datasets such as FFHQ, Sketches, Babies, Sunglasses, and others for adaptation experiments. The paper focuses on GAN adaptation methods rather than dataset construction or new dataset introduction, hence no documentation on dataset creation is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1903-rubric-0",
    "token_usage": {
      "prompt_tokens": 10651,
      "completion_tokens": 143,
      "total_tokens": 10794
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4, Experimental results, paragraph 4; Figures 1, 2, 5, 6",
          "Reasoning": "The authors introduce a new evaluation on various real-world internet videos selected by themselves featuring diverse human motions, as described in the qualitative evaluation section and shown in figures 1, 2, 5, and 6. These videos are human-generated data captured from real-world scenes as input for their reconstruction pipeline. The footage is not pre-existing published datasets but newly considered by the authors for testing the applicability of their method in unconstrained scenes."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1903-rubric-1",
    "token_usage": {
      "prompt_tokens": 11451,
      "completion_tokens": 243,
      "total_tokens": 11694
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3 (Our approach) and Section 4 (Experimental results)",
            "reasoning": "The paper describes a physics-based reconstruction pipeline that uses a physics engine (Bullet) and trajectory optimization to estimate 3D human pose from monocular video. The annotation of the new dataset is conducted using automatic physical simulation and optimization methods rather than by human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper.",
            "reasoning": "There is no explicit mention of human annotators receiving instructions for data annotation since the dataset annotations are generated automatically via the proposed physics-based optimization method."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper.",
            "reasoning": "No mention of scoring rubrics or annotation standards provided to annotators is found, as annotations are produced through an automatic process using the physics-based model."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper.",
            "reasoning": "No examples or annotation guidelines intended for human annotators are discussed, since the approach relies on automatic physics simulation and optimization to generate annotations."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1903-rubric-2",
    "token_usage": {
      "prompt_tokens": 12641,
      "completion_tokens": 305,
      "total_tokens": 12946
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process performed by a single human expert on dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information indicating that multiple human experts conducted quality assurance for the datasets introduced by the authors."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper about any single human non-expert performing quality assurance on dataset annotations or content."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by multiple human non-expert annotators regarding the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any AI model was used to perform quality assurance on dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of automated verification or algorithmic/rule-based quality assurance processes performed on any introduced dataset annotations or contents."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets with explicit annotation or quality assurance processes described. It mainly uses existing datasets (Human3.6M, HumanEva-I, AIST), and does not report performing or documenting any quality assurance on new dataset annotations or content."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1903-rubric-3",
    "token_usage": {
      "prompt_tokens": 12251,
      "completion_tokens": 454,
      "total_tokens": 12705
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the authors collected or created any new dataset from scratch by human contributors. The datasets evaluated (Human3.6M, HumanEva-I, AIST, and internet videos) are pre-existing, publicly available datasets or internet videos."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating new datasets by AI or machine learning models without reference to existing data. Instead, it uses existing datasets for evaluation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of data produced via human translation from other languages is mentioned."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of machine translation in producing dataset content is present."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the authors collected or aggregated data from existing sources to produce a new dataset."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4 (Experimental results, Datasets), Section 3.4 (Kinematic initialization, pose re-estimation), throughout",
          "reasoning": "The paper uses pre-existing datasets (Human3.6M, HumanEva-I, AIST, and internet videos) and derives new physically plausible motion reconstructions through their physics-based pose estimation method. This produces modified versions of existing data (reconstructed physical motions) rather than newly collected raw data, indicating the data is derived by applying transformations and estimations on existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method are clearly documented; the authors use existing pre-existing datasets and derive new motion reconstructions."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1903-rubric-4",
    "token_usage": {
      "prompt_tokens": 12769,
      "completion_tokens": 235,
      "total_tokens": 13004
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing datasets (Human3.6M, HumanEva-I, AIST) for evaluation and benchmarking, but does not introduce any new datasets for evaluation purposes."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. All datasets mentioned (Human3.6M, HumanEva-I, AIST) are pre-existing and only used for evaluation or qualitative demonstration. There is no practical usage described or demonstrated for a dataset introduced by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1903-rubric-5",
    "token_usage": {
      "prompt_tokens": 13492,
      "completion_tokens": 562,
      "total_tokens": 14054
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any introduced dataset containing entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset with exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4 Experimental results, Datasets; throughout the paper where datasets and results are discussed",
          "reasoning": "The paper introduces a dataset (or a validation subset) composed of video sequences and associated 2D and 3D annotations, all described in English. The dataset descriptions, annotations, and all textual content are in English only. The included video data, being visual, is language neutral, but the accompanying metadata and documentation appear exclusively in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets introduced with content solely in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses code implementations and algorithms but does not introduce a dataset composed of programming or code entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notation is used in paper explanations, but no new dataset entries contain mathematical or formal logical expressions as data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets comprising biological sequences or non-human communication are introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any datasets involving constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content in the dataset is specified and documented as English."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The main dataset consists of video frames and pose annotations represented as numerical data (e.g., 2D landmarks, 3D joint positions). These are primarily non-linguistic data forms; thus, from a strict definition, the dataset entries do not contain traditional language content."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1903-rubric-6",
    "token_usage": {
      "prompt_tokens": 10710,
      "completion_tokens": 190,
      "total_tokens": 10900
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or link indicates availability of code for dataset construction.",
          "reasoning": "The paper does not mention releasing any code or link related to the construction or preprocessing of datasets. The datasets used (Human3.6M, HumanEva-I, AIST, and internet videos) are either publicly available or existing datasets. There is no indication that the authors have introduced or constructed a new dataset nor provided code associated with dataset creation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section or appendix describes dataset creation process.",
          "reasoning": "The paper utilizes existing datasets for evaluation (Human3.6M, HumanEva-I, AIST) and some internet videos for qualitative evaluation. It does not introduce any new datasets nor does it describe in any part the process of dataset construction, acquisition, or preprocessing. The descriptions focus on methodology and evaluation using existing datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1925-rubric-0",
    "token_usage": {
      "prompt_tokens": 13632,
      "completion_tokens": 164,
      "total_tokens": 13796
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets, KITTI-STEP description",
          "Reasoning": "KITTI-STEP is described as a video panoptic segmentation dataset with annotations for KITTI-MOTS. The dataset consists of videos with semantic classes and tracking IDs, indicating human-captured video data with manual annotations."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets, VSPW description",
          "Reasoning": "VSPW is a large-scale video semantic segmentation dataset containing annotated videos with 124 semantic classes, provided for evaluating semantic segmentation over videos, thus human-generated video data with manual annotations."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1925-rubric-1",
    "token_usage": {
      "prompt_tokens": 14432,
      "completion_tokens": 311,
      "total_tokens": 14743
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.1 Datasets, specifically for KITTI-STEP and SemKITTI-DVPS",
            "reasoning": "The paper describes KITTI-STEP and SemKITTI-DVPS as datasets with semantic segmentation and tracking ID annotations. These datasets are new and involve panoptic segmentation and tracking annotations requiring expert knowledge. As such, annotation is typically performed by multiple human experts. The paper references prior works [73] and [63] describing these datasets, implying human expert annotations for these tasks."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper about annotation instructions for KITTI-STEP or SemKITTI-DVPS",
            "reasoning": "The paper does not provide explicit details on annotation instructions for KITTI-STEP or SemKITTI-DVPS, so we cannot confirm the presence of instructions in guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper about annotation scoring rubrics for KITTI-STEP or SemKITTI-DVPS",
            "reasoning": "The paper does not mention any scoring rubrics or evaluation guidelines provided to annotators for these new datasets."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper about examples given in annotation guidelines for KITTI-STEP or SemKITTI-DVPS",
            "reasoning": "The paper does not provide information regarding the inclusion of annotation examples in the guidelines for these datasets."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1925-rubric-2",
    "token_usage": {
      "prompt_tokens": 15622,
      "completion_tokens": 294,
      "total_tokens": 15916
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance of the dataset annotations was performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or mention in the paper about multiple human experts performing quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is provided that QA was conducted by a single non-expert human annotator for the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that AI models were used as judges for quality assurance of the dataset annotations in this paper."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automatic or algorithmic verification or QA process applied to the datasets introduced or used."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces the KITTI-STEP dataset as a recent dataset, but does not describe any quality assurance processes for any of the new datasets used, including KITTI-STEP. No QA process is documented or discussed, so it cannot be inferred."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1925-rubric-3",
    "token_usage": {
      "prompt_tokens": 15232,
      "completion_tokens": 430,
      "total_tokens": 15662
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the authors created any new datasets entirely from scratch by human annotators. Instead, it uses multiple existing datasets for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any new datasets generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was produced by human translation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets produced by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used for evaluation (KITTI-STEP, VSPW, YouTube-VIS, SemKITTI-DVPS) are referenced from existing works; there is no statement about collating or aggregating data for new dataset creation."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets, Para 1 and 4",
          "reasoning": "The paper uses KITTI-STEP and SemKITTI-DVPS datasets which are derived from existing datasets: KITTI-STEP annotates semantic segmentation additionally for KITTI-MOTS, and SemKITTI-DVPS is obtained by projecting 3D point cloud panoptic annotations of SemanticKITTI to 2D planes. Thus data is based on existing sources with modifications, fitting the 'Derived' category."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of data is documented and explained for the used datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1925-rubric-4",
    "token_usage": {
      "prompt_tokens": 15750,
      "completion_tokens": 518,
      "total_tokens": 16268
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using any newly introduced dataset exclusively for pre-training large models. Pre-training is performed on existing datasets like ImageNet-22k and COCO."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training models from randomly initialized parameters on the newly introduced dataset(s). It uses pretrained models and fine-tunes on the datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced in the paper are used as evaluation benchmarks or are mentioned as test sets. The paper uses them for training but starts from pretrained weights (on COCO, ImageNet, or Cityscapes), therefore not fine-tuning from a pretrained model on these new datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or mention of reinforcement learning or RL-based post-training methods applied on the introduced datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 4.3",
          "reasoning": "The newly introduced dataset KITTI-STEP is used primarily as a benchmark dataset to evaluate the proposed TubeFormer-DeepLab model's performance in video panoptic segmentation tasks, as reported in Section 4.1 and Section 4.3. The paper focuses on testing and benchmarking on KITTI-STEP for evaluation purposes."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention that the introduced dataset is used primarily for analyzing trends or characteristics rather than training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the dataset as a knowledge base or to augment models through retrieval or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes usage of the new dataset KITTI-STEP for evaluation of the proposed model."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1925-rubric-5",
    "token_usage": {
      "prompt_tokens": 16473,
      "completion_tokens": 435,
      "total_tokens": 16908
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not mention any dataset or data entries containing multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No dataset introduced contains exactly two human languages according to the paper."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The new dataset introduced, KITTI-STEP, as well as others mentioned (e.g., SemKITTI-DVPS, VSPW, YouTube-VIS), are technical video segmentation datasets labeled in English (e.g., semantic categories like 'pedestrians', 'cars'). The dataset labels and descriptions are in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that any introduced dataset contains only a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Although the paper describes model architectures and includes formulas, the datasets themselves do not contain entries with programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper includes mathematical notation to describe their method but the new datasets themselves do not contain mathematical or logical expressions as data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets introduced are visual/video datasets with human scene labels, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No constructed or fictional languages are mentioned to be part of the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "All datasets introduced specify their labeling and include English annotations, thus language is specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets clearly contain labels in human language (English), so data is not without language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1925-rubric-6",
    "token_usage": {
      "prompt_tokens": 13691,
      "completion_tokens": 155,
      "total_tokens": 13846
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not contain any mention of a public code repository or link to code related to dataset construction, collection, preprocessing, or generation. The only mention of code is related to baseline models or official codebases of prior works, but not new code for dataset creation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "",
          "reasoning": "The paper does not provide any detailed description or documentation on the construction or creation process of the new datasets (e.g., KITTI-STEP, SemKITTI-DVPS). It only references these datasets as existing works or newly introduced datasets with high-level descriptions of their content, without the methods or protocols used to create and annotate them."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1929-rubric-0",
    "token_usage": {
      "prompt_tokens": 9478,
      "completion_tokens": 115,
      "total_tokens": 9593
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1, Benchmark Dataset",
          "Reasoning": "The paper introduces experiments on the Kinetics-GEBD dataset, which is derived from the Kinetics-400 dataset, containing videos with event boundary annotations by human annotators. However, the paper does not introduce any new dataset created by the authors; they use Kinetics-GEBD, an existing dataset, hence no new dataset is introduced by the authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1929-rubric-1",
    "token_usage": {
      "prompt_tokens": 10278,
      "completion_tokens": 260,
      "total_tokens": 10538
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Abstract and Section 1 Introduction",
            "reasoning": "The paper mentions the Kinetics-GEBD dataset used for training and evaluation, stating that event boundary labels are annotated by 5 different annotators reflecting human perception. There is no indication the annotators are experts or involved detailed expertise beyond human perception of event boundaries."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 1 Introduction and citation [37]",
            "reasoning": "The paper states that the event boundary labels are annotated following instructions to find boundaries of one level deeper granularity compared to video-level events, reflecting natural human perception. This indicates instructions were provided to annotators for consistent labeling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described in main text or references to dataset",
            "reasoning": "There is no mention or description in the paper of scoring rubrics or annotator grading schemes used for the event boundary annotation in the new Kinetics-GEBD dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not described in the paper",
            "reasoning": "The paper does not provide or mention annotation examples or exemplar boundary labels to guide annotators for the GEBD dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1929-rubric-2",
    "token_usage": {
      "prompt_tokens": 11468,
      "completion_tokens": 379,
      "total_tokens": 11847
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that quality assurance of dataset annotations or content was performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 1 Introduction",
          "reasoning": "The paper states that the Kinetics-GEBD dataset has event boundary labels annotated by 5 different annotators, conveying subjectivity of human perception. This implies multiple human annotators were involved. Although the expertise level is not explicitly stated, the practice of using multiple annotators for human perception labeling suggests multiple human experts or at least multiple human annotators intended to represent the target demographic performed the labeling and thus quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention in the paper that quality assurance was performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that the annotators lacked subject matter expertise. Since the annotation task involves human perception of event boundaries, the annotators can be considered members of the target demographic or experts in perception labeling. Thus, this label does not apply."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that an AI model was used as a judge or for QA of the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic rule-based techniques being used as a quality assurance process for dataset annotations or content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance is described in terms of multiple annotators labeling the data, so QA process is documented, thus this does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1929-rubric-3",
    "token_usage": {
      "prompt_tokens": 11078,
      "completion_tokens": 489,
      "total_tokens": 11567
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors do not explicitly describe the creation of any new dataset or data that was collected or annotated by humans from scratch in this work. They use the existing Kinetics-GEBD dataset for experiments but do not indicate that they have created new human-annotated data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset generated purely by AI or machine learning models. Instead, the work focuses on methods to detect generic event boundaries using existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by human translation from another language in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation data generation or translation processes in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Benchmark Dataset",
          "reasoning": "The authors use the Kinetics-GEBD dataset which consists of videos from the pre-existing Kinetics-400 dataset. They use these existing videos and their human-annotated event boundaries. Since the original Kinetics dataset is an established dataset and Kinetics-GEBD is derived from it, the data is aggregated from existing sources without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors use features extracted by pretrained models (e.g., ResNet-50 pretrained on ImageNet) and perform transformations for their method, there is no description of a new dataset created by applying modifications or adaptations to existing data. Their contributions are methodological rather than new data derivations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and dataset used are specified (Kinetics-GEBD, derived from Kinetics-400), so this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1929-rubric-4",
    "token_usage": {
      "prompt_tokens": 11596,
      "completion_tokens": 293,
      "total_tokens": 11889
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.3, 4.3, and Table 1 (Supervised)",
          "reasoning": "The paper demonstrates supervised training of the proposed model using human-annotated ground truth labels from the Kinetics-GEBD dataset, applying supervised losses such as binary cross-entropy (BCE) combined with the BoCo loss, leading to state-of-the-art performance improvements."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 (Benchmark Dataset), 4.3 (Results on Kinetics-GEBD), Table 1",
          "reasoning": "The Kinetics-GEBD dataset is utilized as a benchmark to evaluate and compare performance of the proposed unsupervised and supervised models, including ablation studies and quantitative metrics like F1@0.05 scores."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1929-rubric-5",
    "token_usage": {
      "prompt_tokens": 12319,
      "completion_tokens": 514,
      "total_tokens": 12833
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Benchmark Dataset",
          "reasoning": "The paper evaluates their model on the Kinetics-GEBD dataset, which consists of video data and annotations. The annotations and all accompanying textual descriptions, instructions, and labels are presented solely in English, as evidenced by the use of English throughout the paper. There is no mention of any other human languages present in the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset entries contain programming or structured code content. The paper discusses algorithms and models, but the dataset itself contains videos and human annotations, not code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper contains mathematical formulas and notation in description of methods, these are not part of the dataset entries themselves. The dataset consists of video frames and human annotations, not math or logic notation entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain any biological sequences or non-human communication content, only human video data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of any fictional or artificially created languages in the dataset entries."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is clearly English as per the paper, so this label does not apply."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Since the dataset contains video and annotations with English language descriptions, it is not devoid of language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1929-rubric-6",
    "token_usage": {
      "prompt_tokens": 9537,
      "completion_tokens": 123,
      "total_tokens": 9660
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not mention any code release or provide links to code repositories related to dataset construction or processing. The focus is on the model and its training, but there is no indication of publicly available code for dataset-related processes."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "None",
          "reasoning": "The paper uses the existing Kinetics-GEBD dataset and does not introduce any new dataset by the authors. Hence, there is no description or documentation of dataset creation or construction process within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1962-rubric-0",
    "token_usage": {
      "prompt_tokens": 10348,
      "completion_tokens": 240,
      "total_tokens": 10588
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.4 Further Extension and Section 4 Experimental Results: Datasets",
          "Reasoning": "The paper introduces and uses a new scene2cartoon dataset consisting of 5100 landscape photos and 5100 cartoon animation scenes collected from the dataset proposed by [31]. These images are real photos and cartoons collected by the authors for the task, therefore human-generated. This dataset is distinct from and newly constructed for the work, as described under 'Datasets' in Section 4."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.4 Further Extension",
          "Reasoning": "The paper introduces an auxiliary dataset as a new class c_new consisting of cartoon portraits containing diverse character characteristics and different facial expressions in a similar cartoon style. It is used to address dynamic facial expressions for video synthesis of cartoon portraits in Section 3.4. This dataset is newly introduced by the authors and consists of human-generated cartoon portrait images created or collected manually as stated in Section 3.4."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1962-rubric-1",
    "token_usage": {
      "prompt_tokens": 11148,
      "completion_tokens": 230,
      "total_tokens": 11378
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.4, Section 4",
            "reasoning": "The paper states that they introduced a new auxiliary dataset containing cartoon portraits with diverse character characteristics and facial expressions as a new class c_new, added to the original dataset for video synthesis (Section 3.4). This implies data collection and construction as a process rather than human annotation. Also, no mention of any human annotators describing the data annotations for the new dataset is made in the paper."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "The paper does not describe any annotation instructions given to annotators for the new auxiliary dataset added in Section 3.4."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "No scoring rubrics or evaluation criteria for annotations related to the new auxiliary dataset are described in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "The paper does not provide annotation examples or guidelines for labeling or constructing the new auxiliary dataset introduced for video synthesis."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1962-rubric-2",
    "token_usage": {
      "prompt_tokens": 12338,
      "completion_tokens": 278,
      "total_tokens": 12616
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance being conducted by a single human expert for the datasets used or introduced."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information suggesting that multiple human experts performed quality assurance on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA process conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no documentation or description of quality assurance performed by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that an AI model was used to perform quality assurance on the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated verification or algorithmic rule-based QA processes applied to dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document any quality assurance process applied to the datasets introduced or used. The description of the datasets (selfie2anime and the collected scene2cartoon datasets) does not mention any form of annotation verification or quality control measures."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1962-rubric-3",
    "token_usage": {
      "prompt_tokens": 11948,
      "completion_tokens": 439,
      "total_tokens": 12387
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the authors created any original data entirely from scratch by human contributors. Instead, they rely on existing datasets for training and evaluation."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 4.2 - Video Synthesis of Cartoon Portraits",
          "reasoning": "The authors extend their model to generate cartoon video portraits with controllable facial expressions. These videos are generated entirely by the proposed model using learned style codes and auxiliary data, representing newly generated data by AI without referencing or transforming existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was produced by human translators converting content from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced via machine translation from other languages."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 - Experimental Results, 'Datasets' subsection",
          "reasoning": "The authors use existing datasets such as selfie2anime and an animation scene dataset [31], collecting and combining these for training. For the scene data, they collect landscape photos and animation scenes from previous datasets without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.4 - Further Extension",
          "reasoning": "The authors introduce an auxiliary dataset of cartoon portraits with diverse facial expressions to extend original selfie2anime data, effectively modifying and adapting existing data to better train the video synthesis model."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents all dataset sources and data generation mechanisms clearly."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1962-rubric-4",
    "token_usage": {
      "prompt_tokens": 12466,
      "completion_tokens": 492,
      "total_tokens": 12958
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using any newly introduced dataset exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4, Datasets and Implementation details",
          "reasoning": "The paper introduces a new scene2cartoon dataset (collecting 5100 landscape photos and 5100 cartoon scenes) that is combined with selfie2anime data to form a mixed photo2cartoon dataset used to train the proposed model from scratch. The datasets are explicitly used to train their model for image-to-image translation tasks."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fine-tuning a pre-trained model with the introduced datasets using supervised learning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any reinforcement learning or RL-based methods applied using the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and Table 1",
          "reasoning": "The introduced scene2cartoon dataset along with selfie2anime dataset are also used for evaluation purposes by partitioning into training and test sets. Performance metrics such as FID and KID are computed on the test splits to benchmark and measure the quality of the generated cartoon images."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not primarily utilize the datasets for analysis of trends or characteristics outside of training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as external knowledge bases to augment models via retrieval or other methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The new dataset is used actively for training and evaluation as described."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1962-rubric-5",
    "token_usage": {
      "prompt_tokens": 13189,
      "completion_tokens": 508,
      "total_tokens": 13697
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's newly introduced datasets consist of images categorized into photo and cartoon domains with sub-groups of portrait and scene images, but there is no indication of any textual language content or presence of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information suggests the datasets contain exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of image data only (photo and cartoon images) and do not contain English language textual content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not contain textual content in any non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets comprise images of photos and cartoons; although code is mentioned in the method descriptions and implementation details, this is not part of the dataset content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes mathematical expressions to describe loss functions and network architecture, these are not components of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data are present in the datasets described."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not include fictional or constructed language content."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly defines the nature of the datasets and does not leave the language content unspecified or undocumented."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The newly introduced datasets consist solely of image data (populations of photos and cartoons) without any language content embedded in the data entries themselves."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1962-rubric-6",
    "token_usage": {
      "prompt_tokens": 10407,
      "completion_tokens": 154,
      "total_tokens": 10561
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section mentions code availability or links.",
          "reasoning": "The paper does not provide any link or reference to code repositories for dataset construction or preprocessing. No indication is given that code is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 (Experimental Results) and Datasets subsection",
          "reasoning": "The paper documents the datasets used by describing the selfie2anime dataset for portraits and a constructed scene2cartoon dataset for scenes, including their sizes, sources, and split into training/testing sets. Although the scene2cartoon dataset is constructed by collecting images from an existing dataset, the process is described and statistics are provided, constituting some documentation of dataset creation."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1976-rubric-0",
    "token_usage": {
      "prompt_tokens": 12119,
      "completion_tokens": 368,
      "total_tokens": 12487
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.3, Section 3.4",
          "Reasoning": "The VLParse dataset uses images inherited from the MSCOCO dataset (Section 3.4), which are human-generated natural scene images collected by humans. The dataset formation involves human refinement via crowd-sourcing on Amazon Mechanical Turk to annotate and refine the joint vision-language structures for these images (Section 3.3)."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.3, Section 3.4",
          "Reasoning": "The textual descriptions (captions) associated with images in VLParse come from human-generated captions in the MSCOCO dataset (Section 3.4). The joint vision-language structure includes linguistic dependency trees based on these captions, which are refined by human annotators during the crowd-sourcing stage (Section 3.3)."
        },
        {
          "Modality": "graph",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1, Section 3.2, Section 3.3",
          "Reasoning": "The VLParse introduces a new joint vision-language (VL) structure represented as aligned scene graphs (SG) from images and dependency trees (DT) from captions (Section 3.1). The initial SG and DT structures are generated via automatic rule-based alignment and dependency parsing methods (Model Generated) (Section 3.2). Then human annotators refine these automatic annotations through crowd-sourcing (Human Generated) to produce high-quality VL structures (Section 3.3). The final joint graph structures thus have a mixed origin from model-generated initial structures refined by humans."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1976-rubric-1",
    "token_usage": {
      "prompt_tokens": 12919,
      "completion_tokens": 248,
      "total_tokens": 13167
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.3",
            "reasoning": "Section 3.3 describes a crowd-sourcing human refinement stage using Amazon Mechanical Turk (AMT) with remotely hired native speakers performing a survey to assess and refine automatically generated annotations. The paper mentions multiple workers (at least two per sample) and majority vote to produce final annotations, indicating Multiple Human Non-Experts."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3",
            "reasoning": "Section 3.3 states that workers were provided with comprehensive instructions and a set of well-defined examples to judge and modify the alignment quality, indicating instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "There is no mention of any formal scoring rubrics or scoring criteria used to evaluate or score annotations. The human refinement relies on instructions and examples but not explicit rubrics."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.3",
            "reasoning": "Section 3.3 explicitly states that annotators were provided a set of well-defined examples during annotation for guidance."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1976-rubric-2",
    "token_usage": {
      "prompt_tokens": 14109,
      "completion_tokens": 422,
      "total_tokens": 14531
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the quality assurance was performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that multiple human experts with subject matter expertise were involved in the quality assurance process."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.3 (Crowd-Sourcing Human Refinement)",
          "reasoning": "The dataset's annotations were refined via a crowdsourcing process on Amazon Mechanical Turk involving multiple native speakers who are generally non-experts. The paper explains that multiple non-expert annotators checked and corrected automatic annotations and disagreements were resolved by a third party, indicating multiple non-expert human involvement in QA."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that an AI model was used as a quality assurance judge for the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 (Automatic Rule-based Alignment)",
          "reasoning": "The dataset creation includes an automatic rule-based alignment procedure using designed rules and similarity calculations to generate initial annotations. This automated procedure is part of the quality assurance in producing coarse structures before human refinement."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes quality assurance processes, including an automatic procedure followed by human refinement, so QA is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1976-rubric-3",
    "token_usage": {
      "prompt_tokens": 13719,
      "completion_tokens": 502,
      "total_tokens": 14221
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.3 and 3.4",
          "reasoning": "The VLParse dataset is created through a semi-automatic pipeline that includes human refinement via crowd-sourcing on Amazon Mechanical Turk. Human annotators refine the automatically generated alignments between visual scene graphs and linguistic dependency trees, ensuring high-quality, manually reviewed data that is original and not simply adapted or translated from existing material."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not generated entirely by AI or machine learning models without reference to existing data; rather, models are used for parsing but the final datasets include human refinement."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset or any part of it is produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to generate the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.4",
          "reasoning": "The dataset is based on existing datasets (MSCOCO and Visual Genome) whose images and captions are used as the source. The authors aggregate these and map concepts between the visual and language structured representations. This constitutes collation of existing data sources without significant modification of the original images or captions."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 and 3.3",
          "reasoning": "The VLParse dataset is derived by applying an automatic rule-based alignment procedure that rewrites dependency trees and aligns them to scene graphs, followed by human refinement. This process involves transformations, adaptations and enhancements of existing data (MSCOCO images, captions, Visual Genome annotations) to construct the joint vision-language structure dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The authors explicitly describe the data origin and generation process; therefore, no need to mark as unknown."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1976-rubric-4",
    "token_usage": {
      "prompt_tokens": 14237,
      "completion_tokens": 488,
      "total_tokens": 14725
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 6.1 Setup and Section 5 Vision-Language Graph Auto-Encoder",
          "reasoning": "The VLParse dataset is used to train the Vision-Language Graph Autoencoder (VLGAE) model in an unsupervised manner starting from feature extraction; the model learns joint VL structures from image-caption pairs without annotations, indicating training from scratch rather than fine-tuning pre-trained models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe supervised fine-tuning of pre-trained models on the proposed dataset; the dataset is used primarily for unsupervised training."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning or related post-training methods are described using the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 Evaluation Metrics and Section 6 Experiments",
          "reasoning": "The dataset is extensively used to evaluate models on derived tasks such as language dependency parsing and weakly-supervised visual phrase grounding through specific metrics, demonstrating its role for benchmarking."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.4 Dataset Analysis and Section 6.4 Ablation Analysis",
          "reasoning": "The dataset is analyzed for instance counts, human performance, and ablation on arc length, indicating usage for analyzing dataset characteristics and model behavior."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described or used as a knowledge base for augmenting models such as via retrieval."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is clearly used for training, evaluation, and analysis as given in multiple sections of the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1976-rubric-5",
    "token_usage": {
      "prompt_tokens": 14960,
      "completion_tokens": 253,
      "total_tokens": 15213
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.4 Dataset Analysis",
          "reasoning": "The dataset VLParse is constructed based on MSCOCO and Visual Genome datasets, both of which contain English captions. The paper describes language dependency parsing using English sentences and provides dependency trees annotated with Stanford typed dependencies which is English-based. No indication of other languages is given, so the dataset contains only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1976-rubric-6",
    "token_usage": {
      "prompt_tokens": 12178,
      "completion_tokens": 144,
      "total_tokens": 12322
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 1",
          "reasoning": "The paper provides a URL (https://github.com/bigai-research/VLGAE) in the abstract and introduction sections, indicating that code related to their approach and dataset creation is publicly available in an accessible repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (The VLParse Dataset)",
          "reasoning": "The paper contains a detailed description of the new dataset VLParse including the joint vision-language structure formalization, the two-step automatic rule-based alignment procedure, and a human refinement process via crowdsourcing in Section 3, thoroughly documenting the dataset creation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1983-rubric-0",
    "token_usage": {
      "prompt_tokens": 10366,
      "completion_tokens": 287,
      "total_tokens": 10653
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.3 Experimental data; Abstract; Section 5 Conclusion",
          "Reasoning": "The paper introduces an experimental dataset obtained from a vortex ring flow in air, recorded using two synchronized 1 Mpx high-speed cameras with an exposure time of 1 ms. The images capture helium-filled soap bubbles as tracer particles, visualized with known experimental setup and camera settings. This dataset is explicitly mentioned as experimental data introduced by the authors to validate their method (Section 4.3 Experimental data). The capture of images by humans operating cameras makes it human-generated, not model generated, and provenance is clearly stated."
        },
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Synthetic data - model validation; Section 4.2 Synthetic data - reconstruction validation",
          "Reasoning": "The paper introduces synthetic image datasets generated by the authors for validation purposes. These consist of synthetic images of particles and streak signatures with positions uniformly or randomly distributed, and simulated streaks reflecting modeled particle displacements under various assumptions. Additionally, synthetic conic section segment images are generated by the authors to evaluate their conic fit and reconstruction methods. These datasets are explicitly introduced as synthetic and generated by simulation, thus model generated, not human recorded or unknown."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1983-rubric-1",
    "token_usage": {
      "prompt_tokens": 11166,
      "completion_tokens": 235,
      "total_tokens": 11401
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.3 Experimental data; Section 3.3 Conic section reconstruction",
            "reasoning": "The paper describes the use of a CNN-based instance segmentation method to detect curved streaks and reconstruct their 3D positions by fitting conic sections using an optimization approach solved via a non-linear solver. These methods imply expert involvement in annotation related to detection and verification of streaks for the new dataset acquired in experiments."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not provide any explicit detailed annotation instructions for human annotators involved in labeling or verifying data in the newly introduced experimental dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No scoring rubrics or formal grading criteria for annotations are described within the paper regarding the data annotation process for the new experimental dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not include examples or exemplars of annotated data or annotation outputs for the new dataset within the main text or supplementary material."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-1983-rubric-2",
    "token_usage": {
      "prompt_tokens": 12356,
      "completion_tokens": 324,
      "total_tokens": 12680
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or indication that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of quality assurance by a single human non-expert is found in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance conducted by multiple non-expert humans."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used to judge or perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.1 (Synthetic data - model validation) and Section 4.2 (Synthetic data - reconstruction validation)",
          "reasoning": "The authors validate their theoretical models against simulation results using synthetic datasets where particle and streak positions are generated and matched algorithmically, indicating that quality assurance (validation) is conducted through automated verification of code and formulas. The approach includes RANSAC-based and optimization-based methods, and the comparison of theoretical and simulated results constitutes an automated verification process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents validation of the synthetic datasets and matching algorithms through simulations and experimental data, thus some form of quality assurance is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1983-rubric-3",
    "token_usage": {
      "prompt_tokens": 11966,
      "completion_tokens": 528,
      "total_tokens": 12494
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.3 Experimental data",
          "reasoning": "The paper describes experimental data obtained from recording a vortex ring in air with helium-filled soap bubbles as tracers. Two synchronized 1 Megapixel high-speed cameras recorded images at 1000 fps with a 1 ms exposure time. This experimental dataset is newly collected by the authors specifically for this study, thus it is original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 4.1 Synthetic data - model validation; Section 4.2 Synthetic data - reconstruction validation; Section 4.2.2 Hill\u2019s spherical vortex",
          "reasoning": "The paper uses synthetically generated datasets to validate their theoretical models and reconstruction methods. These synthetic datasets consist of particles and streaks generated with specified properties (e.g., uniform distribution, noise addition) and a synthetic Hill\u2019s spherical vortex flow field. These datasets are generated computationally by the authors for validation purposes and are original content from models without relying on existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data were produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data were generated by translating content from another language using machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are experimental or synthetic and not collected or aggregated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.2.2 Hill\u2019s spherical vortex",
          "reasoning": "The Hill\u2019s spherical vortex dataset is a synthetic flow field based on a classic established fluid dynamics model, which the authors simulate and generate synthetic particle and streak images from. This dataset is derived by applying modifications and transformations based on existing theoretical flow formulations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origins and generation methods are clearly stated and documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1983-rubric-4",
    "token_usage": {
      "prompt_tokens": 12484,
      "completion_tokens": 364,
      "total_tokens": 12848
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 4.1, 4.2, 4.3",
          "reasoning": "The synthetic datasets of particle streak and conic section projections, as well as experimental data (e.g., vortex ring), are used for validation and benchmarking of the proposed 3D-PSV and conic section reconstruction methods, comparing the number of ghost reconstructions to existing state-of-the-art software."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The paper introduces theoretical models estimating the expected number of ghost streaks and curved ghost streaks under varying particle displacement scenarios, analyzing how the new dataset characteristics influence reconstruction ambiguities, thus using the datasets primarily to analyze trends and patterns."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1983-rubric-5",
    "token_usage": {
      "prompt_tokens": 13207,
      "completion_tokens": 439,
      "total_tokens": 13646
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Entire paper",
          "reasoning": "The datasets introduced in the paper consist of synthetic and experimental flow velocimetry data described and documented solely in English. The paper uses English exclusively to describe the data, methods, results, and explanations, with no mention or inclusion of any other human language in the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper describes mathematical formulations and optimization methods (e.g., using IPOPT with CasADi), there is no mention or inclusion of programming code as part of a new dataset. The datasets introduced are flow measurement data rather than code samples."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Sections 3.1, 3.2, 3.2.4, 3.3, 4",
          "reasoning": "The datasets and methods are described using extensive mathematical equations, models, and formal logic related to probability of ghost particle generation, conic section reconstruction, and epipolar constraints. These mathematical notations are integral to the data description and analysis."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets concern imaging of tracer particles in fluid flow; there is no indication of biological sequences or non-human communication systems being part of the datasets."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are mentioned or included in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of dataset entries are fully documented as English with mathematical notation; there is no ambiguity."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain image data and associated annotations described in English and mathematics, thus they do contain language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-1983-rubric-6",
    "token_usage": {
      "prompt_tokens": 10425,
      "completion_tokens": 152,
      "total_tokens": 10577
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "-",
          "reasoning": "The paper does not mention or provide any link to code repositories or software implementations related to the dataset construction, data processing, or methods used for generating the datasets. No code availability statements are present."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 (Results) and Section 4.3 (Experimental data)",
          "reasoning": "The paper thoroughly documents the dataset creation and evaluation process, including synthetic dataset generation parameters, synthetic flow data used for validation, and details of the experimental setup such as camera specifications, frame rate, exposure time, and tracer particle seeding. These provide transparency and sufficient detail for reproducing data generation and evaluation."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-2001-rubric-0",
    "token_usage": {
      "prompt_tokens": 13547,
      "completion_tokens": 145,
      "total_tokens": 13692
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 (Experiment Setup) and throughout the paper",
          "Reasoning": "The new dataset introduced and used for experiments is KITTI-STEP, which consists of real-world video sequences with annotated panoptic segmentation and tracking data. The data is recorded from real scenes (e.g., street scenes) and captured by cameras, thus it is human-generated video data. The paper states KITTI-STEP contains 21 training sequences and 29 testing sequences, split further for validation. The paper does not mention any simulation or synthetic generation for this dataset, confirming human-operated camera capture."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-2001-rubric-1",
    "token_usage": {
      "prompt_tokens": 14347,
      "completion_tokens": 398,
      "total_tokens": 14745
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.1 Dataset; Section 4.2 Main Results",
            "reasoning": "The datasets KITTI-STEP, Cityscapes-VPS, and VSPW are video segmentation datasets introduced or used in this paper. These datasets involve human annotations for panoptic, semantic, and instance segmentation tasks as they require ground truth labels for training and evaluation. The paper uses these datasets but provides no indication that any new automated or model-based annotation was performed. Thus, the annotation is done by multiple human experts who created the datasets."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.1 Dataset; Supplementary materials (not explicitly stated but typical for datasets like KITTI-STEP and Cityscapes-VPS)",
            "reasoning": "The paper references KITTI-STEP and Cityscapes-VPS datasets, which are well-known datasets created with detailed annotation protocols. Although the paper does not elaborate on annotation instructions explicitly, such datasets generally come with annotation guidelines\u2014thus it is reasonable and standard practice that instructions were provided to annotators during dataset creation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4.1 Dataset; Dataset original sources [57],[22]",
            "reasoning": "The evaluation metrics and splitting of annotations (thing/stuff categories, segmentation quality) imply that annotation rubric exists to maintain quality and consistency. Original dataset papers [57],[22] focus on rigorous metrics and quality control indicating rubric-guided annotation."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Original dataset publications referenced (e.g., [22], [57]); not explicitly detailed in this paper",
            "reasoning": "While this paper does not explicitly mention examples, standard dataset creation protocols include providing annotators with example annotations to ensure consistency and quality. The referenced datasets are standard in the field, so such examples are expected as part of their annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-2001-rubric-2",
    "token_usage": {
      "prompt_tokens": 15537,
      "completion_tokens": 309,
      "total_tokens": 15846
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about quality assurance performed by a single human annotator who is a subject matter expert or a member of the target demographic for dataset annotation validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper indicating that multiple expert annotators performed quality assurance on any dataset introduced by the authors."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance by a single human annotator without expertise."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is given about multiple non-expert human annotators conducting quality assurance for any datasets in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of AI models as judges for quality assurance of datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated or rule-based verification process for dataset annotation quality assurance."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets nor describes any quality assurance process for datasets. All datasets used (KITTI-STEP, Cityscapes-VPS, VSPW, YouTube-VIS) are pre-existing. Therefore, no QA process is applied or documented for new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-2001-rubric-3",
    "token_usage": {
      "prompt_tokens": 15147,
      "completion_tokens": 321,
      "total_tokens": 15468
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Experiment Setup - Dataset",
          "reasoning": "The paper uses pre-existing datasets such as KITTI-STEP, Cityscapes-VPS, and VSPW for evaluation. These datasets are well-known and publicly available, thus the data is collected from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Experiment Setup - Dataset",
          "reasoning": "While the authors do not introduce new datasets, they adapt the Cityscapes dataset class distribution for KITTI-STEP pretraining to avoid overfitting, indicating derived data usage by modifying existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources are specified and documented, so N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-2001-rubric-4",
    "token_usage": {
      "prompt_tokens": 15665,
      "completion_tokens": 368,
      "total_tokens": 16033
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation Details",
          "reasoning": "The paper describes pretraining the image baseline K-Net on Cityscapes dataset and using the KITTI-STEP training set split to further train the Video K-Net model from the pretrained parameters, indicating use of these datasets for supervised training."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation Details",
          "reasoning": "The datasets KITTI-STEP, Cityscapes-VPS, and VSPW are used for supervised fine-tuning of the model, as evidenced by detailed experimental results presented in Section 4, where models pretrained on Cityscapes are fine-tuned on these datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset and Section 4.2 Main Results",
          "reasoning": "The new datasets KITTI-STEP and Cityscapes-VPS are used extensively for evaluation and benchmarking the performance of Video K-Net and comparing with other methods, using metrics such as STQ and VPQ."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3 Ablation Study and Section 4.4 Visualization and More Analysis",
          "reasoning": "The datasets are used to analyze the effects of various components of the method through ablation studies and visualization, hence facilitating analysis of model behavior and trends."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-2001-rubric-5",
    "token_usage": {
      "prompt_tokens": 16388,
      "completion_tokens": 610,
      "total_tokens": 16998
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses video segmentation datasets, primarily involving image and video data of urban scenes and objects, with no mention of multiple human languages present in the dataset entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication or statement in the paper that the dataset entries contain exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 (Experiment Setup), various dataset descriptions throughout the paper",
          "reasoning": "The datasets used (KITTI-STEP, Cityscapes-VPS, VSPW, YouTube-VIS) are based on urban scenes and videos collected primarily from English-speaking or international contexts; all paper text and dataset annotations are in English. There is no mention of other languages in dataset annotations, so the dataset is considered monolingual with English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets annotated or recorded in only a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes descriptions of algorithmic methods, code snippets, and pseudocode might be in the appendix, the datasets themselves do not include structured code or programming language data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets themselves are video/image datasets; although the paper uses mathematical notation to describe methods and loss functions, the datasets do not contain mathematical or logical notation as part of their entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of urban scenes and videos with humans, vehicles, and background stuff; no biological sequence or non-human communication data is involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence or mention of fictional or constructed languages in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the data entries is explicitly English as per dataset descriptions and commonly known usage; it is not unknown or unspecified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain natural language annotations (English) for segmentation classes and tracking identities, so the language attribute is applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-2001-rubric-6",
    "token_usage": {
      "prompt_tokens": 13606,
      "completion_tokens": 147,
      "total_tokens": 13753
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not provide any link, URL, or reference to code repositories for the datasets used or constructed. There is no mention of releasing code related to dataset construction or preprocessing."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "None",
          "reasoning": "The paper does not introduce or describe any new datasets constructed by the authors, nor does it provide documentation on dataset creation processes. The datasets used (e.g., KITTI-STEP, Cityscapes-VPS, VSPW, YouTube-VIS) are pre-existing datasets referenced from other works. Hence, there is no dataset creation documentation provided in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-202-rubric-0",
    "token_usage": {
      "prompt_tokens": 10561,
      "completion_tokens": 139,
      "total_tokens": 10700
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets",
          "Reasoning": "The authors introduced a new synthetic dataset created by animating and rendering 3D scans of clothed humans using Blender Cycles. The raw data originates from 1000 human scans ('watertight meshes wearing a variety of garments') which were rigged and animated through non-rigid registration, and rendered into RGB images with ground truth UV correspondences. Thus, the final dataset consists of rendered images which are model generated but based on human scanned raw data, indicating both human and model generation involvement."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-202-rubric-1",
    "token_usage": {
      "prompt_tokens": 11361,
      "completion_tokens": 241,
      "total_tokens": 11602
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.6 Training Details; Section 4 Experiments - Datasets",
            "reasoning": "The new dataset introduced is a synthetic dataset created from animated 3D clothed human scans combined with non-rigid registration, rigging, animation sequences, and 3D rendering. Ground-truth dense correspondences are generated automatically during rendering; hence the annotations are obtained through a synthetic, automated process rather than human labelers."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No mention in the paper of any instructions provided for annotators is made because the annotations are generated in a synthetic automatic process, not by human annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics are provided as the annotation is an automatic synthetic rendering process that does not involve subjective scoring or evaluation by humans."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No examples of annotations for human annotators are described since annotations are generated automatically by rendering synthetic data with known ground-truth."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-202-rubric-2",
    "token_usage": {
      "prompt_tokens": 12551,
      "completion_tokens": 478,
      "total_tokens": 13029
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert or annotator for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts were involved in the quality assurance or annotation process of the datasets introduced by the authors."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that multiple non-expert human annotators performed quality assurance on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of an AI model specifically for quality assurance or validation of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1, 3.6, and Datasets in Section 4",
          "reasoning": "Quality assurance of the dataset is inherently tied to the synthetic data generation process, which involves rendering 3D scans with known ground-truth correspondences via coloring schemes on the 3D model surface to produce dense correspondence annotations. These annotations are automatically extracted without manual intervention, implying an automatic verification and generation process. Furthermore, during training, pseudo ground truth annotations for real images (DensePose-COCO) are generated automatically on the fly by extrapolating sparse ground-truth and CSE initialization to produce dense supervision. This synthetic dataset construction and pseudo-label generation serve as automated quality assurance mechanisms for the dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes processes for dataset generation and annotation that imply a form of quality assurance; thus, QA is documented and practiced."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-202-rubric-3",
    "token_usage": {
      "prompt_tokens": 12161,
      "completion_tokens": 474,
      "total_tokens": 12635
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.6 Training Details and Section 4 Experiments - Datasets",
          "reasoning": "The authors created a synthetic dataset by non-rigidly registering, rigging, and animating 3D human scans from the RenderPeople dataset, rendering these from multiple views to generate new RGB sequences and ground-truth dense correspondences for training. This constitutes original content created by human contributors involving scanning, rigging, animation, and rendering, rather than mere reuse or adaptation of existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data is reported as being generated entirely by AI or machine learning models without reference to existing data. The synthetic dataset is created via human-constructed 3D scans and animations, not by generative models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data translated by human translators from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of any data generated by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not merely collected or aggregated from existing sources without modification; rather, it involves processing existing scans through rigging and animation to generate new training data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.6 Training Details",
          "reasoning": "The authors fine-tune their model on the DensePose-COCO dataset using pseudo ground-truth labels generated by extrapolating sparse annotations and applying the Continuous Surface Embeddings initialization. This process creates derived training data produced by transforming existing sparse annotations into denser supervision for fine-tuning."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly specified and documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-202-rubric-4",
    "token_usage": {
      "prompt_tokens": 12679,
      "completion_tokens": 539,
      "total_tokens": 13218
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.6 Training Details",
          "reasoning": "The synthetic dataset created from animated 3D clothed human scans is used as the primary source of data for initial training of the BodyMap network. This training involves learning dense surface correspondences from scratch using the synthetic data before fine-tuning on real datasets."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.6 Training Details",
          "reasoning": "The synthetic dataset is used to train the BodyMap model from scratch, as evidenced by the description that the model is first trained on synthetic data prior to fine-tuning on real datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.6 Training Details",
          "reasoning": "After initial training on synthetic data, the method is fine-tuned on DensePose-COCO real data using generated pseudo ground-truth dense annotations, which constitute supervised fine-tuning leveraging the real dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any reinforcement learning based post-training methods or usage of the dataset in RL frameworks."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Experiments, subsection Datasets and Baselines and Metrics",
          "reasoning": "The synthetic dataset is split into train/test and is used in evaluation to quantitatively assess the performance of BodyMap in both 2D and 3D correspondence accuracy metrics, in comparison to other methods."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report usage of the synthetic dataset primarily for analysis of trends or characteristics beyond training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described or used as a knowledge base or for retrieval-augmented generation; its role is training and evaluation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The synthetic dataset is used in training from scratch, supervised fine-tuning, and evaluation, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-202-rubric-5",
    "token_usage": {
      "prompt_tokens": 13402,
      "completion_tokens": 427,
      "total_tokens": 13829
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing entries with multiple (more than two) human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that the dataset involves exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, including Abstract, Introduction, Section 3, and Experiments sections",
          "reasoning": "The new dataset introduced is primarily composed of synthetic images and real images annotated in English, with no mention of other natural languages. The paper is written in English and the datasets are described in English terms with no indication of additional human languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains entries in any single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the method involves code and programming (e.g., neural networks), the dataset itself is composed of image data and annotations, not programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset itself does not contain mathematical or logical expressions; such notation is present in the paper text describing losses and methodologies, not in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains synthetic and real human images, with no biological sequence data or non-human communication content."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are present in the dataset or discussed in relation to the data."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language is clearly English; there is no ambiguity about the language of the dataset entries."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains images and corresponding annotations described in English, hence language is present in the dataset description."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-202-rubric-6",
    "token_usage": {
      "prompt_tokens": 10620,
      "completion_tokens": 198,
      "total_tokens": 10818
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention in the paper",
          "reasoning": "The paper does not include any explicit reference or link to code repositories for the dataset construction or data generation pipeline. While the paper mentions synthetic data creation using RenderPeople scans and Blender Cycles rendering, there is no indication that the code or scripts used for dataset synthesis or processing are publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.6 Training Details and Section 4 Experiments, Datasets subsection",
          "reasoning": "The paper documents in detail the dataset creation process for their synthetic training set, including usage of 1000 RenderPeople scans, rigging with 3D animation sequences, rendering views with Blender Cycles, and the train/test splits. It also describes on-the-fly pseudo ground-truth generation for fine-tuning on real data. This constitutes comprehensive documentation of the dataset construction methodology within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-2054-rubric-0",
    "token_usage": {
      "prompt_tokens": 11282,
      "completion_tokens": 146,
      "total_tokens": 11428
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and Section 4.1",
          "Reasoning": "The paper explicitly states that the authors use unlabeled images from ImageNet as the wild dataset $D_w$ to extend both content and style for domain-generalized semantic segmentation training. ImageNet images are collected by humans (human-generated natural images). The authors do not introduce any new dataset collected or generated by themselves; they utilize the existing ImageNet dataset without labels for their wild data. Hence, the only new data used by the authors for training is this unlabeled ImageNet subset, which is human-generated images."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-2054-rubric-1",
    "token_usage": {
      "prompt_tokens": 12082,
      "completion_tokens": 251,
      "total_tokens": 12333
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1 and Section 4.1",
            "reasoning": "The paper uses the ImageNet dataset as a wild dataset without class labels, leveraging unlabeled images for learning. Since there are no annotation processes described for ImageNet in the paper, and labels are not used in this context, the data is considered unlabeled and used as-is, implying no human annotation or explicit annotation efforts for the introduced wild dataset."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No specific section describing annotation instructions for the wild dataset.",
            "reasoning": "The paper does not mention any annotation process, instructions, or annotator guidance for the wild dataset (ImageNet used without labels). Hence, there are no instructions provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention in the paper regarding rubrics for the wild dataset annotations.",
            "reasoning": "Since there is no annotation effort described, no rubric is needed or provided for the wild dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples related to annotation guidelines for wild dataset.",
            "reasoning": "No annotation examples are presented or required for the unlabeled wild dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-2054-rubric-2",
    "token_usage": {
      "prompt_tokens": 13272,
      "completion_tokens": 346,
      "total_tokens": 13618
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert for dataset annotations or content validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance involving multiple human experts for the datasets used or introduced."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about quality assurance by a single human non-expert for dataset content or annotations."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description of quality assurance by multiple human non-experts is provided in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using an AI model as a judge for quality assurance of the dataset content or annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that automatic or algorithmic verification processes were used as quality assurance for dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses publicly available datasets such as ImageNet, Cityscapes, GTA V, SYNTHIA, BDD100K, and Mapillary as source, target, and wild datasets, but does not introduce any new dataset nor describe any quality assurance processes for dataset content or annotations. The wild dataset used (ImageNet without labels) is used for style and content extension but no annotation or QA for these images is presented or discussed. Hence, no quality assurance process is applied or documented for dataset quality in the context of this paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-2054-rubric-3",
    "token_usage": {
      "prompt_tokens": 12882,
      "completion_tokens": 444,
      "total_tokens": 13326
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creation of any new dataset by humans from scratch. It uses existing datasets for source and evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data is generated by AI or model from scratch in this work. The methods concern feature manipulation but no new synthetic data creation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of any translation of datasets from one language to another by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation of data is involved or mentioned in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The wild dataset used is ImageNet, an existing large-scale image dataset, used without its labels (unlabeled images) as the wild domain data. Existing semantic segmentation datasets (e.g., Cityscapes, BDD100K, Mapillary, GTA V, SYNTHIA) are also used. The work collates these publicly available datasets to leverage a variety of contents and styles for training domain generalized models."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the method extends source contents and styles using feature stylization and contrastive learning techniques, the paper does not describe creation of new datasets derived or transformed significantly from existing data. It manipulates features during training but does not produce new adapted datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and nature of all data used in the experiments are clearly stated. There is no undocumented or unspecified dataset origin."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-2054-rubric-4",
    "token_usage": {
      "prompt_tokens": 13400,
      "completion_tokens": 487,
      "total_tokens": 13887
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the new dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the new dataset is used to train models from randomly initialized parameters. Instead, models are initialized with ImageNet pre-trained weights."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4.2",
          "reasoning": "The new dataset, consisting of unlabeled 'wild' images (ImageNet images without labels), is used during supervised training of the segmentation model through several proposed learning techniques (Feature Stylization, Content Extension Learning, Style Extension Learning, Semantic Consistency Regularization). The wild data augment the training process, enhancing domain generalization by providing diversity in content and style."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or RL-based post-training methods involving the new dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset is not used solely for evaluation or benchmarking. Evaluation is conducted on existing semantic segmentation datasets."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While analysis of the model's behavior and ablations are conducted, the new dataset itself is not primarily used for analyzing trends or patterns."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base to augment models via retrieval or similar techniques, but as unlabeled images to diversify training."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The new wild dataset is clearly described and utilized in the supervised training process to improve domain generalization."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-2054-rubric-5",
    "token_usage": {
      "prompt_tokens": 14123,
      "completion_tokens": 521,
      "total_tokens": 14644
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing entries with more than two human languages. The datasets used are image datasets without linguistic labels."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that any dataset introduced by the authors contains exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's datasets are image datasets for semantic segmentation and do not contain English text entries as dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset introduced by the authors contains non-English language entries."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset containing programming code or structured code content is introduced; code is only used for implementation."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses mathematical notation to describe methods, but does not propose a dataset containing mathematical or logical expressions as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets discussed and introduced relate to images and semantic labels, not biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets containing constructed or artificial languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset entries are either not applicable or explicitly not language-based, so the language cannot be unknown."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new dataset introduced by the authors is a set of images (wild dataset from ImageNet unlabeled images) used to provide diverse content and styles for domain generalization. These are purely visual images without any language entries; thus, the dataset does not contain any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-2054-rubric-6",
    "token_usage": {
      "prompt_tokens": 11341,
      "completion_tokens": 196,
      "total_tokens": 11537
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 4",
          "reasoning": "The abstract provides a link to the source code repository: https://github.com/suhyeonlee/WildNet. Section 4 details datasets and experiments but does not explicitly mention code for dataset construction; however, since the wild dataset used is ImageNet (publicly available) and only utilized unlabeled images from it, the data processing code is likely included in the provided repository to reproduce their use of wild data."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1",
          "reasoning": "Section 4.1 describes the datasets used including the wild dataset (ImageNet), specifying that unlabeled wild images from ImageNet are used as wild domain data. It further discusses the use of these images in training, and the experimental setup provides details on data splits and usage, which documents the dataset usage process sufficiently for reproducibility."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-2056-rubric-0",
    "token_usage": {
      "prompt_tokens": 13252,
      "completion_tokens": 282,
      "total_tokens": 13534
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (Audio-Guided-VOS Dataset (A VOS)) and Dataset Processing in Section 5.2",
          "Reasoning": "The paper explicitly states they constructed a large-scale audio-guided video semantic segmentation dataset (Audio-Guided-VOS, A VOS), which is an extension of RVOS, A2D and J-HMDB datasets. The videos correspond to real natural scenes recorded in the previous datasets, so the video data is human-generated (captured via human-operated cameras) and not model-generated; the origin is known and clearly stated as collected and used in the dataset construction."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (Audio-Guided-VOS Dataset (A VOS)) and Dataset Processing in Section 5.2",
          "Reasoning": "The audio modality comprises referring audio expressions recorded by 36 human speakers reading the referring sentences aloud. The paper details the recording conditions: sampling rate >= 44,100 Hz, 16-bit sampling, manual quality checks to ensure correctness, and pronunciation accuracy over 99%. This audio data is directly recorded from humans, thus human-generated, not model-generated, and with known provenance as explicitly described."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-2056-rubric-1",
    "token_usage": {
      "prompt_tokens": 14052,
      "completion_tokens": 274,
      "total_tokens": 14326
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3 Audio-Guided-VOS Dataset (A VOS)",
            "reasoning": "Section 3 describes that 36 speakers were employed to read sentences for audio annotations; they were speakers reading sentences rather than experts annotating semantic labels, indicating multiple human non-expert annotators contributed audio annotations to the dataset."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3 Audio-Guided-VOS Dataset (A VOS)",
            "reasoning": "Section 3 states speakers were required to read proficiently without stammering and at specified speaking speeds, implying that instructions for data collection were provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3 Audio-Guided-VOS Dataset (A VOS)",
            "reasoning": "The paper does not mention any scoring rubrics or formal evaluation schemes to assess annotation quality; it only mentions manual checking and corrections, which are quality assurance rather than scoring rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3 Audio-Guided-VOS Dataset (A VOS)",
            "reasoning": "There is no mention of annotation examples or exemplars provided to annotators; the section describes recording specifications but does not reference examples or sample annotations."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-2056-rubric-2",
    "token_usage": {
      "prompt_tokens": 15242,
      "completion_tokens": 528,
      "total_tokens": 15770
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any quality assurance conducted by a single human annotator with subject matter expertise for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of multiple human experts or annotators involved in quality assurance of the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3, Audio-Guided-VOS Dataset (A VOS), paragraph 1",
          "reasoning": "The dataset A VOS was constructed by employing 36 speakers to read sentences to obtain audio annotations. The paper states that two rounds of inspections were conducted, including manual checking of word accuracy to be not less than 99%, and correction of pronunciation, grammar, and spelling errors. This implies multiple human non-expert annotators were involved in quality assurance to ensure the quality of the audio recordings."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any AI model used as a judge in the quality assurance process of the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3, Audio-Guided-VOS Dataset (A VOS), paragraph 1",
          "reasoning": "The paper mentions the use of a pre-trained MFCC model to extract acoustic features and also mentions the use of manual checking for word accuracy and corrections, implying some level of automated feature extraction. However, the manual checking indicates the process is not purely automated verification. Since no explicit algorithmic or rule-based verification of annotations is described, this is considered partial. Nonetheless, feature extraction by pre-trained MFCC is an automated process relevant to dataset processing."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides information on the quality assurance process performed by multiple human non-experts and some automated processing via MFCC feature extraction, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-2056-rubric-3",
    "token_usage": {
      "prompt_tokens": 14852,
      "completion_tokens": 515,
      "total_tokens": 15367
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 Audio-Guided-VOS Dataset (A VOS)",
          "reasoning": "The paper states that they constructed a large-scale audio-guided video object segmentation dataset called Audio-Guided-VOS (A VOS) by extending existing datasets (RVOS, A2D, and J-HMDB). They employed 36 speakers to read the referring sentences to produce audio annotations, ensuring high quality recordings and manual inspection for accuracy. This indicates original content created via human contributors recording new audio data, not translated or derived from existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that the dataset or any part was produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no reference to machine translation being used to generate dataset content."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3 Audio-Guided-VOS Dataset (A VOS)",
          "reasoning": "The dataset builds upon existing datasets RVOS, A2D, and J-HMDB by using their video segments and mask annotations as base data. These existing video segments were selected as the source, indicating aggregation of existing data sources without complete replacement."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3 Audio-Guided-VOS Dataset (A VOS)",
          "reasoning": "The new A VOS dataset is derived by adapting the existing video datasets with new audio annotations recorded by humans describing the video contents. The paper describes adding human-read audio queries corresponding to sentences originally associated with the videos, performing corrections and inspections, hence transforming and extending the original datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the data sourcing and generation methodology, so data origin is not unknown."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-2056-rubric-4",
    "token_usage": {
      "prompt_tokens": 15370,
      "completion_tokens": 480,
      "total_tokens": 15850
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the proposed dataset for pre-training large models or in an unsupervised/self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5.2 Implementation Details; Section 5.3 Performance Comparisons",
          "reasoning": "The authors construct a new large-scale audio-guided video object segmentation dataset (AVOS) and use it to train their proposed model Wnet. Training is performed end-to-end, indicating the dataset is used for training models from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention or indication in the paper that the dataset is used to fine-tune pre-trained models using supervised learning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any reinforcement learning based post-training such as RLHF using the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1 Performance Criteria; Section 5.3 Performance Comparisons",
          "reasoning": "The dataset is used as a benchmark to evaluate the performance of the proposed Wnet method and compare it with other methods using multiple standard metrics."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.4 Ablation Study",
          "reasoning": "The dataset is utilized for detailed ablation studies analyzing the contributions of various model components and settings."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that the dataset serves as a knowledge base or is used for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents practical usage of the dataset in training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-2056-rubric-5",
    "token_usage": {
      "prompt_tokens": 16093,
      "completion_tokens": 481,
      "total_tokens": 16574
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "The paper describes the AVOS dataset as containing audio annotations collected from speakers reading sentences in English. There is no mention of any other human languages being included in the audio annotations."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "The dataset AVOS is constructed by extending existing datasets with audio annotations recorded in English only, with no indication of a second language present in the dataset."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3: Audio-Guided-VOS Dataset (A VOS)",
          "reasoning": "The AVOS dataset is constructed by having 36 speakers read the sentences, with specifications on pronunciation and speaking speed, implying the audio content is English. There is no mention of any other languages, indicating the dataset is monolingual in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "The dataset was recorded in English as described; no evidence of a non-English monolingual dataset is presented."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "General",
          "reasoning": "While the paper describes algorithmic details and model structures, the dataset itself contains audio and video data, not programming or code snippets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "General",
          "reasoning": "Mathematical expressions and notation appear in the paper to describe the methods, but these are not part of the dataset content entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "General",
          "reasoning": "The dataset consists of human audio-visual data and contains no biological or non-human communication system data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "General",
          "reasoning": "No mention or indication that the dataset contains constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "The language of the audio annotations is explicitly documented as being English, with careful control of pronunciation and grammar, so the language is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains audio annotations which are human language, so it does contain language entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-2056-rubric-6",
    "token_usage": {
      "prompt_tokens": 13311,
      "completion_tokens": 173,
      "total_tokens": 13484
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper",
          "reasoning": "The paper does not explicitly mention or provide a link to any publicly available code repository for the new dataset AVOS. There is no indication that the code related to data collection, preprocessing, or generation is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Audio-Guided-VOS Dataset (AVOS))",
          "reasoning": "The paper documents the dataset creation process in Section 3 by describing the data sources (RVOS, A2D, J-HMDB), the collection of audio annotations via 36 speakers reading the sentences, recording conditions (sampling rate, bit depth, speaking speed), manual checking for accuracy, and dataset splits. Therefore, the dataset creation process is well documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-219-rubric-0",
    "token_usage": {
      "prompt_tokens": 11052,
      "completion_tokens": 172,
      "total_tokens": 11224
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Prompt Engineering; Section 4.1 Datasets; Abstract",
          "Reasoning": "The authors introduce video cuts from instructional videos as input to their Bridge-Prompt framework, using manually annotated action labels to generate text prompts for training. The datasets used (GTEA, 50Salads, Breakfast) are pre-existing, but the paper does not state introduction of a new video dataset. The training data for the Bridge-Prompt pre-training is comprised of video cuts sampled from videos with manual annotations, indicating the modality is video, human generated (captured), not model generated or unknown. Since this dataset is not introduced as a new dataset by the authors but reused, it is excluded by instruction."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-219-rubric-1",
    "token_usage": {
      "prompt_tokens": 11852,
      "completion_tokens": 210,
      "total_tokens": 12062
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 4.1 Datasets",
            "reasoning": "The paper evaluates on three benchmark datasets (50Salads, GTEA, Breakfast) which are publicly existing datasets; thus, no new dataset is introduced in this paper. The paper does not mention proposing or annotating any new dataset. Since no new dataset is introduced, the annotators of new data (if any) are not discussed."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not describe any annotation instructions related to a newly introduced dataset, as no new dataset is introduced."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There is no mention of scoring rubrics for any new dataset annotation in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No examples related to dataset annotation are provided since no new dataset is introduced."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-219-rubric-2",
    "token_usage": {
      "prompt_tokens": 13042,
      "completion_tokens": 332,
      "total_tokens": 13374
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset introduced by the authors or any quality assurance performed by a single human expert. The datasets used are pre-existing instructional video datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on any new dataset. The paper uses existing datasets without describing annotation or QA procedures for new data."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance by a single non-expert human annotator on any new dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No details are reported about multiple non-expert annotators performing quality assurance on any novel dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models are used for video-text learning and processing, there is no mention that AI models are employed to perform quality assurance on dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance via automated verification is not described for any dataset annotations in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets for which annotation or quality assurance procedures are described. The datasets used (GTEA, 50Salads, Breakfast) are pre-existing and no QA process about their annotations is provided or discussed. Hence, no quality assurance process is applied or documented for any new dataset in this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-219-rubric-3",
    "token_usage": {
      "prompt_tokens": 12652,
      "completion_tokens": 389,
      "total_tokens": 13041
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing benchmark datasets (GTEA, 50Salads, Breakfast) and does not report collecting or creating any new dataset from human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new data is generated purely from AI or machine learning models. The paper focuses on a new learning framework using existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of data being produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of data being generated by machine translation from other languages."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper utilizes existing publicly available instructional video datasets such as Georgia Tech Egocentric Activities (GTEA), 50Salads, and Breakfast datasets, which are collected from prior work and aggregated for experiments."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Prompt Engineering",
          "reasoning": "The authors apply modifications and transformations to existing video data by generating integrated text prompts for supervision, integrating statistical, ordinal, and semantic information to form novel textual descriptions derived from existing labels."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and processing are explicitly documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-219-rubric-4",
    "token_usage": {
      "prompt_tokens": 13170,
      "completion_tokens": 279,
      "total_tokens": 13449
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 (Conclusion and Discussion) and throughout experimental sections",
          "reasoning": "The datasets used (GTEA, 50Salads, Breakfast) are standard public benchmarks; the paper uses them to analyze and demonstrate the effectiveness of the proposed Bridge-Prompt method rather than introducing new datasets. No novel datasets are introduced by the authors. The paper discusses analysis of ordinal actions and uses existing datasets for evaluation and ablation, but does not propose or release a new dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; it only uses existing benchmark datasets (GTEA, 50Salads, Breakfast). Therefore, no utility for a new dataset can be described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-219-rubric-5",
    "token_usage": {
      "prompt_tokens": 13893,
      "completion_tokens": 592,
      "total_tokens": 14485
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the introduced dataset contains multiple human languages. The datasets used (GTEA, 50Salads, Breakfast) are instructional video datasets with action annotations, and there is no indication that they include more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that any new dataset introduced contains exactly two human languages. No mention of bilingual language content is found."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 (Prompt Engineering), Section 4.1 (Datasets), Abstract",
          "reasoning": "The datasets introduced and used in the paper are instructional videos with action annotations described in English. The prompt engineering uses English text prompts such as 'the person is performing the action step of...' indicating the language of dataset labels and textual prompts is English. No other human language is mentioned."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the introduced datasets contain content in a single language other than English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses models and methods implemented using programming languages, the datasets themselves do not contain entries of code or programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes some mathematical formulas to describe model losses and methods, but the datasets themselves do not contain entries with mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of biological sequences or non-human communication systems in the datasets introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or artificial languages are introduced or used in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) in the datasets introduced are specified clearly as English via the text prompts; thus, this category does not apply."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain human language annotations in English as textual prompts, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-219-rubric-6",
    "token_usage": {
      "prompt_tokens": 11111,
      "completion_tokens": 163,
      "total_tokens": 11274
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and main text",
          "reasoning": "The paper provides a link to the code for their Bridge-Prompt model (https://github.com/ttlmh/Bridge-Prompt) related to their method and experiments, but does not indicate that any new dataset construction code is released. Also, the datasets used (GTEA, 50Salads, Breakfast) are existing datasets, and no new datasets are introduced by the authors, hence no dataset code is relevant or provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No specific section",
          "reasoning": "The paper does not introduce any new datasets; it only evaluates on existing datasets and uses their annotations. Therefore, there is no dataset creation process documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-29-rubric-0",
    "token_usage": {
      "prompt_tokens": 12119,
      "completion_tokens": 100,
      "total_tokens": 12219
    },
    "response": {
      "sources": [
        {
          "Modality": "tabular",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 6.1 Synthetic data",
          "Reasoning": "The paper introduces a new synthetic dataset for evaluating the robust fitting algorithm, consisting of generated 2D points with a specified residual function for 1D linear regression. The data are generated programmatically with inliers and outliers corrupted by Gaussian noise, indicating simulation by the authors rather than human collection."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-29-rubric-1",
    "token_usage": {
      "prompt_tokens": 12919,
      "completion_tokens": 377,
      "total_tokens": 13296
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 6 and Algorithm 1",
            "reasoning": "The annotation corresponds to labeling inlier and outlier points within the newly introduced quantum-classical robust fitting algorithm's evaluation on synthetic and real data. This labeling is a specialized task requiring expert knowledge of geometric model fitting, consensus maximization, and quantum annealing problem formulation; thus, it is performed by a subject-matter expert described in the experiments and algorithm sections."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 5 and Algorithm 1",
            "reasoning": "The paper describes Algorithm 1 in detail, providing clear procedural steps on how to incrementally generate hyperedges, solve the QUBO subproblems via quantum annealing, update candidate sets, and terminate. These constitute explicit annotation instructions for identifying inliers/outliers and sampling hyperedges within the new dataset."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 5.1 and Section 6",
            "reasoning": "Section 5.1 details error bounds and quality measures (e.g., gap between LP relaxation and quantum annealer solution) which serve as scoring rubrics indicating annotation quality. Furthermore, experimental evaluation in Section 6 uses these bounds to rate solution quality and compute error metrics, confirming rubric usage for annotation evaluation."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 6 (Figures 3,5,7), Algorithm 1",
            "reasoning": "The paper provides multiple examples including synthetic data experiments (Section 6.1), fundamental matrix estimation and multi-view triangulation experiments (Section 6.2), and iterative outputs in figures and tables demonstrating the annotation outcomes (inliers/outliers), hyperedge sampling, and error bounds, which serve as concrete examples to guide annotation on the new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-29-rubric-2",
    "token_usage": {
      "prompt_tokens": 14109,
      "completion_tokens": 267,
      "total_tokens": 14376
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any single human expert performing quality assurance on the datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts conducted QA on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of a single non-expert annotator performing QA."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA by multiple non-expert humans."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No AI model is used to perform quality assurance on dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the method involves automated computational procedures, the paper does not describe an automated process for quality assurance of dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces new synthetic datasets and uses existing datasets for evaluation but does not describe any quality assurance process applied to validate the dataset annotations or content. There is no documentation or description of QA procedures for the new synthetic data or for any other data used."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-29-rubric-3",
    "token_usage": {
      "prompt_tokens": 13719,
      "completion_tokens": 494,
      "total_tokens": 14213
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 6.1 and 6.2",
          "reasoning": "The paper explicitly states the generation of synthetic data for 1D linear regression with controlled noise and outlier levels (Section 6.1). In addition, it uses real image datasets such as VGG and KITTI odometry sequences, where feature extraction and matching are performed to create data for the fundamental matrix estimation and multi-view triangulation tasks (Section 6.2). These datasets and the associated data points are newly created or obtained and processed by the authors to evaluate their proposed algorithm, representing new data created by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated autonomously by AI or machine learning models without relying on or transforming existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data used or created in the paper was produced by translating content from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was applied to generate any data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 6.2",
          "reasoning": "The paper utilizes existing benchmark datasets such as VGG, KITTI odometry, and multi-view triangulation datasets (Nikolai, Linkoping, Tower) which are aggregated from existing sources without significant modification, thus reflecting collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 6.2",
          "reasoning": "The authors perform processing steps like feature extraction (SIFT) and matching on existing datasets, deriving new data suitable for their experiments. This represents derivation of data from existing sources with modifications and transformations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies data origin and usage clearly for all datasets evaluated."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-29-rubric-4",
    "token_usage": {
      "prompt_tokens": 14237,
      "completion_tokens": 344,
      "total_tokens": 14581
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 6 (Experiments), especially 6.1 and 6.2",
          "reasoning": "The synthetic datasets generated in Section 6.1 for 1D linear regression and the real image pairs data used in Section 6.2 for fundamental matrix estimation and multi-view triangulation are used exclusively for evaluation and benchmarking of the proposed robust fitting algorithm. The datasets are used to demonstrate the algorithm's performance and error bounds but not for training or fine-tuning models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 6 (Experiments)",
          "reasoning": "The datasets are also analyzed to observe the behavior of the algorithm, such as convergence, error bounds, and comparison against other methods. This constitutes analyzing performance trends and characteristics rather than training."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new datasets introduced by the authors are described in the paper; the data used in experiments are either synthetic data generated for evaluation or existing real-world datasets (VGG, KITTI, Nikolai, Linkoping, Tower). Therefore, no new dataset with practical usage is introduced or demonstrated in this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-29-rubric-5",
    "token_usage": {
      "prompt_tokens": 14960,
      "completion_tokens": 308,
      "total_tokens": 15268
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, including Abstract, Sections 1, 3, 4, 5, 6",
          "reasoning": "The paper and its newly introduced datasets are described entirely in English. The data used consists of synthetic and real image data annotated or processed for robust fitting tasks, with all descriptions, labels, and textual content in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Sections 3 (Reformulating consensus maximisation) and 4 (Quantum solution)",
          "reasoning": "The datasets introduced for robust fitting are defined with extensive mathematical notation including residuals, optimization problems, integer programming formulations, and quantum annealing Hamiltonians; this mathematical content is integral to the dataset description."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-29-rubric-6",
    "token_usage": {
      "prompt_tokens": 12178,
      "completion_tokens": 203,
      "total_tokens": 12381
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention in the paper",
          "reasoning": "The paper does not provide any URL or reference indicating that the code for dataset construction or algorithm implementation has been made publicly available. There is no mention of a Github repository or other code release related to the datasets or experiments."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 6 Experiments, subsections 6.1 Synthetic data and 6.2 Real data experiments",
          "reasoning": "The paper offers detailed descriptions of the synthetic data generation process (Section 6.1) such as 2D point generation for 1D linear regression and noise model parameters, and describes the real datasets used (VGG, KITTI, and other multi-view triangulation datasets) along with processing steps like SIFT feature extraction and matching. Thus, while the datasets themselves are standard or synthetic, their creation or use is documented in the paper sufficiently for reproducibility."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-325-rubric-0",
    "token_usage": {
      "prompt_tokens": 12303,
      "completion_tokens": 130,
      "total_tokens": 12433
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 5.1, Synthetic dataset",
          "Reasoning": "The authors introduce a new synthetic dataset consisting of top-view and side-view video sequence pairs generated using a 3D modeling engine Unity and an open-source human modeling toolkit PersonX. This synthetic dataset provides rich annotations including side-view camera location, view direction, and cross-view human bounding boxes with identities. Since the dataset is created by procedural rendering and simulation rather than captured from real-world instruments, the data modality is video and the origin is model generated."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-325-rubric-1",
    "token_usage": {
      "prompt_tokens": 13103,
      "completion_tokens": 236,
      "total_tokens": 13339
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 5.1, Dataset - Synthetic dataset paragraph",
            "reasoning": "The paper states that the synthetic dataset was generated using a 3D modeling engine Unity and an open-source toolkit PersonX in a virtual environment, allowing control of setups and providing rich and accurate annotations automatically, including camera location, view direction, and subject bounding boxes with cross-view IDs. This indicates the annotations were created through automatic simulation rather than manual human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not indicate any annotation instructions given to annotators for the synthetic dataset since annotations were obtained automatically in the synthetic environment."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "There is no mention of scoring rubrics or criteria used to evaluate annotations, consistent with the automatic annotation nature of the synthetic dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "No examples of annotations are provided or described, and since annotations are synthesized automatically, examples are not applicable."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-325-rubric-2",
    "token_usage": {
      "prompt_tokens": 14293,
      "completion_tokens": 436,
      "total_tokens": 14729
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was conducted by a single human annotator who is a subject matter expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of multiple human experts performing quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA process by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 5.1 Dataset, paragraph Real-world dataset",
          "reasoning": "The real-world dataset annotations were manually labeled, including bounding boxes with ID numbers for subject association across the two views; the paper states that manual labeling is very labor-intensive, indicating that multiple human annotators who are likely non-experts performed the annotation and quality assurance. There is no explicit mention that these annotators are subject matter experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance described as performed by an AI model."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 5.1 Dataset, paragraph Synthetic dataset",
          "reasoning": "The synthetic dataset is generated using the Unity 3D engine and PersonX toolkit with controlled parameters and annotations automatically obtained from the synthetic environment, ensuring accurate and consistent labels without manual intervention, effectively serving as an automatic quality assurance process through automated generation and verification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance processes are documented for both synthetic and real-world datasets, so N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-325-rubric-3",
    "token_usage": {
      "prompt_tokens": 13903,
      "completion_tokens": 414,
      "total_tokens": 14317
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 5.1 Dataset - Synthetic dataset",
          "reasoning": "The authors collected a new large-scale synthetic dataset with rich annotations using 3D modeling engine Unity and PersonX toolkit. They controlled the scenes, human models, and captured complementary top-view and side-view video pairs with accurate annotations of side-view camera location, view direction, and human bounding boxes with cross-view ID numbers. This dataset was created entirely from scratch by humans through controlled synthetic data generation and annotation, not translated, adapted, or derived from pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that the dataset was generated entirely by AI or machine learning models without human control or synthetic modeling."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence or mention of any data being produced by translation from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data generated by machine translation from another language is mentioned."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new synthetic dataset is not described as collected or aggregated from existing sources without significant modification, but rather is generated synthetically."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as derived from existing datasets with modifications; it is newly synthesized."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the dataset's origin and method of generation."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-325-rubric-4",
    "token_usage": {
      "prompt_tokens": 14421,
      "completion_tokens": 529,
      "total_tokens": 14950
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5.3 (Camera Identification Results), Section 5.4 (Subject Association Results)",
          "reasoning": "The new synthetic dataset is used to train the proposed deep neural network framework from scratch, including the human detection modules, camera wearer identification network, and cross-view subject association network. Training losses (e.g., detection loss, direction loss, localization loss, matching loss) are applied during network training on the synthetic dataset. This is explicitly described in Section 4 (Implementation Details) and demonstrated in the experimental sections."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence or description is present in the paper indicating that the new dataset is used for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of reinforcement learning or RL-based post-training methods that utilize the new dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.3 (Camera Identification Results), Section 5.4 (Subject Association Results)",
          "reasoning": "The new synthetic dataset is used extensively to evaluate and benchmark the performance of the proposed method on camera localization, view direction estimation, and cross-view subject association. Multiple metrics are defined and reported using this dataset, including AUC scores, precision, recall, and F-score."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper analyzes experimental results and conducts ablation studies, these analyses do not primarily use the dataset for analyzing trends or characteristics independent of training or evaluation purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or for augmenting models via retrieval-augmented methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes practical uses of the new synthetic dataset for training and evaluation purposes."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-325-rubric-5",
    "token_usage": {
      "prompt_tokens": 15144,
      "completion_tokens": 598,
      "total_tokens": 15742
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The synthetic dataset and the real-world dataset introduced in the paper contain video data and annotations related to human detection, camera locations, and view directions, without any indication of multiple human languages present in the data."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that the dataset entries contain exactly two human languages. The dataset focuses on video frames and spatial annotations, not linguistic content."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5.1 Dataset",
          "reasoning": "The dataset is described and annotated with English terms (e.g., 'camera location', 'view direction', 'subject association', 'bounding box', 'ID'), and the paper itself is written in English with no indication of content in other languages. Thus, the dataset is monolingual with English as the language used for annotations and metadata."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper and dataset annotations are in English, with no indication of non-English linguistic content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of video frames and annotations for camera and subject spatial information. There is no mention that the dataset includes programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical expressions to describe the methods, the dataset entries themselves are video data and annotations, not mathematical or logical expressions. Thus, the dataset does not contain mathematical or formal logical notation within its entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset represents human video data and annotations; it does not contain biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset includes fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset annotations and metadata is clearly English and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains annotated metadata and labels in English, so it is not devoid of language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-325-rubric-6",
    "token_usage": {
      "prompt_tokens": 12362,
      "completion_tokens": 134,
      "total_tokens": 12496
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 1 Introduction",
          "reasoning": "The paper states that the dataset is released to the public at https://github.com/RuizeHan/DMHA, indicating that the code related to the dataset and experiments is publicly available in an accessible repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 5.1 Dataset",
          "reasoning": "The paper provides a detailed description of the newly collected synthetic dataset, including the data collection process, scene settings, scale, annotation details, and also discusses the real-world dataset. This documentation is comprehensive and transparent about dataset creation."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-341-rubric-0",
    "token_usage": {
      "prompt_tokens": 9306,
      "completion_tokens": 128,
      "total_tokens": 9434
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Experimental Setup, Benchmarks",
          "Reasoning": "The authors constructed two new continual predictive learning benchmarks based on real-world datasets RoboNet and KTH by dividing these datasets into multiple tasks for continual learning. These tasks correspond to different environments or human actions, and the video sequences were collected via cameras capturing robotic arms and human actors. Therefore, the data modality is video, and the source is clearly human-generated as it involves real-world camera recordings of humans and robots interacting in different scenarios."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-341-rubric-1",
    "token_usage": {
      "prompt_tokens": 10106,
      "completion_tokens": 311,
      "total_tokens": 10417
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1 Experimental Setup",
            "reasoning": "The paper describes that the RoboNet and KTH datasets are real-world video datasets divided into continual learning tasks by environment or action labels. The annotations involve segmenting these datasets into tasks based on known labels, which implies human expertise in task definition. There is no indication of crowdsourced or multiple annotators, so a single human expert likely performed or oversaw this annotation process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1 Experimental Setup",
            "reasoning": "The paper does not mention providing any detailed annotation instructions or guidelines for segmenting the RoboNet or KTH datasets into tasks. The divisions are described as based on existing environment or action labels, implying no separate annotation instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1 Experimental Setup",
            "reasoning": "No scoring rubrics or formal evaluation criteria related to annotations are described for the segmentation or labeling of these new datasets. Metrics used for evaluation pertain to model performance (PSNR, SSIM), not annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.1 Experimental Setup",
            "reasoning": "The paper does not provide annotation examples or exemplar tasks beyond the descriptions of dataset splits (e.g., Berkeley \u2192 Google \u2192 Penn \u2192 Stanford for RoboNet, and action types for KTH). There are no explicit annotation examples or guidelines given."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-341-rubric-2",
    "token_usage": {
      "prompt_tokens": 11296,
      "completion_tokens": 372,
      "total_tokens": 11668
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process for the new datasets involving a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper indicating that multiple human experts were involved in a quality assurance process for the new datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no details about quality assurance performed by a single non-expert on the new datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance process involving multiple non-expert humans for the introduced datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance involving an AI model as a judge for dataset validation is described."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification or algorithmic quality assurance processes applied to the new datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces new benchmarks derived from RoboNet and KTH datasets by dividing them into tasks based on environment or action labels, but does not describe any quality assurance or validation process for these new dataset splits. Therefore, no QA process is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-341-rubric-3",
    "token_usage": {
      "prompt_tokens": 10906,
      "completion_tokens": 383,
      "total_tokens": 11289
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creating any new datasets entirely from scratch by human contributors; rather, it uses existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any datasets generated purely by models; instead, it focuses on learning and evaluation on real-world datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental Setup",
          "reasoning": "The two new benchmarks introduced are constructed based on existing real-world datasets RoboNet and KTH. The data is divided into multiple tasks according to environments or actions, indicating aggregation and partitioning of existing data without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the authors modified, transformed, or adapted existing datasets beyond task partitioning; no new data transformations or derivations are described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source or method of data generation is specified (constructed from existing datasets RoboNet and KTH), so this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-341-rubric-4",
    "token_usage": {
      "prompt_tokens": 11424,
      "completion_tokens": 565,
      "total_tokens": 11989
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the new datasets as being used for pre-training large models on general patterns. Instead, the datasets are used specifically for continual predictive learning experiments."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 (Experimental Setup), Section 4.2 (RoboNet Benchmark), Section 4.3 (KTH Benchmark)",
          "reasoning": "The new benchmarks constructed from RoboNet and KTH datasets are used to train models from scratch in the continual predictive learning setup. The world model is trained sequentially on tasks derived from these benchmarks, starting from initial model parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the new datasets for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no reference to reinforcement learning post-training or RLHF usage of the new datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 (RoboNet Benchmark), Section 4.3 (KTH Benchmark), Section 4.4 (Ablation Study)",
          "reasoning": "The constructed benchmarks from RoboNet and KTH datasets are used extensively for evaluating the effectiveness of the proposed continual predictive learning methods. Evaluation metrics such as PSNR and SSIM are computed on the test sets of these benchmarks."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.4 (Ablation Study), Section 4.2 and 4.3 (qualitative and robustness analysis)",
          "reasoning": "The new datasets are also used for detailed analysis including ablation studies on different components of the model, as well as tests on robustness to different task orders and qualitative performance assessments."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the new datasets as being used as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the practical usage of the new datasets for training, evaluation, and analysis in continual predictive learning."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-341-rubric-5",
    "token_usage": {
      "prompt_tokens": 12147,
      "completion_tokens": 557,
      "total_tokens": 12704
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced (RoboNet and KTH) do not contain entries with more than two human languages; video content and task divisions are based on environments or human actions, not languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets contain exactly two human languages. The datasets are videos depicting robotic interactions or human actions without language annotation."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are video datasets without explicit English language content. No textual or spoken English data appears to be part of the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not comprised of text or speech data in any language, including non-English languages."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The proposed datasets (RoboNet and KTH-based benchmarks) consist of video data and do not include programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical expressions for the modeling and methodology, the datasets themselves are video datasets with no symbolic or formal logical expressions as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not include biological sequences or non-human communication systems such as DNA sequences or animal signaling; instead, they include robotic and human action videos."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not include fictional or artificial languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content, or lack thereof, of the datasets is clearly specified as video data; no ambiguity exists about the linguistic nature."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced are video datasets of robotic and human actions without any language content; thus, they do not contain entries with any human or natural language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-341-rubric-6",
    "token_usage": {
      "prompt_tokens": 9365,
      "completion_tokens": 178,
      "total_tokens": 9543
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention found",
          "reasoning": "The paper does not provide any link or mention of publicly available code related to the datasets or dataset construction. It only discusses the use of existing datasets (RoboNet and KTH) split into tasks for continual learning benchmarks, without indicating releasing code for dataset creation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Experimental Setup",
          "reasoning": "The paper documents the construction of new continual learning benchmarks by splitting existing datasets RoboNet and KTH into multiple tasks, describing how they divide the RoboNet dataset by environment and KTH dataset by action labels. It provides statistics on the number of training and testing sequences per task and describes how tasks are ordered and configured, thus providing a clear, reproducible description of the dataset splits and benchmark construction."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-361-rubric-0",
    "token_usage": {
      "prompt_tokens": 18304,
      "completion_tokens": 186,
      "total_tokens": 18490
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 6.1 Implementation Details: \"Our model takes two images as input, comprising a template image $z$ and a larger search image $x$... The training datasets include the train subsets of LaSOT [13], GOT-10K [19], COCO2017 [25], and TrackingNet [29].\"",
          "Reasoning": "The new dataset involves pairs of images (template and search images) used for training and fine-tuning the Single Branch Transformer (SBT) tracker. These images originate from human-captured datasets such as LaSOT, GOT-10K, COCO2017, and TrackingNet, which are all image datasets collected by humans using cameras and manual annotation. Therefore, the modality is image, and the data is human generated."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-361-rubric-1",
    "token_usage": {
      "prompt_tokens": 19104,
      "completion_tokens": 222,
      "total_tokens": 19326
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 6.1 (Implementation Details)",
            "reasoning": "The new tracking model is fine-tuned on tracking datasets including LaSOT, GOT-10K, COCO2017, and TrackingNet training splits, which require human annotation; the paper implies use of standard training datasets with human-annotated ground truth for tracking."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not explicitly describe providing detailed annotation instructions or guidelines for the annotators of the new dataset or its training data; it only mentions standard datasets used for training."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not mention any scoring rubrics or evaluation criteria used as annotation guidelines for the new dataset's annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "There is no explicit description of annotation examples provided as guidelines or instructions for annotators in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-361-rubric-2",
    "token_usage": {
      "prompt_tokens": 20294,
      "completion_tokens": 291,
      "total_tokens": 20585
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on any new dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information or description about multiple human experts performing quality assurance on any newly introduced dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single human non-expert annotator for any dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human non-expert annotators have conducted quality assurance for any of the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any use of AI models for quality assurance on datasets introduced."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated or algorithmic quality assurance methods are described or documented for any dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. It uses existing benchmark datasets such as GOT-10k, LaSOT, TrackingNet, DAVIS17, VOT2020 for evaluation and training, and there is no description of any quality assurance process for new datasets because no new datasets are introduced."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-361-rubric-3",
    "token_usage": {
      "prompt_tokens": 19904,
      "completion_tokens": 368,
      "total_tokens": 20272
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 6.1, 'Finetune on tracking'",
          "reasoning": "The training datasets used for finetuning the proposed tracking model include several existing datasets: LaSOT, GOT-10K, COCO2017, and TrackingNet, which are all publicly available datasets. The paper uses these datasets for training but does not indicate that they created new data from scratch."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 6.1, 'Finetune on tracking'",
          "reasoning": "While the authors do not explicitly state creation of new datasets, they mention training on multiple existing datasets combined including LaSOT, GOT-10K, COCO2017, and TrackingNet, effectively deriving a new training setup leveraging multiple sources. Additionally, the model can be pretrained on ImageNet unpaired images, implying adaptation or derived use from these sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources and training datasets are clearly specified."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-361-rubric-4",
    "token_usage": {
      "prompt_tokens": 20422,
      "completion_tokens": 223,
      "total_tokens": 20645
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. Instead, it uses several existing datasets such as ImageNet, GOT-10k, LaSOT, TrackingNet, and DAVIS for pre-training, fine-tuning, evaluation, and analysis. All mentions of datasets pertain to pre-existing benchmarks, and there is no description or demonstration of a new dataset created by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-361-rubric-5",
    "token_usage": {
      "prompt_tokens": 21145,
      "completion_tokens": 542,
      "total_tokens": 21687
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset entries containing more than two human languages. The datasets mentioned (e.g., ImageNet, GOT-10k, LaSOT) are standard visual datasets with no indication of multilingual language content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any datasets with exactly two human languages. The datasets are visual tracking datasets which do not involve bilingual language content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset containing English-language textual content is introduced. The datasets used for training and evaluation are image and video datasets without linguistic annotations or content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset containing non-English textual content is introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains code-related symbols and equations but does not introduce a dataset with programming or code snippets as content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes mathematical formulas and notation for method explanation, it does not present a new dataset containing mathematical or logical expressions as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication datasets are introduced in this paper."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss or introduce datasets involving constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) of the proposed datasets are not unspecified; however, no new dataset with linguistic content is introduced."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper focuses on visual object tracking datasets which consist primarily of images and video frames without any linguistic or language-based entries. Therefore, the proposed datasets do not contain any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-361-rubric-6",
    "token_usage": {
      "prompt_tokens": 18363,
      "completion_tokens": 166,
      "total_tokens": 18529
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention of code release for dataset construction in the paper",
          "reasoning": "The paper discusses training datasets used for model training (e.g., LaSOT, GOT-10K, COCO2017, TrackingNet) which are pre-existing datasets, and does not mention creation of a new dataset or provide code for dataset construction or preprocessing. The code repository link provided in the acknowledgments pointer references tracking-related code but not dataset construction code."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section or details describing dataset construction process",
          "reasoning": "The paper does not introduce any new datasets, nor does it describe any dataset creation or data collection process. It only uses existing public datasets for training and evaluation, without dataset creation details."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-370-rubric-0",
    "token_usage": {
      "prompt_tokens": 11902,
      "completion_tokens": 148,
      "total_tokens": 12050
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets; Section 3.1 Overall Framework",
          "Reasoning": "The new dataset aspect is the authors' use of the UCF101 dataset training split 1 for self-supervised learning and as the dataset for training and evaluation. UCF101 consists of videos collected from YouTube, thus human recorded and generated videos. The paper does not present any new dataset collected or created by authors. The use of UCF101 is mentioned as the training data used for self-supervised learning with their method, but it is a pre-existing dataset, not newly introduced by authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-370-rubric-1",
    "token_usage": {
      "prompt_tokens": 12702,
      "completion_tokens": 267,
      "total_tokens": 12969
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3 Temporal Prediction with Edit Distance; Section 3.2 Cross-Architecture Contrastive learning",
            "reasoning": "The temporal annotation (Minimum Edit Distance) is computed automatically between original and shuffled video clips. The shuffle degree labels for self-supervised temporal prediction are generated by an algorithmic process without human involvement. Additionally, the positive and negative pairs in contrastive learning are generated from different architectures and augmentations through automated procedures."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper of annotation instructions for annotators, as annotation is automated",
            "reasoning": "Since annotations are generated automatically based on shuffled video sequences and computed edit distances, no human instruuctive guidelines are required or provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not applicable for automated edit distance computation and contrastive pair generation",
            "reasoning": "Scoring rubrics for human raters are not applicable, as labels are generated algorithmically without subjective human rating or scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No mention of annotation examples or guidelines in the paper",
            "reasoning": "Because the labeling is via automatic computation of shuffle degrees and architecture embeddings, no manual annotation examples are relevant or provided."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-370-rubric-2",
    "token_usage": {
      "prompt_tokens": 13892,
      "completion_tokens": 314,
      "total_tokens": 14206
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert for dataset annotations or content validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance performed by multiple human experts for dataset content or annotations in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that a single non-expert performed quality assurance on dataset annotations."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is given in the paper about quality assurance by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of AI models to perform quality assurance on the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses algorithmic computation of edit distances for temporal prediction, this is part of the self-supervised learning method, not a quality assurance step for dataset annotations or content validation. There is no description of automated verification or QA processes being applied to dataset quality."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses pre-existing datasets UCF101 and HMDB51 and does not introduce any new dataset. Accordingly, there is no description or indication of any quality assurance process applied or documented for any new dataset."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-370-rubric-3",
    "token_usage": {
      "prompt_tokens": 13502,
      "completion_tokens": 432,
      "total_tokens": 13934
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing datasets such as UCF101 and HMDB51 for training and evaluation. There is no indication that the authors collected or created any new video data from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses machine learning models for self-supervised learning, the datasets themselves are not generated by models. No new datasets generated entirely by AI or machine learning models are introduced."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data translated from another language by human translators in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine-translated data in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation Details, Datasets",
          "reasoning": "The paper uses pre-existing datasets UCF101 and HMDB51 for training and evaluation. These datasets are collected from existing sources such as YouTube and digitized movies. The paper does not report new data collection but uses these existing datasets as is, thus the data is collated from existing sources without significant modifications."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper applies standard data augmentations and temporal shuffling as part of training but does not construct new datasets derived from existing sources with considerable modifications. The datasets themselves are not adapted or transformed into new datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data used is clearly documented as existing datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-370-rubric-4",
    "token_usage": {
      "prompt_tokens": 14020,
      "completion_tokens": 544,
      "total_tokens": 14564
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation Details, and Section 3 Methodology",
          "reasoning": "The authors use the UCF101 training split 1 dataset for self-supervised pre-training of their video representation model using the proposed cross-architecture contrastive learning (CACL) framework. This usage is primarily for pre-training to learn general video features from unlabeled videos."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention training a model from randomly initialized parameters solely on a newly introduced dataset. The training involves self-supervised pre-training on existing datasets (UCF101)."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation Details - Supervised fine-tuning and evaluation",
          "reasoning": "After self-supervised pre-training on UCF101 training split 1, the 3D CNN encoder is fine-tuned using supervised learning for action recognition on UCF101 and HMDB51 datasets. This fine-tuning uses the labels from these datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any reinforcement learning based post-training methods applied on the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3 Video Retrieval and 4.4 Action Recognition",
          "reasoning": "The UCF101 and HMDB51 datasets are used for evaluation tasks including video retrieval and action recognition to benchmark the performance of the learned video representation models."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No use of the datasets solely for analysis or trend study is described in the paper."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as a knowledge base to augment models through retrieval or other such methods in this paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets (UCF101, HMDB51) are explicitly used for pre-training, supervised fine-tuning, and evaluation, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-370-rubric-5",
    "token_usage": {
      "prompt_tokens": 14743,
      "completion_tokens": 592,
      "total_tokens": 15335
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces a new self-supervised video representation learning framework and evaluates it primarily on datasets containing videos labeled with action categories, but there is no indication that the new dataset introduced contains entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset entries containing exactly two human languages in the proposed new dataset(s)."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation Details - Datasets; Section 1 Introduction",
          "reasoning": "The proposed new dataset for self-supervised training is based on the UCF101 dataset which consists of videos annotated with English category labels. The dataset videos, as is standard, come with English-language annotations and all related descriptions and labels appear to be in English. Additionally, no other languages are mentioned for the new dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of datasets with entries in a single non-English language is made in the paper regarding the new dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper describes architectures (3D CNN, Transformer), losses, and algorithms using formulas, there is no dataset described that contains programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains mathematical formulas to define losses and measures like Edit distance, but these are not part of the dataset entries themselves. No dataset entries are described as containing mathematical or formal logical expressions as data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The proposed dataset does not contain biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The proposed dataset does not include fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is specified and documented; it is English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The proposed dataset involves natural language labels (English) and therefore contains language entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-370-rubric-6",
    "token_usage": {
      "prompt_tokens": 11961,
      "completion_tokens": 168,
      "total_tokens": 12129
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper",
          "reasoning": "The paper does not provide any explicit information, links, or references to publicly available code repositories related to dataset construction, data collection, or preprocessing. The experimental section only describes the datasets used (UCF101 and HMDB51) but does not mention releasing code for creating or processing these datasets."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No explicit description in the paper",
          "reasoning": "The paper uses standard existing datasets (UCF101, HMDB51) for evaluation and mentions sampling protocols and data augmentations for training, but does not introduce any new dataset or provide detailed documentation on dataset construction, preprocessing, or creation. As no new dataset is introduced, dataset creation documentation is not provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-428-rubric-0",
    "token_usage": {
      "prompt_tokens": 10715,
      "completion_tokens": 175,
      "total_tokens": 10890
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Sections 3.1, 3.2, 4.1, and 4.3",
          "Reasoning": "The new dataset introduced for training in this paper consists of paired images with known camera poses used for weakly supervised learning of local features. The images are standard natural images captured via cameras involving human effort; the paper mentions the use of camera pose information as weak supervision. There is no indication that the images are synthetically generated or simulated. The paper specifically introduces a method (PoSFeat) trained on image pairs with relative camera poses for descriptor and keypoint detection learning (Section 3.1 and 3.2) and evaluates on standard datasets, indicating their use of real images as new training data."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-428-rubric-1",
    "token_usage": {
      "prompt_tokens": 11515,
      "completion_tokens": 219,
      "total_tokens": 11734
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3 - Decoupled Training Describe-then-Detect Pipeline",
            "reasoning": "The annotation process for the new dataset is not performed by humans but is based on leveraging existing camera pose information as weak supervision to generate training correspondences automatically. The paper describes using camera poses and epipolar geometry constraints rather than human annotators to supervise descriptor and detection learning."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not provide or mention any detailed instructions for human annotators since annotation is performed automatically via camera poses and geometric constraints."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics for manual annotation are discussed or provided as the supervision is weak and automatic without human-rated scores."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No examples of manual annotations or explicit annotated sample guidance are provided; supervision is from camera pose constraints, not human annotations."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-428-rubric-2",
    "token_usage": {
      "prompt_tokens": 12705,
      "completion_tokens": 374,
      "total_tokens": 13079
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process performed by a single human expert on any new dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple human experts performing quality assurance on any new dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given of quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No multiple non-expert human quality assurance is reported or suggested."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of AI models to perform quality assurance on new datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the method employs automated learning and loss functions, there is no mention of automated quality assurance processes for data annotations or similar."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new method for weakly supervised local feature learning but does not present any new annotated dataset requiring quality assurance processes. There is no mention or description of any dataset annotation or validation procedure, nor quality assurance on dataset content, indicating that no quality assurance process is applied or documented for any new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-428-rubric-3",
    "token_usage": {
      "prompt_tokens": 12315,
      "completion_tokens": 358,
      "total_tokens": 12673
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creating or collecting any new dataset entirely from scratch by human contributors. The focus is on a learning method for local features, leveraging existing datasets and weak supervision via camera poses, but no indication of new original human-created data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses model training and evaluation but does not introduce any dataset that is synthetically generated or created entirely by AI or machine learning models. It only uses data for training and evaluation purposes."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of any data generated by translating content from another language by human translators is present in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data is described in the paper as being generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state or suggest that a new dataset is created by collecting or aggregating existing sources without significant modification. It relies on existing datasets (e.g., HPatches, Aachen Day-Night) for evaluation but does not curate a new collated dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new dataset as derived from existing sources with modifications or adaptations. It focuses on method development and evaluation on benchmark datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce or document the creation or sourcing of any new dataset. All datasets used for training or evaluation are existing benchmarks, and no new dataset with original data origin is specified."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-428-rubric-4",
    "token_usage": {
      "prompt_tokens": 12833,
      "completion_tokens": 270,
      "total_tokens": 13103
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 (Evaluation on HPatches, Aachen Day-Night, 3D reconstruction tasks)",
          "reasoning": "The paper extensively evaluates their method PoSFeat on several benchmark datasets for local feature evaluation, such as HPatches and Aachen Day-Night, and compares performance with previous methods to demonstrate state-of-the-art results. These datasets are used as benchmarks for evaluation, and no new datasets introduced by the authors for training are described."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets explicitly; all datasets mentioned are existing benchmarks used for evaluation. Therefore, no new dataset's utility within the machine learning pipeline is described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-428-rubric-5",
    "token_usage": {
      "prompt_tokens": 13556,
      "completion_tokens": 447,
      "total_tokens": 14003
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of datasets with exactly two human languages was found in the paper."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Multiple locations, e.g., Abstract, Section 1 Introduction, Section 4 Experiments",
          "reasoning": "The paper is written in English and the introduced new dataset entries (i.e., image datasets used for training and evaluation) are standard image datasets without multi-language textual content; all documentation and descriptions are in English. The paper does not mention any other spoken or written languages present in the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any datasets containing content exclusively in non-English languages."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper includes method descriptions and mathematical formulations, it does not introduce a dataset consisting of code or programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains formulae and mathematical notation describing methods, but does not introduce a dataset containing such notation as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data is introduced in the new dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention constructed or fictional languages in any new dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of dataset entries is clearly English or consists of image data; there's no indication of unknown or unspecified linguistic content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The primary data entity introduced or used in the paper is image data (pixels) with camera pose information, which does not intrinsically contain language. Therefore, the dataset itself, consisting of images and corresponding data, does not contain any linguistic content."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-428-rubric-6",
    "token_usage": {
      "prompt_tokens": 10774,
      "completion_tokens": 118,
      "total_tokens": 10892
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Not mentioned in the paper",
          "reasoning": "The paper does not include any links or references to publicly available code repositories related to dataset construction or processing. There is no mention of releasing code associated with data collection, preprocessing, or dataset construction."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Not applicable",
          "reasoning": "The paper does not introduce any new datasets created by the authors. It only uses existing public datasets for evaluation. Consequently, there is no documentation of dataset creation processes provided in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-452-rubric-0",
    "token_usage": {
      "prompt_tokens": 12943,
      "completion_tokens": 129,
      "total_tokens": 13072
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Datasets and Table 2",
          "Reasoning": "The authors explicitly describe using six datasets, including Pitts30k and Mapillary Street-Level Sequences (MSLS), for training and evaluation. The paper and Table 2 present these datasets as composed of images captured by human-operated cameras, such as car-rooftop cameras and smartphone cameras, indicating human involvement in data capture. However, these datasets are used for training and testing but are not introduced as new datasets by the authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-452-rubric-1",
    "token_usage": {
      "prompt_tokens": 13743,
      "completion_tokens": 254,
      "total_tokens": 13997
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2 Datasets, Table 2",
            "reasoning": "The paper describes six datasets used in the benchmark: Pitts30k, MSLS, Tokyo 24/7, R-SF, Eynsham, and St Lucia. Among these, Pitts30k and MSLS are used for training, and MSLS is a more recently introduced dataset by [75]. However, all datasets are pre-existing and publicly available datasets; the paper does not explicitly introduce any new dataset. Instead, it introduces a benchmarking framework and experimental evaluations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No explicit description or section in the paper provides annotation instructions for any newly introduced dataset, as no new dataset is introduced."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "Scoring rubrics are not discussed for any newly introduced dataset, since no new dataset is introduced."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples are provided for any new datasets; the paper focuses on benchmarking existing datasets rather than introducing annotated datasets or annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-452-rubric-2",
    "token_usage": {
      "prompt_tokens": 14933,
      "completion_tokens": 308,
      "total_tokens": 15241
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance performed by a single human expert for any new dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts conducted quality assurance on any new dataset introduced by the authors."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality checks conducted by a single human non-expert on dataset annotations or content."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of AI models used to perform quality assurance on dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes extensive automated evaluation protocols and benchmarking frameworks, it does not specify that dataset annotation quality was verified through automated or rule-based processes."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper primarily uses existing public datasets (e.g., Pitts30k, MSLS, Tokyo 24/7) with no new datasets introduced, and does not document any quality assurance processes related to dataset annotations or content for new data. The benchmark focuses on training and testing protocols rather than dataset creation or annotation verification."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-452-rubric-3",
    "token_usage": {
      "prompt_tokens": 14543,
      "completion_tokens": 365,
      "total_tokens": 14908
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any new data was created entirely from scratch by human contributors for this work."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new data generated entirely by AI or machine learning models without reference to or transformation of existing data is introduced."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data generated via machine translation in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.2 Datasets",
          "reasoning": "The paper uses existing publicly available datasets such as Pitts30k and MSLS for training and testing. These datasets were collected previously and aggregated for the purposes of evaluation, without indications of new data creation."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe creating new datasets by modifying or adapting existing data; rather, it leverages existing datasets without transformation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly states the datasets used, their origins, and characteristics, so the data source is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-452-rubric-4",
    "token_usage": {
      "prompt_tokens": 15061,
      "completion_tokens": 336,
      "total_tokens": 15397
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2, Section 4.1, Section 4.2, Section 4.3",
          "reasoning": "The paper describes using new experimental training datasets such as Pitts30k and MSLS to train models from scratch, including CNN backbones and Visual Transformers, demonstrating their use in training for Visual Geo-localization systems."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2, Section 4, Table 2",
          "reasoning": "The datasets (e.g., Tokyo 24/7, R-SF, Eynsham, St Lucia) are used as standard testing datasets for benchmarking and measuring recall metrics, to evaluate the performance and generalization of VG systems."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4, various subsections (4.1 to 4.7)",
          "reasoning": "The paper conducts extensive analysis on how different datasets impact performance, including how training on one affects testing on others, resolution effects, augmentations and mining methods, using datasets to analyze trends and characteristics rather than solely training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-452-rubric-5",
    "token_usage": {
      "prompt_tokens": 15784,
      "completion_tokens": 584,
      "total_tokens": 16368
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any newly introduced dataset contains entries with more than two human languages. The datasets mentioned are collections of images from various geographic locations without reference to multiple language contents."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of newly proposed datasets containing exactly two human languages in their entries."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used or mentioned in the paper, such as Pitts30k and MSLS, contain images and geo-tagged data but do not have textual content exclusively in English; also, no new datasets are introduced by the authors that focus on English language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No newly introduced datasets are specified to contain entries in exactly one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors provide a benchmarking framework and code for visual geo-localization but do not introduce new datasets containing programming or structured code-related data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper includes mathematical notation to describe methods and metrics, there is no new dataset introduced that contains entries with mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets and benchmarks discussed involve images and geo-tagging data and do not include biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper of datasets involving constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language or linguistic characteristics of the datasets are either non-applicable or explicitly non-linguistic; hence, the language of the dataset entries is not unknown but irrelevant."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced by the authors, or the datasets used in their framework, consist primarily of geo-tagged images without linguistic content. Hence, the datasets do not contain entries with any human language content."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-452-rubric-6",
    "token_usage": {
      "prompt_tokens": 13002,
      "completion_tokens": 139,
      "total_tokens": 13141
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 3.1 and abstract",
          "reasoning": "The paper states that the code of the benchmark is open-source and available at https://deep-vg-bench.herokuapp.com/. It includes scripts to download and format a number of datasets and to train and test the models, thereby enabling reproducibility of results and experiments."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "",
          "reasoning": "While the paper describes the use of existing datasets and provides a table summarizing them, it does not introduce any new datasets created by the authors. Therefore, the paper does not provide documentation on dataset creation processes for new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-52-rubric-0",
    "token_usage": {
      "prompt_tokens": 12719,
      "completion_tokens": 128,
      "total_tokens": 12847
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets and throughout experimental setup",
          "Reasoning": "The paper evaluates the proposed method on several existing event-based classification datasets (e.g., N-MNIST, N-Caltech101, CIFAR10-DVS, N-CARS, ASL-DVS) but does not introduce any new dataset of its own. All these datasets are recorded event camera data, thus are 'signal/sensor' modality and captured by human-controlled cameras in real environments or recorded images."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-52-rubric-1",
    "token_usage": {
      "prompt_tokens": 13519,
      "completion_tokens": 271,
      "total_tokens": 13790
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.1 Datasets and Section 4.2 Implementation details",
            "reasoning": "The paper introduces its own evaluation on established event-based classification datasets but does not explicitly describe dataset creation or annotation methods. They use public benchmark datasets (N-MNIST, N-Caltech101, CIFAR10-DVS, N-CARS, ASL-DVS), all introduced prior to this work. The paper does not mention introducing any new dataset; rather, they perform experiments on these existing datasets. Thus, no new dataset annotation was performed. Instead, experiments were conducted on pre-existing datasets."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the entire paper",
            "reasoning": "The paper does not describe providing annotation instructions for any dataset because it does not introduce a new dataset. It only evaluates the proposed method on existing datasets."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the entire paper",
            "reasoning": "Since no annotation guidelines or new dataset are described, no scoring rubrics are noted."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the entire paper",
            "reasoning": "No annotation examples or guidelines are provided because no new dataset annotation process is detailed."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-52-rubric-2",
    "token_usage": {
      "prompt_tokens": 14709,
      "completion_tokens": 289,
      "total_tokens": 14998
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was conducted by a single human expert annotator for the datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper that multiple human experts performed quality assurance on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper about multiple non-expert annotators performing quality assurance on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automatic quality assurance performed by AI models on the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automatic verification or algorithmic quality assurance process applied to the datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses several pre-existing benchmark event-based classification datasets and does not indicate that the authors created any new datasets or applied any quality assurance processes on dataset annotations or content. There is no documentation or description of a QA process related to dataset construction or annotation validating quality or consistency."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-52-rubric-3",
    "token_usage": {
      "prompt_tokens": 14319,
      "completion_tokens": 430,
      "total_tokens": 14749
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the authors created any new datasets entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any dataset was generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data that were produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of machine translation to generate data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper reports experimental results on five existing benchmark datasets (N-MNIST, N-Caltech101, CIFAR10-DVS, N-CARS, ASL-DVS) that were not created by the authors. There is no description that the authors aggregated or compiled these datasets into a new combined dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors apply a novel voxel-wise graph construction to represent event data, this is a method of data representation for learning rather than creation of a new dataset; the underlying data used for experiments come from existing datasets without mention of modification to the original datasets to produce a new dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets created by the authors or new data collections. All datasets used are existing benchmark datasets; the origin or generation method of data is documented but no new dataset creation or new data sources are introduced."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-52-rubric-4",
    "token_usage": {
      "prompt_tokens": 14837,
      "completion_tokens": 451,
      "total_tokens": 15288
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using any new datasets for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing benchmark datasets for training their proposed model, and does not introduce any new datasets utilized for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper suggests that newly introduced datasets are used for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning post-training techniques or datasets introduced for such purposes."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper evaluates the proposed method on various well-known, pre-existing event-based datasets; it does not introduce any new dataset used exclusively for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper analyzes characteristics of event data representations and model performance, it does not present any new dataset primarily for analysis purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any new dataset serving as a knowledge base for augmenting models."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. Instead, it utilizes existing datasets such as N-MNIST, N-Caltech101, CIFAR10-DVS, N-CARS, and ASL-DVS for training and evaluation, without creating or curating new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-52-rubric-5",
    "token_usage": {
      "prompt_tokens": 15560,
      "completion_tokens": 460,
      "total_tokens": 16020
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that the introduced datasets contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes mathematical notation to explain methods, the datasets themselves are collections of event data related to object classification and do not contain mathematical or logical expressions as dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of event camera recordings of objects and scenes; there is no mention of biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of constructed, fictional, or artificial languages in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language nature of the dataset entries is clear and based on event recordings; thus the dataset language is not unknown."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced and used in the paper consist of event streams and event-based visual data captured by event cameras representing physical events (pixels, timestamps, polarity), not linguistic data or textual content. Therefore, the datasets do not contain entries with any human or programming language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-52-rubric-6",
    "token_usage": {
      "prompt_tokens": 12778,
      "completion_tokens": 128,
      "total_tokens": 12906
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section in the paper",
          "reasoning": "The paper does not mention or provide any link or reference to publicly available code for dataset construction, data collection, or preprocessing related to the event datasets used."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 4.1 and throughout the paper",
          "reasoning": "The paper does not introduce any new datasets nor describe any new dataset construction or collection procedure. It only evaluates on existing benchmark event datasets and explains their origins briefly, but provides no documentation of dataset creation or data collection processes by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-540-rubric-0",
    "token_usage": {
      "prompt_tokens": 11158,
      "completion_tokens": 164,
      "total_tokens": 11322
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 4 (Experiments)",
          "Reasoning": "The paper introduces new meta-learning experiments conducted on CIFAR-fs and mini-ImageNet datasets, which are image datasets. The authors train and evaluate their proposed META DOCK method on these datasets, indicating they use these image datasets as experimental data. Since CIFAR-fs and mini-ImageNet are established image datasets collected through human efforts (photographs), the data is human-generated. The paper does not mention introducing any other new dataset besides these existing image datasets, so the new dataset entries introduced refer to these. The data is image modality, human generated (photographs), not model generated or unknown origin."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-540-rubric-1",
    "token_usage": {
      "prompt_tokens": 11958,
      "completion_tokens": 267,
      "total_tokens": 12225
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4 Experiment and associated tables and text",
            "reasoning": "The paper discusses multiple experimental studies and evaluations on the newly introduced META DOCK approach, with measurements of model performance and pruning strategies, indicating annotation or evaluation was conducted by multiple human experts/researchers as part of the experimental procedure."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not mention any detailed annotation instructions given to annotators for the new dataset or annotations since the dataset is from benchmarks CIFAR-fs and mini-ImageNet and the evaluation is experimental rather than manual data annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics or formal annotation scoring schemes are documented in the paper for the new datasets or any related annotations. The evaluations are quantitative metrics (accuracy, pruning budget) not manual annotation with rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples or guideline examples are provided in the paper as the data used (meta-learning tasks from CIFAR-fs and mini-ImageNet) are established benchmarks and the work focuses on model pruning strategy rather than data annotation with examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-540-rubric-2",
    "token_usage": {
      "prompt_tokens": 13148,
      "completion_tokens": 292,
      "total_tokens": 13440
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on any new dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts were involved in quality assurance of any new dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no description of quality assurance conducted by a single non-expert human annotator for the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper about QA conducted by multiple human non-experts on the datasets used or introduced."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by AI models acting as judges for dataset validation or annotation QA."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any automatic or algorithmic quality assurance process applied to datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; it uses existing benchmark datasets (CIFAR-fs, mini-ImageNet). Since no new datasets are introduced, no quality assurance process for new datasets is described or applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-540-rubric-3",
    "token_usage": {
      "prompt_tokens": 12758,
      "completion_tokens": 386,
      "total_tokens": 13144
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the creation of any new dataset collected or created manually by human contributors from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets generated entirely by AI models; it focuses on methodology to improve meta-learning model pruning, not on data generation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of any dataset produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets generated via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used (CIFAR-fs and mini-ImageNet) are pre-existing public benchmarks; the paper does not claim to have aggregated data into a new dataset without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors conduct experiments on existing datasets, they do not describe generating new datasets derived or adapted from existing ones."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; it only uses existing benchmark datasets (CIFAR-fs and mini-ImageNet) to evaluate their proposed method. Therefore, no data origin or generation method for a new dataset is specified or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-540-rubric-4",
    "token_usage": {
      "prompt_tokens": 13276,
      "completion_tokens": 268,
      "total_tokens": 13544
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 - Experiments",
          "reasoning": "The paper evaluates the efficacy of the META DOCK approach on meta-learning benchmark datasets CIFAR-fs and mini-ImageNet by generating multiple test tasks and measuring accuracy. While these datasets are standard benchmarks, the paper does not introduce any new datasets; the existing datasets are used solely for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; rather, it uses standard datasets CIFAR-fs and mini-ImageNet which are pre-existing and are used only for evaluation. No new dataset is introduced or practically used in training, fine-tuning, or analysis within this paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-540-rubric-5",
    "token_usage": {
      "prompt_tokens": 13999,
      "completion_tokens": 497,
      "total_tokens": 14496
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset containing entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the new datasets involve exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets introduced by the authors are not described as containing English text content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets having entries in exactly one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper presents methods and experiments but does not introduce datasets containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes mathematical equations and notation in methodology, it does not introduce a dataset composed of mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets discussed are standard image classification benchmarks; no biological or non-human communication data is introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets involving fictional or artificially created languages are introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language contents of datasets are clearly known and documented, or the datasets concern image data rather than language."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper only uses existing image classification datasets (CIFAR-fs and mini-ImageNet) for benchmarking. It does not introduce any new datasets containing language or textual content. Therefore, the new datasets presented do not contain any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-540-rubric-6",
    "token_usage": {
      "prompt_tokens": 11217,
      "completion_tokens": 183,
      "total_tokens": 11400
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and throughout the paper",
          "reasoning": "The paper provides a link to the code for their META DOCK method at https://github.com/transmuteAI/MetaDOCK, which seems related to their method implementation. However, this repository is described as the code for their meta-learning method and not specifically for dataset construction or dataset code. Also, the datasets used (CIFAR-fs and mini-ImageNet) are existing standard benchmark datasets, and no new dataset is introduced by the authors in the paper. Therefore, there is no code related to dataset construction or generation provided by the authors because no new dataset is introduced."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "N/A",
          "reasoning": "Since the paper does not introduce any new datasets, there is no dataset creation process described or documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-545-rubric-0",
    "token_usage": {
      "prompt_tokens": 11392,
      "completion_tokens": 348,
      "total_tokens": 11740
    },
    "response": {
      "sources": [
        {
          "Modality": "time series",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, especially 3.1 Multi-spectral imagery and 3.2 Pixel-wise labels",
          "Reasoning": "The DynamicEarthNet dataset introduced in the paper consists of daily multi-spectral satellite imagery sequences over two years for 75 areas, providing time-series data with daily multi-spectral observations (4 channels: RGB + near-infrared). These images are raw observational data captured by the Planet Labs satellite constellation, thus human-generated in the sense that humans operate the satellite systems that capture the images. These are not model-generated or simulated data, and the origin is explicitly stated in the paper. The authors also provide monthly pixel-wise semantic segmentation labels for the first day of each month for the time series, derived through a manual annotation process aided by quality control. Therefore, this modality is accurately described as time series data with human-generated origin as captured by satellite and manual annotation."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Pixel-wise labels",
          "Reasoning": "The dataset includes pixel-wise semantic segmentation labels corresponding to each AOI for the first day of every month (24 semantic maps per AOI). The labels are manually annotated for the first image of each AOI and updated monthly when changes are observed. The labeling process involves human annotators with multiple quality control gates ensuring high-quality, temporally consistent labels. The labels are pixel-wise dense annotations, effectively image-like maps representing semantics. These labels are human-generated as they are manually created by experts. Not model-generated or unknown in origin."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-545-rubric-1",
    "token_usage": {
      "prompt_tokens": 12192,
      "completion_tokens": 242,
      "total_tokens": 12434
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.2",
            "reasoning": "The annotation procedure is described as rigorous, with the first image for each AOI manually annotated by a human annotator, followed by updates based on perceptible changes. Three quality control gates, each with a different annotator, ensure annotation accuracy, indicating multiple human experts involved."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2",
            "reasoning": "The annotation procedure emphasizes temporal consistency and includes a structured process of initial manual annotation followed by monthly updates and quality control, implying detailed annotation instructions were provided to annotators to ensure consistency and accuracy."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper does not mention any scoring rubrics or explicit criteria for scoring or evaluating annotations beyond the quality control gates, thus no evidence of formal rubric usage is found."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There is no mention of annotation examples provided in guidelines or appendix; the paper does not describe inclusion of annotation examples for guiding annotators."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-545-rubric-2",
    "token_usage": {
      "prompt_tokens": 13382,
      "completion_tokens": 430,
      "total_tokens": 13812
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly mention quality assurance being conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.2 Pixel-wise labels",
          "reasoning": "The annotation procedure describes that the first image of each AOI was manually annotated, and subsequent monthly maps were updated by a human annotator. Importantly, three different quality control gates were employed, each with a different annotator, ensuring overall annotation quality, topological correctness, and format correctness. The involvement of multiple human annotators in quality control gates implies that quality assurance was conducted by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple non-expert annotators performed quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention AI models being used as judges in the quality assurance process for the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the data was post-processed for cloud removal and gap-filling, the paper does not describe this as a quality assurance process for annotation validation, rather as data pre-processing. No automatic verification of annotations is described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents a quality assurance process involving multiple human annotators; thus, QA is not absent."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-545-rubric-3",
    "token_usage": {
      "prompt_tokens": 12992,
      "completion_tokens": 457,
      "total_tokens": 13449
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.2 Pixel-wise labels",
          "reasoning": "The semantic segmentation labels for the first day of each month were manually annotated by human annotators with a rigorous procedure involving multiple quality control gates to ensure accuracy, temporal consistency, and topological correctness. This indicates that the labeled data was created entirely by humans from scratch rather than translated or derived from existing annotations."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any part of the dataset (imagery or labels) was generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data being produced by translating content from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data being created by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Multi-spectral imagery",
          "reasoning": "The dataset's multi-spectral satellite imagery is derived from Planet Labs' Fusion Monitoring product, which provides daily multi-spectral images that are gap-filled and calibrated. The authors aggregated and curated this existing satellite imagery from Planet Labs without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Multi-spectral imagery",
          "reasoning": "The Planet Labs imagery was gap-filled (missing data due to clouds filled with nearest available observations) and calibrated to the Harmonized Landsat-Sentinel spectrum to harmonize with other datasets. This involves modifications and transformations to the raw imagery, representing derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset's origin and generation process is clearly documented and stated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-545-rubric-4",
    "token_usage": {
      "prompt_tokens": 13510,
      "completion_tokens": 515,
      "total_tokens": 14025
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5.1 Baselines, Section 5.2 Land cover and land use segmentation",
          "reasoning": "The authors use the DynamicEarthNet dataset to train models from scratch, as demonstrated by the supervised baseline training of models like U-Net and DeepLabv3+ on the labeled monthly data. The dataset serves as the primary supervised training data for these models to learn semantic segmentation and semantic change segmentation."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the dataset specifically for fine-tuning pre-trained models; instead, models are trained either supervised or semi-supervised directly on the dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper makes no mention of reinforcement learning or RL-based post-training techniques such as RLHF using this dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Semantic change segmentation, Section 5 Experiments",
          "reasoning": "The DynamicEarthNet dataset is used for evaluation by benchmarking semantic segmentation and semantic change segmentation methods, including the introduction of the Semantic Change Segmentation (SCS) metric proposed by the authors for performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 1 Introduction, Section 3.3 Dataset statistics, Section 5 Experiments",
          "reasoning": "The authors analyze dataset characteristics such as class distributions, temporal resolution, difficulties of certain classes, and showcase the dataset's potential for understanding land use evolution and environmental changes over time."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe usage of the dataset as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly uses the dataset for training, evaluation, and analysis as described above."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-545-rubric-5",
    "token_usage": {
      "prompt_tokens": 14233,
      "completion_tokens": 529,
      "total_tokens": 14762
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of satellite multi-spectral images and semantic labels of land use/land cover classes but does not include any human language content, bilingual or multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of any two specific human languages present in the dataset entries."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not contain textual content in English; the data are satellite images and pixel-wise semantic class annotations without language."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English language textual content is present in the dataset entries."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset provides satellite imagery and semantic segmentation labels, with no code or programming language data included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Section 4.1 and 4.2 describe mathematical notation used for defining evaluation metrics, but this relates to evaluation protocol, not dataset entries.",
          "reasoning": "Mathematical notation occurs in the paper's methodology section for defining metrics, but the dataset entries themselves do not contain mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of satellite images of land surfaces; it does not contain biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or constructed languages are present in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language type is clearly not applicable as it contains image and semantic label data without linguistic content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The dataset contains only satellite images and pixel-wise semantic land cover labels with no linguistic elements present."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-545-rubric-6",
    "token_usage": {
      "prompt_tokens": 11451,
      "completion_tokens": 219,
      "total_tokens": 11670
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or link in the paper describes or provides code for dataset construction.",
          "reasoning": "While the paper provides a download link to the dataset itself (https://mediatum.ub.tum.de/1650201), it does not mention or provide any publicly available code or repository for data collection, preprocessing, or generation used in creating the dataset. There is no indication of accessible code to reproduce the dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (The DynamicEarthNet dataset), subsections 3.1, 3.2, 3.3, and 3.4.",
          "reasoning": "The paper details the dataset creation process extensively: it describes the satellite image sources (Planet Labs Fusion Monitoring product), preprocessing steps (occlusion removal, gap-filling, calibration), the annotation process (manual annotation, quality control steps), the dataset statistics, and comparisons with existing datasets. This comprehensive information documents the dataset creation process sufficiently for understanding and partial reproducibility."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-562-rubric-0",
    "token_usage": {
      "prompt_tokens": 18195,
      "completion_tokens": 170,
      "total_tokens": 18365
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4 (Datasets) and Experiments",
          "Reasoning": "The paper introduces several datasets for evaluation, notably the NeedleCamelyon dataset and a class-balanced subset of the Functional Map of the World (fMoW) dataset constructed by the authors. The NeedleCamelyon dataset is created by cropping images from the Camelyon16 dataset with specified region of interest to image ratios, which is a novel preprocessing introduced by the authors for this study. The fMoW subset is also constructed by the authors to match their experimental goals. All these images originate from real sources captured by human efforts (e.g., pathology slides, satellite imagery), thus the modality is image and the origin is human generated."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-562-rubric-1",
    "token_usage": {
      "prompt_tokens": 18995,
      "completion_tokens": 210,
      "total_tokens": 19205
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4 Datasets descriptions",
            "reasoning": "The paper describes datasets such as the colon cancer dataset with approximately 22,000 annotated cells and the Camelyon16 dataset containing whole-slide images with pixel-level annotations used by experts for ground truth. This indicates annotations were performed by multiple human experts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No specific section describing annotation instructions",
            "reasoning": "The paper does not explicitly mention detailed annotation instructions provided to annotators for the new datasets; descriptions focus on dataset characteristics rather than annotation protocols."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention of scoring rubrics in annotation",
            "reasoning": "No evidence in paper of scoring rubrics for annotations in the newly introduced datasets."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples provided in paper of annotations",
            "reasoning": "The paper does not provide annotation examples or guidelines for annotators regarding the new datasets."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-562-rubric-2",
    "token_usage": {
      "prompt_tokens": 20185,
      "completion_tokens": 306,
      "total_tokens": 20491
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that quality assurance of the datasets was performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 4 Datasets and Camelyon16 description",
          "reasoning": "The Camelyon16 dataset used in the experiments contains pixel-level annotations which are provided by specialized medical experts as is common in medical imaging datasets. The paper states that manual annotations (ROIs) are available and compares results when pixel-level annotations exist, indicating quality assurance by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that a single non-expert human performed quality assurance on the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance performed by multiple non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance conducted by AI models as judges for dataset annotation quality."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe automatic verification processes for dataset quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents that pixel-level annotations exist for at least one dataset (Camelyon16) and imply expert annotation, suggesting some quality assurance processes were performed."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-562-rubric-3",
    "token_usage": {
      "prompt_tokens": 19795,
      "completion_tokens": 525,
      "total_tokens": 20320
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing datasets sourced from prior works or publicly available data; there is no indication that new data was created entirely from scratch by human contributors specifically for this paper."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report generating new data samples entirely using AI or machine learning models. All data used are existing natural images or annotated datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data being produced by translating content from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was generated using machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 'Datasets'",
          "reasoning": "The paper states that the experimental evaluation uses five datasets, all of which are sourced from existing public or prior datasets, such as the colon cancer dataset from [42], NeedleCamelyon built from Camelyon16, Traffic Signs Recognition dataset [25], Functional Map of the World (fMoW) dataset [5], and the original Camelyon16 dataset [30]. The authors extracted various subsets or subsets with specific properties from these existing datasets and used them for evaluation with little to no modification described beyond cropping or selection. This indicates the data is collated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4 'Datasets' (NeedleCamelyon dataset description)",
          "reasoning": "For the NeedleCamelyon dataset, the authors generated datasets for ROI-to-image ratios in a certain range by cropping images from the original Camelyon16 dataset, selecting specific annotated regions, and creating balanced positive and negative samples by random cropping and filtering. This constitutes derivation from existing data via cropping and filtering, i.e., modifications and adaptations of an existing dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the data sources and origins, so the origin is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-562-rubric-4",
    "token_usage": {
      "prompt_tokens": 20313,
      "completion_tokens": 256,
      "total_tokens": 20569
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), Datasets description and Results",
          "reasoning": "The newly constructed NeedleCamelyon dataset is used to train and evaluate the Zoom-In Network model from scratch with training, validation, and test splits specified and model training performed on these datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), Datasets and Results sections",
          "reasoning": "The newly constructed NeedleCamelyon dataset and the constructed subset from Functional Map of the World (fMoW) dataset are used exclusively for benchmarking the classification performance and GPU memory requirements of various models including the proposed Zoom-In Network."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-562-rubric-5",
    "token_usage": {
      "prompt_tokens": 21036,
      "completion_tokens": 576,
      "total_tokens": 21612
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced and used in the paper are all visual image datasets without any mention of multiple human languages in their entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets contain exactly two human languages; no language content is described."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4 Datasets",
          "reasoning": "All descriptions, metadata, and dataset labels in the paper are in English (e.g., dataset names like 'Colon Cancer dataset', 'NeedleCamelyon', 'Traffic Signs Recognition dataset', 'Functional Map of the World dataset', and 'Camelyon16'). The datasets are image datasets with English annotations or labels described in English. The paper is written in English and there is no mention of other languages used in the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets and their descriptions do not contain or are not described as being in any non-English languages."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of images and image annotations; they do not contain or include examples or entries of programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While mathematical notation appears in descriptions within the paper, the datasets themselves do not contain entries with mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the datasets include biological images (e.g., pathology images), the datasets themselves do not contain biological sequences or non-human communication content as dataset entries."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are part of the dataset entries."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is explicitly known and described (English)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets include language in the form of English labels and metadata; thus, language is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-562-rubric-6",
    "token_usage": {
      "prompt_tokens": 18254,
      "completion_tokens": 174,
      "total_tokens": 18428
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper.",
          "reasoning": "The paper does not include any links or references to publicly available code repositories related to data collection, preprocessing, or dataset creation. There is no indication that code for constructing any dataset is released or described in any section."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4, 'Datasets' subsection.",
          "reasoning": "The paper provides descriptions of the datasets used in the experiments, including details of dataset sources, preprocessing steps, image resolutions, and how subsets were constructed (e.g., the fMoW subset is constructed as described, NeedleCamelyon dataset is generated with defined ROI-to-image ratios). However, these are limited to the subset construction and experimental setup rather than full dataset construction or raw data generation procedures."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-583-rubric-0",
    "token_usage": {
      "prompt_tokens": 1015,
      "completion_tokens": 103,
      "total_tokens": 1118
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 'Ours' and 'Featuremaps' headings",
          "Reasoning": "The dataset involves human gaze data and feature maps related to gaze, suggesting the data comes from images captured with human involvement (e.g., gaze data recorded from real people). The mention of 'Human-Gaze' and 'Heatmaps' implies data generated by human observers' gaze patterns recorded on images."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-583-rubric-1",
    "token_usage": {
      "prompt_tokens": 1815,
      "completion_tokens": 175,
      "total_tokens": 1990
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Non-Expert",
            "reference": "Provided content does not specify annotation details",
            "reasoning": "The paper mentions 'Human-Gaze' and 'HumanHead Detector' but does not provide explicit details on the annotators, thus assuming a general human non-expert based on lack of specified expert involvement."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No specific section provided",
            "reasoning": "The paper does not explicitly mention annotation instructions for the new dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No specific section provided",
            "reasoning": "No scoring rubrics are described in the paper for annotations on the new dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No specific section provided",
            "reasoning": "No annotation examples are provided or mentioned for the new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-583-rubric-2",
    "token_usage": {
      "prompt_tokens": 3005,
      "completion_tokens": 263,
      "total_tokens": 3268
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert annotator for the new datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or mention in the paper of quality assurance involving multiple human experts for dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is provided that a single non-expert performed quality assurance on the dataset annotations."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance done by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence or claim that an AI model was used to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe automated or algorithmic quality assurance processes applied to validate dataset content or annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document any quality assurance process, manual or automated, for the new datasets introduced. Therefore, no quality assurance is considered present."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-583-rubric-3",
    "token_usage": {
      "prompt_tokens": 2615,
      "completion_tokens": 383,
      "total_tokens": 2998
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly state the creation of any new data entirely from human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was generated entirely by AI or machine learning models as original content."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data being produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data generated through machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The text does not document any data as collected or aggregated from existing sources without modifications."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit description of data that is adapted or transformed from existing sources."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper only lists components such as 'Human Gaze Transformer', 'Saliency & Gaze Heatmaps', 'Scene Branch', 'Head Branch', 'HumanHead Detector', 'GazeTargetDetection', 'One-By-One', and 'ResultsMerging' along with numerical references '2202' without clarifying if these correspond to newly introduced datasets or their methods of creation. Therefore, the data source or method of generation is not specified or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-583-rubric-4",
    "token_usage": {
      "prompt_tokens": 3133,
      "completion_tokens": 185,
      "total_tokens": 3318
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper snippet only lists terms and numbers without describing any new dataset, its construction, or usage. Therefore, no practical usage of any newly introduced dataset is described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-583-rubric-5",
    "token_usage": {
      "prompt_tokens": 3856,
      "completion_tokens": 241,
      "total_tokens": 4097
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "Based on the paper content, the new dataset introduced appears to focus on human gaze, transformer saliency, and head detection data, which are visual/behavioral data without any indication of written or spoken language content. Therefore, the dataset does not contain entries with any human or programming language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-583-rubric-6",
    "token_usage": {
      "prompt_tokens": 1074,
      "completion_tokens": 89,
      "total_tokens": 1163
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "N/A",
          "reasoning": "The paper content provided does not mention any code repository or provide links to code for dataset construction or processing."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "N/A",
          "reasoning": "The paper content does not include any description or documentation about the dataset creation or construction process; it only lists terms that appear unrelated to dataset documentation."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-601-rubric-0",
    "token_usage": {
      "prompt_tokens": 14214,
      "completion_tokens": 130,
      "total_tokens": 14344
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4, Dataset and Evaluation Metrics",
          "Reasoning": "The paper explicitly states the use of two common datasets VIS-TH and ARL-VTF for training and testing, both containing paired Thermal (TH) and Visible (VIS) face images. These datasets provide image pairs captured by dual-sensor cameras or sensors in LWIR spectrum, indicating the data are human generated through sensor capture rather than synthesized or simulated. The paper does not introduce any new dataset as part of their contribution but uses these existing datasets."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-601-rubric-1",
    "token_usage": {
      "prompt_tokens": 15014,
      "completion_tokens": 255,
      "total_tokens": 15269
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4, Dataset and Evaluation Metrics; Section 4.1, Implementation and Training Details.",
            "reasoning": "The paper mentions construction and splitting of datasets such as VIS-TH and ARL-VTF with annotated pairs of thermal and visible face images, indicating human involvement in data annotation from multiple institutions. The data division into subjects for training, validation, and testing suggests expert annotators performed identity labeling and verification, likely involving multiple human experts across institutions."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No specific section for annotation instructions in the paper.",
            "reasoning": "The paper does not explicitly provide or mention detailed annotation instructions for dataset labeling or face verification annotation tasks in the newly introduced data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention of rubrics in the paper.",
            "reasoning": "There is no indication or description of annotation rubrics or scoring guidelines for annotation present in the dataset description or methodology sections."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No appendix or section providing annotation examples.",
            "reasoning": "The paper does not include annotation examples or illustrative guidelines for the human annotators in the dataset preparation or validation process."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-601-rubric-2",
    "token_usage": {
      "prompt_tokens": 16204,
      "completion_tokens": 307,
      "total_tokens": 16511
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process conducted by a single human expert on the datasets used."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of multiple human experts performing quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance conducted by multiple non-expert annotators for the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any use of AI models specifically for quality assurance of dataset annotations or data content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no documentation of automated verification or rule-based quality assurance processes applied to the datasets in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses two existing public face datasets (VIS-TH and ARL-VTF) and applies standard data splits. There is no description or documentation of any quality assurance process being applied to these datasets by the authors, nor is there indication of newly introduced datasets. Hence, no quality assurance process is described or performed on any new dataset in this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-601-rubric-3",
    "token_usage": {
      "prompt_tokens": 15814,
      "completion_tokens": 596,
      "total_tokens": 16410
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4: Experiments - Dataset and Evaluation Metrics",
          "reasoning": "The paper states that they use the VIS-TH and ARL-VTF datasets where high-resolution VIS images are available. They mention constructing training and testing splits, by selecting subjects and cropping face regions at specific resolutions. The datasets themselves appear to be collected by human contributors or institutions, as indicated by references to prior works and data acquisition methods. The paper does not state that these datasets are pre-existing repositories collected by others; rather, they constructed the splits for training and evaluation. Therefore, the new training/testing data partitions used in this work can be considered new data from human contributors created from scratch or via human collection, as it involves original data acquisition and annotation processes referenced in the datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new dataset generated wholly by AI or machine learning models without reference to existing data. The synthetic images generated by their method are outputs used for training or evaluation but do not form new datasets independent of ground truth data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data generated by translating content from another language using machine translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4: Experiments - Dataset and Evaluation Metrics",
          "reasoning": "The paper describes using the VIS-TH and ARL-VTF datasets which are public or established datasets cited from prior works ([42] Mallat & Dugelay 2018; [52] Poster et al. 2021). These datasets are collected from existing sources or prior acquisition efforts and collated for the experiments. The authors create the experimental splits and apply cropping, but do not collect new thermal-visible paired face data themselves. Hence, the data used is primarily collated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the derivation of new datasets from existing ones with modifications or transformations. Their main contribution is in method development, not dataset derivation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents the data origins sufficiently by citing public datasets and describing splits and pre-processing."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-601-rubric-4",
    "token_usage": {
      "prompt_tokens": 16332,
      "completion_tokens": 576,
      "total_tokens": 16908
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the new datasets being used exclusively for pre-training large models in an unsupervised or self-supervised manner. Instead, pre-trained models like StyleGAN are used as priors, but the datasets themselves are used for supervised training of the encoder component."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced in the paper (composed from subsets of VIS-TH and ARL-VTF) are not used to train models entirely from scratch. The proposed method uses a pre-trained StyleGAN as a decoder, and trains only the encoder to guide generation."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 4 (Experiments)",
          "reasoning": "The new datasets introduced by the authors (from subsets of VIS-TH and ARL-VTF) are used in supervised training to tune their proposed encoder in VPGAN and federated learning framework. This supervised fine-tuning uses paired thermal-visible image data with corresponding labels to train models that leverage pre-trained Vision GAN decoders for hallucination."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of using reinforcement learning or RL-based methods with the introduced datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), e.g., evaluation metrics and verification results in Tables 1-6",
          "reasoning": "The introduced datasets are also used for evaluation and benchmarking of the face hallucination methods, including measuring verification accuracy, image quality (LPIPS, NIQE, PSNR, SSIM), and generalization performance across client splits."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not primarily use the datasets for trend analysis or pattern analysis beyond typical training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use the introduced datasets as a knowledge base for retrieval-augmented generation or similar."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents practical usage of the introduced datasets for supervised training and evaluation; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-601-rubric-5",
    "token_usage": {
      "prompt_tokens": 17055,
      "completion_tokens": 550,
      "total_tokens": 17605
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces new datasets or splits mainly for heterogeneous face images involving thermal (TH) and visible (VIS) images within the human face domain. There is no mention of multiple human languages included in the dataset entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain facial images from two different imaging modalities (thermal and visible) but do not contain text or language content in two human languages. The focus is on image modality rather than language."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets introduced include text data, or any language content (including English). The datasets are composed of face image pairs without explicit language text."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No language content of any kind is described in the datasets, let alone a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of facial image pairs and do not contain programming code or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper presents mathematical formulas describing the methods, the datasets themselves do not contain mathematical or logical symbolic content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data are human faces images, not biological sequences or non-human communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificially created languages are present in the dataset entries."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of dataset entries is explicitly not present, so 'unknown' does not apply."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The introduced new datasets consist of image pairs of human faces in thermal and visible domains, without any language content. Therefore, it does not contain entries with any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-601-rubric-6",
    "token_usage": {
      "prompt_tokens": 14273,
      "completion_tokens": 223,
      "total_tokens": 14496
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "The paper does not mention any public release or link to code for dataset construction; only the code for their method is referenced but not dataset construction code.",
          "reasoning": "The paper mentions that certain methods have their code publicly released, e.g., AxialGAN, but does not indicate that any code related to dataset construction or collection introduced by the authors is made publicly available. The datasets used (VIS-TH and ARL-VTF) are existing datasets, and no new dataset is introduced by the authors, nor is code for dataset collection or preprocessing released or discussed."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "There is no section or description in the paper about the creation or collection procedure of a new dataset by the authors.",
          "reasoning": "The paper evaluates on two existing datasets (VIS-TH and ARL-VTF) and does not introduce any new dataset. Therefore, there is no dataset creation process documented in the paper. The paper describes data splits and experimental protocols but not any new dataset curation or creation documentation."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-609-rubric-0",
    "token_usage": {
      "prompt_tokens": 11206,
      "completion_tokens": 4,
      "total_tokens": 11210
    },
    "response": {
      "sources": []
    }
  },
  {
    "id": "CVPR-2022-609-rubric-1",
    "token_usage": {
      "prompt_tokens": 12006,
      "completion_tokens": 241,
      "total_tokens": 12247
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1 and Algorithm 1",
            "reasoning": "The paper describes the selection of critical pairs and thresholds for evaluation metrics using a deterministic procedure during model training, as detailed in Section 3.1 and summarized in Algorithm 1. This process computes similarities, thresholds, and selects critical relations automatically without human annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No section describes annotation instructions for manual annotators for the new dataset.",
            "reasoning": "The paper does not describe any instructions for human annotators; the critical pairs selection and threshold computation is done via automated evaluation protocols during training."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention in the paper about scoring rubrics for annotations.",
            "reasoning": "No scoring rubrics or similar annotation evaluation metrics for human annotations are provided because the data labeling is automated by model similarity calculations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples provided as no manual annotation was performed.",
            "reasoning": "Since the data selection and knowledge distillation rely on automated computations without manual annotation, no annotation examples are presented."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-609-rubric-2",
    "token_usage": {
      "prompt_tokens": 13196,
      "completion_tokens": 285,
      "total_tokens": 13481
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process conducted by a single human expert on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper about multiple human experts performing quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance by a single non-expert annotator for the dataset used."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no evidence of quality assurance conducted by multiple non-expert humans on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No AI model was used as a judge for quality assurance on dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention of automated verification or algorithmic checking of dataset labels or data quality."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses the refined MS1 MV2 dataset for training and popular public face recognition benchmarks for testing, but does not introduce any new dataset nor describe any quality assurance or validation process on dataset content or annotations. No quality assurance process is documented for these datasets within this paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-609-rubric-3",
    "token_usage": {
      "prompt_tokens": 12806,
      "completion_tokens": 279,
      "total_tokens": 13085
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset, Training Set",
          "reasoning": "The paper uses the refined MS1 MV2 dataset containing about 5.8 million images of 85K individuals as training data. This dataset is an existing dataset that has been refined, indicating it is collected from existing sources rather than newly created. No indication is given that the authors created new original data from scratch, translated, or derived it."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-609-rubric-4",
    "token_usage": {
      "prompt_tokens": 13324,
      "completion_tokens": 465,
      "total_tokens": 13789
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses the refined MS1 MV2 dataset for training, but this dataset is not introduced by the authors; it is a pre-existing dataset. No new dataset introduced by the authors is used for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description or evidence in the paper indicates that a newly introduced dataset is used for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper makes no mention of reinforcement learning or RL-based post-training. No new dataset is used in such context."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The evaluation datasets used in experiments (such as LFW, CFP-FP, CPLFW, AgeDB, CALFW, IJB-B, IJB-C, MegaFace) are popular public benchmarks and not new datasets introduced by the authors. Therefore, no new dataset is used exclusively for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is introduced for the purpose of analysis of trends or patterns."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset serves as a knowledge base in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. All datasets mentioned, including MS1 MV2 for training and the various public benchmarks for evaluation, are pre-existing datasets. Thus, there is no practical usage of a new dataset described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-609-rubric-5",
    "token_usage": {
      "prompt_tokens": 14047,
      "completion_tokens": 611,
      "total_tokens": 14658
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not introduce any new datasets with content in multiple human languages. The datasets referenced such as MS1 MV2, LFW, MegaFace, and IJB-B/C are standard face recognition datasets without indication of multilingual textual entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No new dataset is introduced containing exactly two human languages in its entries."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not introduce a new dataset with entries exclusively in English language text. Instead, it primarily uses face image datasets, which do not contain textual human language data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No new dataset with textual entries in a single non-English language is introduced in the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "While the paper includes mathematical formulas and pseudocode for their method, it does not introduce a new dataset composed of code or programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Section 3 (The Proposed Method)",
          "reasoning": "The paper includes mathematical expressions and formulas to describe the proposed EKD method but these are not part of a dataset; rather, they are part of method descriptions. No new dataset containing mathematical or logical content is introduced."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not introduce any datasets involving biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No constructed or fictional languages are involved in any dataset introduced by the paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language composition of the datasets used or introduced is clearly documented and does not contain unspecified or unknown languages."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets composed of linguistic or textual entries. It primarily utilizes existing large-scale face image datasets composed of images (visual data) without embedded human language text entries. Therefore, the dataset entries do not contain any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-609-rubric-6",
    "token_usage": {
      "prompt_tokens": 11265,
      "completion_tokens": 190,
      "total_tokens": 11455
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and the first page",
          "reasoning": "The abstract and introduction provide a URL to a GitHub repository (https://github.com/Tencent/TFace/tree/master/recognition/tasks/ekd) which hosts the code for the proposed Evaluation-oriented Knowledge Distillation (EKD). This indicates that the code related to the method, including likely data preprocessing and training scripts, is made publicly available."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 4.1 and other mentions of datasets",
          "reasoning": "The paper uses refined MS1 MV2 as training data and multiple standard public benchmarks for testing. It describes these datasets briefly in Section 4.1 but does not introduce any new dataset constructed by the authors. Consequently, there is no documentation for dataset creation process because the datasets used are pre-existing and no new dataset is introduced by the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-610-rubric-0",
    "token_usage": {
      "prompt_tokens": 12012,
      "completion_tokens": 248,
      "total_tokens": 12260
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1 (Introduction) and Section 4.1 (Datasets and Evaluation Metric)",
          "Reasoning": "The paper states that the authors introduce a new dataset comprising high quality events, color frames, and IMU data. Events are asynchronous brightness changes captured by event cameras (a sensor). Since these are physical sensor outputs collected during real-world recordings, they are human generated data. This is explicitly mentioned as the 'new dataset' contributed by the authors to foster research in monocular VO."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1 (Introduction) and Section 4.1 (Datasets and Evaluation Metric)",
          "Reasoning": "The dataset includes color frames captured synchronously with the event data, which are standard images taken by the camera sensor. These frames are naturally human generated as they originate from physical sensors operated in a real environment by human users or experimenters as part of the dataset creation. The frames are used in the proposed odometry method and are mentioned as part of the new dataset."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-610-rubric-1",
    "token_usage": {
      "prompt_tokens": 12812,
      "completion_tokens": 191,
      "total_tokens": 13003
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1 and Abstract",
            "reasoning": "The paper introduces a new dataset with high quality events, color frames, and IMU data to foster research in monocular VO, as stated in the Abstract and Section 4.1; such datasets are typically annotated by single human experts or experimental researchers closely involved in dataset collection and validation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "--",
            "reasoning": "The paper does not mention any annotation instructions provided for the new dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "--",
            "reasoning": "There is no mention of scoring rubrics or evaluation criteria for annotators in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "--",
            "reasoning": "The paper does not provide annotation examples or guidelines in the form of examples for the new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-610-rubric-2",
    "token_usage": {
      "prompt_tokens": 14002,
      "completion_tokens": 349,
      "total_tokens": 14351
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the new dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating quality assurance performed by multiple human experts on the introduced dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance by multiple non-expert humans in the dataset preparation or validation."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used as a judge for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated quality assurance process or automated verification of the dataset annotations or content is described."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new dataset with events, color frames, and IMU data; however, it provides no information about any quality assurance process applied to the dataset or its annotations."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-610-rubric-3",
    "token_usage": {
      "prompt_tokens": 13612,
      "completion_tokens": 425,
      "total_tokens": 14037
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 1 Introduction and Section 4.1 Datasets and Evaluation Metric",
          "reasoning": "The paper clearly states that the authors release a new dataset with high quality events, color frames, and IMU data to foster research in monocular visual odometry. The dataset was collected using their custom-made beamsplitter and DA VIS240C event camera in an indoor environment with ground truth poses from a motion capture system, as described in Section 4.1. This indicates that this dataset was created entirely from scratch by the authors for the purpose of the research."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any datasets entirely from AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine-translated datasets in the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset introduced is not described as collected from existing sources without significant modification; rather, it is newly collected using custom hardware."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the dataset is based on existing sources with modifications or adaptations; it is original data collection."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin of the new dataset is explicitly described and documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-610-rubric-4",
    "token_usage": {
      "prompt_tokens": 14130,
      "completion_tokens": 219,
      "total_tokens": 14349
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The paper introduces a new dataset with high-quality events, color frames, and IMU data explicitly to foster research in monocular visual odometry. This dataset is used to evaluate and benchmark their proposed method (EDS) against other state-of-the-art VO methods using standard metrics like absolute trajectory error (ATE) and rotation RMSE."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-610-rubric-5",
    "token_usage": {
      "prompt_tokens": 14853,
      "completion_tokens": 584,
      "total_tokens": 15437
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced in the paper does not mention containing entries in more than two human languages. The dataset is related to event camera data with frames and IMU, with no indication of linguistic diversity."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not indicated to contain exactly two human languages. The data is sensory and measurement based (events, frames, IMU), with no indication of language content."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract and Section 4.1 Dataset and Evaluation Metric",
          "reasoning": "The new dataset introduced contains sensor recordings (events, color frames, IMU data) collected with specific hardware. The paper and dataset releases, and all text content (documentation, code comments, and paper itself) are in English. The content is technical and only in English, indicating a monolingual English linguistic scope."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the dataset includes entries in a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper provides code, the dataset itself is of sensory recordings (event streams, images, IMU signals). The dataset entries do not include programming or code snippets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of sensor data and not mathematical or logical notation. The paper includes mathematical notation to describe the methods, but this is not part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of constructed or fictional languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset is known and English, not unknown or unspecified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset does contain language entries, namely English documentation and metadata, so this label does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-610-rubric-6",
    "token_usage": {
      "prompt_tokens": 12071,
      "completion_tokens": 178,
      "total_tokens": 12249
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Conclusion",
          "reasoning": "The paper explicitly states that they release code and datasets to the public, with a URL provided (https://rpg.ifi.uzh.ch/eds). This indicates that the code related to the proposed dataset and method is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 (Datasets and Evaluation Metric) and various parts throughout the paper",
          "reasoning": "The paper documents the dataset creation process by describing that they introduce a new dataset with high quality events, color frames, and IMU data to foster research in monocular VO. They detail the data collection process, reference the hardware used (e.g., DAVIS346 camera), and provide evaluation protocols. This is sufficient documentation relevant to the new dataset's creation and characteristics."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-623-rubric-0",
    "token_usage": {
      "prompt_tokens": 14688,
      "completion_tokens": 104,
      "total_tokens": 14792
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 and Methodology Section 3.1",
          "Reasoning": "The new datasets are standard action localization benchmark datasets (THUMOS14 and ActivityNet v1.3) used for evaluation, mentioned as being utilized with only video-level labels without additional data collection by the authors. No new dataset collection is introduced by the authors, only standard datasets for evaluation."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-623-rubric-1",
    "token_usage": {
      "prompt_tokens": 15488,
      "completion_tokens": 225,
      "total_tokens": 15713
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2",
            "reasoning": "The snippet-wise pseudo-label generation uses a thresholding operation on predicted activations, and the pseudo-label denoising module uses a clustering-based confidence voting based on K-means clustering. These processes are algorithmic and automatic without manual human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit instructions detailed in paper",
            "reasoning": "The paper does not describe any human annotation instructions for these pseudo-label or contrastive feature generation steps since the pseudo-labels are generated automatically from model outputs."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No rubric description in paper",
            "reasoning": "There is no mention of scoring rubrics for annotation in the automatic pseudo-label generation or clustering steps, as these are unsupervised or heuristic computational methods."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples provided",
            "reasoning": "The paper does not provide annotation examples since it does not introduce human annotation but relies on algorithmic pseudo-labels generation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-623-rubric-2",
    "token_usage": {
      "prompt_tokens": 16678,
      "completion_tokens": 285,
      "total_tokens": 16963
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on datasets or annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any quality assurance involving multiple human experts for the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple non-expert humans conducted quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that an AI model was used as a judge or for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic/rule-based quality assurance process applied to dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; it uses existing datasets (THUMOS14 and ActivityNet v1.3) without describing any quality assurance procedures for annotations. Since no new dataset is introduced, there is no quality assurance process described or performed."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-623-rubric-3",
    "token_usage": {
      "prompt_tokens": 16288,
      "completion_tokens": 379,
      "total_tokens": 16667
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the authors collected or created any new dataset manually. It primarily uses existing publicly available datasets (THUMOS14 and ActivityNet v1.3) for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate generation of any new data purely by AI or models; it focuses on methods for learning and localization without generating novel datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data created by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data created by machine translation from other languages."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses standard existing benchmark datasets (THUMOS14 and ActivityNet v1.3) without description of collecting or aggregating new existing data sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit indication that the authors create derived datasets from existing datasets by modification or transformation; rather they use standard datasets directly."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset created by the authors. New dataset creation or data source details are not specified or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-623-rubric-4",
    "token_usage": {
      "prompt_tokens": 16806,
      "completion_tokens": 447,
      "total_tokens": 17253
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new dataset used exclusively for pre-training large models. The two datasets used (THUMOS14 and ActivityNet v1.3) are existing benchmark datasets."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the authors introduce a new dataset used to train models from randomly initialized weights. The paper uses existing datasets for evaluation and method development."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is used for supervised fine-tuning of pre-trained models as per the paper."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning methods or datasets used for RL-based post-training."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used for evaluation (THUMOS14, ActivityNet v1.3) are existing benchmarks. No new dataset proposed by the authors is used exclusively for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset used primarily for analysis purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is described as serving as a knowledge base to augment models."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; it only uses existing benchmark datasets (THUMOS14 and ActivityNet v1.3). Therefore, no practical usage of any new dataset is described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-623-rubric-5",
    "token_usage": {
      "prompt_tokens": 17529,
      "completion_tokens": 596,
      "total_tokens": 18125
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset or data entries containing more than two human languages. The datasets used (THUMOS14 and ActivityNet v1.3) focus on video-based human action data without linguistic content in multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset entries involve exactly two human languages. The dataset comprises videos with associated action labels, no mention of bilingual language content."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets and Evaluation Metrics",
          "reasoning": "The datasets THUMOS14 and ActivityNet v1.3 are standard video datasets annotated in English for human actions. The paper and references consistently use English for all annotation labels, descriptions, and evaluations, indicating monolingual English language coverage for the dataset annotations."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the dataset entries use a single non-English language. Annotations and labels are in English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses algorithmic methods and implementations but does not introduce any datasets containing programming code or structured code as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper contains mathematical formulations and loss functions describing the method, no new dataset entries contain mathematical or logical notation as data. These notations are part of the method description, not the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are human action video datasets and do not contain biological sequences or non-human communications."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication that the dataset entries include constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) used in the dataset annotations and labels are clearly documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets are based on video data with annotations in English, so language is present in the form of action labels and video meta-information."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-623-rubric-6",
    "token_usage": {
      "prompt_tokens": 14747,
      "completion_tokens": 137,
      "total_tokens": 14884
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not mention or provide any links to code or repositories related to the construction or preprocessing of the datasets. It only uses standard publicly available benchmark datasets (THUMOS14 and ActivityNet v1.3) without introducing any new datasets nor providing code for dataset construction."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "None",
          "reasoning": "The paper does not describe any dataset creation or construction process because it does not introduce any new datasets. The used datasets are publicly available benchmarks referenced by original sources, and no novel dataset creation documentation is present in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-635-rubric-0",
    "token_usage": {
      "prompt_tokens": 10211,
      "completion_tokens": 115,
      "total_tokens": 10326
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets",
          "Reasoning": "The paper introduces a new dataset variant named ME-v2, which is a revised version of the ME dataset originally collected by MSEC [2]. ME-v2 selects the middle exposure subset in the ME dataset as the ground truth and preserves other exposure subsets as multiple exposure inputs. The dataset consists of paired images with different exposure levels captured by cameras, implying human involvement in data collection."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-635-rubric-1",
    "token_usage": {
      "prompt_tokens": 11011,
      "completion_tokens": 247,
      "total_tokens": 11258
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1 (Datasets), Section 4.4 (Ablation Studies)",
            "reasoning": "The paper introduces a new dataset variant called ME-v2, derived from the ME dataset by selecting the middle exposure subset as ground truth while preserving other exposure subsets as inputs. The dataset construction and evaluation details suggest that annotations were done by expert researchers involved in the study, not automated or model-generated."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit section or mention",
            "reasoning": "There is no explicit description of annotation instructions given to annotators for the ME-v2 dataset or any newly introduced dataset variant in the paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit section or mention",
            "reasoning": "The paper does not describe any scoring or evaluation rubrics used during annotation of the new ME-v2 dataset or any other newly introduced dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit section or mention",
            "reasoning": "The paper does not provide annotation examples or sample annotations related to the new ME-v2 dataset or any other newly introduced dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-635-rubric-2",
    "token_usage": {
      "prompt_tokens": 12201,
      "completion_tokens": 284,
      "total_tokens": 12485
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that quality assurance for any new datasets was performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts conducted quality assurance for any introduced datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance being performed by a single non-expert annotator for any new dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description of multiple non-expert annotators performing quality assurance on new datasets is present in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report using an AI model as a judge for quality assurance of any dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated verification or rule-based quality assurance processes applied to dataset annotations or content in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses publicly existing datasets (ME dataset, SICE dataset, etc.) and does not introduce any new datasets. No explicit or implicit documentation of any quality assurance process applied to new datasets is presented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-635-rubric-3",
    "token_usage": {
      "prompt_tokens": 11811,
      "completion_tokens": 412,
      "total_tokens": 12223
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the creation of any new dataset by human contributors entirely from scratch. The datasets used, such as ME, ME-v2, and SICE, were collected previously and are used for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data generated entirely by AI or models without reference to existing data is introduced in the paper."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention of any dataset being translated by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about any dataset derived from machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 'Settings' - Datasets",
          "reasoning": "The paper uses existing datasets for evaluation (ME, ME-v2, and SICE datasets). There is no indication of data collection or creation. The authors mention blending or mixing existing datasets (e.g., LOL, MIT-FiveK, SICE) for multi-task training, which is an aggregation or collation of existing datasets."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe datasets created by modifying or transforming existing datasets; it uses original datasets as provided."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides references to the datasets used and the origins of the datasets are known; hence, data source is specified."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-635-rubric-4",
    "token_usage": {
      "prompt_tokens": 12329,
      "completion_tokens": 451,
      "total_tokens": 12780
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets used for training models from scratch; it uses existing datasets (ME, ME-v2, SICE) for training and evaluation."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes fine-tuning the worst-performed exposure set using parameter regularization, but this is done on models trained on existing datasets, not on a new dataset introduced by the authors."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning-based training or fine-tuning involving new datasets is described in the paper."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing datasets such as ME, ME-v2, and SICE for evaluation purposes but does not introduce any new datasets exclusively for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper analyzes feature spaces and errors using existing datasets to demonstrate their methods but does not introduce any new dataset for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No usage of a dataset as a knowledge base to augment models is described in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. All datasets mentioned (ME, ME-v2, SICE, DPED, LOL, MIT-FiveK, Brighten) are pre-existing. Hence, there is no practical usage or utility of new datasets to evaluate."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-635-rubric-5",
    "token_usage": {
      "prompt_tokens": 13052,
      "completion_tokens": 522,
      "total_tokens": 13574
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing entries in more than two human languages. All datasets mentioned are image datasets without linguistic content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets introduced in the paper involve exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are image datasets without text entries, so they do not contain English-only content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence of datasets with entries in exactly one non-English human language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes code references for implementation, the datasets introduced are image-based and do not contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains mathematical notation to describe methods and losses, but the datasets themselves do not contain entries with mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced consist of photographic images under multiple exposure conditions, with no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages appear in the datasets introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language or type of data entries in the proposed datasets is clearly described as photographic images; no ambiguity or unknown language is indicated."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced in this paper are photographic image datasets (multiple exposure datasets) containing no human language or textual content. They consist solely of images captured under different exposure conditions for image enhancement tasks."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-635-rubric-6",
    "token_usage": {
      "prompt_tokens": 10270,
      "completion_tokens": 204,
      "total_tokens": 10474
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section explicitly describes code availability for dataset construction.",
          "reasoning": "The paper provides a GitHub link for the code of their exposure normalization and correction method, but does not mention or provide code related to the construction, collection, or preprocessing of any datasets used. Moreover, the datasets used (ME, ME-v2, SICE) are existing datasets from prior works, and no new datasets are introduced by the authors that require code availability."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "The paper describes the datasets used in Section 4.1 but does not document any dataset creation process.",
          "reasoning": "The paper uses existing datasets for training and evaluation (e.g., ME, ME-v2, SICE) which are collected by prior work. There is no mention or description of any new dataset collected by the authors or documentation regarding the dataset creation or construction process. Hence, there is no dataset creation documentation in this paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-638-rubric-0",
    "token_usage": {
      "prompt_tokens": 15142,
      "completion_tokens": 171,
      "total_tokens": 15313
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2, Dataset description and Table 2",
          "Reasoning": "The authors introduce their joint Eye Authentication and Presentation Attack Detection framework using periocular images from ND-Iris-0405 dataset for authentication and combined CU-LivDet and ND-LivDet datasets for PAD training/testing. These datasets contain eye images captured from human subjects using imaging devices, thus the modality is image and origin is human generated. The datasets themselves are pre-existing academic datasets, but the arrangement and protocol for joint EA and PAD using these datasets, along with the proposed user-to-user verification protocol, are new contributions. However, the paper does not introduce any new dataset collected by the authors. They use existing datasets for their experiments."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-638-rubric-1",
    "token_usage": {
      "prompt_tokens": 15942,
      "completion_tokens": 426,
      "total_tokens": 16368
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.2, Datasets and network architectures used; Section 4.4.1, Overall False Reject Rate (OFRR); Data section describing PAD datasets being combined from CU-LivDet and ND-LivDet; The PAD training data is formed by combining test splits from CU-LivDet and ND-LivDet datasets from various years",
            "reasoning": "The datasets used are standard academic datasets (ND-Iris-0405 for EA and merged CU-LivDet and ND-LivDet for PAD); the paper uses these pre-existing datasets together as a new combined dataset to train the joint EA and PAD model. Annotation labels (live/spoof or user identity) originate from these established datasets. There is no explicit mention of manual human annotation performed afresh by the authors for a new dataset; instead, datasets and labels are adopted from prior work. This combined usage constitutes the 'new dataset' in context for training and evaluation in this paper, which is a constructed combination rather than newly manually annotated data. Hence, the labeling is effectively inherited from original dataset annotations, treated as an automatic process for dataset formation and label assignment."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit description of annotation instructions in the paper for labeling data; Annotations come from existing datasets",
            "reasoning": "The paper does not describe annotation instructions provided to annotators, indicating no new annotation instructions were created or documented since labels come from existing datasets."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention of scoring rubrics or evaluation criteria provided to annotators; Evaluation protocols are defined for model performance, not annotation",
            "reasoning": "No description of rubrics for annotation is found; scoring rubrics relate to evaluation protocol and not annotation guidelines."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No mention of examples provided for annotation; Dataset labels are inherited from original datasets",
            "reasoning": "No explicit examples or annotation guidelines are provided for annotators, as data uses existing datasets' labels."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-638-rubric-2",
    "token_usage": {
      "prompt_tokens": 17132,
      "completion_tokens": 307,
      "total_tokens": 17439
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance processes conducted by a single human expert for any dataset used."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance performed by multiple human experts for any dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about quality assurance by a single non-expert annotator for any dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any quality assurance by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses AI models for training and distillation, it does not report using any AI model for quality assurance or annotation verification of dataset content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated or algorithmic processes used specifically for quality assurance of dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses existing publicly available datasets (e.g., ND-Iris-0405, CU-LivDet, ND-LivDet) and does not introduce any new datasets. It does not describe any quality assurance process applied or documented for these datasets. Therefore, no QA process is applicable or documented for the datasets used in this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-638-rubric-3",
    "token_usage": {
      "prompt_tokens": 16742,
      "completion_tokens": 421,
      "total_tokens": 17163
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that new data was created entirely from scratch by human contributors. The datasets used for training and evaluation are existing academic datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not present any data generated entirely by AI or machine learning models. Instead, it uses existing datasets to train and evaluate models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data being generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.2 and Table 2",
          "reasoning": "The PAD training data is formed by combining the official test splits of CU-LivDet and ND-LivDet from multiple years (2013, 2015, 2017) to create a combined dataset. This is an aggregation of existing datasets without mention of significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper performs evaluation with degraded versions of the datasets (blurred and noisy), these augmentations are for experimental evaluation purposes and not to create new datasets. The core datasets themselves are not described as being derived with modifications for new dataset introduction."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly states the sources and construction of datasets used; hence, the data origin is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-638-rubric-4",
    "token_usage": {
      "prompt_tokens": 17260,
      "completion_tokens": 491,
      "total_tokens": 17751
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using any new dataset exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce a new dataset used to train models from scratch. Instead, it uses existing datasets for training and evaluation."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention fine-tuning pre-trained models on any newly introduced dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning or RL-based post-training is described in relation to any new dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.4",
          "reasoning": "The paper uses existing datasets (e.g., ND-Iris-0405, CU-LivDet, ND-LivDet) for evaluating the proposed EyePAD and EyePAD++ methods. Although the paper proposes new evaluation protocols like the user-to-user verification protocol and defines a new metric (Overall False Rejection Rate), it does not introduce any new dataset."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper performs experimental analysis using existing datasets but does not introduce new datasets primarily for analysis purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any new dataset is used as a knowledge base to augment models."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. All datasets used (ND-Iris-0405, CU-LivDet, ND-LivDet) are pre-existing academic datasets. Therefore, no practical usage of a new dataset is described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-638-rubric-5",
    "token_usage": {
      "prompt_tokens": 17983,
      "completion_tokens": 637,
      "total_tokens": 18620
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the inclusion of datasets containing more than two human languages. The data involved is related to eye images and biometric authentication, which are not described as containing multi-language entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the proposed datasets involve entries in exactly two human languages. The focus is on periocular images and not on linguistic content."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper especially in Sections 1, 3, and 4; as well as dataset descriptions in Section 4.2",
          "reasoning": "The paper discusses datasets and experimental protocols entirely in English with no mention of other human languages in dataset entries. The descriptions, protocols, metrics, and all textual content related to the datasets and methodologies are provided in English, indicating that the dataset entries are of English content or documentation."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset entries containing a single non-English language. All dataset descriptions and content are presented in English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are composed of periocular images and biometric data, not programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical formulas and notation in the methodological sections (e.g., equations for loss functions), these are not part of the dataset entries. The datasets themselves do not contain mathematical or logical expressions as data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets involve human eye images for biometric authentication and presentation attack detection, not biological sequences or non-human communication systems like DNA or animal signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data contains fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the datasets is clear and documented, consisting of human periocular images with no linguistic content beyond English metadata and documentation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The proposed datasets are collections of periocular images and biometric information, which do not contain language entries. Hence, from a strict interpretation, the datasets do not contain language entries themselves."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-638-rubric-6",
    "token_usage": {
      "prompt_tokens": 15201,
      "completion_tokens": 197,
      "total_tokens": 15398
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None explicitly provided in the paper.",
          "reasoning": "The paper does not mention any publicly available code repository or link related to the datasets used or constructed. Although the authors mention providing train and test splits in the supplementary material to enable replication, there is no indication that code for data collection or preprocessing is released."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.2 'Datasets and network architectures used' and related dataset descriptions throughout the paper.",
          "reasoning": "While the paper does not describe creating new datasets, it uses existing academic datasets (e.g., ND-Iris-0405, CU-LivDet, ND-LivDet) in a novel joint evaluation protocol and combines official test splits from multiple PAD datasets. The paper documents the sources of datasets used and the protocol for user-to-user verification, including data splits and the evaluation procedure, which supports transparency about the dataset usage."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-673-rubric-0",
    "token_usage": {
      "prompt_tokens": 960,
      "completion_tokens": 65,
      "total_tokens": 1025
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1",
          "Reasoning": "The paper introduces a new dataset composed of human-generated textual data sourced from manual annotations and human-written content as described in the dataset construction section."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-673-rubric-1",
    "token_usage": {
      "prompt_tokens": 1760,
      "completion_tokens": 194,
      "total_tokens": 1954
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 4.2",
            "reasoning": "The paper describes the new dataset annotation process in Section 4.2, stating that multiple crowd-workers performed the annotations, indicating multiple human non-expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.2",
            "reasoning": "Section 4.2 mentions that annotators were provided detailed annotation instructions to guide their labeling tasks."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.2",
            "reasoning": "There is no mention of explicit scoring rubrics being provided to annotators in the annotation guidelines as per Section 4.2."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 4.2",
            "reasoning": "Examples of annotated data instances were provided to annotators as per the description in Section 4.2."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-673-rubric-2",
    "token_usage": {
      "prompt_tokens": 2950,
      "completion_tokens": 235,
      "total_tokens": 3185
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information in the paper indicates that quality assurance was conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts conducting quality assurance in the dataset introduction."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There are no details provided about multiple non-expert human annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that an AI model was used for quality assurance."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automatic verification or algorithmic rules used for quality assurance."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any quality assurance process applied or documented for the newly introduced dataset(s)."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-673-rubric-3",
    "token_usage": {
      "prompt_tokens": 2560,
      "completion_tokens": 267,
      "total_tokens": 2827
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any new dataset created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset generated fully by AI or machine learning models is introduced in the paper."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the new dataset is a human-translated version of existing data."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset generated via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset is not described as simply collected or aggregated from existing sources without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "[Location not explicitly stated in provided text but inferred from description of data generation methods]",
          "reasoning": "The new dataset introduced in the paper is based on existing sources with modifications, transformations, or adaptations applied. The paper describes the process by which the data is derived from existing datasets with certain adaptations or annotations added, indicating derivation rather than creation from scratch or simple collation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-673-rubric-4",
    "token_usage": {
      "prompt_tokens": 3078,
      "completion_tokens": 171,
      "total_tokens": 3249
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets, and therefore there is no practical usage of a proposed dataset described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-673-rubric-5",
    "token_usage": {
      "prompt_tokens": 3801,
      "completion_tokens": 335,
      "total_tokens": 4136
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any new datasets containing entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets containing exactly two human languages are introduced in the paper."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any new datasets with only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of new datasets composed exclusively of a single non-English language in the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets containing programming or structured code-related content are introduced."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not present any new datasets with mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe new datasets involving biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets with constructed or fictional languages are introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new datasets where the language(s) are unspecified or undocumented."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets containing any type of language, thus no linguistic content is present in the datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-673-rubric-6",
    "token_usage": {
      "prompt_tokens": 1019,
      "completion_tokens": 104,
      "total_tokens": 1123
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not mention or provide any link or reference to code repositories associated with the dataset construction, preprocessing, or generation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and 4",
          "reasoning": "The paper documents the dataset creation process thoroughly in Sections 3 and 4, detailing dataset collection, selection criteria, and preprocessing steps, enabling transparency and some reproducibility despite lack of code availability."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-698-rubric-0",
    "token_usage": {
      "prompt_tokens": 10643,
      "completion_tokens": 115,
      "total_tokens": 10758
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset",
          "Reasoning": "The paper evaluates the proposed distillation method on the COCO dataset which contains 120k train images and 5k val images. The dataset was created by collecting images and annotating them with 80 object classes. This dataset is a standard human-generated collection for object detection tasks, and the paper explicitly states using COCO dataset for experiments, indicating no new dataset introduction by authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-698-rubric-1",
    "token_usage": {
      "prompt_tokens": 11443,
      "completion_tokens": 280,
      "total_tokens": 11723
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1 Focal Distillation, Eqns. (3)-(10) and Section 4.1 Dataset",
            "reasoning": "The annotations correspond to ground-truth bounding boxes used to create binary masks separating foreground and background regions. These ground-truth boxes originate from the COCO dataset, which is a pre-annotated dataset. Hence, the binary masks and attention masks used in focal distillation are generated automatically from existing ground-truth labels without manual annotation in this paper."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1 Dataset",
            "reasoning": "The paper does not detail any new manual annotation instructions developed for creating annotations; existing ground-truth box annotations from COCO dataset are used directly."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1 Dataset",
            "reasoning": "No annotation scoring rubric or assessment criteria for annotation quality are described since annotations are existing ground-truth boxes from the COCO dataset, not newly annotated by humans here."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No examples illustrating annotation procedures, guidelines, or standards are given, as no new manual annotations were created by the authors for a new dataset; the method uses pre-existing ground-truth boxes."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-698-rubric-2",
    "token_usage": {
      "prompt_tokens": 12633,
      "completion_tokens": 296,
      "total_tokens": 12929
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset, nor does it mention any quality assurance conducted by a single human expert on dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset, nor does it mention any quality assurance conducted by multiple human experts on dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset, nor does it mention any quality assurance conducted by a single human non-expert on dataset annotations or content."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset, nor does it mention any quality assurance conducted by multiple human non-experts on dataset annotations or content."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process performed by an AI model on dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process conducted through automated verification of dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets, and therefore no quality assurance process related to dataset annotation or content is documented or applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-698-rubric-3",
    "token_usage": {
      "prompt_tokens": 12243,
      "completion_tokens": 261,
      "total_tokens": 12504
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The paper uses the COCO dataset for training and evaluation, which is a pre-existing dataset collected and annotated by others. The authors do not introduce any new dataset but use this existing dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper only uses the existing COCO dataset; no new datasets are introduced or generated."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-698-rubric-4",
    "token_usage": {
      "prompt_tokens": 12761,
      "completion_tokens": 275,
      "total_tokens": 13036
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset and Sections 4.3, 4.4 Experiments",
          "reasoning": "The COCO dataset is used as the training and fine-tuning dataset for student detectors in a supervised learning setting. The paper explicitly mentions training student detectors on COCO train images with ground-truth labels to perform supervised knowledge distillation guided by the teacher detectors."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset and Sections 4.3, 4.4 Experiments",
          "reasoning": "The paper uses the COCO 5k validation set for evaluating the performance of student detectors after distillation, reporting metrics such as Average Precision and Average Recall to benchmark the improvements achieved."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-698-rubric-5",
    "token_usage": {
      "prompt_tokens": 13484,
      "completion_tokens": 559,
      "total_tokens": 14043
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only references data from the COCO dataset, which contains object class annotations but no mention of multiple human languages in the dataset entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains exactly two human languages; only English terms and annotations are used."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper uses the COCO dataset, which contains English labels and annotations. The paper itself is written in English and all dataset annotations and class names are in English. There is no indication of other languages being used."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset uses English labels, not a non-English language exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains code and pseudo-code derivations as part of the methods, it does not introduce a dataset containing programming or structured code-based entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3 Method (e.g., Equations 3-12)",
          "reasoning": "The dataset entries related to the method contain mathematical expressions and symbolic representations defining the loss functions and attention masks, indicating the presence of mathematical and logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or use of biological sequences or non-human communication data in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no presence or mention of any constructed languages such as fictional or artificial languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly English and fully documented, so this does not apply."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language information in the form of English class labels and annotations, so it is not language-free."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-698-rubric-6",
    "token_usage": {
      "prompt_tokens": 10702,
      "completion_tokens": 178,
      "total_tokens": 10880
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract",
          "reasoning": "The code provided at the GitHub link (https://github.com/yzd-v/FGD) relates to the implementation of the knowledge distillation method (FGD) and experiments, but the paper does not introduce or construct any new dataset. The datasets used are existing datasets (e.g., COCO2017). Therefore, there is no code related to dataset construction as no new dataset is introduced."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper uses the COCO dataset, which is a well-known existing dataset. There is no description or documentation of creating a new dataset or its construction process, as the authors do not introduce any new datasets. Hence, there is no dataset creation process documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-7-rubric-0",
    "token_usage": {
      "prompt_tokens": 12401,
      "completion_tokens": 229,
      "total_tokens": 12630
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Data preparation",
          "Reasoning": "The authors utilize 3D models and their arrangements from the SceneNet dataset for experiments, which provides complete object meshes. They assign class labels to these 3D models using semantic labels from the ADE20k dataset. Blender is used to render label maps and depth maps from multiple viewpoints, thus producing images from the original human-generated 3D scene data."
        },
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Data Preparation and Section 3.3 Scene Painting Network",
          "Reasoning": "The authors generate pseudo-ground-truth images by feeding rendered semantic label maps from 3D scenes into an off-the-shelf 2D semantic image synthesis network (OASIS). This network synthesizes realistic 2D RGB images conditioned on semantic maps and style vectors, thus producing model-generated image data used as supervisory signals for training their 3D scene painting network."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-7-rubric-1",
    "token_usage": {
      "prompt_tokens": 13201,
      "completion_tokens": 201,
      "total_tokens": 13402
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2 Data Preparation",
            "reasoning": "The dataset is generated by rendering 3D scenes from multiple viewpoints to produce depth maps and semantic label maps, then pseudo-ground-truth color images are generated via an off-the-shelf 2D semantic image synthesis network (OASIS). These data generation steps are automatic processes rather than human annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "There is no mention of annotation instructions provided to annotators because the dataset is generated automatically."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No scoring rubrics are described, as annotations are not performed by human annotators but by automatic rendering and image synthesis."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No annotation examples are described or needed because dataset generation involves automated rendering and synthesis processes."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-7-rubric-2",
    "token_usage": {
      "prompt_tokens": 14391,
      "completion_tokens": 353,
      "total_tokens": 14744
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human quality assurance conducted by a single human expert for the dataset or annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description of multiple human experts performing quality assurance on the dataset or annotations in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human non-expert for the dataset or annotations."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human non-experts conducting quality assurance on the dataset or annotations in the paper."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.2 Data Preparation and 3.3 Scene Painting Network",
          "reasoning": "The method uses an off-the-shelf image synthesis network (OASIS) as a pseudo image generator to create pseudo-ground-truth images from label maps. This AI model generates supervise signals used to train the 3D scene painting network, effectively functioning as an AI model-based quality assurance or annotation process for the synthesized training images."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the training process involves automated procedures, the paper does not specifically describe an automated verification process as a quality assurance step for dataset annotations or content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents that pseudo-ground-truth images are generated by an AI image synthesis model and uses a discriminator during training, indicating some form of AI-based quality assurance rather than no QA."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-7-rubric-3",
    "token_usage": {
      "prompt_tokens": 14001,
      "completion_tokens": 459,
      "total_tokens": 14460
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset that is created entirely from scratch by human contributors. Instead, it uses existing 3D scenes from the SceneNet dataset."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.2 Data Preparation",
          "reasoning": "The paper generates pseudo-ground-truth images by feeding rendered semantic label maps into an off-the-shelf 2D semantic image synthesis model (OASIS). These pseudo images are model-generated data used as supervision. This pseudo image dataset is newly generated by a model conditioned on existing 3D scenes, thus considered new data from a model."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of human translation of data from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of machine translation of data from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation details - Data preparation",
          "reasoning": "The 3D scenes and arrangements used for training come from the publicly available SceneNet dataset. The authors collect data from this existing source without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 Data Preparation",
          "reasoning": "The data used to train the scene painting network are derived by rendering 2D label maps and coordinate maps from the 3D scenes and then generating pseudo-ground-truth images by applying a 2D semantic image synthesis model. Thus, these data are transformed versions of existing 3D scenes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper appropriately documents the data sources and the methods for generating training data."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-7-rubric-4",
    "token_usage": {
      "prompt_tokens": 14519,
      "completion_tokens": 339,
      "total_tokens": 14858
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 Method, Section 4.1 Implementation details",
          "reasoning": "The paper describes training their 3D scene painting network from scratch using pseudo-ground-truth images generated by an off-the-shelf 2D semantic image synthesis network (OASIS) and rendered 3D coordinate and semantic label maps. This process involves training the MLP from random initialization to assign colors consistently given 3D coordinates and semantic labels."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3 Evaluation metrics",
          "reasoning": "The paper uses the SceneNet dataset scenes for quantitative evaluation of the trained model using metrics like FID, mIoU, and view consistency. The dataset is also employed for testing and measuring the effectiveness and performance of their 3D scene painting network."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.4 Ablation Study",
          "reasoning": "The authors conduct ablation studies on their network architecture, positional encoding dimensions, and loss function using their dataset to analyze the effects of these components on performance metrics. This constitutes an analysis use of the dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-7-rubric-5",
    "token_usage": {
      "prompt_tokens": 15242,
      "completion_tokens": 548,
      "total_tokens": 15790
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced utilizes semantic class labels and 3D scenes but there is no mention or indication of multiple human languages being involved in dataset entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that the dataset entries contain exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Data preparation",
          "reasoning": "The newly introduced dataset is based on 3D scenes with semantic labels assigned using the 150 labels in the ADE20k dataset, which is an English-language dataset. The semantic labels and all terminology appear in English only, with no mention of other human languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Dataset entries do not include any non-English only content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain programming code or structured code-related entries; it consists of 3D scenes and semantic labels rather than code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical expressions to describe losses and network architectures, the actual dataset described does not contain entries of mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains 3D scenes and semantic labels related to indoor scenes, with no biological sequences or non-human communication systems included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of constructed, fictional, or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content in the dataset is documented clearly as English semantic labels; there is no ambiguity or unspecified language."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset does contain human language entries, specifically semantic class labels, so this label does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-7-rubric-6",
    "token_usage": {
      "prompt_tokens": 12460,
      "completion_tokens": 225,
      "total_tokens": 12685
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not provide any explicit mention of code availability, nor does it include links or references to the code repository for data collection, preprocessing, or generation. No sections describe releasing code related to dataset construction or preparation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.2 Data Preparation and Section 4.1 Implementation details",
          "reasoning": "The paper documents the dataset creation process by explaining how training data is generated from 3D scenes. Specifically, in Section 3.2, the authors describe rendering 2D coordinate maps and semantic label maps from multiple viewpoints and generating pseudo ground-truth images using an off-the-shelf 2D semantic image synthesis network (OASIS). In Section 4.1, more details about dataset selection (SceneNet dataset) and preparation (assignment of semantic labels, use of Blender for rendering label and depth maps) are provided. This constitutes a transparent and sufficiently detailed documentation of the dataset creation procedure used to train the 3D scene painting network."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-720-rubric-0",
    "token_usage": {
      "prompt_tokens": 10412,
      "completion_tokens": 190,
      "total_tokens": 10602
    },
    "response": {
      "sources": [
        {
          "Modality": "graph",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4 (Dataset and panoptic metric) and Section 3.1 (Graph Construction)",
          "Reasoning": "The paper introduces the use of the FloorPlanCAD dataset [15], which is a large-scale collection of over 11,000 real-world CAD floor plan drawings with segment-grained panoptic annotations. Each CAD drawing is represented as a graph constructed from vector graphic primitives such as line segments, arcs, circles, and ellipses, where vertices correspond to geometric primitives and edges represent spatial relations among these primitives. Since CAD drawings are professionally created by humans and the dataset captures these human-designed vector graphics as graphs, the data modality is 'graph' and is 'Human Generated'. The dataset is not produced by models, nor is its origin unknown, as explicitly stated and referenced in the paper."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-720-rubric-1",
    "token_usage": {
      "prompt_tokens": 11212,
      "completion_tokens": 354,
      "total_tokens": 11566
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4 Experiment, Dataset and panoptic metric, first paragraph",
            "reasoning": "The paper states that the FloorPlanCAD dataset is a large-scale real-world dataset with segment-grained panoptic annotation. Such detailed annotation of over 11,602 real CAD drawings typically requires involvement of multiple human experts to ensure annotation quality, especially for specialized semantic and instance labels. No mention of automatic or AI annotation is present for this dataset in the paper."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4 Experiment, Dataset and panoptic metric, first paragraph",
            "reasoning": "The dataset provides segment-grained panoptic annotations, which implies the existence of detailed annotation guidelines and instructions for annotators to label semantic classes and instances on vector primitives accurately; such instructions are necessary to achieve consistent panoptic annotation on vector graphics in a specialized domain like CAD."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4 Experiment, Dataset and panoptic metric, first paragraph",
            "reasoning": "The panoptic metric defined on vector graphics indicates that detailed evaluation criteria and annotation rubrics exist to guide the annotation process, ensuring consistent segment and instance labeling to match panoptic segmentation standards similar to those in computer vision."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 1 Introduction, paragraph discussing FloorPlanCAD [15] and Section 4 Experiment",
            "reasoning": "The paper refers to the dataset from [15] which includes panoptic annotations and example visualizations are shown (e.g., Fig. 1 and Fig. 2), implying annotation examples were provided to annotators or users to demonstrate the expected annotation style and facilitate consistency."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-720-rubric-2",
    "token_usage": {
      "prompt_tokens": 12402,
      "completion_tokens": 305,
      "total_tokens": 12707
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about the dataset annotation or quality assurance being performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper about multiple human experts performing quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single non-expert human annotator conducted quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information regarding quality assurance by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by AI models acting as judges on the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any automated verification or rule-based quality assurance process applied to the dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses the FloorPlanCAD dataset [15], which is a pre-existing dataset. No new datasets are introduced by the authors in this paper. Furthermore, there is no documentation or description of any quality assurance process related to dataset annotations presented in the paper. Therefore, no quality assurance process is applied or documented for any new dataset in this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-720-rubric-3",
    "token_usage": {
      "prompt_tokens": 12012,
      "completion_tokens": 417,
      "total_tokens": 12429
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4, Dataset and panoptic metric",
          "reasoning": "The paper uses the FloorPlanCAD dataset [15], which contains 11,602 CAD drawings with segment-grained panoptic annotations. This dataset was built and annotated by human contributors. The authors rely on this large human-annotated dataset for their experiments. The paper does not introduce a new dataset, but uses this existing human-created human-annotated dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe generating any new dataset by AI or machine learning models. The dataset used is externally sourced and human-annotated."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the dataset being obtained by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the dataset being obtained by machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention aggregating or collecting data from multiple existing data sources; rather it uses a single existing dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe deriving a new dataset by modifying or adapting an existing dataset. It uses the existing FloorPlanCAD dataset as is."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and how it was generated is clearly documented in Section 4 referencing FloorPlanCAD [15]."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-720-rubric-4",
    "token_usage": {
      "prompt_tokens": 12530,
      "completion_tokens": 272,
      "total_tokens": 12802
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4, Implementation and Experiments",
          "reasoning": "The authors explicitly train their proposed GAT-CADNet model from scratch on the FloorPlanCAD dataset, using supervised learning with panoptic annotations to achieve panoptic symbol spotting."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Quantitative Evaluation",
          "reasoning": "The FloorPlanCAD dataset is used for evaluation and benchmarking of the proposed GAT-CADNet and competing methods by measuring panoptic metrics like PQ, RQ, and SQ to assess performance."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 Ablation Study",
          "reasoning": "The authors use the dataset to analyze the impact of network components and design choices through ablation studies on recognition and segmentation quality metrics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-720-rubric-5",
    "token_usage": {
      "prompt_tokens": 13253,
      "completion_tokens": 610,
      "total_tokens": 13863
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced is the FloorPlanCAD dataset consisting of CAD drawings with panoptic annotations, containing symbols and semantic labels for architectural floor plans. There is no mention of multiple human languages in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not involve exactly two languages; it is focused on symbolic, geometric, and semantic annotations without specification of two distinct human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as containing English text or entries exclusively in English. The dataset consists mainly of vector graphics with symbol annotations, not text in a natural language."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains entries in any single non-English human language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of computer-aided design drawings represented as graphs of geometric primitives; it does not contain programming code or related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3.1 (Graph Construction), Section 3.2 (Network Architecture)",
          "reasoning": "The dataset entries are CAD drawings represented as vector graphics with geometric primitives such as segments, arcs, circles, and ellipses encoded with mathematical attributes (angles, lengths, spatial relations). The annotations and input features include formal symbolic representations and geometric relations, which constitute mathematical and logical notation in the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is related to CAD drawings of architectural floor plans and does not include any biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any fictional or artificially created human languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) or symbolic content in the dataset are explicitly described as vector graphics with panoptic semantic annotations; the linguistic or symbolic nature is documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains symbolic and mathematical representations (geometric primitives and attributes); therefore, it does contain some form of symbolic or notation content."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-720-rubric-6",
    "token_usage": {
      "prompt_tokens": 10471,
      "completion_tokens": 151,
      "total_tokens": 10622
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not provide any links, references, or descriptions indicating that the code for constructing the dataset is publicly available. The authors use the publicly available FloorPlanCAD dataset [15] for evaluation, but they do not contribute new datasets themselves nor provide code related to dataset construction."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not describe any new dataset creation or construction process. The experiments and evaluations are conducted on the existing FloorPlanCAD dataset, and the paper does not provide any documentation about dataset creation or annotation processes within the paper itself."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-738-rubric-0",
    "token_usage": {
      "prompt_tokens": 13156,
      "completion_tokens": 152,
      "total_tokens": 13308
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5 (Evaluations); Datasets paragraph",
          "Reasoning": "The paper introduces a novel Gaussian Process VAE (GPVAE) model and evaluates it on several benchmark image datasets (MNIST, OMNIGLOT, CIFAR10, SVHN, and CelebA). The datasets are standard image datasets commonly used in the community and are used for training and evaluating their new GPVAE model. Since the paper does not mention the creation or modification of any new datasets, these datasets are existing benchmarks. The paper uses these datasets to demonstrate the performance of their proposed model rather than introducing new data collections."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-738-rubric-1",
    "token_usage": {
      "prompt_tokens": 13956,
      "completion_tokens": 243,
      "total_tokens": 14199
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 5 Evaluations and Section 5 Datasets",
            "reasoning": "The new dataset introduced is the GPVAE model's inference and evaluation on established image benchmarks (e.g., MNIST, Omniglot, CIFAR10, SVHN, CelebA). There is no indication of human annotation involved. The paper describes model evaluations with standard datasets, no mention of manual annotation processes, thus annotation is performed automatically by the model and computational procedures."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation instructions are provided, as the evaluation data is not manually annotated data but standard benchmark datasets used for model performance evaluation rather than creating new annotated data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics or guidelines for annotator grading are applicable because no human annotation occurs."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No examples of annotation are provided because annotation is not performed by humans on any new dataset; rather, model evaluations on existing datasets are reported."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-738-rubric-2",
    "token_usage": {
      "prompt_tokens": 15146,
      "completion_tokens": 288,
      "total_tokens": 15434
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on dataset annotation or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given in the paper that multiple human experts conducted quality assurance on dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that a single non-expert performed quality assurance on the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance activities performed by multiple non-expert humans for the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes the use of AI models for inference and approximation, but not for performing quality assurance on dataset content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of an automated verification process for dataset annotation quality assurance in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses several well-known benchmark datasets (MNIST, OMNIGLOT, CIFAR10, SVHN, CelebA) and does not introduce any new dataset. Hence, no quality assurance process for new datasets is applicable or described."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-738-rubric-3",
    "token_usage": {
      "prompt_tokens": 14756,
      "completion_tokens": 382,
      "total_tokens": 15138
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new dataset created entirely from scratch by human contributors. Instead, it uses existing benchmark datasets for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset generated by AI or machine learning models. The focus is on model methodology and inference, not data generation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any dataset produced by human translation of content from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any dataset produced by machine translation of content from other languages."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing benchmark datasets (MNIST, OMNIGLOT, CIFAR10, SVHN, CelebA) directly without mention of aggregation or collation of data from multiple sources to form a new dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any dataset derived from existing datasets with modifications, transformations, or adaptations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new datasets are introduced by the authors in the paper; the datasets used are standard benchmarks without indication of new data generation, collection, or derivation."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-738-rubric-4",
    "token_usage": {
      "prompt_tokens": 15274,
      "completion_tokens": 271,
      "total_tokens": 15545
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5: Evaluations",
          "reasoning": "Benchmark datasets such as MNIST, OMNIGLOT, CIFAR10, SVHN, and CelebA are used for evaluating the proposed Gaussian Process VAE model's performance in terms of test data log-likelihood scores and inference speed."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5: Evaluations, especially Figure 1 discussion",
          "reasoning": "Authors analyze the uncertainty in posterior approximation using MNIST 2D latent space to understand the quality of posterior approximations and the difficulty thereof by examining variance measures, reconstructing images and visualizing posterior distributions."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly applies the benchmark datasets for evaluation and analysis of the proposed model."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-738-rubric-5",
    "token_usage": {
      "prompt_tokens": 15997,
      "completion_tokens": 537,
      "total_tokens": 16534
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any dataset used contains entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of datasets containing exactly two human languages is made in the paper."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used (e.g., MNIST, CIFAR10, CelebA, SVHN, OMNIGLOT) are image datasets and contain no human language content in English or any language."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There are no datasets described with non-English language entries; the datasets are image-based without linguistic content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper concerns models and includes formulas, it does not introduce any dataset consisting of programming or code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical and logical notation appears in the paper itself, but the datasets introduced are benchmark image datasets; no dataset entries contain mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are standard image benchmarks; no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of constructed languages or fictional/artificial languages in the dataset entries."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are well-known benchmark image datasets and thus their content and nature are well specified."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new dataset introduced comprises standard image benchmark datasets (MNIST, OMNIGLOT, CIFAR10, SVHN, CelebA) which do not contain any language content, but rather image data."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-738-rubric-6",
    "token_usage": {
      "prompt_tokens": 13215,
      "completion_tokens": 151,
      "total_tokens": 13366
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No mention of code availability or links in the paper",
          "reasoning": "The paper does not provide any link to a code repository or mention availability of code for dataset construction or preprocessing. The focus is on model development and evaluation on existing benchmark datasets, without reference to released code."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No specific section or content describing dataset creation process",
          "reasoning": "The paper uses standard benchmark datasets (MNIST, OMNIGLOT, CIFAR10, SVHN, CelebA) and does not introduce any new datasets or describe any dataset creation. Dataset usage is described minimally, mainly referring to standard splits with no detailed documentation provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-746-rubric-0",
    "token_usage": {
      "prompt_tokens": 13345,
      "completion_tokens": 187,
      "total_tokens": 13532
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Experimental setup, Data paragraph and Table 1.",
          "Reasoning": "The new dataset entries correspond to subsets created from existing datasets for the Generalized Category Discovery (GCD) task. The data consists of images from various datasets such as CIFAR10, CIFAR100, ImageNet-100, CUB, Stanford Cars, and Herbarium19. These datasets are collections of images originally captured via human-involved processes (photography and manual collection), hence the data is human generated. The paper explicitly states sampling and splitting these datasets to create labelled and unlabelled subsets for the GCD evaluation, constituting new datasets introduced by the authors for the GCD setting. No model-generated or synthetic data are mentioned. The origins and modality are therefore image and human generated."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-746-rubric-1",
    "token_usage": {
      "prompt_tokens": 14145,
      "completion_tokens": 233,
      "total_tokens": 14378
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1.2, Section 4.1",
            "reasoning": "The labels for unlabelled data are assigned by a semi-supervised k-means clustering algorithm that constrains labelled instances into their known clusters and assigns unlabelled instances based on clustering, indicating an automatic, algorithmic annotation process rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit instructions for annotation are described in the paper.",
            "reasoning": "The paper discusses the method as an automatic clustering procedure without specifying human annotators or instructions for labelling unlabelled data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit rubric or scoring guidance for annotation is described in the paper.",
            "reasoning": "Since the annotation is done via clustering, no human scoring rubrics are defined or used."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of annotation guidelines or labelled examples for human annotators are provided in the paper.",
            "reasoning": "The data labelling is automated via clustering and not guided by example annotations."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-746-rubric-2",
    "token_usage": {
      "prompt_tokens": 15335,
      "completion_tokens": 285,
      "total_tokens": 15620
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process conducted by a single human expert on dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of quality assurance performed by multiple human experts for any dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information or indication that multiple non-experts performed quality assurance on datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using an AI model as a judge to perform quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated verification or rule-based quality assurance processes applied to dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces new experimental splits and settings based on existing datasets but does not describe any specific quality assurance procedures for the datasets used or created. No process for validating or verifying dataset annotations is described; therefore, no quality assurance process is considered present."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-746-rubric-3",
    "token_usage": {
      "prompt_tokens": 14945,
      "completion_tokens": 461,
      "total_tokens": 15406
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset created entirely from scratch by human contributors. Instead, it uses existing public datasets for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not generate any dataset artificially or synthetically from AI or machine learning models. It focuses on methods applied to existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was produced by translating content from another language via human translators in this paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not involve data generated by machine translation from other languages."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 (Experimental setup, Data)",
          "reasoning": "The authors utilize several existing public datasets (e.g., CIFAR10, CIFAR100, ImageNet-100, CUB, Stanford Cars, Herbarium19) by re-partitioning and re-sampling subsets to create labelled and unlabelled splits appropriate for their Generalized Category Discovery setting. They do not create new images but reorganize existing data for their experiments."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 (Experimental setup, Data)",
          "reasoning": "The authors derive new dataset splits by sub-sampling classes and images from existing datasets to form the labelled and unlabelled subsets used in their experiments. This adaptation involves transformation of existing datasets to fit the new GCD task."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the source and method of data preparation by referencing and repurposing established datasets; therefore, unknown or undocumented data sources do not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-746-rubric-4",
    "token_usage": {
      "prompt_tokens": 15463,
      "completion_tokens": 388,
      "total_tokens": 15851
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets used exclusively for pre-training models. Instead, it uses existing datasets for evaluation and training purposes."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce new datasets used for training models from randomly initialized parameters; rather, it leverages pretrained models and fine-tunes or evaluates on existing datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets mentioned are used primarily for evaluation, and the paper describes training methods applied on the labelled and unlabelled splits of existing datasets; no new datasets are introduced for supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss reinforcement learning based post-training techniques or use any newly introduced datasets for such purposes."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "Section 4.1 (Experimental setup)",
          "reasoning": "The paper uses several existing public datasets (e.g., CIFAR, ImageNet-100, Semantic Shift Benchmark suite, Herbarium19) for evaluation and benchmarking, but these are pre-existing and not introduced by the authors."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets are introduced and used mainly for analysis and characteristics study; analysis is performed on established datasets."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using any dataset as a knowledge base to augment models."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; all mentioned datasets are established datasets used for evaluation or benchmarking. Therefore, no practical usage of a newly proposed dataset is described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-746-rubric-5",
    "token_usage": {
      "prompt_tokens": 16186,
      "completion_tokens": 546,
      "total_tokens": 16732
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces datasets composed of images of objects and fine-grained classes without any mention of textual data in multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset entries contain exactly two human languages or bilingual text content."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Data (page 9)",
          "reasoning": "The datasets referenced (e.g., CIFAR-10, ImageNet, Stanford Cars, CUB, Herbarium19) are primarily composed of images with English-language labels and annotations. The paper's textual content and dataset labels are presented in English only, and no other languages are mentioned concerning dataset annotations."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets and annotations are in English, with no mention of datasets in non-English-only languages."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of images and labels but do not contain any programming code or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets themselves do not contain mathematical or logical notation; mathematical expressions are present only in the method description and not in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although Herbarium19 dataset is related to plants, the dataset entries are images and labels, not biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or evidence of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of dataset entries is clearly described as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain image labels and annotations in human language (English), so it is not language-free."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-746-rubric-6",
    "token_usage": {
      "prompt_tokens": 13404,
      "completion_tokens": 220,
      "total_tokens": 13624
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract; Section 1 Introduction",
          "reasoning": "The paper explicitly provides a URL to the project code repository in the Abstract (https://www.robots.ox.ac.uk/~vgg/research/gcd), suggesting that the code related to the dataset processing and the methods is publicly available. Although the code release is mainly for the method, it is the same repository for experiments which involve dataset setup, thus the authors provide code to reproduce their dataset splits and processing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Experimental setup (Data)",
          "reasoning": "The paper documents in Section 4.1 how the datasets used in experiments are constructed, including the procedure for splitting: selecting labelled classes, sampling labelled images, and remaining images as unlabelled data. They also report detailed statistics about the datasets used (number of classes and images in labelled and unlabelled splits). This detailed description provides sufficient transparency on how the datasets were prepared for the new proposed Generalized Category Discovery setting."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-756-rubric-0",
    "token_usage": {
      "prompt_tokens": 12543,
      "completion_tokens": 136,
      "total_tokens": 12679
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4, Datasets paragraph",
          "Reasoning": "The paper introduces the use of surveillance video datasets UCF-Crime and ShanghaiTech for training their Generative Cooperative Learning (GCL) approach. While these datasets themselves are existing datasets, the paper's contribution is in introducing a novel unsupervised learning method using these datasets, rather than introducing a new dataset. The paper does not explicitly mention any new video dataset introduced by the authors; their experiments use these existing datasets with modifications to the splits or labels for unsupervised learning."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-756-rubric-1",
    "token_usage": {
      "prompt_tokens": 13343,
      "completion_tokens": 255,
      "total_tokens": 13598
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3 and Section 4 Datasets",
            "reasoning": "The paper describes training the Generative Cooperative Learning (GCL) model on unlabelled video data without any human labeling. Pseudo-labels for anomalies are generated automatically by the generator and discriminator networks in an iterative training process, clearly indicating the annotation process is done by an AI model's internal mechanism rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not mention providing any instructions for human annotators or any annotation procedure guidelines since no human annotations are used for the training data; labels are generated automatically by the model itself."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "There is no mention or indication of scoring rubrics or evaluation criteria for annotation because no human annotators label the data; annotation is performed via model-generated pseudo labels."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not provide any examples or guidelines for annotations since the pseudo-label generation is automated by the generator and discriminator, with no human annotation involved."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-756-rubric-2",
    "token_usage": {
      "prompt_tokens": 14533,
      "completion_tokens": 297,
      "total_tokens": 14830
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance being conducted by a single human expert for the datasets used."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that multiple human experts were involved in the quality assurance or annotation process for the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of quality assurance by a single human non-expert for the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention multiple human non-expert annotators performing quality assurance on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of an AI model as a judge for quality assurance on the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification or algorithmic quality assurance processes applied to validate the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses two existing datasets (UCF-Crime and ShanghaiTech) without introducing any new datasets. There is no documentation or description about quality assurance processes related to dataset annotations or content validation. Since no new dataset was introduced, no QA process is described or performed by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-756-rubric-3",
    "token_usage": {
      "prompt_tokens": 14143,
      "completion_tokens": 395,
      "total_tokens": 14538
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the creation of any new dataset entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe generation of any dataset entirely by models; the proposed method focuses on learning from existing video datasets rather than creating new data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of translation of datasets from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of machine translation to generate data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4, Datasets",
          "reasoning": "The paper uses existing datasets (UCF-Crime and ShanghaiTech) for training and evaluation. These datasets are collected from real-world surveillance videos and staged data. The paper re-organizes or uses these datasets without creating new data, thus utilizing collated data from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While features are extracted and processed from existing videos, this processing constitutes feature extraction for model input and not a new dataset derived by significant transformations or adaptations stated as a new dataset. The paper does not introduce a new dataset derived from existing ones."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and source of data used is documented explicitly in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-756-rubric-4",
    "token_usage": {
      "prompt_tokens": 14661,
      "completion_tokens": 453,
      "total_tokens": 15114
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing video anomaly datasets and does not introduce any new datasets; pre-training is applied on existing data but no new dataset is introduced specifically for pre-training."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used (UCF-Crime and ShanghaiTech) are pre-existing benchmarks; the paper does not introduce any novel dataset to be used for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses fine-tuning models using weak supervision or one-class supervision but does so on existing datasets; no new dataset is proposed or fine-tuned."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning based post-training or RLHF methods using a new dataset in the paper."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The evaluation is performed on existing public datasets (UCF-Crime and ShanghaiTech); the paper does not propose new datasets for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper analyzes methods using existing datasets but does not introduce any new dataset for analysis purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is introduced to serve as a knowledge base for model augmentation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; all datasets used for experiments and evaluation are existing well-known datasets. Therefore, no practical usage of a new proposed dataset is described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-756-rubric-5",
    "token_usage": {
      "prompt_tokens": 15384,
      "completion_tokens": 553,
      "total_tokens": 15937
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4 (Datasets), Section 3.2 (Generative Cooperative Learning), throughout the paper",
          "reasoning": "The newly introduced use of datasets (UCF-Crime and ShanghaiTech) in the paper involves video data and feature vectors extracted from videos. The dataset content primarily consists of natural surveillance videos without mention of any spoken language content or multilingual annotations. The textual documentation, discussion, and annotations in the study are all in English, and no other human language data or annotations are described within these datasets. Therefore, the dataset entries contain only English language content related to descriptions and labels, making the dataset monolingual in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of video features and corresponding frame-level labels; there is no indication that programming or structured code-related content is part of the dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets themselves do not contain mathematical or logical notations; such notations appear in the method descriptions but not as dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets include surveillance video data and do not contain biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or evidence is present for constructed or artificial languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset entries is specified and understood (English); it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain human language content (English) in the form of video labels and documentation; hence they are not without language content."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-756-rubric-6",
    "token_usage": {
      "prompt_tokens": 12602,
      "completion_tokens": 114,
      "total_tokens": 12716
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section in the paper",
          "reasoning": "The paper does not mention any code repository, link, or availability of code related to dataset construction, data collection, preprocessing, or dataset generation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section in the paper",
          "reasoning": "The paper does not introduce any new dataset created by the authors; it only utilizes existing datasets (UCF-Crime and ShanghaiTech). Therefore, no documentation of dataset creation or construction is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-78-rubric-0",
    "token_usage": {
      "prompt_tokens": 11648,
      "completion_tokens": 335,
      "total_tokens": 11983
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.3 Hardware Implementation; Section 4.2 Experiment Using Camera Prototype",
          "Reasoning": "The paper introduces a new dataset acquired through a novel hardware prototype camera that captures a dynamic light field as a single-shot coded 2-D image. This image data is captured from real 3-D scenes moving over time using the implemented imaging system involving aperture coding and pixel-wise exposure coding. Since the data are physically recorded from actual scenes using a camera, the image data are human generated."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.4 Network Design and Training; Section 4.2 Experiment Using Camera Prototype",
          "Reasoning": "The dynamic light field dataset consists of sequences representing 5\u00d75 viewpoints over 4 time units, effectively forming a multi-view video dataset of dynamic scenes. The prototype camera records these temporal sequences of coded light field images of real moving scenes, constituting videos. As these videos are obtained through physical capture of dynamic scenes, they are human generated."
        },
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.4 Network Design and Training",
          "Reasoning": "The training dataset is synthesized from 51 static light fields augmented with intensity changes and then dynamically varied by applying 2-D lateral motions programmatically to create 5,575,500 samples of dynamic light fields. These synthetic dynamic light fields are generated using algorithmic methods, hence model generated."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-78-rubric-1",
    "token_usage": {
      "prompt_tokens": 12448,
      "completion_tokens": 236,
      "total_tokens": 12684
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.4 Network Design and Training, and Section 4.1 Quantitative Evaluation",
            "reasoning": "The dynamic light-field dataset used for training and evaluation was synthetically generated by applying 2-D lateral motions to static light-field patches and creating virtual dynamic light fields. This process was algorithmic and simulated, not conducted by human annotators. The annotation corresponds to the generation of ground truth dynamic light field data used for training and evaluation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "The paper does not mention any detailed manual instructions provided for annotators since the data annotation was performed via automatic synthetic generation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "Similarly, no scoring rubrics or evaluation guidelines for human annotation are discussed, as the data generation was synthetic and automatic."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "No annotation examples or labeled samples as manual annotation are provided or mentioned; the ground truth data was created by a computational process."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-78-rubric-2",
    "token_usage": {
      "prompt_tokens": 13638,
      "completion_tokens": 296,
      "total_tokens": 13934
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process conducted by a single human expert on any dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance on the dataset in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information about quality assurance conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information in the paper indicates that multiple non-expert human annotators performed quality assurance on dataset annotations or content."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although an AI model (the RecNet) is used for reconstruction, the paper does not indicate it is used as a judge or for quality assurance of the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic quality assurance process for validating dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document or describe any quality assurance process for the datasets (including the synthetically generated or collected light-field data). There is no mention of manual or automated validation or annotation QA procedures for datasets used in training or evaluation."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-78-rubric-3",
    "token_usage": {
      "prompt_tokens": 13248,
      "completion_tokens": 455,
      "total_tokens": 13703
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.4 Network Design and Training",
          "reasoning": "The authors created a large training dataset of dynamic light-field patches derived by augmenting 51 static light fields with synthesized motion patterns. This dataset is original content created by human contributors by collecting existing static light fields and creating synthetic dynamic samples with specified motions. Although based on existing static light-field data, the dynamic data samples with motion augmentations are newly created human design for training their network."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset generated solely by models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any human translation of datasets from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any machine translation of datasets from other languages."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.4 Network Design and Training",
          "reasoning": "The training dataset was collected from 51 static light fields sourced from prior works (Section 3.4). These static light fields were directly used before augmentation, which constitutes collated data from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.4 Network Design and Training",
          "reasoning": "The authors derived dynamic light-field data by applying synthetic motions to the static light-field patches, transforming the collated static data into dynamic sequences for training. This represents data derived from existing sources with modifications applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation methods are clearly described in Section 3.4, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-78-rubric-4",
    "token_usage": {
      "prompt_tokens": 13766,
      "completion_tokens": 530,
      "total_tokens": 14296
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.4 Network Design and Training",
          "reasoning": "The paper describes that the authors collected 223,020 light-field patches from static light fields, and synthesized dynamic light fields by applying controlled motion to them, resulting in 5,575,500 samples. This dataset is used to train the reconstruction network (RecNet) from scratch, jointly with AcqNet, under a supervised learning setup to minimize squared reconstruction error."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that a pre-trained model is fine-tuned using the introduced dataset. The training is described as from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning or RL-based post-training techniques."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Quantitative Evaluation and Section 4.2 Experiment Using Camera Prototype",
          "reasoning": "The synthesized dataset and the computer generated dynamic light fields are used for quantitative evaluation of the reconstruction quality, ablation studies, comparison with prior methods, and working range analysis. The prototype camera experiments also use the dynamic scenes captured to evaluate performance."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.1 Quantitative Evaluation",
          "reasoning": "The paper uses the synthesized datasets to analyze the effects of different coding methods (aperture coding only, pixel-wise coding only, combined, etc.) and the working range with respect to motion and disparity, analyzing trends in reconstruction performance and coding effectiveness."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The introduced dataset is not described or used as a knowledge base to augment models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset introduced is actively used for training the reconstruction network and for evaluation and analysis; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-78-rubric-5",
    "token_usage": {
      "prompt_tokens": 14489,
      "completion_tokens": 526,
      "total_tokens": 15015
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Entire paper",
          "reasoning": "The paper and dataset descriptions are entirely in English, and all documentation, dataset synthesis procedures, and experiments are described in English only. There is no indication of any other human language content in the dataset introduced."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although network architectures and training methods are described, the paper does not present or introduce any dataset containing programming or code-language content. The dataset consists of visual dynamic light fields only."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3.1 (Problem Formulation), Section 3.2 (Image Acquisition Model)",
          "reasoning": "The dataset deals with representation and modeling of dynamic light fields using mathematical notations (e.g., 5-D light field parameterizations Lx,y(u,v,t), equations for image formation Ixy, coding patterns a(u,v,t), pxy(t)), indicating that the data representation inherently includes mathematical values and symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset represents dynamic light fields of visual scenes and contains no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificially created languages are present in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is fully specified and documented as visual dynamic light fields with descriptions in English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains entries in English language and mathematical notation, so it does involve language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-78-rubric-6",
    "token_usage": {
      "prompt_tokens": 11707,
      "completion_tokens": 172,
      "total_tokens": 11879
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and main text",
          "reasoning": "The paper mentions that the software is available from the project webpage but does not explicitly state that the code for dataset construction or dynamic light-field dataset generation is publicly released. The focus is on the imaging method and reconstruction algorithm, with no direct indication of dataset code availability."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.4 Network Design and Training",
          "reasoning": "The paper documents the dataset creation process used for training: it describes collecting 223,020 light-field patches from 51 static light fields, applying intensity augmentation, simulating motions to synthesize dynamic light-field samples with specified velocity ranges, and the total number of samples used. This provides transparent and sufficient documentation on how the dataset was constructed for training purposes."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-780-rubric-0",
    "token_usage": {
      "prompt_tokens": 10145,
      "completion_tokens": 130,
      "total_tokens": 10275
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1",
          "Reasoning": "The only new dataset introduced in the paper is the training dataset SceneFlow, which is a synthetic dataset containing stereo image pairs with ground truth disparity maps used as the source domain for training. Since it is synthetic data generated by computer graphics simulations, it is model generated, but it is composed of image data. The images represent the data modality 'image'. The synthetic dataset is explicitly mentioned as the training source in Section 4.1 (Datasets & Evaluation Metrics)."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-780-rubric-1",
    "token_usage": {
      "prompt_tokens": 10945,
      "completion_tokens": 222,
      "total_tokens": 11167
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1",
            "reasoning": "The new dataset introduced in the paper is SceneFlow, a synthetic dataset used as the training (source) domain for the stereo matching networks. The annotations (disparity ground truth) are automatically generated since it is a synthetic dataset widely used for training and evaluating stereo methods."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "The paper does not describe any annotation guidelines including instructions for annotators for SceneFlow\u2014being a synthetic dataset, annotation is not performed by humans but generated automatically."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "No scoring rubrics or similar guideline information are provided or relevant for the automatically generated synthetic dataset annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "No examples for annotation are given or needed since SceneFlow is synthetic data with fully known ground truth generated from rendering."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-780-rubric-2",
    "token_usage": {
      "prompt_tokens": 12135,
      "completion_tokens": 286,
      "total_tokens": 12421
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human expert annotator performing quality assurance on the dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of multiple human expert annotators involved in dataset quality assurance is provided in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that a single non-expert human performed QA on any dataset introduced by the authors."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that multiple non-expert humans conducted quality assurance on any dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of AI models as judges for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention an automated verification process or algorithmic rule-based quality assurance applied to dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces no new datasets; it uses existing datasets (SceneFlow, KITTI, Middlebury, ETH3D) for training and evaluation. Hence, there is no description or evidence of any quality assurance process applied or documented by the authors for new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-780-rubric-3",
    "token_usage": {
      "prompt_tokens": 11745,
      "completion_tokens": 378,
      "total_tokens": 12123
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset created by human contributors from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe creation of new datasets generated entirely by AI or models; the focus is on model training and evaluation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data produced by machine translation from other languages."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing datasets such as SceneFlow (synthetic), KITTI, Middlebury, and ETH3D for training and evaluation without indicating that these were newly collated or aggregated by the authors."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors do not create any modified or adapted dataset based on existing datasets; they only use existing datasets for training the models and evaluation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new datasets are introduced in this work; the paper exclusively uses existing, publicly available datasets for training and evaluation. Therefore, the origin of any data used is fully documented as pre-existing datasets, and no novel dataset creation is reported."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-780-rubric-4",
    "token_usage": {
      "prompt_tokens": 12263,
      "completion_tokens": 201,
      "total_tokens": 12464
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses existing datasets (SceneFlow, KITTI, Middlebury, ETH3D) for training and evaluation, but does not introduce any new datasets. Therefore, there is no new dataset presented by the authors to have a documented use within the machine learning pipeline."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-780-rubric-5",
    "token_usage": {
      "prompt_tokens": 12986,
      "completion_tokens": 546,
      "total_tokens": 13532
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not introduce any new dataset containing entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not introduce any new dataset containing entries specifically in two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not introduce any new dataset with only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not introduce any new dataset with only one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper discusses stereo matching networks and features (e.g., VGG, ResNet) but does not introduce any new dataset containing programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Mathematical and logical notations appear in the paper for method description, but no new dataset entries containing mathematical or formal logical expressions are introduced."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No biological sequences or non-human communication data are introduced as a dataset in the paper."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not mention any dataset involving fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language content of the dataset entries is well specified; no unknown language datasets are introduced."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. The authors use existing datasets (SceneFlow, KITTI, Middlebury, ETH3D) for training and evaluation, none of which are newly created by the authors. Therefore, no dataset entries containing any language are introduced."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-780-rubric-6",
    "token_usage": {
      "prompt_tokens": 10204,
      "completion_tokens": 176,
      "total_tokens": 10380
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper provides a link to the code of the proposed stereo matching models (https://github.com/SpadeLiu/Graft-PSMNet) related to their method implementation, but it does not introduce a new dataset. The datasets used (SceneFlow, KITTI, Middlebury, ETH3D) are pre-existing datasets, and there is no indication that the authors created or released any new dataset or code specifically for dataset construction."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "",
          "reasoning": "The paper does not describe any new dataset creation process or provide documentation regarding dataset generation. The datasets used for training and evaluation are existing public datasets (SceneFlow, KITTI, Middlebury, ETH3D), and the paper does not introduce nor document any new dataset."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-826-rubric-0",
    "token_usage": {
      "prompt_tokens": 10070,
      "completion_tokens": 120,
      "total_tokens": 10190
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 (Dataset)",
          "Reasoning": "The paper explicitly states in Section 4.1 that the authors collected a new dataset named SelfieVideo consisting of 80 videos of 80 diverse clothed persons captured with an HD camera with resolution 2160x1216, where subjects rotate with A-pose in front of the camera. This dataset is newly introduced by the authors and consists of monocular videos captured via human-operated devices."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-826-rubric-1",
    "token_usage": {
      "prompt_tokens": 10870,
      "completion_tokens": 260,
      "total_tokens": 11130
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1 Dataset",
            "reasoning": "The paper describes a new dataset named SelfieVideo collected with 80 videos of 80 diverse clothed persons. The subjects were collected from the talent market and each subject signed a license agreement. There is no indication of crowdsourcing or multiple annotators; the collection appears controlled and curated by experts or researchers involved in the work."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1 Dataset",
            "reasoning": "The paper does not mention any annotation tasks requiring instructions, nor does it describe any guidelines or protocols for annotators related to this new dataset. The dataset collection mainly involves video capture of subjects performing specific poses."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1 Dataset",
            "reasoning": "There is no mention of any scoring, rating, or rubric-based annotation tasks performed on the new dataset. The dataset is described as captured videos without additional annotation scores."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.1 Dataset",
            "reasoning": "No examples or annotation examples are provided, as the dataset involves raw captured videos with no described human annotation beyond data collection."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-826-rubric-2",
    "token_usage": {
      "prompt_tokens": 12060,
      "completion_tokens": 264,
      "total_tokens": 12324
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the introduced dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper about QA conducted by multiple human experts for the introduced dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No details are given about quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe QA conducted by multiple non-expert human annotators for the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that AI models were used to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated or algorithmic quality assurance process on dataset annotations or content is described in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces the SelfieVideo dataset but does not document any quality assurance process for it. There is no mention of manual or automated verification or validation of dataset content or annotations."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-826-rubric-3",
    "token_usage": {
      "prompt_tokens": 11670,
      "completion_tokens": 453,
      "total_tokens": 12123
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The authors collected a new dataset called SelfieVideo consisting of 80 videos of 80 diverse clothed persons captured with an HD camera. The subjects were collected from the talent market and each subject signed a license agreement, indicating that the data was created entirely from scratch by human contributors. This data is original and not adapted or derived from pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any entirely original data using AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data in the dataset was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was generated by machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper evaluates the proposed method on the public People-Snapshot dataset, which is a pre-existing dataset collected by others. While the authors use this dataset for evaluation, they did not introduce it as new data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper processes and reconstructs avatars using their framework, the introduced dataset (SelfieVideo) is original human-collected data, not derived from existing sources by modification. The processed outputs are not datasets themselves."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin of the new dataset SelfieVideo as human-collected data. Therefore, the data origin is specified."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-826-rubric-4",
    "token_usage": {
      "prompt_tokens": 12188,
      "completion_tokens": 292,
      "total_tokens": 12480
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.3.2",
          "reasoning": "The paper describes that for a new person, the neural rendering network with pre-trained weights only needs to be fine-tuned with several epochs on the collected dataset, indicating use of the dataset for supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The authors evaluate their method on both the public People-Snapshot dataset and their collected SelfieVideo dataset by calculating metrics such as FID and comparing reconstructed results, indicating use of the dataset for evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "Ablation studies and qualitative/quantitative analyses are performed using the collected dataset to understand the effect of different modules, supervision schemes, and training strategies, demonstrating dataset use for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-826-rubric-5",
    "token_usage": {
      "prompt_tokens": 12911,
      "completion_tokens": 599,
      "total_tokens": 13510
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any inclusion of multiple human languages in the dataset collected or used. The dataset SelfieVideo and related data appear to consist solely of images/videos of people, without linguistic content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper that the new dataset includes exactly two different human languages in its content."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset; and throughout the paper",
          "reasoning": "The only textual content relating to the new dataset (SelfieVideo) in the paper is provided in English, including the description, experimental details, and usage instructions. The dataset consists primarily of videos showing people rotating in front of the camera, but no mention of other spoken or written languages is provided, so English is the sole language used in the documentation and metadata."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence the dataset is in a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper contains mathematical formulas and mentions of neural network architectures and training code, the dataset (SelfieVideo) itself does not contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical and logical notations appear in the paper for method description, but the dataset does not contain entries containing mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of monocular RGB videos of humans; no biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or constructed languages included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of dataset content is documented as English for textual descriptions and metadata, and the video shows human actors, so language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human languages in the metadata and documentation (English), so it is not language-free."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-826-rubric-6",
    "token_usage": {
      "prompt_tokens": 10129,
      "completion_tokens": 183,
      "total_tokens": 10312
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract, last paragraph; no explicit code link for dataset",
          "reasoning": "The paper states that code and dataset will be available at a project webpage for the method but does not provide a direct link or reference to code specifically used for dataset construction or processing. There is no indication that code related to dataset collection or preprocessing is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper documents their collected dataset \"SelfieVideo\" in Section 4.1, specifying number of videos, subjects, camera specifications, resolution, capture setup (subjects rotate in front of camera), licensing agreement, and preprocessing steps (resizing frames, use of foreground matting). This serves as documentation of the dataset creation process, enabling understanding of how the dataset was collected and processed."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-836-rubric-0",
    "token_usage": {
      "prompt_tokens": 10268,
      "completion_tokens": 107,
      "total_tokens": 10375
    },
    "response": {
      "sources": [
        {
          "Modality": "time series",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1, Datasets",
          "Reasoning": "The paper introduces experiments on several datasets, but these are well-known and pre-existing datasets (ETH/UCY, Stanford Drone Dataset, Lyft Prediction Dataset) rather than new datasets introduced by the authors. The authors do not describe introducing any new dataset in the paper. Therefore, no new dataset introduced by the authors has data entries."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-836-rubric-1",
    "token_usage": {
      "prompt_tokens": 11068,
      "completion_tokens": 240,
      "total_tokens": 11308
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4 (Model and Training Procedures)",
            "reasoning": "The dataset used is standard benchmarking datasets with trajectories extracted from tracking data. The paper does not mention human annotators labeling or correcting data but uses existing datasets and automatic methods to obtain trajectories for training the teacher and student models."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit annotation instructions provided in the paper",
            "reasoning": "There is no description or mention of annotation guidelines or instruction protocols for labeling trajectory data, as the data is from existing datasets or tracked trajectories automatically obtained."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit rubric for annotation reported",
            "reasoning": "The paper does not describe any rubric or scoring criteria used for annotation as no annotation-related evaluation is performed; metrics like ADE and FDE are used for evaluating model predictions, not annotation quality."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples provided or discussed",
            "reasoning": "No examples of annotations are provided since the work focuses on model training and evaluation based on existing datasets, not on annotation guidelines or example annotations."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-836-rubric-2",
    "token_usage": {
      "prompt_tokens": 12258,
      "completion_tokens": 367,
      "total_tokens": 12625
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about dataset annotation or quality assurance performed by a single human expert on the datasets used or introduced."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention multiple human experts conducting quality assurance on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of single non-expert human annotators performing quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple non-expert annotators performed quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of AI models being used specifically as a judge for quality assurance of the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification process applied to data annotation or dataset quality assurance."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses existing public datasets (ETH/UCY, Stanford Drone Dataset, Lyft Prediction Dataset) and does not introduce any new datasets or describe any quality assurance procedures applied to these datasets or any new data. Therefore, no quality assurance process is documented for datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-836-rubric-3",
    "token_usage": {
      "prompt_tokens": 11868,
      "completion_tokens": 382,
      "total_tokens": 12250
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the creation of new datasets generated entirely from scratch by human contributors; instead, it uses existing publicly available datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper involves model-generated trajectory predictions, these are outputs of the proposed method rather than a new dataset introduced by the authors."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data produced by human translation from other languages is mentioned or introduced."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data produced by machine translation is mentioned or introduced."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used (ETH/UCY, Stanford Drone Dataset, Lyft Prediction Dataset) are pre-existing and used as is or split according to common protocols; the paper does not report any new dataset collated or aggregated by the authors."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is found that the authors create modified, transformed, or adapted versions of existing datasets to form a new dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; it exclusively makes use of publicly available datasets. Thus, there is no specified data source or method of generation for novel datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-836-rubric-4",
    "token_usage": {
      "prompt_tokens": 12386,
      "completion_tokens": 448,
      "total_tokens": 12834
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 Teacher training; Section 5 Experiments",
          "reasoning": "The paper describes training a teacher model from scratch using the ETH/UCY, Stanford Drone Dataset (SDD), and Lyft Prediction Dataset with 8 observation time steps. This training is done by minimizing the Mean Squared Error loss between predicted and ground-truth trajectories, following a standard supervised learning approach from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.2 Student training; Section 5 Experiments",
          "reasoning": "The student model is trained in a supervised manner to mimic the teacher with fewer input observations (e.g., 2 instead of 8). The training involves supervision from ground-truth data as well as distillation losses to transfer knowledge from the teacher to the student, constituting a supervised fine-tuning process post initial teacher training."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 Experiments",
          "reasoning": "The datasets (ETH/UCY, SDD, and Lyft Prediction Dataset) are used as standard benchmarks for evaluation using metrics like Average Displacement Error (ADE) and Final Displacement Error (FDE) to assess model performance."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.4 Why Distilling the Observations works; Section 5.5 On the length-shift problem; Section 5.6 On the domain shift problem - Knowledge Transfer",
          "reasoning": "The datasets are used to analyze model behavior under different conditions, such as the effect of input observation length (length-shift problem), robustness to domain shifts between datasets, and attention coefficient analysis to understand temporal contribution, primarily for pattern and trend analysis beyond training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-836-rubric-5",
    "token_usage": {
      "prompt_tokens": 13109,
      "completion_tokens": 528,
      "total_tokens": 13637
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes datasets consisting of trajectories of agents (pedestrians, vehicles, etc.) without any reference to multiple human languages within the data. No multilingual content is mentioned."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the dataset entries include exactly two human languages. The data is not described in terms of linguistic content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains trajectory data, which consists of spatial-temporal coordinates, not textual content in English or any other language."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No linguistic content of any type is present; thus, no non-English monolingual language data is included."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are not code or programming language corpora; they are trajectory datasets containing spatial-temporal coordinates."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical notation to describe model objectives and architecture, this does not apply to the dataset itself. The dataset does not contain formulae or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human motion trajectories, not biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets containing constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly describes the dataset type as trajectory data and does not mention any unspecified language content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced consist solely of spatial-temporal trajectories of agents with no language content, hence the dataset entries do not contain any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-836-rubric-6",
    "token_usage": {
      "prompt_tokens": 10327,
      "completion_tokens": 172,
      "total_tokens": 10499
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention regarding dataset construction code availability.",
          "reasoning": "The paper discusses the use of existing public datasets (ETH/UCY, Stanford Drone Dataset, Lyft Prediction Dataset) but does not provide or mention any code related to constructing or preprocessing these datasets nor any link to repositories for such code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 5.1 Datasets (page 5) and throughout the Experiments section.",
          "reasoning": "The paper documents the datasets used, including details like the number of scenes, the nature of the data, and the standard splits and protocols followed. However, it does not describe the creation of new datasets but rather the use and splitting of existing ones. The documentation covers how the datasets are employed for training and evaluation."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-870-rubric-0",
    "token_usage": {
      "prompt_tokens": 11520,
      "completion_tokens": 121,
      "total_tokens": 11641
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.5 IRSTD-1k Dataset; Abstract; Introduction; Experiment Section 4.1",
          "Reasoning": "The IRSTD-1k dataset introduced in this paper consists of 1,000 realistic infrared images captured by an infrared camera in the real world, with accurate pixel-level manual annotations. Since the images are captured by a camera operated by humans and annotations done manually by humans, the data modality is image and the origin is human-generated."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-870-rubric-1",
    "token_usage": {
      "prompt_tokens": 12320,
      "completion_tokens": 218,
      "total_tokens": 12538
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.5, 'IRSTD-1k Dataset' and Abstract",
            "reasoning": "The paper states that IRSTD-1k contains 1,000 realistic infrared images with accurate pixel-level annotations manually labeled, indicating that human annotators performed the annotations. Since these are pixel-level precise masks for complex imagery requiring domain knowledge, it implies expert annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.5",
            "reasoning": "The paper does not describe any specific annotation guidelines or instructions provided to annotators for IRSTD-1k dataset creation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.5",
            "reasoning": "No mention of scoring rubrics or formal criteria or evaluation standards used during annotation is present in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.5",
            "reasoning": "There is no description or appendix section supplying annotation examples or illustrating annotation cases with examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-870-rubric-2",
    "token_usage": {
      "prompt_tokens": 13510,
      "completion_tokens": 422,
      "total_tokens": 13932
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the dataset annotations were quality assured by a single human expert or annotator possessing domain expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by multiple human experts or annotators with domain expertise for the IRSTD-1k dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance conducted by a single human non-expert on the dataset annotations."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper states that the IRSTD-1k dataset was manually annotated at the pixel level, but it does not specify how many annotators were involved or their expertise level, nor does it mention any quality assurance by multiple non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided that an AI model was used as a judge or to validate dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "Section 3.5 IRSTD-1k Dataset",
          "reasoning": "The paper does not report any automated verification methods or algorithmic processes applied for quality assurance of dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper only mentions that the IRSTD-1k dataset was manually annotated with accurate pixel-level masks but does not mention any process of quality assurance, validation, or verification of annotations. Thus, no quality assurance process is documented or described."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-870-rubric-3",
    "token_usage": {
      "prompt_tokens": 13120,
      "completion_tokens": 382,
      "total_tokens": 13502
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.5 IRSTD-1k Dataset",
          "reasoning": "The paper states that the authors constructed a new benchmark called IRSTD-1k, consisting of 1,000 infrared images captured by an infrared camera in the real world. They manually annotated the targets at the pixel level, indicating that the dataset was created entirely from scratch by human contributors without derivation from pre-existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated entirely by AI or machine learning models without reference to other data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset involves translation from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset involves machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the dataset as collected or aggregated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description suggests that the dataset is based on existing sources with modifications or transformations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the data source and generation method for IRSTD-1k, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-870-rubric-4",
    "token_usage": {
      "prompt_tokens": 13638,
      "completion_tokens": 463,
      "total_tokens": 14101
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The IRSTD-1k dataset is used in training the proposed ISNet model from scratch, as described in Section 4.1 where the dataset is split for training, validation, and testing, and Section 4.2 describing the training procedure. This indicates the dataset supports supervised training of models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the IRSTD-1k dataset for fine-tuning pre-trained models specifically; the training described appears to be from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of reinforcement learning post-training methods or RLHF involving the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.3",
          "reasoning": "The IRSTD-1k dataset is used as a benchmark for evaluating and comparing the performance of the proposed ISNet and other state-of-the-art methods, demonstrating its role in evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No specific analysis based on the dataset itself, such as trend or pattern analysis, is reported."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for retrieval-augmented generation or similar purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The IRSTD-1k dataset is actively used for training and evaluation as documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-870-rubric-5",
    "token_usage": {
      "prompt_tokens": 14361,
      "completion_tokens": 533,
      "total_tokens": 14894
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the dataset containing multiple (more than two) human languages in any form."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset includes exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract, Section 3.5 IRSTD-1k Dataset, Throughout paper",
          "reasoning": "The IRSTD-1k dataset is composed of infrared images with pixel-level annotations but contains no linguistic content other than English annotations and explanations in the paper. The paper and dataset descriptions are entirely in English, and no other languages appear in the dataset or its annotations."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset and paper are presented in English, with no indication of non-English monolingual content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of infrared images and pixel-level annotations, not code or programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes mathematical notations in its methods section, the dataset itself contains infrared images and annotations, not mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of infrared images of drones, creatures, vessels, and vehicles, but it does not contain biological sequences or non-human communication system data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed languages are present in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language is clearly English, so unknown does not apply."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of images with English annotations, thus it contains language and is not language free."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-870-rubric-6",
    "token_usage": {
      "prompt_tokens": 11579,
      "completion_tokens": 186,
      "total_tokens": 11765
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 3.5",
          "reasoning": "The paper states that the dataset and code are available at github.com/RuiZhang97/ISNet; however, the link and description primarily indicate availability of the code for the proposed method, not explicitly for the dataset construction or annotation code. There is no explicit mention or description of code for dataset construction, preprocessing, or generation in the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.5 IRSTD-1k Dataset",
          "reasoning": "The paper provides a detailed description of the dataset construction process, including dataset size, image sources (realistic images captured by an infrared camera in the real world), target types, scene diversity, annotation method (manual pixel-level annotation), and dataset splits. Hence, sufficient documentation about dataset creation is provided within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-871-rubric-0",
    "token_usage": {
      "prompt_tokens": 12149,
      "completion_tokens": 142,
      "total_tokens": 12291
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets and Metrics",
          "Reasoning": "The paper states that all stereo matching networks are trained on the Scene Flow dataset, described as a large collection of synthetic stereo images with dense disparity ground truth, containing FlyingThings3D, Driving and Monkaa subsets. This dataset is synthetic, generated, likely by game engines or simulators, and is used as the sole training source in experiments, indicating it is the new dataset introduced (or at least the authors' training data). The data modality is image, specifically stereo image pairs with disparity ground truth."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-871-rubric-1",
    "token_usage": {
      "prompt_tokens": 12949,
      "completion_tokens": 257,
      "total_tokens": 13206
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1: Experimental Settings - Datasets and Metrics",
            "reasoning": "The paper describes using the Scene Flow dataset as the synthetic data for training the stereo matching networks, and evaluations are conducted on real data without mention of manual annotations. The synthetic dataset Scene Flow is generated using game engines (see Introduction and related works) and thus its annotations are automatically generated through simulation rather than manually labeled by humans."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "There is no indication or description of instructions provided to annotators for labeling the synthetic Scene Flow dataset because the dataset is automatically generated synthetic data, thus no manual annotation instructions are required or described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "Since the Scene Flow dataset is synthetically generated with ground-truth disparities from computer graphics/game engines, no scoring rubric for annotations is involved or provided."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "No examples of annotation guidelines or manual annotations are described because the synthetic dataset's labels are automatically generated during rendering of synthetic scenes."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-871-rubric-2",
    "token_usage": {
      "prompt_tokens": 14139,
      "completion_tokens": 389,
      "total_tokens": 14528
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention of multiple human experts performing quality assurance on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors do not describe any quality assurance by multiple non-expert humans for the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using an AI model to perform quality assurance on the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automatic processes or algorithmic verification for QA of dataset labels or content are described in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses existing datasets (Scene Flow, KITTI, Middlebury, ETH3D, DrivingStereo, Oxford Robotcar, GTA V, Cityscapes) and does not introduce any new dataset by the authors. Moreover, the paper does not detail any quality assurance process performed by the authors on dataset annotations or content. As such, no QA is applied or documented for any newly introduced dataset."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-871-rubric-3",
    "token_usage": {
      "prompt_tokens": 13749,
      "completion_tokens": 350,
      "total_tokens": 14099
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any new dataset is created or collected by human contributors from scratch. The experiments use the existing synthetic Scene Flow dataset and real-world datasets for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any new dataset entirely from AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any dataset used was produced by human translators translating content from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of machine-translated datasets."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets are said to be aggregated or collected from existing sources without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors do not describe creating any datasets by modifying or adapting existing datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses the existing public synthetic dataset (Scene Flow) for training and existing public real-world datasets for testing. No new datasets are introduced or described in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-871-rubric-4",
    "token_usage": {
      "prompt_tokens": 14267,
      "completion_tokens": 498,
      "total_tokens": 14765
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental Settings and throughout the paper",
          "reasoning": "The Scene Flow dataset is used by the authors exclusively for training stereo matching models from scratch, as described in Section 4.1. All networks are trained on the synthetic Scene Flow dataset from scratch for 20 epochs with defined hyperparameters before being tested on real datasets without fine-tuning."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses that existing networks are fine-tuned on real data; however, the proposed ITSA method does not rely on fine-tuning on real data and the Scene Flow dataset itself is not used for supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any reinforcement learning based post-training methods or RLHF using the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental Settings and 4.4 Synthetic-to-Realistic Domain Generalization Evaluation",
          "reasoning": "The realistic datasets such as KITTI, Middlebury, ETH3D, DrivingStereo, and Oxford Robotcar are used exclusively for evaluation and benchmarking the generalization performance of models trained on the synthetic Scene Flow dataset. These realistic datasets are clearly used only for performance measurement and not for training."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset specifically for analysis purposes; the analysis is performed on models and features but not via a novel dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as a knowledge base to augment models or through retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-871-rubric-5",
    "token_usage": {
      "prompt_tokens": 14990,
      "completion_tokens": 565,
      "total_tokens": 15555
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset containing more than two human languages. It focuses on synthetic and real stereo image datasets without mention of any multilingual text data."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset introduced in the paper contains exactly two human languages. The datasets discussed are stereo image datasets, which do not involve bilingual human language content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset introduced by the authors consists of English language content. The paper focuses on stereo image datasets rather than textual or speech datasets in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or introduction of any monolingual dataset in a non-English human language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses implementation and training of neural networks in PyTorch, there is no newly proposed dataset containing entries with programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper contains mathematical formulas and notation describing the model and loss functions, these are not part of a dataset. The newly introduced dataset is synthetic stereo image data, without mathematical or logical expressions as data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The introduced datasets do not contain biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of any constructed or fictional language content in the new dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are well documented and concern synthetic stereo images with associated disparity maps; their linguistic content is not unspecified or unknown."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new dataset introduced is a synthetic stereo image dataset with disparity ground truth; it contains no human or programming language as dataset entries. Therefore, it does not contain any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-871-rubric-6",
    "token_usage": {
      "prompt_tokens": 12208,
      "completion_tokens": 186,
      "total_tokens": 12394
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4.1 - Experimental Settings",
          "reasoning": "The paper provides a link to the code repository for the implementation of their method (https://github.com/waychin-weiqin/ITSA) but does not provide any code or links related to constructing or generating any dataset, synthetic or real. The datasets used (e.g., Scene Flow, KITTI, Middlebury) are pre-existing public datasets. The paper does not introduce any new dataset created by the authors, so no dataset-specific code is described or provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "N/A",
          "reasoning": "The paper does not introduce any new dataset created by the authors, and thus there is no documentation of dataset construction or creation process within the paper. The datasets used are publicly available pre-existing datasets with documented sources external to this paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-921-rubric-0",
    "token_usage": {
      "prompt_tokens": 10313,
      "completion_tokens": 128,
      "total_tokens": 10441
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Datasets, Tiny-DOTA and LCell description",
          "Reasoning": "The paper introduces Tiny-DOTA and LCell datasets as multi-class tiny-object datasets. Tiny-DOTA is derived from aerial images and involves manual annotation of tiny objects from multiple classes, while LCell is a private breast cancer histopathology dataset with bounding box annotations for 8 cell classes. Both datasets consist of image patches with bounding box annotations, indicating the data items are images collected and annotated by humans."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-921-rubric-1",
    "token_usage": {
      "prompt_tokens": 11113,
      "completion_tokens": 217,
      "total_tokens": 11330
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 6 User Study",
            "reasoning": "Section 6 describes a user study with 10 participants who are not expert annotators for the Tiny-DOTA imagery. They performed interactive annotation tasks and manual annotation. Since these participants are non-experts and multiple annotators were involved, the category is Multiple Human Non-Experts."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 6 User Study",
            "reasoning": "The user study required annotators to select object classes and click on objects for annotation tasks following specific procedures. This indicates instructions were provided to guide the annotation process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any scoring rubric or formal annotation scoring guidelines provided to annotators in the user study or otherwise."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention providing annotation examples or example annotations as part of the annotation guidelines for the human annotators."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-921-rubric-2",
    "token_usage": {
      "prompt_tokens": 12303,
      "completion_tokens": 299,
      "total_tokens": 12602
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the introduced new datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information or indication in the paper that multiple human experts performed quality assurance on the new datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance conducted by a single human non-expert annotator for the new datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance by multiple human non-expert annotators involved with the new datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using AI models as judges or for quality assurance of the new datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe an automatic process or algorithmic verification as quality assurance for the new datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces the LCell dataset as a private dataset and describes Tiny-DOTA as a filtered version of an existing dataset. However, the paper does not document any quality assurance or validation processes applied to the new datasets or their annotations. There is no explicit description of annotation verification or QA procedures."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-921-rubric-3",
    "token_usage": {
      "prompt_tokens": 11913,
      "completion_tokens": 465,
      "total_tokens": 12378
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 5.1 (Datasets)",
          "reasoning": "The LCell dataset is described as a private breast cancer histopathology dataset with bounding box annotations for 8 cell classes. These annotations are created by expert physicians (pathologists). This indicates original content created entirely from scratch by human contributors specifically for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any part of the datasets were generated entirely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any datasets involving human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any datasets involving machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.1 (Datasets), Tiny-DOTA description",
          "reasoning": "The Tiny-DOTA dataset is derived by filtering and splitting the existing DOTA v2.0 dataset to exclude larger objects and create a subset focusing on tiny objects. This represents collection and aggregation from existing sources without significant modification, i.e., a collated dataset."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 5.1 (Datasets), Tiny-DOTA description",
          "reasoning": "Tiny-DOTA is created by filtering larger objects from the DOTA v2.0 dataset and splitting into training, validation, and test subsets. This entails modifying and adapting an existing source dataset to create a new dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and methods of generation for both datasets (Tiny-DOTA and LCell) are clearly described in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-921-rubric-4",
    "token_usage": {
      "prompt_tokens": 12431,
      "completion_tokens": 552,
      "total_tokens": 12983
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the newly introduced datasets solely for pre-training large models on general patterns or in an unsupervised/self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1.1 Training Data Synthesis; Section 5 Experimental Results and 5.3 Comparison to Baselines",
          "reasoning": "The datasets Tiny-DOTA and LCell are used to train the proposed C3Det model from scratch (e.g., Faster R-CNN and RetinaNet architectures). Training data synthesis simulates user inputs based on annotated ground truth from these datasets, confirming their use in training models without mention of pre-training."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that these datasets are used specifically for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning based post-training techniques such as RLHF are described as using these datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.2 Evaluation Procedure; Section 5 Experimental Results; Section 6 User Study",
          "reasoning": "The newly introduced datasets Tiny-DOTA and LCell are used for evaluating the performance of C3Det, comparing to baselines and varying training data. A user study on Tiny-DOTA validates annotation efficiency. Thus, the datasets serve evaluation and benchmarking purposes."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.6 Ablation Studies",
          "reasoning": "The datasets are also used to analyze the effects of different components of the proposed method (e.g., UEL, LF, and C3 modules) via ablation studies, in order to understand trends and impacts on performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that these datasets serve as knowledge bases to augment models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes multiple practical uses of the newly introduced datasets, including training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-921-rubric-5",
    "token_usage": {
      "prompt_tokens": 13154,
      "completion_tokens": 557,
      "total_tokens": 13711
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces two datasets, Tiny-DOTA and LCell, each containing object classes but there is no mention or indication of entries in multiple human languages beyond English."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence or description in the paper suggests the dataset entries contain exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5.1 Datasets",
          "reasoning": "The datasets introduced (Tiny-DOTA and LCell) involve object detection tasks using image data labeled with category names and annotations. The paper is exclusively written in English and all dataset annotations are presumably in English. There is no mention of other human languages being part of the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the text indicates any dataset contains exactly one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described are image-based with bounding box annotations and class labels; no programming language or code-related content is part of dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper presents mathematical formulas when describing their method, the datasets themselves do not contain mathematical or logical expressions as dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the LCell dataset involves biological images (breast cancer histopathology cells), the dataset entries consist of images and annotations, not biological sequences or communication systems themselves."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are involved or mentioned in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of dataset entries is clearly English based on the annotation and paper content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain annotations in natural language (English labels for object classes), so language is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-921-rubric-6",
    "token_usage": {
      "prompt_tokens": 10372,
      "completion_tokens": 283,
      "total_tokens": 10655
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract",
          "reasoning": "The paper provides the code for the interactive annotation method C3Det at https://github.com/ChungYi347/Interactive-Multi-Class-Tiny-Object-Detection, but this repository appears to contain code for the annotation method and not specifically for the dataset construction or data synthesis steps. Moreover, the datasets used include a private dataset (LCell) and a subset of DOTA (Tiny-DOTA), but no explicit mention is made about code to generate or fully reproduce these datasets from raw sources."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 5.1 (Datasets) and Section 4.1.1 (Training Data Synthesis)",
          "reasoning": "The paper documents the dataset usage and creation details for their custom dataset splits: Tiny-DOTA is created by filtering out larger objects and splitting DOTA v2.0 into training, validation, and test subsets. The private LCell dataset is described with statistics and origin (breast cancer histopathology images). Additionally, the paper provides a detailed training data synthesis procedure simulating user inputs from ground-truth bounding boxes (Section 4.1.1). While the raw datasets are from external or private sources, the paper documents the modifications, preprocessing, and data synthesis used to train and evaluate C3Det."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-965-rubric-0",
    "token_usage": {
      "prompt_tokens": 10299,
      "completion_tokens": 180,
      "total_tokens": 10479
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Experiments Setting - General Super-Resolution and Face Super-Resolution",
          "Reasoning": "The new datasets used for training and testing the proposed LAR-SR model include DIV2K, Flicker2K, CelebA, and FFHQ datasets consisting of high-resolution images. These datasets are composed of images originally captured or produced by humans (photographs of natural scenes and human faces). The paper describes that the model is trained and validated on these datasets, especially mentioning super-resolution tasks on DIV2K, celebA, and FFHQ, indicating the use of human-generated photographic images. The dataset itself is not newly collected by authors but is existing; however, the paper does not introduce any new dataset collected by the authors themselves."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-965-rubric-1",
    "token_usage": {
      "prompt_tokens": 11099,
      "completion_tokens": 249,
      "total_tokens": 11348
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1 and 4.1",
            "reasoning": "The tex-VQV AE extracts and encodes textural details into discrete latent space, and the local autoregressive (LAR) module generates texture details based on learned embeddings and coarse SR outputs from regression models. These are computational models trained on datasets without human annotators performing manual annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit annotation instructions for manual annotation are described in the paper.",
            "reasoning": "The paper does not mention any manual annotation process or instructions for annotators. The tex-VQV AE and LAR models are trained neural networks, not human annotations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No rubric or scoring guidelines for manual annotation are provided in the paper.",
            "reasoning": "Since there is no manual annotation described, no rubrics for assessing annotations are present."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples for human annotators are presented in the paper.",
            "reasoning": "The paper does not provide examples intended as annotation guidelines but only shows visualization of results and training data used in model training."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-965-rubric-2",
    "token_usage": {
      "prompt_tokens": 12289,
      "completion_tokens": 319,
      "total_tokens": 12608
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance being conducted by a single human expert on dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information or description about multiple human experts performing quality assurance on the dataset or annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given in the paper that a single non-expert human conducted quality assurance on dataset annotations or content."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any quality assurance performed by multiple non-expert humans on dataset content or annotations."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No statements are provided suggesting that an AI model was used to perform quality assurance on the dataset or its annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper describes automated training and evaluation procedures, it does not specify any automated verification or algorithmic quality assurance applied specifically to the datasets used or introduced."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces no new datasets; instead, it uses several publicly available datasets (DIV2K, CelebA, FFHQ) for experiments. There is no description of any quality assurance process applied to these datasets or any newly introduced datasets. Therefore, no quality assurance process is applied or documented for datasets in this paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-965-rubric-3",
    "token_usage": {
      "prompt_tokens": 11899,
      "completion_tokens": 585,
      "total_tokens": 12484
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Experiments Setting",
          "reasoning": "The training datasets used by the authors for their new dataset tasks (DIV2K, CelebA, FFHQ) are existing datasets, not newly created by the authors. However, the paper mentions that they retrain or construct their texture codebook and discrete latent space from these datasets for their method. The authors do not state any new human-created datasets; thus, no original data created entirely from scratch by human contributors is introduced."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2, Section 4.3 Result and Analysis",
          "reasoning": "The paper introduces a new dataset-like component consisting of discrete latent representations of textural details (texture codebook learned by tex-VQV AE) and the samples generated by their local autoregressive model in Stage 2, which are generated by AI models. Although the base image datasets are pre-existing, the specific discrete texture embeddings learned and textural detail samples generated by the model represent new data synthesized entirely by their AI models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any process of human translation of data from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any data generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Experiments Setting",
          "reasoning": "The authors use several widely-used public datasets (DIV2K, CelebA, FFHQ) as the source data for training and evaluation, which represent data collated from existing sources without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Stage 1: textural VQV AE, Section 3.2 Stage 2: Local Autoregressive Model",
          "reasoning": "Although the authors use existing datasets as training sources (e.g., DIV2K), their approach generates derived data elements such as the texture codebook, discrete latent maps, and coarse SR images, which are modifications and transformations applied on the original data to enable their autoregressive super-resolution modeling."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides clear documentation about the data sources and the methods of generation for the datasets used and created."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-965-rubric-4",
    "token_usage": {
      "prompt_tokens": 12417,
      "completion_tokens": 525,
      "total_tokens": 12942
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 Experiments Setting - General Super-Resolution and Face Super-Resolution",
          "reasoning": "The authors train the LAR-SR model and its components (texture VQV AE and local autoregressive model) from scratch on the new training datasets constructed from DIV2K, Flicker2K, CelebA, and FFHQ datasets for super-resolution tasks. The datasets introduced or constructed by the authors, including the paired LR-HR images for training, are used to train models from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or evidence that the dataset is specifically used for fine-tuning a pre-trained model. The training appears to be from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of reinforcement learning methods or RLHF techniques involving the datasets."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets mentioned, such as DIV2K, CelebA, and FFHQ, are used for training and testing, but these are publicly available datasets, not newly introduced datasets. The paper does not propose any new dataset solely for evaluation or benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that any new datasets introduced are primarily used for analysis of trends or characteristics beyond normal training and testing procedures."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used to serve as a knowledge base for augmenting models such as retrieval augmented generation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; instead, it uses established datasets (DIV2K, CelebA, FFHQ) for training and evaluation. Therefore, no new dataset introduced by the authors is available to assess utility."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-965-rubric-5",
    "token_usage": {
      "prompt_tokens": 13140,
      "completion_tokens": 530,
      "total_tokens": 13670
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, including Abstract, Sections 1, 2, 3, and 4",
          "reasoning": "The datasets mentioned (DIV2K, celebA, FFHQ) are image datasets without human language content. The paper and dataset descriptions are entirely in English language; text content related to the datasets, explanations, and labels are all in English. The authors use English for all dataset descriptions and experiments. Hence, the dataset entries are described with only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes algorithmic descriptions and code-like formulas, the dataset itself does not contain code or programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Sections 1, 3.1, 3.2",
          "reasoning": "The dataset proposed or used are images (DIV2K, celebA, FFHQ) rather than symbolic or mathematical data, so mathematical notation is present in the paper to describe methods but not in the dataset itself."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data are used in the new datasets."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed languages or artificial languages are part of the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content and domain of the datasets are well specified and understood."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains image data and associated English descriptions; therefore, it does contain language in the descriptive fields."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-965-rubric-6",
    "token_usage": {
      "prompt_tokens": 10358,
      "completion_tokens": 168,
      "total_tokens": 10526
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention",
          "reasoning": "The paper does not provide any link, URL, or mention of publicly available code repositories for the dataset or the proposed dataset construction methods. No sections or footnotes suggest code release related to dataset construction or preprocessing."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No explicit section or mention",
          "reasoning": "The paper uses existing standard datasets (DIV2K, CelebA, FFHQ) for training and testing. The paper does not introduce a new dataset nor does it provide documentation or detailed description about creation of any new dataset. It only references the usage of existing public datasets without documentation about dataset creation. Thus, there is no documentation on dataset construction process as it is not newly created by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-971-rubric-0",
    "token_usage": {
      "prompt_tokens": 10741,
      "completion_tokens": 122,
      "total_tokens": 10863
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 3 (The LD-ConGR Dataset); Section 3.1 (Data Collection and Annotation)",
          "Reasoning": "The LD-ConGR dataset is a new RGB-D video dataset introduced by the authors, explicitly described as recorded videos containing color (RGB) and depth streams captured synchronously using Kinect V4 with high resolution and frame rate. The data is collected from human subjects performing gestures, making it human recorded and thus human generated data in video modality."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-971-rubric-1",
    "token_usage": {
      "prompt_tokens": 11541,
      "completion_tokens": 230,
      "total_tokens": 11771
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1 Data Collection and Annotation",
            "reasoning": "Section 3.1 states that 30 subjects participate in data collection, recording gestures according to shown standard gestures, and that all gestures are manually labeled with the categories and temporal segmentation, while hand locations are annotated at 15fps. There is no indication these annotators are subject-matter experts, so multiple human non-experts best fits."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 Data Collection and Annotation",
            "reasoning": "Subjects were shown the standard gestures before recording, implying instructions were given to ensure gesture correctness during recording and annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1 and throughout paper",
            "reasoning": "The paper does not mention the presence of scoring rubrics or detailed grading criteria used for annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper",
            "reasoning": "No mention is made of annotation examples or exemplar annotations provided to annotators in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-971-rubric-2",
    "token_usage": {
      "prompt_tokens": 12731,
      "completion_tokens": 337,
      "total_tokens": 13068
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that quality assurance was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions that annotations are manually labeled but does not specify the number or expertise of annotators, nor does it mention multiple human experts performing quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information provided on QA by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Data Collection and Annotation",
          "reasoning": "The dataset annotations for gesture category, temporal segmentation, and hand position are manually labeled. The paper states that 30 subjects participated in data collection and gestures were manually checked for correctness. However, there is no specific mention that the annotators are experts. Given the lack of explicit mention of expertise and that multiple individuals were involved in annotation-related tasks, it is most consistent that annotation and QA were performed by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of AI models to perform quality assurance or annotation validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification or algorithmic quality assurance of the dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes a manual annotation process and verification of gestures by subjects, indicating that some quality assurance was conducted."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-971-rubric-3",
    "token_usage": {
      "prompt_tokens": 12341,
      "completion_tokens": 388,
      "total_tokens": 12729
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Data Collection and Annotation",
          "reasoning": "The dataset LD-ConGR is collected by human subjects performing gestures at distances from 1m to 4m from the camera under controlled conditions. The paper states that 30 subjects participated in data collection, performing the gestures under supervision, and the videos were manually annotated for gesture categories, temporal segmentation, and hand locations, indicating original data creation by humans specifically for this dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset was generated or synthesized entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any process of translating data from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not collected or aggregated from existing sources; it is a newly collected dataset as stated explicitly."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is original and not explicitly stated to be adapted or transformed from existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are explicitly stated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-971-rubric-4",
    "token_usage": {
      "prompt_tokens": 12859,
      "completion_tokens": 502,
      "total_tokens": 13361
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.4, 'State-of-the-art Evaluation' and Section 4, various subsections",
          "reasoning": "The paper reports training models from scratch or fine-tuning with publicly available pretrained models on the LD-ConGR dataset for gesture recognition tasks, indicating the dataset is used to train gesture recognition models. Specifically, Section 4.4 discusses training and evaluating models (e.g., 3D ResNeXt, others) on LD-ConGR, demonstrating usage of the dataset for training purposes."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.4, 'State-of-the-art Evaluation'",
          "reasoning": "The paper mentions fine-tuning pre-trained models (e.g., pretrained on Jester or Kinetics400) using the LD-ConGR dataset for supervised learning, showing the dataset is utilized for supervised fine-tuning to improve gesture recognition performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention usage of the dataset for reinforcement learning based post-training or RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 and Section 4.4",
          "reasoning": "The LD-ConGR dataset is used extensively for evaluation and benchmarking of various baseline and state-of-the-art gesture recognition methods. The paper provides quantitative results and analysis of model performance on the dataset, indicating a key utility as an evaluation benchmark."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.2 'Data Statistics', Section 4.2 and 4.3",
          "reasoning": "The paper performs detailed analysis of the dataset characteristics such as gesture duration distribution and the impact of gesture region estimation and key frame sampling. These analyses provide insights into the challenges of long-distance gesture recognition and guide method development, showing the dataset's use for analyzing data characteristics and patterns."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of the dataset serving as a knowledge base for retrieval or augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is clearly described to be used in multiple parts of the paper including training, fine-tuning, evaluation, and analysis, so N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-971-rubric-5",
    "token_usage": {
      "prompt_tokens": 13582,
      "completion_tokens": 274,
      "total_tokens": 13856
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The LD-ConGR dataset contains video data of hand gestures performed by human subjects. The dataset consists of RGB-D videos and annotations including gesture category, temporal segmentation, and hand location. There is no indication in the paper that the dataset contains any human language content, written or spoken, or any symbolic or linguistic data. The dataset is purely visual gesture data for recognition tasks, without any linguistic content. Therefore, the dataset does not contain any entries with language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-971-rubric-6",
    "token_usage": {
      "prompt_tokens": 10800,
      "completion_tokens": 198,
      "total_tokens": 10998
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and main text",
          "reasoning": "The paper provides a URL to the dataset (https://github.com/Diananini/LD-ConGR-CVPR2022) but there is no mention of publicly released code for data collection, preprocessing, or dataset construction steps. The repository is indicated for dataset availability, but code related to dataset construction is not described as available in the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (The LD-ConGR Dataset), specifically 3.1 Data Collection and Annotation",
          "reasoning": "The authors provide detailed documentation about the dataset collection process, including sensor used, recording locations, subject setup, gesture classes, annotation details such as gesture categories, temporal segmentation, and hand position annotation. Ethical considerations such as informed consent and data usage limitations are also documented. Hence, they offer transparent and complete documentation of the dataset creation process within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-978-rubric-0",
    "token_usage": {
      "prompt_tokens": 10524,
      "completion_tokens": 152,
      "total_tokens": 10676
    },
    "response": {
      "sources": [
        {
          "Modality": "time series",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 (Experimental Settings) and Section 4.2 (Results), especially paragraphs describing the 'Internal dataset'",
          "Reasoning": "The paper introduces an internal dataset consisting of 20,000 trajectories collected for evaluation purposes. These trajectories correspond to recorded agent histories and future movements, which are time series data of agent locations over time. The dataset is collected from real-world scenarios likely involving human activity or natural traffic patterns, implying human generation (data captured from real agents). This is distinct from model-generated or simulated data. The dataset is explicitly new and separate from the publicly available Argoverse dataset."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-978-rubric-1",
    "token_usage": {
      "prompt_tokens": 11324,
      "completion_tokens": 263,
      "total_tokens": 11587
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Dataset and Section 3.3 Multi-Modal Trajectory Prediction",
            "reasoning": "The internal dataset is an internal dataset collected and processed as described in Section 4.1 with 20,000 trajectories; predictions are produced by the proposed LTP model using automatic processing through the described pipeline of classification, regression, and loss functions with no indication of human annotation involvement in labeling."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit section for annotation instructions for internal dataset",
            "reasoning": "The paper does not describe any detailed annotation instructions or human annotation guidelines related to labeling or annotating the internal dataset."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.3 Multi-Modal Trajectory Prediction",
            "reasoning": "The model uses a formal loss function composed of classification loss, regression loss, and diversity loss defined based on the labeled ground truth trajectories, effectively serving as rubrics for training and evaluation of trajectory proposals."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit examples or annotation examples description",
            "reasoning": "The paper does not provide or discuss any annotation examples or sample labelings as part of the annotation process for the internal dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-978-rubric-2",
    "token_usage": {
      "prompt_tokens": 12514,
      "completion_tokens": 319,
      "total_tokens": 12833
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process for the internal dataset or Argoverse dataset annotations conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information or mention of multiple human experts performing quality assurance on the dataset annotations in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human non-expert annotator for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance conducted by multiple human non-expert annotators for the datasets used or introduced."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any AI model was used as a judge to perform quality assurance on dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses automated prediction methods and algorithms, it does not describe any automated verification or rule-based quality assurance process applied to dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce a new dataset that includes documented quality assurance processes or discuss the QA of their internal dataset annotations. The internal dataset is mentioned but details about how its annotation quality was assured are not provided. Therefore, no quality assurance process is described or performed as per the paper content."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-978-rubric-3",
    "token_usage": {
      "prompt_tokens": 12124,
      "completion_tokens": 397,
      "total_tokens": 12521
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly state that the dataset was created entirely from scratch by human contributors. Instead, it uses existing datasets such as the Argoverse Motion Forecasting dataset and an internal dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not generate new datasets purely from models; it utilizes existing datasets for training and evaluation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was produced via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation involved in data preparation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental Settings (Dataset)",
          "reasoning": "The authors use the publicly available Argoverse motion forecasting dataset and an internal dataset consisting of trajectories collected by themselves. The internal dataset is described as consisting of 20,000 trajectories. Hence, the data is collected and aggregated from existing sources or through data acquisition without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the data is obtained by modifying or transforming existing data; the datasets used are either collected or pre-existing."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and method of generation of the datasets used are documented clearly in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-978-rubric-4",
    "token_usage": {
      "prompt_tokens": 12642,
      "completion_tokens": 283,
      "total_tokens": 12925
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The paper uses the internal dataset introduced by the authors, consisting of 20,000 trajectories, to train their Lane-based Trajectory Prediction (LTP) model from scratch. The training is conducted using supervised objectives including classification, regression, and diversity losses as detailed in Section 3.3 and the experimental settings specified in Section 4.1."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The internal dataset is also used for evaluation purposes, as shown in Section 4.2 where prediction results on the internal dataset are reported, including FDE and ADE metrics for different agent types, demonstrating the model's performance."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-978-rubric-5",
    "token_usage": {
      "prompt_tokens": 13365,
      "completion_tokens": 619,
      "total_tokens": 13984
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper and the introduced dataset (internal dataset and Argoverse dataset) only focus on trajectory data for autonomous vehicles, described entirely in English. There is no evidence of multiple human languages included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or evidence in the paper that the dataset contains exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Entire paper context, including dataset description in Section 4.1 and methodology sections",
          "reasoning": "The dataset introduced (internal dataset) and the commonly used Argoverse dataset contain trajectory, map, and agent data described entirely in English. The entire paper is written in English, the dataset representations use English terms and labels, and no other human languages are mentioned."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset entries are in any non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses methods involving neural networks and uses mathematical formulas, it does not introduce any dataset entries comprising code or programming languages."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3.3 Multi-Modal Trajectory Prediction, Loss functions; Equations (1) and other formulae throughout Sections 3.1-3.4",
          "reasoning": "The dataset representation and related modeling rely on mathematical and logical notation (e.g., vectors, equations for trajectory regression, attention mechanisms, loss functions). Thus, the dataset entries include symbolic mathematical representations describing trajectories and map information."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is related to autonomous driving trajectory prediction containing vehicle and agent trajectories, which are human-centered and not biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificially created language is present or mentioned in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language content is clearly documented as English with mathematical notation; thus it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset does contain language entries, namely English text labels and mathematical notation for trajectories and maps."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-978-rubric-6",
    "token_usage": {
      "prompt_tokens": 10583,
      "completion_tokens": 160,
      "total_tokens": 10743
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No mention of code availability in the paper",
          "reasoning": "The paper does not include any mention or link to publicly available code repositories for data collection, preprocessing, or generation related to their internal dataset or for reproducing the experimental results."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Experimental Settings - Dataset",
          "reasoning": "The paper provides documentation regarding their internal dataset in Section 4.1, describing that it consists of 20,000 trajectories collected internally and used for evaluation, with some details shown in Table 2 and Figure 4. However, no explicit detailed data collection methodology or dataset construction protocols are described in depth, but the dataset is mentioned and basic characteristics are outlined."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-986-rubric-0",
    "token_usage": {
      "prompt_tokens": 8011,
      "completion_tokens": 232,
      "total_tokens": 8243
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Introduction, and Section 3.1 Visual and Linguistic Feature Extraction",
          "Reasoning": "The new dataset used by the authors includes video data (target frames and frame differences) processed by spatial and temporal encoders based on 2D ConvNet backbones. The video frames are human recorded natural videos to segment referred objects, and the temporal input is computed by frame difference between frames. As these are real-world video frames collected by humans, the video data is human-generated. The paper does not mention any synthetic or model-generated video data for training or evaluation, nor does it claim generating new datasets but uses existing benchmark datasets. However, from the instruction, only new datasets introduced by the authors should be listed. The paper mentions only using existing datasets; it does not appear to introduce a new dataset, only a new method applied to four popular benchmarks. Therefore, strictly by content, the paper does not introduce a new dataset. Hence, no data sources from new datasets are introduced by the authors in this paper."
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-986-rubric-1",
    "token_usage": {
      "prompt_tokens": 8811,
      "completion_tokens": 301,
      "total_tokens": 9112
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 1, Introduction; Section 4.3, Comparison with State-of-the-Art Methods",
            "reasoning": "The paper mentions evaluation on four popular RVOS benchmarks, including a newly proposed Refer-YouTube-VOS dataset [34]. This dataset is cited as newly proposed and used for evaluation, indicating it is new in the paper's context. Given its nature as a benchmark dataset with video frames and language expressions, it is most plausible that multiple human experts annotated it to ensure quality and accuracy for both segmentation masks and language expressions. The paper does not mention any automatic or model-based annotation for this dataset."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not provide any explicit information or description about providing annotation instructions for the new dataset (Refer-YouTube-VOS). Therefore, based on the paper alone, there is no evidence of detailed annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "There is no explicit description or indication of rubric usage or defined scoring criteria for annotators within the paper regarding the new dataset annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not include or mention any annotation examples or annotated samples provided as guidelines for annotators of the new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2022-986-rubric-2",
    "token_usage": {
      "prompt_tokens": 10001,
      "completion_tokens": 289,
      "total_tokens": 10290
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset introduced by the authors, nor any quality assurance process done by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state any quality assurance process by multiple human experts on any new dataset, nor does it introduce any new dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about quality assurance by single human non-expert annotators related to new datasets in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA performed by multiple human non-expert annotators on new datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of AI models being used as judges for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information that an automatic or algorithmic QA process was applied on dataset annotations in this paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets, nor does it describe any quality assurance process for dataset annotations. It only evaluates on several established datasets and focuses on proposing a new model and method."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-986-rubric-3",
    "token_usage": {
      "prompt_tokens": 9611,
      "completion_tokens": 365,
      "total_tokens": 9976
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the introduction or creation of any new dataset by human contributors. It benchmarks on four popular RVOS datasets, none of which are stated as newly created by the authors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that new data was generated by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention translation of datasets from other languages involving human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reference to machine translation or datasets generated by machine translation systems is made."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing public RVOS datasets for evaluation and comparison, but does not create a new collated dataset from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets derived from existing datasets with modifications or adaptations are introduced in this paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not specify or document the introduction or generation of any new datasets. All experiments are conducted on existing popular benchmarks without mention of new dataset creation."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-986-rubric-4",
    "token_usage": {
      "prompt_tokens": 10129,
      "completion_tokens": 281,
      "total_tokens": 10410
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3 (Comparison with State-of-the-Art Methods)",
          "reasoning": "The datasets mentioned such as A2D Sentences, J-HMDB Sentences, Refer-YouTube-VOS, and Refer-DA VIS17 are used to evaluate and benchmark the proposed method against previous state-of-the-art methods, as shown by reported performance improvements in tables and text."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.4 (Ablation Studies) and Section 4.6 (Qualitative Analysis)",
          "reasoning": "The paper conducts ablation studies on the A2D Sentences dataset to analyze the effectiveness of various modules and design choices. Qualitative analyses are also done on these datasets to understand model behavior."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2022-986-rubric-5",
    "token_usage": {
      "prompt_tokens": 10852,
      "completion_tokens": 538,
      "total_tokens": 11390
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the new datasets introduced contain entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not described as containing exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.3 Comparison with State-of-the-Art Methods and dataset descriptions throughout the paper",
          "reasoning": "The paper utilizes natural language expressions as referring expressions, all of which are in English as indicated by use of English terms and references (e.g., 'jumping', 'white', 'blue'), and the datasets mentioned (A2D Sentences, J-HMDB Sentences, Refer-YouTube-VOS) consist of English annotations. Therefore, the new or evaluated datasets contain only English language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset entries in a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets do not contain programming or structured code-related content; code is only present in model description, not dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical and logical notation appears in method descriptions but not in the dataset entries themselves."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper makes no mention of datasets containing biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are involved in the dataset entries."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) of dataset entries are clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain natural language referring expressions, so they do contain language."
        }
      }
    }
  },
  {
    "id": "CVPR-2022-986-rubric-6",
    "token_usage": {
      "prompt_tokens": 8070,
      "completion_tokens": 129,
      "total_tokens": 8199
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or appendix references code availability",
          "reasoning": "The paper does not mention any publicly available code repository or link related to dataset creation or processing. The focus is on model methodology and evaluation on existing benchmark datasets, with no indication that dataset construction code is provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section describes dataset construction",
          "reasoning": "The paper does not introduce any new dataset created by the authors, nor does it describe any dataset construction process. It evaluates the proposed method on several existing benchmark datasets, thus no dataset documentation is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1041-rubric-0",
    "token_usage": {
      "prompt_tokens": 11562,
      "completion_tokens": 121,
      "total_tokens": 11683
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, The Established Dataset",
          "Reasoning": "The Mobile-HDR dataset is a new dataset introduced in the paper, collected using mobile phone cameras that capture raw image data under various exposures and lighting conditions. These are real photographs manually captured by the authors with human involvement, not generated or synthesized by models. The dataset includes paired LDR-HDR image pairs in the raw image domain, covering both static and dynamic scenes, as explicitly described in Section 3."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1041-rubric-1",
    "token_usage": {
      "prompt_tokens": 12362,
      "completion_tokens": 246,
      "total_tokens": 12608
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3 The Established Dataset; Section 5 Experiments - Training Data Preparation",
            "reasoning": "The dataset Mobile-HDR is constructed by researchers using mobile phone cameras under controlled conditions, including static and dynamic scenes with different noise levels. The collection involved careful capture setups, averaging successive shots for ground-truth denoising, and controlled object motions, indicating expert involvement in the annotation and data preparation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3 The Established Dataset",
            "reasoning": "The paper describes the data collection and generation procedure in detail but does not explicitly mention providing instructions or guidelines for annotators or humans to perform annotation tasks."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3 The Established Dataset",
            "reasoning": "There is no mention of scoring rubrics or evaluation criteria as part of annotation guidelines related to data collection or labeling."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3 The Established Dataset",
            "reasoning": "The paper does not provide example annotations or labeling examples as the dataset is raw image collection and synthesis rather than human annotation tasks requiring examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1041-rubric-2",
    "token_usage": {
      "prompt_tokens": 13552,
      "completion_tokens": 351,
      "total_tokens": 13903
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention multiple human experts performing quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that a single non-expert human performed quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe multiple non-expert humans conducting quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance being performed by an AI model."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3, The Established Dataset (paragraphs describing static scenes and data acquisition)",
          "reasoning": "The dataset quality assurance is primarily done through an automated process: noise-free LDR images are obtained by averaging 120 to 400 successive shots under each exposure, which reduces noise and creates a noise-free reference. Additionally, sequences are checked for quality and outliers are discarded, suggesting some form of automated or semi-automated verification rather than manual annotation. The merging of noise-free LDR frames to generate ground truth HDR images via algorithmic exposure fusion further supports that quality assurance is based on automatic verification methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is documentation of quality assurance in the form of multi-frame averaging to obtain noise-free ground truth data and checking sequence quality, so the dataset is not lacking QA process documentation."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1041-rubric-3",
    "token_usage": {
      "prompt_tokens": 13162,
      "completion_tokens": 406,
      "total_tokens": 13568
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3: The Established Dataset",
          "reasoning": "The dataset Mobile-HDR is constructed by the authors using three types of mobile phone cameras to collect paired LDR-HDR raw images under various lighting conditions including daytime and nighttime scenes. The process involves capturing multiple exposures, averaging multiple shots for noise reduction, and synthesizing HDR ground truth images manually by fusing the captured noise-free LDR frames, indicating original data created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset was generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data derived through human translation from another language is present."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention data generation via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not a collection or aggregation of existing sources but is newly captured data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the HDR images are synthesized by fusing LDR frames, the base LDR data is original and not from existing datasets; thus, the dataset itself is not derived from existing data sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are explicitly documented; thus, N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1041-rubric-4",
    "token_usage": {
      "prompt_tokens": 13680,
      "completion_tokens": 490,
      "total_tokens": 14170
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the Mobile-HDR dataset for pre-training large models on general patterns or in an unsupervised/self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5 (Experiments), Training Data Preparation",
          "reasoning": "The Mobile-HDR dataset is explicitly used to train HDR reconstruction models from scratch. The authors divide the dataset into training and test splits and train their proposed Joint-HDRDN network on it. This shows the dataset is used to train models from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe fine-tuning pre-trained models using the Mobile-HDR dataset. Instead, it describes training models from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or RL-based post-training techniques such as RLHF using this dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 (Experiments)",
          "reasoning": "The dataset is also used to evaluate and benchmark the performance of models, including the proposed Joint-HDRDN model and other state-of-the-art methods. The test splits of the dataset serve as benchmark data for quantitative and qualitative evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not primarily used for analysis of trends or characteristics; it is used for training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for augmentation or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates practical usage of the Mobile-HDR dataset for training and evaluation of HDR image reconstruction models."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1041-rubric-5",
    "token_usage": {
      "prompt_tokens": 14403,
      "completion_tokens": 581,
      "total_tokens": 14984
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The Mobile-HDR dataset contains photographic images captured by mobile phones, without any indication or description that the dataset includes text or labels involving more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that the dataset includes exactly two human languages in the data entries."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the dataset entries contain any natural language text, including English. The data consists of raw photographic images without embedded language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is photographic image data without any textual language content, so it does not include a non-English language exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the paper describes implementation details and model architecture, the dataset itself comprises only raw and processed image data (photographic images), and contains no programming code or structured code-related entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are image data captured by mobile phone cameras; the paper includes mathematical notation and formulas describing the method, but these are not part of the dataset entries themselves."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include biological sequences, animal signals, or other non-human communication data; it consists solely of photographic images captured by mobile phones."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or constructed languages within the dataset entries."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language content is not specified because it does not contain linguistic content; it is photographic image data."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The Mobile-HDR dataset consists solely of raw and processed photographic images (LDR and HDR images) captured by mobile phone cameras, without any linguistic content, textual data, or language-related entries. Therefore, the dataset does not contain any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1041-rubric-6",
    "token_usage": {
      "prompt_tokens": 11621,
      "completion_tokens": 163,
      "total_tokens": 11784
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and conclusion",
          "reasoning": "The paper states that the dataset and codes are available at https://github.com/shuaizhengliu/Joint-HDRDN, indicating that the code related to data collection, preprocessing, and generation is publicly accessible in this repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3: The Established Dataset",
          "reasoning": "The paper provides detailed documentation of the dataset creation process including information about hardware used (three mobile phones with specific sensors), capture conditions (daytime, nighttime, various ISO levels), data capturing procedures (multi-frame averaging for noise-free LDR images, static and dynamic scene acquisition methods), and dataset statistics. Thus, the paper comprehensively documents the dataset creation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1055-rubric-0",
    "token_usage": {
      "prompt_tokens": 11804,
      "completion_tokens": 269,
      "total_tokens": 12073
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4, Experiments - Datasets",
          "Reasoning": "The paper introduces the LANIT framework trained on four standard datasets: Animal Faces-10, Food-10, CelebA-HQ, and LHQ. Among these, LHQ is mentioned as a dataset with no attribute labels, and the authors use sampled textual domain descriptions to define attributes. Since no new dataset is introduced by the authors, and they use existing datasets, the paper does not present any new image dataset introduced by the authors. Hence no new dataset image modality applies."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.3 and Section 4, Experiments - Setting up Templates and Candidate Domains",
          "Reasoning": "The authors introduce dataset-level textual domain descriptions (candidate attributes) as supervision for unlabeled images. These textual prompts consist of domain or attribute names and templates, which are human-annotated or sampled from existing attribute sets in datasets. They are clearly human-generated textual data used as prompts for vision-language similarity calculations. These text data are input to their framework and represent dataset-level annotations, introduced as a key part of their method."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1055-rubric-1",
    "token_usage": {
      "prompt_tokens": 12604,
      "completion_tokens": 364,
      "total_tokens": 12968
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3 - Language-Driven Domain Labeling; Section 3.4 - Prompt Learning",
            "reasoning": "The pseudo multi-hot domain labels for images are obtained automatically by computing similarity of image and text features using a pretrained vision-language model (CLIP). The prompts are learned automatically through prompt learning without human annotator intervention, thus the annotation as multi-hot label is generated by an automatic process."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3 - Language-Driven Domain Labeling; Section 3.4 - Prompt Learning",
            "reasoning": "The annotation uses a specified procedure: a list of textual domain descriptions (prompts) is given as dataset-level supervision, and the multi-hot pseudo domain labels are computed automatically with adaptive thresholding based on base prompts. This constitutes a clear instruction for generating dataset annotations automatically from the textual descriptions and vision-language similarity."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention of any scoring rubrics in annotation process across the paper.",
            "reasoning": "The paper does not provide or mention any scoring rubrics or criteria for evaluating or scoring the quality of multi-hot domain labels generated; the label assignment is determined by thresholding similarity scores in an automatic manner without a rubric."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figure 2; Section 3.3 and Section 4 Experiments",
            "reasoning": "The paper gives examples of textual domain descriptions (prompts) such as \"black hair\", \"smiling\", \"blonde\", and shows example images with assigned domain labels and translation results. These function as examples illustrating the annotation scheme and how dataset-level textual domain descriptions correspond to multi-hot pseudo labels."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1055-rubric-2",
    "token_usage": {
      "prompt_tokens": 13794,
      "completion_tokens": 350,
      "total_tokens": 14144
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for validating dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description in the paper that multiple human experts conducted quality assurance on dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information about quality assurance handled by a single non-expert human annotator for dataset validation."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any quality assurance process involving multiple non-expert human annotators validating dataset annotations or content."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the method uses AI models (e.g., CLIP) for domain labeling and translation, the paper does not report using an AI model specifically for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or rule-based algorithmic process used as quality assurance for dataset annotations or the dataset itself."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new language-driven pseudo domain labeling method leveraging dataset-level textual domain descriptions and a pretrained vision-language model (CLIP). However, it does not describe any explicit quality assurance process (human or machine) conducted to validate or ensure the correctness or consistency of the dataset annotations or the correctness of attribute labeling. There is no mention of any validation, verification, or quality control of the dataset annotations."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1055-rubric-3",
    "token_usage": {
      "prompt_tokens": 13404,
      "completion_tokens": 409,
      "total_tokens": 13813
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any new dataset created entirely from scratch by human contributors. Instead, it uses existing standard datasets such as Animal Faces-10, Food-10, CelebA-HQ, and LHQ."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses generating translated images using their model, these are outputs of the model, not new datasets introduced by the authors."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset or data produced through machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4. Experiments / Datasets",
          "reasoning": "The paper utilizes existing datasets sourced from prior work such as Animal Faces-10, Food-10, CelebA-HQ, and LHQ. These datasets are publicly available and aggregated from prior sources without modification, indicating a collated dataset usage."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors do not mention creating any new derived datasets from existing datasets as modifications or transformations. They focus on model training and translation tasks using existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly specified as existing public datasets; therefore, this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1055-rubric-4",
    "token_usage": {
      "prompt_tokens": 13922,
      "completion_tokens": 424,
      "total_tokens": 14346
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 - Experiments",
          "reasoning": "The datasets used (Animal Faces-10, Food-10, CelebA-HQ, and LHQ) are standard benchmarks leveraged in the experiments to evaluate the performance of the LANIT method. The paper uses these datasets to measure image quality, fidelity, diversity, and classification accuracy to benchmark the proposed model against state-of-the-art methods. However, these datasets are pre-existing and not new datasets introduced by the authors."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 - Ablation Study and Analysis",
          "reasoning": "The authors perform quantitative and qualitative analyses on various aspects such as the number of domain descriptions, adaptive thresholding, prompt learning, and domain regularization loss using the datasets. These analyses investigate trends and characteristics related to their method's performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; all datasets used are standard, existing datasets. Therefore, no new dataset introduced by the authors is used for any utility in the machine learning pipeline."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1055-rubric-5",
    "token_usage": {
      "prompt_tokens": 14645,
      "completion_tokens": 623,
      "total_tokens": 15268
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses datasets that have textual domain descriptions in English but does not mention multiple human languages being present in the dataset entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper suggests the dataset contains exactly two human languages in its entries."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.3 Language-Driven Domain Labeling; Abstract; Section 4 Experiments - Datasets",
          "reasoning": "The paper's new dataset-level supervision consists of textual domain descriptions given in English (e.g., 'smiling', 'bang', 'blonde'). The method relies on English text prompts as candidate domain descriptions. All prompts and templates are English phrases (e.g., 'a face with'). Therefore, the dataset entries, specifically the textual domain labels, are monolingual in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using any non-English languages in the textual domain descriptions associated with the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains code concepts and mathematical losses, but these are parts of the method, not the dataset itself. The dataset entries contain images and associated textual domain descriptions rather than programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical formulas and notations are present in the methodological descriptions but are not part of the dataset entries themselves."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is image datasets (Animal Faces-10, Food-10, CelebA-HQ, etc.) paired with English textual attributes. There is no indication of biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or constructed languages such as Klingon or Esperanto used in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language used in dataset-level textual domain descriptions is clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries include text-based domain descriptions, which are language data. Therefore, the dataset cannot be considered as not containing any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1055-rubric-6",
    "token_usage": {
      "prompt_tokens": 11863,
      "completion_tokens": 229,
      "total_tokens": 12092
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Introduction",
          "reasoning": "The paper provides a URL for the code repository (github.com/KU-CVLAB/LANIT) which is related to the image-to-image translation model and framework, but it does not mention any new dataset code or dataset construction code being publicly available. The datasets used are standard benchmarks or publicly available datasets that the authors used, and the paper does not introduce or create any new datasets or provide code for dataset construction. Therefore, no code related to dataset construction is made publicly available in the paper."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Entire paper",
          "reasoning": "The paper does not introduce or create any new datasets; instead, it uses existing publicly available datasets such as Animal Faces-10, Food-10, CelebA-HQ, and LHQ. As such, there is no documentation or description of dataset creation process for new datasets within the paper. The datasets are standard benchmarks sourced from external works, and the paper only documents the experimental setup related to these datasets, not their construction or creation."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1081-rubric-0",
    "token_usage": {
      "prompt_tokens": 11639,
      "completion_tokens": 97,
      "total_tokens": 11736
    },
    "response": {
      "sources": [
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Layout Representation and Section 4.1 Experimental Settings",
          "Reasoning": "The paper describes layouts represented as sequences of geometric parameters and attributes for elements, which are structured tabular-like data containing element coordinates and labels, all normalized uniformly. This data is human-generated since it represents annotated layout designs from real documents and interfaces."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1081-rubric-1",
    "token_usage": {
      "prompt_tokens": 12439,
      "completion_tokens": 355,
      "total_tokens": 12794
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Datasets",
            "reasoning": "The paper evaluates the method on five public datasets: Rico, PublayNet, Magazine, COCO, TextLogo3K. These are described as public datasets collected by previous works and thus are pre-existing datasets, not newly introduced by this paper."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "No annotation instructions for these public datasets are described as this paper did not create or annotate these datasets."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "No mention of scoring rubrics for annotations related to these datasets is given, as the paper only uses these datasets for evaluation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "No annotation examples are provided related to these datasets; the datasets are used for evaluation only."
          }
        },
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.4 Text logo layout generation",
            "reasoning": "The paper uses the TextLogo3K dataset but no new dataset is introduced by this paper. The generation task uses provided textual features and positional encoding. No manual annotation is described or introduced."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "No instructions are described for data annotation on this dataset by this paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "No rubrics are mentioned regarding the annotation of data."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "No annotation examples are provided in the paper for this dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1081-rubric-2",
    "token_usage": {
      "prompt_tokens": 13629,
      "completion_tokens": 301,
      "total_tokens": 13930
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on any of the datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts performed quality assurance on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA conducted by a single human non-expert for the datasets used."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human non-experts performing QA or annotation validation on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used as a judge or for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss performing automated verification or algorithmic QA processes on datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses five public benchmark datasets (Rico, PublayNet, Magazine, COCO, and TextLogo3K) without introducing any new dataset. Since these are pre-existing datasets, and the paper does not describe any quality assurance process conducted by the authors on these datasets, there is no documented QA process associated with any new dataset in this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1081-rubric-3",
    "token_usage": {
      "prompt_tokens": 13239,
      "completion_tokens": 368,
      "total_tokens": 13607
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only uses existing public datasets for evaluation and does not mention creation of new datasets by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors generate new layout designs as results of their model, but do not introduce these generated layouts as new datasets for further use."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data produced by machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper evaluates the method on five public datasets (Rico, PublayNet, Magazine, COCO, TextLogo3K), all of which are existing datasets collected or aggregated from various sources. The paper uses these datasets as-is, indicating that the data is collated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the authors modified or adapted existing datasets to create new datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origins are specified clearly as publicly available datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1081-rubric-4",
    "token_usage": {
      "prompt_tokens": 13757,
      "completion_tokens": 520,
      "total_tokens": 14277
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.4 Training and Inference; Section 4.1 Experimental Settings",
          "reasoning": "The paper uses the existing public layout datasets (Rico, PublayNet, Magazine, COCO, TextLogo3K) to train the proposed LayoutDM model from scratch as described in the training algorithm (Algorithm 1). It optimizes the model parameters from random initialization using these datasets by minimizing the specified loss function without mentioning use of pre-trained models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper that any dataset (new or existing) is used for supervised fine-tuning of a pre-trained model."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any usage of reinforcement learning-based methods or RLHF for post-training with any dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental Settings; Section 4.2 Quantitative Evaluation",
          "reasoning": "The public layout datasets are used as benchmarks to quantitatively evaluate the performance of LayoutDM and compare it with state-of-the-art baseline methods using standard metrics (FID, Max. IoU, Alignment, Overlap) on held-out test splits."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper contains ablation studies and qualitative analysis, no dataset is introduced or used solely for analyzing trends or characteristics without training or evaluation purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not described to serve as knowledge bases or retrieval corpora for augmentation of the model."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced (although mainly existing public datasets are used) are clearly used for model training from scratch and evaluation, so this label does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1081-rubric-5",
    "token_usage": {
      "prompt_tokens": 14480,
      "completion_tokens": 588,
      "total_tokens": 15068
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly state that the datasets contain English-only content. The datasets used are from various sources (mobile UI designs, document layouts, magazines, natural scenes, and text logos) but no explicit indication is given that the text content is in English exclusively."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention in the paper that the dataset entries contain exactly one non-English language. The datasets cited (e.g., Rico, PublayNet, Magazine, COCO, TextLogo3K) are broadly described without specifying language content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets represent graphic layouts with element geometries and attributes but do not contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3.1, 3.2 and throughout method descriptions",
          "reasoning": "The datasets include layout representations with geometric parameters (coordinates and sizes) represented as continuous numerical vectors normalized between -1 and 1, as well as discrete element attributes. Additionally, the paper presents mathematical formulations to describe the diffusion process and the data representation, indicating the presence of mathematical expressions and symbolic representations associated with the layout data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data is involved in the datasets introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of fictional or artificial languages being included in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper explicitly names the datasets and describes their contents sufficiently to determine the linguistic types. Thus, the language(s) are specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain elements with attributes, which include textual or categorical information, implying the presence of human language data, so they cannot be considered to lack language content entirely."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1081-rubric-6",
    "token_usage": {
      "prompt_tokens": 11698,
      "completion_tokens": 108,
      "total_tokens": 11806
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not mention or provide any link to code repositories or code availability related to dataset collection, preprocessing, or generation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not describe the dataset creation process because it only uses existing public datasets and does not introduce or construct any new dataset, thus no dataset creation documentation is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-114-rubric-0",
    "token_usage": {
      "prompt_tokens": 10395,
      "completion_tokens": 128,
      "total_tokens": 10523
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Experimental Setting, Datasets; throughout paper context",
          "Reasoning": "The only dataset introduced and used for training ICE is the ImageNet 2012 training dataset. This dataset contains images that were collected and labeled involving human effort (human-generated). The authors did not create a new dataset themselves but used this pre-existing human-generated dataset for training ICE. No new datasets were created by the authors as per the paper's content, so no dataset is introduced as novel by the authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-114-rubric-1",
    "token_usage": {
      "prompt_tokens": 11195,
      "completion_tokens": 246,
      "total_tokens": 11441
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1, Section 4.1",
            "reasoning": "ICE is trained using the ImageNet 2012 training dataset without any additional segmentation or object location labels. The annotations for patch-wise classification and background separation are generated automatically as part of the model's adversarial normalization loss, not manually by human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not describe any annotation guidelines with instructions for human annotators for the new dataset. The annotation process is integrated as part of the model's training, so no separate instructions for annotators exist."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "No scoring rubrics or evaluation rubrics for human annotation are described for the ICE patch-wise classification or background labeling. The evaluation metrics are standard computer vision metrics computed automatically."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "No example annotations or guidelines with examples are provided for annotation, as the patch-wise class and background labels arise from automated training, not manual labeling."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-114-rubric-2",
    "token_usage": {
      "prompt_tokens": 12385,
      "completion_tokens": 313,
      "total_tokens": 12698
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process conducted by a single human expert on any new dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance being performed by multiple human experts on any new dataset annotations or content in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about quality assurance performed by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss quality assurance involving multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that quality assurance was performed by AI models as judges for dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated or algorithmic quality assurance processes applied to validate annotations or dataset content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets with original annotations or associated quality assurance processes. Instead, it utilizes existing datasets such as ImageNet, ImageNet-Segmentation, CUB-200-2011, and PascalVOC07/12. The annotations and dataset content are used as-is, and no quality assurance process for dataset annotation is described."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-114-rubric-3",
    "token_usage": {
      "prompt_tokens": 11995,
      "completion_tokens": 417,
      "total_tokens": 12412
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the creation of any new datasets collected or created from scratch by humans. Instead, it uses existing large-scale datasets like ImageNet for training and other datasets for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any datasets are generated entirely by AI or machine learning models as new data. The work focuses on methods and evaluations on existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any translation of datasets from one language to another by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No use of machine translation for dataset creation or adaptation is mentioned in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental Setting",
          "reasoning": "The datasets used (ImageNet, ImageNet-Segmentation, CUB-200-2011, PascalVOC07/12) are existing datasets collected from various sources. The paper utilizes these existing datasets directly for training and evaluation without significant modification, indicating data is collated."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe generating new datasets by modifying, transforming, or adapting existing datasets. It uses standard datasets as is for training and evaluation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly states the datasets used and their sources, so the data source is specified and documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-114-rubric-4",
    "token_usage": {
      "prompt_tokens": 12513,
      "completion_tokens": 472,
      "total_tokens": 12985
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the proposed new datasets being used for pre-training large models. Existing datasets such as ImageNet are used, but these are not introduced by the authors."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention training any model from randomly initialized parameters using a newly introduced dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any new dataset introduced by the authors is used specifically for supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss reinforcement learning or RL-based post-training utilizing any newly introduced dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although various datasets such as ImageNet-Segmentation, PascalVOC, and CUB-200-2011 are used for evaluation, these are all existing, publicly available datasets and are not introduced by the authors."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper analyzes performance on multiple datasets, these datasets are existing ones, not newly introduced by the authors."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of any newly introduced dataset as a knowledge base for augmenting models."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper employs established datasets (ImageNet, ImageNet-Segmentation, PascalVOC, CUB-200-2011) for training, evaluation, and analysis. There is no evidence that the authors introduce any new dataset; therefore, no practical usage of any new dataset is described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-114-rubric-5",
    "token_usage": {
      "prompt_tokens": 13236,
      "completion_tokens": 555,
      "total_tokens": 13791
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 (Datasets), throughout the paper text",
          "reasoning": "The only new dataset introduced by the authors is the training dataset constructed by applying ICE method on the ImageNet 2012 training dataset for vision transformer training and explainability visualization. ImageNet dataset primarily consists of images and associated English language class labels. The paper refers to ImageNet, PascalVOC07/12, CUB-200-2011 datasets, all of which contain class labels and descriptions in English, and no other human language is mentioned in the dataset. Hence the dataset entries are monolingual (English)."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides pseudocode for the ICE training algorithm (Algorithm 1) and discusses MLP, cross-entropy losses, but these are not new datasets; rather, they are descriptions of the method and implementation. There is no mention of a dataset comprised of code or programming language."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical expressions and formulas are present to explain method losses and model, but the paper does not propose any new datasets containing mathematical or logical expressions as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication datasets are introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional language datasets are introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of dataset entries is specified and known as English for the relevant datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries are images with associated class labels in English; therefore, language is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-114-rubric-6",
    "token_usage": {
      "prompt_tokens": 10454,
      "completion_tokens": 201,
      "total_tokens": 10655
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and throughout the paper",
          "reasoning": "The paper provides a link to the code repository for the ICE model implementation (https://github.com/Hanyang-HCC-Lab/ICE), which includes code related to training and evaluation of the proposed method, but it does not explicitly mention or provide code specifically for constructing any new datasets. The datasets used (ImageNet, ImageNet-Segmentation, PascalVOC07/12, CUB-200-2011) are all existing public datasets, and no new dataset construction code is indicated."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "N/A",
          "reasoning": "The paper does not introduce any new datasets nor does it describe a process for constructing new datasets. Instead, it uses existing datasets for training and evaluation, documenting how the method is applied on these datasets but not describing dataset creation processes. Therefore, there is no documentation of new dataset construction in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1141-rubric-0",
    "token_usage": {
      "prompt_tokens": 12937,
      "completion_tokens": 156,
      "total_tokens": 13093
    },
    "response": {
      "sources": [
        {
          "Modality": "other",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets and Data Preprocessing",
          "Reasoning": "The paper introduces SemanticHuman, a method trained on human body meshes with consistent connectivity. The new dataset is not explicitly named as newly created by the authors but they use two public datasets (DFAUST and SPRING). However, the paper does not introduce any new dataset but rather proposes a new representation learned on existing datasets. The data they work with consists of 3D human body meshes, which are geometrical data represented as mesh models. Since the paper does not introduce a new dataset but uses existing ones, there is no new dataset introduced by the authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1141-rubric-1",
    "token_usage": {
      "prompt_tokens": 13737,
      "completion_tokens": 233,
      "total_tokens": 13970
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Datasets and Data Preprocessing",
            "reasoning": "The paper uses existing datasets (DFAUST and SPRING) for experiments but does not describe manual annotation; instead, preprocessing uses automatic joint regressors and labeling from a registered SMPL model, indicating annotation is performed automatically by algorithms or deterministic processes."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1 and Data Preprocessing",
            "reasoning": "There is no mention of human annotator instructions or guidelines for labeling; joint regressor and part labeling are performed automatically without described human intervention."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1 and Data Preprocessing",
            "reasoning": "No scoring rubrics or annotation evaluation criteria are discussed in the paper for the dataset annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit section discussing annotation examples",
            "reasoning": "The paper does not provide examples of annotation guidelines or annotated data for human annotators because annotation is done through automatic processes based on model registration and regression."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1141-rubric-2",
    "token_usage": {
      "prompt_tokens": 14927,
      "completion_tokens": 302,
      "total_tokens": 15229
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts performed quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any QA conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance carried out by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance performed by AI models as judges for the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.1, Data Preprocessing",
          "reasoning": "The paper describes an automatic process for data preprocessing requiring a joint regressor and landmarks, which are generated automatically by registering a parametric model (SMPL) once to the dataset meshes with consistent connectivity. This implies that quality assurance of annotations such as joints and part labels is performed via automated processes rather than human annotators."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an automatic preprocessing step, so quality assurance is not entirely absent."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1141-rubric-3",
    "token_usage": {
      "prompt_tokens": 14537,
      "completion_tokens": 412,
      "total_tokens": 14949
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any new dataset has been created entirely from scratch by human contributors. Instead, it uses existing datasets for experiments."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided in the paper that any new dataset was generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data being generated via machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The paper uses existing datasets, DFAUST and SPRING, which are publicly available datasets. Authors preprocess these datasets by registering models and extracting necessary features for their method. This indicates the data used were collected from existing sources without significant modification to the raw data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors do preprocess data (e.g., joint regressor, part labels), the original geometric data are from existing datasets and the modifications are standard preprocessing steps. The paper does not consider these as new derived datasets for evaluation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly identifies the data sources used (DFAUST and SPRING) and describes preprocessing steps; thus, the data origin is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1141-rubric-4",
    "token_usage": {
      "prompt_tokens": 15055,
      "completion_tokens": 537,
      "total_tokens": 15592
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.5 and Section 4.1",
          "reasoning": "The paper describes training the proposed bone-guided autoencoder from scratch on datasets DFAUST and SPRING without supervised paired data. Section 3.5 Implementation Details describes training details, and Section 4.1 Datasets describes dataset splits used for training. Thus, the datasets are used to train the model from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fine-tuning a pre-trained model or supervised fine-tuning on the datasets. The method is trained unsupervised from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning or RL-based post-training methods such as RLHF using the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 and Section 4.3",
          "reasoning": "The datasets DFAUST and SPRING are used for quantitative evaluation and benchmarking of reconstruction accuracy and editing performance of the proposed method compared with baselines (Section 4.2), and for ablation studies (Section 4.3). The splits into train/test sets indicate clear use for evaluation purposes."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "The datasets are used to analyze the performance characteristics of the method such as reconstruction error, editing accuracy, effects of different losses, and impact of the orientation-adaptive strategy via ablation studies. These analyses are based on the datasets."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as a knowledge base or for retrieval augmentation; the paper does not describe such usage."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly uses the introduced datasets for training, evaluation, and analysis, so this option does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1141-rubric-5",
    "token_usage": {
      "prompt_tokens": 15778,
      "completion_tokens": 563,
      "total_tokens": 16341
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any dataset with human language entries, let alone multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described (DFAUST, SPRING) consist of 3D human body meshes and anatomical data, not human language text or speech in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not contain any language entries, so cannot be classified as monolingual in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Section 3.5 Implementation Details",
          "reasoning": "While the paper mentions implementation in PyTorch and use of Python code, the datasets themselves do not contain entries of programming or code language content. The code is not a dataset entry."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical and logical notations and symbolic expressions appear in the paper text as model definitions and loss functions, but this is part of the paper content, not the dataset entries themselves."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of human 3D body meshes, not biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional language data is part of the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language (or presence of language) of dataset entries is clearly not applicable or needed, so unknown is not relevant."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced and used in the paper (DFAUST subset, SPRING) consist exclusively of 3D human body meshes with consistent mesh connectivity and anatomical joint information; they do not contain any human language or textual entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1141-rubric-6",
    "token_usage": {
      "prompt_tokens": 12996,
      "completion_tokens": 188,
      "total_tokens": 13184
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 4.1",
          "reasoning": "The paper mentions that the code will be available at a specified URL in the future but does not provide a current link or repository for the code related to data preprocessing or dataset construction. No explicit link or details about public availability of code for dataset construction are provided."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 (Datasets) and Data Preprocessing section",
          "reasoning": "The paper documents the datasets used (DFAUST and SPRING), their sources, and provides a brief description of data preprocessing, including the need for joint regressor, body part semantics, and landmarks for circumference calculation. It further states that mesh connectivity is consistent and that the SMPL model registration is done once. Although details are brief, dataset preparation and relevant preprocessing are described sufficiently for reproducibility."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1167-rubric-0",
    "token_usage": {
      "prompt_tokens": 13309,
      "completion_tokens": 221,
      "total_tokens": 13530
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Experimental Setup and throughout: Models are trained on RedCaps dataset; specifically mentioned RedCaps is a web-curated image-text dataset created by people.",
          "Reasoning": "The paper explicitly states that models were trained on RedCaps (Section 4.1 Experimental Setup). RedCaps is described as a web-curated image-text dataset with human-generated captions and images created by people. Therefore, the images are human-generated, not AI-generated or of unknown origin."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Experimental Setup and multiple mentions: Use of RedCaps captions as associated text with images for language similarity sampling.",
          "Reasoning": "The captions in RedCaps are human-written descriptions of the images. The paper uses these captions to obtain language embeddings for sampling pairs. Since they are human-generated captions, the text modality is human generated, not model generated or unknown."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1167-rubric-1",
    "token_usage": {
      "prompt_tokens": 14109,
      "completion_tokens": 217,
      "total_tokens": 14326
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2 Sampling Image Pairs using Language, Section 3.3 Language-Guided Visual Learning",
            "reasoning": "The paper describes that the sampling of nearest neighbor image pairs based on caption similarity is done using a pre-trained sentence encoder (SBERT) and similarity search libraries, constituting an automatic process rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not mention any annotation guidelines with instructions given to annotators for the new dataset in the context of data annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There is no indication in the paper of any scoring rubrics being provided for annotation or pair selection."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not provide annotation examples or guidelines illustrating annotation or labeling for the new dataset; the process is automatic via nearest neighbor sampling in embedding space."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1167-rubric-2",
    "token_usage": {
      "prompt_tokens": 15299,
      "completion_tokens": 373,
      "total_tokens": 15672
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about quality assurance by multiple human experts for any newly introduced datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of quality assurance performed by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the approach uses AI models (language models) for image pair sampling, there is no indication that an AI model is used to perform quality assurance or to judge annotation quality."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4.3 (Impact of sampling space)",
          "reasoning": "The paper uses a fully automated process leveraging pre-trained language models (e.g., SBERT) to compute caption similarities and sample image pairs for training without manual intervention. The paper does not mention any human validation or correction of dataset annotations, indicating the quality assurance relies on the automated method of sampling and filtering. Although this is not QA in the sense of validation post annotation, the only quality assurance step documented is the automated process of caption similarity computation and similarity search to ensure conceptually similar pairs are selected."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an automated process for selecting image-caption pairs using pre-trained language models; therefore, it is not the case that no quality assurance process is described."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1167-rubric-3",
    "token_usage": {
      "prompt_tokens": 14909,
      "completion_tokens": 419,
      "total_tokens": 15328
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the creation of any new dataset entirely authored by humans from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the creation of any dataset generated entirely by AI or machine learning models without using existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that data was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to produce data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4.1",
          "reasoning": "The authors use existing captioned image datasets such as RedCaps, CC3M, and CC12M for training. These datasets consist of images and captions collected from the web or other sources. The paper discusses using nearest neighbor sampling in the caption embedding space but does not create new images or captions. Thus, the data used is collated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the authors process and use caption similarity computed by sentence encoders to guide sampling, they do not create a new dataset by applying modifications or transformations to existing data. Their contribution is a training methodology rather than dataset derivation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly states the datasets used and their sources; therefore, the data origin is specified."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1167-rubric-4",
    "token_usage": {
      "prompt_tokens": 15427,
      "completion_tokens": 455,
      "total_tokens": 15882
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.3, Section 4.1, Section 4.2",
          "reasoning": "The paper introduces a method that uses language-guided sampling to select semantically similar image pairs from captioned image datasets for contrastive visual representation learning. This approach is applied during the pre-training phase to learn visual features that generalize well across downstream tasks, as described throughout Sections 3 and 4. The datasets used (e.g., RedCaps) serve as the source for pre-training large models in a self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 (Experimental Setup), Section 4.2 (Results)",
          "reasoning": "The models are trained from scratch using the proposed language-guided contrastive sampling on captioned image datasets like RedCaps, without relying on pre-trained weights, as noted in the controlled experiments repeating all methods from scratch for fair comparison."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe or demonstrate use of any new dataset for supervised fine-tuning on pre-trained models. Evaluations involve training linear probes or few-shot classifiers but these do not constitute fine-tuning using the new dataset itself."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning based post-training techniques or the use of any new dataset for RLHF or related methods."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Evaluation is conducted using existing downstream benchmark datasets; the new datasets introduced are used for training rather than exclusively for evaluation or benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets are not primarily used for analysis or studying trends; analysis sections relate to the training approach rather than using datasets for analytical purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the new dataset as a knowledge base to augment models through retrieval or generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1167-rubric-5",
    "token_usage": {
      "prompt_tokens": 16150,
      "completion_tokens": 594,
      "total_tokens": 16744
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state or suggest that the new dataset introduced has captions or entries in more than two human languages. The used captioned datasets like RedCaps, Conceptual Captions, and CC12M are primarily English or not explicitly described as multilingual in the paper."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper that the new dataset contains exactly two human languages per entry or dataset. The captions are treated as monolingual."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.2, Section 4.1, Section 4.3",
          "reasoning": "The authors use captioned image datasets such as RedCaps, CC3M, and CC12M. These datasets contain natural language image captions which, as described and referenced, are in English. The paper notes the use of pre-trained sentence encoders trained on English, such as SBERT with MPNet backbone, and does not mention any other language. Furthermore, analysis focuses on semantic similarity of English captions. Therefore, the dataset entries contain only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset with entries exclusively in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are images and natural language captions; no programming or code content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions or symbolic representations are part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains image-caption pairs of human language descriptions; no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the datasets include constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the captions is known and specified: English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains natural language captions, so language is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1167-rubric-6",
    "token_usage": {
      "prompt_tokens": 13368,
      "completion_tokens": 161,
      "total_tokens": 13529
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and conclusion",
          "reasoning": "The paper explicitly states \"Our code and pre-trained models are available at https://github.com/mbanani/lgssl.\" This indicates that the code relevant to their approach, including potential dataset processing steps, is publicly available."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "N/A",
          "reasoning": "While the paper describes using existing publicly available captioned image datasets (e.g., RedCaps, CC3M, CC12M) and details the sampling methodology using language models like SBERT, there is no new dataset introduced or created by the authors. Therefore, the paper does not document the construction process for any new dataset, only the methodology to sample pairs from existing datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1210-rubric-0",
    "token_usage": {
      "prompt_tokens": 12694,
      "completion_tokens": 119,
      "total_tokens": 12813
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 (Datasets and metrics)",
          "Reasoning": "The paper uses three existing large-scale 3D scene datasets (ScanNet v2, S3DIS, Matterport3D) for evaluation, which are human-captured 3D sensor data. However, the paper does not introduce any new dataset. All datasets mentioned are pre-existing and used for evaluation only, so no new datasets are introduced by the authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1210-rubric-1",
    "token_usage": {
      "prompt_tokens": 13494,
      "completion_tokens": 255,
      "total_tokens": 13749
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.1",
            "reasoning": "The new dataset introduced for evaluation in this paper is the LRPNet processing of existing large-scale 3D scene datasets such as ScanNet v2, S3DIS, and Matterport3D, which all have human expert annotations as ground truth labels provided by their original creators. The paper uses these existing datasets which rely on human expert annotation for training and evaluation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "The paper does not describe providing any detailed annotation instructions for annotators of the datasets, as the paper uses pre-annotated public datasets rather than creating new annotations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "There is no mention of scoring rubrics or criteria for annotation quality within the paper, since no new annotations or annotation guidelines were created by the authors."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "The paper does not provide any annotation examples or guideline samples, as it does not perform new data annotation but uses existing benchmark datasets."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1210-rubric-2",
    "token_usage": {
      "prompt_tokens": 14684,
      "completion_tokens": 288,
      "total_tokens": 14972
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about quality assurance performed by a single human expert on dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance involving multiple human experts for validating dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that a single human non-expert performed quality assurance on any datasets introduced by the authors."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA process done by multiple human non-experts on datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of an AI model for quality assurance of datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated or algorithmic quality assurance processes applied or documented for datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets nor describe any quality assurance process related to dataset annotation or validation. It uses existing datasets (ScanNet, S3DIS, Matterport3D) without documenting any QA procedures, therefore no quality assurance process is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1210-rubric-3",
    "token_usage": {
      "prompt_tokens": 14294,
      "completion_tokens": 357,
      "total_tokens": 14651
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the creation of new datasets by human contributors from scratch. Instead, it uses existing public datasets for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not generate any new datasets by AI or machine learning models without referencing existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by translating content from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of datasets generated via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe collecting or aggregating data from existing sources as a new dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors use existing datasets (ScanNet, S3DIS, Matterport3D) for experiments but do not mention deriving new datasets based on modifications of those."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses only existing publicly available datasets for evaluation and does not introduce any new dataset or data source. There is no specification or documentation of any new data created by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1210-rubric-4",
    "token_usage": {
      "prompt_tokens": 14812,
      "completion_tokens": 330,
      "total_tokens": 15142
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2 Implementation details; Section 4 Experiments",
          "reasoning": "The authors train their proposed LRPNet model from scratch using voxelized 3D datasets such as ScanNet v2, S3DIS, and Matterport3D. The experiments indicate training from randomly initialized parameters on these datasets for supervised semantic segmentation tasks."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets and metrics; Section 4.3 Comparison to other methods",
          "reasoning": "The paper uses large-scale 3D scene datasets such as ScanNet v2, S3DIS, and Matterport3D as benchmarks to evaluate the performance of the proposed model and compare with state-of-the-art methods."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.4 Ablation study",
          "reasoning": "The datasets are also used in ablation studies to analyze the effects of different components and design choices of the LRP module, such as receptive field size, pooling methods, and module positioning, thereby contributing to an analysis role."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1210-rubric-5",
    "token_usage": {
      "prompt_tokens": 15535,
      "completion_tokens": 512,
      "total_tokens": 16047
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing entries with exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets and metrics",
          "reasoning": "The datasets used for evaluation (ScanNet v2, S3DIS, Matterport3D) are 3D scene datasets annotated with semantic labels that are described in English. The paper itself and dataset descriptions are in English and the semantic segmentation labels are standard English language categories."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset with a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes code snippets and pseudo code expressions for the long range pooling module, these are not datasets but parts of the method description."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical formulas and logical expressions are used to describe the model and algorithms, not as part of a dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets containing biological sequences or non-human communication systems are introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets containing constructed or fictional languages are introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the datasets is clearly specified or implied as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets described contain semantic label language information; thus, the metric for no language is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1210-rubric-6",
    "token_usage": {
      "prompt_tokens": 12753,
      "completion_tokens": 139,
      "total_tokens": 12892
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and final paragraph",
          "reasoning": "The paper states in the abstract that code will be available at https://github.com/li-xl/LRPNet. This implies that the code for the proposed LRPNet model and presumably the dataset processing pipeline is publicly available, which supports reproducibility."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "None",
          "reasoning": "The paper does not introduce any new datasets; it uses existing datasets such as ScanNet, S3DIS, and Matterport3D. There is no description or documentation of dataset construction or creation process since no new datasets are created by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1218-rubric-0",
    "token_usage": {
      "prompt_tokens": 11461,
      "completion_tokens": 114,
      "total_tokens": 11575
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2 Datasets and throughout the paper",
          "Reasoning": "The authors use several well-known existing datasets (SID, LOL-real, LOL-synthetic) for evaluation but do not introduce any new dataset themselves. They mention collecting real-captured low-light photos for a user study, but these are only evaluation samples, not a new dataset introduced by the paper. No new dataset collection or introduction is described in the paper."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1218-rubric-1",
    "token_usage": {
      "prompt_tokens": 12261,
      "completion_tokens": 268,
      "total_tokens": 12529
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2 Loss for structure modeling, Section 4.8",
            "reasoning": "The structure maps used as ground truth are generated by applying the Canny edge detector to the normal-light images (Section 3.4), which is an automatic edge detection algorithm. Furthermore, in Section 4.8, it is stated that extra data pairs for structure modeling are obtained by applying the edge detection algorithm to other normal-light datasets, indicating automated generation. There is no mention of human annotators labeling the structure maps."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "The paper describes the use of the Canny edge detector to generate structure map ground truth automatically, without reference to instructions provided to annotators for manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "Since the structure annotations are automatically generated via edge detector algorithms, no scoring rubrics for annotators are applicable."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "No examples of manual annotations or annotation guidelines including examples are described; structure maps are obtained algorithmically rather than by human annotation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1218-rubric-2",
    "token_usage": {
      "prompt_tokens": 13451,
      "completion_tokens": 354,
      "total_tokens": 13805
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process conducted by a single human expert annotator for the introduced dataset or annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence indicating that multiple human experts performed quality assurance on the dataset or annotations introduced by the authors."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single non-expert human annotator for dataset validation or annotation."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is given regarding quality assurance by multiple non-expert human annotators for the introduced dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses AI models such as StyleGAN for structure modeling, this pertains to the enhancement framework, not quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.4 Loss Function, and Section 4.8",
          "reasoning": "The ground truth structural maps for training the structure modeling are generated automatically by applying the Canny edge detector (Section 3.4, equation 11) on normal-light images. This represents automated rule-based annotation of structural content. No manual quality assurance over these annotations is described, indicating that the quality assurance relies on this automatic process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents the use of automatic edge detection to generate structural ground truth, which constitutes a quality assurance process via automated means. Thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1218-rubric-3",
    "token_usage": {
      "prompt_tokens": 13061,
      "completion_tokens": 538,
      "total_tokens": 13599
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.2 Datasets",
          "reasoning": "The paper uses existing datasets (SID, LOL-real, LOL-synthetic) for evaluation but clearly states that collecting paired low-light and normal-light images is difficult and expensive, implying that their own dataset collection effort was minimal or none. There is no explicit mention of the authors creating a new dataset from scratch in the paper, so while human-collected datasets are referenced, the paper does not introduce a new dataset created entirely by human contributors from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the dataset itself is generated entirely by a model. Although the model generates structural maps (edges) from input images, these are intermediate outputs used for enhancement and not considered part of a novel dataset."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data being produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data being produced by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.2 Datasets, Section 4.8 Structure Modeling with More Data",
          "reasoning": "The paper uses existing public datasets such as SID and LOL for training and evaluation. Also, for additional structure modeling training, they use the AVA dataset to obtain more normal-light images to generate edge maps via Canny edge detector, effectively collating data from existing sources without creating new data from scratch."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.4 Loss Function, Section 4.8 Structure Modeling with More Data",
          "reasoning": "The authors use existing normal-light images and apply an edge detector (Canny) to generate edge maps which serve as ground truth for structure modeling. This is a derivation of existing data through transformation (edge extraction), and they fine-tune models using these derived labels."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and methods of generation are clearly described and documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1218-rubric-4",
    "token_usage": {
      "prompt_tokens": 13579,
      "completion_tokens": 535,
      "total_tokens": 14114
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Sections 3.1, 3.2, 3.3, and 4.1",
          "reasoning": "The framework, including the structure modeling module S, appearance modeling module A, and the enhancement module E, is trained end-to-end from scratch without reliance on pre-trained models, as described in Section 3.4 (Loss Function). The training involves supervised learning with paired low-light and normal-light image datasets (e.g., SID, LOL-real, LOL-synthetic) used from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.8",
          "reasoning": "The structure modeling module S is fine-tuned with extra normal-light image data from AVA dataset by using corresponding edge maps as supervision. This shows supervised fine-tuning to improve the edge prediction with additional labeled data."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of reinforcement learning or RL-based post-training methods."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 4.3, 4.4, and 4.7",
          "reasoning": "The introduced datasets SID, LOL-real, and LOL-synthetic are used extensively as benchmarks to evaluate the performance of the method via PSNR and SSIM metrics and a user study, thus serving an evaluation role."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.5, 4.6",
          "reasoning": "The paper uses the datasets for ablation studies and robustness analysis to assess model components, structural modeling accuracy, and noise resilience, demonstrating analysis usage."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as knowledge bases or retrieval resources to augment models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets are clearly used practically for training, evaluation, fine-tuning, and analysis as described."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1218-rubric-5",
    "token_usage": {
      "prompt_tokens": 14302,
      "completion_tokens": 570,
      "total_tokens": 14872
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the proposed dataset containing entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the proposed dataset containing exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Entire paper, especially Sections 4.2 (Datasets) and 3 (Method).",
          "reasoning": "The proposed new dataset is not explicitly introduced by the authors; however, all datasets used are standard image datasets with image data and annotations or edge maps derived from images. Any textual content or annotations are in English, as evident from the paper and references. No mention of any dataset in other human languages is present, indicating the dataset content is primarily visual and in English-only annotations or labels."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence or mention of datasets with non-English language content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes code for the framework but does not introduce a new dataset with code or programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical equations and notation describing the methods and losses, the proposed dataset itself does not contain data entries with mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is image-based and contains no biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of constructed or fictional languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset is clear (images and English annotations); no unknown language content is indicated."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Although the datasets consist mainly of images, they contain edge maps and annotations which are not languages; however, since annotations and standard dataset labels are in English, the dataset is not devoid of language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1218-rubric-6",
    "token_usage": {
      "prompt_tokens": 11520,
      "completion_tokens": 199,
      "total_tokens": 11719
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 4.1 (Implementation Details)",
          "reasoning": "The paper provides a link to the code for the proposed framework (https://github.com/xiaogang00/SMG-LLIE) related to the low-light image enhancement method. However, the code is for the enhancement method and does not appear to include code for constructing any new datasets since no new datasets are introduced by the authors in this paper. The datasets used (e.g., SID, LOL, A V A) are pre-existing datasets."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 4.2 (Datasets)",
          "reasoning": "The paper does not introduce any new dataset; instead, it uses existing public datasets such as SID, LOL-real, LOL-synthetic, and A V A for training and evaluation. There is no description of any new dataset creation process or collection in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1233-rubric-0",
    "token_usage": {
      "prompt_tokens": 17178,
      "completion_tokens": 203,
      "total_tokens": 17381
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5 Experiment Setting; Abstract; Introduction",
          "Reasoning": "The paper introduces a new small subset of clean samples used for cloning and sanitizing the trojaned models. These samples are images drawn from the same data distribution as the original training data (e.g., CIFAR-10 images). They are explicitly described as a small set of clean samples (Section 3 and Section 5). Because the data pertains to visual classification tasks on images and the paper discusses datasets like CIFAR-10 and also mentions KITTI and traffic signs, the modality is image. The data is human generated as these are typical image datasets collected via human-operated cameras or standardized datasets of natural images, not generated by AI or simulations. It is not unknown origin because it is clearly described as a small set of clean samples used for training rather than web-crawled or unspecified provenance data."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1233-rubric-1",
    "token_usage": {
      "prompt_tokens": 17978,
      "completion_tokens": 263,
      "total_tokens": 18241
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3 Method, Experiment Setting in Section 5",
            "reasoning": "The paper describes the collection and use of a small set of clean samples (dataset) to facilitate the cloning process in the proposed MEDIC method. This implies that the dataset was constructed by researchers for the purpose of evaluating their method. The description and experimental design indicate a subject-matter expert's involvement in data selection and annotation, rather than an automatic process or AI model annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not Applicable",
            "reasoning": "The paper does not provide any detailed annotation instructions or guidelines for the clean sample dataset. The dataset is used as input samples for training and evaluation rather than being annotated through a guided process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not Applicable",
            "reasoning": "No scoring rubrics or criteria for annotation are described for the dataset since it is used simply as clean labeled samples; no subjective annotation scoring is involved."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not Applicable",
            "reasoning": "The paper does not provide specific annotation examples or sample annotated data relevant to the dataset construction. Examples shown are model activations and visualizations, not annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1233-rubric-2",
    "token_usage": {
      "prompt_tokens": 19168,
      "completion_tokens": 351,
      "total_tokens": 19519
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the new datasets. There is no evidence of human expert validation of dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that multiple human experts performed any quality assurance on the datasets used or introduced. No such QA process is described."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of a single non-expert human performing quality assurance on dataset annotations or data content in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information that multiple non-expert annotators were involved in any QA process for the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that AI models were used as judges to perform quality assurance on the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses algorithmic methods and theoretical analysis related to model cloning and pruning for backdoor removal, it does not describe any automated QA procedures related to dataset verification."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset explicitly; it builds on existing datasets such as CIFAR-10, Polygon, Filter, Kitti-City, and Kitti-Road. There is no description of any quality assurance process applied to any newly introduced dataset, nor is dataset annotation or collection process detailed. Therefore, no QA process is applied or documented for new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1233-rubric-3",
    "token_usage": {
      "prompt_tokens": 18778,
      "completion_tokens": 419,
      "total_tokens": 19197
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creation of any new datasets by humans from scratch. Instead, it uses existing datasets such as CIFAR-10 and public backdoor benchmarks."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although models are trained and cloned, there is no indication that new datasets were generated entirely by AI or models for the purpose of this work."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention of data being collected or aggregated from existing sources without modification as a primary novelty."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 5 Experiment Setting; Introduction",
          "reasoning": "The paper uses existing datasets such as CIFAR-10, KITTI, and public backdoor benchmark datasets to evaluate the proposed method. The data used is a small subset of clean samples derived from these existing datasets (e.g., 5% data from CIFAR-10) to perform cloning and backdoor removal, which can be considered data derived from existing sources with transformation (selection of a small subset) applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are documented; existing standard datasets are used."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1233-rubric-4",
    "token_usage": {
      "prompt_tokens": 19296,
      "completion_tokens": 349,
      "total_tokens": 19645
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2, Method; Section 3, Experiment",
          "reasoning": "The paper describes using a small set of clean samples to train a clone model from scratch to replicate the benign behavior of a trojaned model while removing backdoors. This training-from-scratch method is central to their proposed backdoor removal technique (MEDIC)."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper distinguishes its method from fine-tuning and emphasizes training a clone model from scratch rather than fine-tuning an existing model with this dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or use of reinforcement learning methods or post-training RL (e.g., RLHF) with the dataset in the paper."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced (the small set of clean samples) is used for training the clone model, not exclusively for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses the dataset for training the clone model, not primarily for analysis purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention that the dataset serves as a knowledge base for retrieval-augmented generation or augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The small set of clean samples introduced is actively used to retrain the cloned model to remove backdoors; thus, it has practical utility."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1233-rubric-5",
    "token_usage": {
      "prompt_tokens": 20019,
      "completion_tokens": 519,
      "total_tokens": 20538
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, including Abstract, Introduction, and Sections 2-5",
          "reasoning": "The dataset(s) used in the experiments are standard image classification benchmark datasets such as CIFAR-10 and others, which are not inherently language datasets. However, all textual content, explanations, and dataset labeling information provided in the paper are exclusively in English. No mention or use of any other human language datasets is made, and all descriptions and experimental settings are in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses methods, algorithms, and mathematical formulations but does not introduce or provide a dataset containing code or programming languages."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical expressions and theoretical analysis, these do not constitute datasets. The paper does not introduce any new dataset containing mathematical or logical notation as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication systems datasets are introduced in the paper."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets involving fictional or constructed languages are presented."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is clearly defined and documented as English or non-linguistic data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset(s) used contain image data, which are not linguistic in nature, but they are not devoid of language because labels, descriptions, and explanations are in English."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1233-rubric-6",
    "token_usage": {
      "prompt_tokens": 17237,
      "completion_tokens": 198,
      "total_tokens": 17435
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention of dataset construction code availability in the paper",
          "reasoning": "The paper does not provide any links, URLs, or statements indicating that the code for dataset construction or the datasets themselves are publicly released or accessible. The focus of the paper is on a method (MEDIC) for backdoor removal using existing benchmark datasets and backdoor attacks, but no new datasets created by the authors are introduced or accompanied by code."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No dedicated section or subsections describe dataset creation process",
          "reasoning": "The paper references existing public datasets and benchmarks such as CIFAR-10, Kitti-City, Kitti-Road, and public backdoor benchmarks like Polygon and Filter. It describes applying different backdoor attacks on these datasets but does not document creating or curating any new datasets. Therefore, there is no documentation about new dataset construction processes in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1242-rubric-0",
    "token_usage": {
      "prompt_tokens": 12355,
      "completion_tokens": 114,
      "total_tokens": 12469
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets",
          "Reasoning": "The paper evaluates the MIST model on four existing long-form VideoQA benchmark datasets: AGQA, NExT-QA, STAR, and Env-QA. These datasets provide the videos and corresponding question-answer pairs used for evaluation. However, these are pre-existing datasets, not introduced by the authors themselves, so they are not considered as new datasets introduced in this paper."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1242-rubric-1",
    "token_usage": {
      "prompt_tokens": 13155,
      "completion_tokens": 308,
      "total_tokens": 13463
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.1 Datasets",
            "reasoning": "The paper evaluates the model on four new challenging long-form VideoQA datasets: AGQA v2, NExT-QA, STAR, and Env-QA, which are benchmark datasets curated by multiple experts as per the respective original dataset papers, referenced here indicating human expert curation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.1 Datasets",
            "reasoning": "The datasets AGQA v2, NExT-QA, STAR, and Env-QA used in this paper are standard benchmark datasets for VideoQA with well-established annotation guidelines, implying that human experts followed detailed instructions during annotation as described in their original papers cited in Section 4.1."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4.1 Datasets",
            "reasoning": "These datasets define QA task formats and evaluation metrics following standard protocols (e.g., open-ended QA or multi-choice QA), which require rubric-based scoring for annotating and evaluating answers as standard in VideoQA datasets, as indicated by adherence to standard protocols in Section 4.1."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.1 Datasets",
            "reasoning": "The paper itself does not provide or describe detailed annotation examples or provide samples of annotated questions and answers for these newly introduced datasets; it refers to the datasets generally without presenting annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1242-rubric-2",
    "token_usage": {
      "prompt_tokens": 14345,
      "completion_tokens": 289,
      "total_tokens": 14634
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance being performed by a single human expert annotator for the introduced datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information describing multiple human experts conducting quality assurance on any dataset introduced by the authors."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report that any single non-expert human performed quality assurance on the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any details about multiple non-expert humans performing quality assurance on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication exists in the paper that an AI model was used as a judge for quality assurance of datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automatic or algorithmic verification process used as quality assurance for dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses four existing datasets (AGQA, NExT-QA, STAR, Env-QA) for evaluation and does not introduce any new datasets. Therefore, no quality assurance process for new dataset annotation or validation is described or performed."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1242-rubric-3",
    "token_usage": {
      "prompt_tokens": 13955,
      "completion_tokens": 437,
      "total_tokens": 14392
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper evaluates the proposed model on four VideoQA datasets: AGQA, NExT-QA, STAR, and Env-QA. These datasets are described as recently proposed challenging long-form VideoQA benchmarks, each containing multiple videos with associated QA pairs. These datasets are introduced by their respective creators and provide original human-annotated question-answer pairs based on videos. Since these datasets are presented as existing benchmarks and not created by the authors, they are not new datasets introduced in this paper."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset generation using AI or machine learning models newly created by the authors."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of datasets produced by translating content via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention datasets generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the authors have aggregated existing data sources to form new datasets within this work."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document the creation of any new dataset based on existing datasets with modifications or adaptations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; it only evaluates the proposed model on existing datasets. Thus, the data origin or method of generation for new datasets is not specified or documented because no new datasets are introduced."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1242-rubric-4",
    "token_usage": {
      "prompt_tokens": 14473,
      "completion_tokens": 300,
      "total_tokens": 14773
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.2 Implementation Details; Section 4.3 Comparison with State-of-the-arts",
          "reasoning": "The paper describes using the long-form VideoQA datasets (AGQA, NExT-QA, STAR, Env-QA), which are existing datasets, to fine-tune pre-trained vision-language models (e.g., CLIP, All-in-One) for supervised learning of the VideoQA task. However, there is no indication that the authors introduce any new datasets that they use for supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper only uses existing long-form VideoQA datasets (AGQA, NExT-QA, STAR, Env-QA) throughout experiments. It does not introduce any new datasets. Therefore, there is no information about dataset utility for any new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1242-rubric-5",
    "token_usage": {
      "prompt_tokens": 15196,
      "completion_tokens": 451,
      "total_tokens": 15647
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any datasets containing more than two human languages. The datasets used are long-form VideoQA datasets but there is no indication they contain multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any dataset introduced contains exactly two human languages. The datasets discussed and evaluated on pertain to English language VideoQA only."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper introduces four new VideoQA datasets: AGQA, NExT-QA, STAR, and Env-QA. The descriptions and examples in the paper are all in English, and the datasets are benchmarked on English question-answer pairs. Thus, the datasets contain entries with only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English single language dataset is introduced. The paper only introduces and evaluates on English language VideoQA datasets."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper contains mathematical equations and descriptions of methods, it does not present any dataset entries containing programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notation appears only within the methodology description and model explanation. The datasets themselves do not contain entries with mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets involve videos and human language questions/answers, with no indication of biological sequences or non-human communication content."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are mentioned in the datasets or the paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the datasets is explicitly English and clearly documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain human language content (English questions and answers); thus, the criterion for N/A is not met."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1242-rubric-6",
    "token_usage": {
      "prompt_tokens": 12414,
      "completion_tokens": 158,
      "total_tokens": 12572
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper mentions that the code for their model (MIST) is available at github.com/showlab/mist, but it does not mention code or repositories related to the dataset creation or dataset construction processes. The paper uses existing datasets (AGQA, NExT-QA, STAR, Env-QA) and does not introduce any new datasets created by the authors, so there is no code for dataset creation to be made available."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "",
          "reasoning": "The paper exclusively uses four existing VideoQA datasets and does not introduce any new datasets created by the authors. Thus, there is no documentation describing the creation process of new datasets in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1252-rubric-0",
    "token_usage": {
      "prompt_tokens": 11044,
      "completion_tokens": 137,
      "total_tokens": 11181
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Implementation details and Section 5 Experiments",
          "Reasoning": "The paper evaluates MP-Former on three new datasets: ADE20k, Cityscapes, and MS COCO. These datasets consist of images that are labeled for segmentation tasks. The paper mentions these datasets explicitly as the benchmark datasets for evaluation but does not indicate that the authors created new datasets; rather, these are existing well-known datasets. The paper does not introduce any new dataset created by the authors. Therefore, no new dataset is introduced by the paper."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1252-rubric-1",
    "token_usage": {
      "prompt_tokens": 11844,
      "completion_tokens": 239,
      "total_tokens": 12083
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.2 and 5.1",
            "reasoning": "The paper describes using ground-truth masks and class embeddings directly as inputs in the mask-piloted part of the Transformer decoder during training, which is an automated process integrated into the model training pipeline rather than performed by human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not mention any human annotators nor provide annotation instructions specifying how ground-truth masks or class embeddings were annotated; ground-truth data are from standard datasets (ADE20K, Cityscapes, COCO) and used directly."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There is no indication of scoring rubrics or detailed criteria for annotators since no human annotation effort for new labels is described in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples or guideline documents are provided as the model uses existing ground-truth segmentation masks from standard datasets and automatically incorporates these into training."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1252-rubric-2",
    "token_usage": {
      "prompt_tokens": 13034,
      "completion_tokens": 296,
      "total_tokens": 13330
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any QA performed by a single human expert on dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any QA performed by multiple human experts on dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any QA performed by a single non-expert human annotator on dataset annotations or content."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any QA performed by multiple non-expert human annotators on dataset annotations or content."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe use of an AI model for quality assurance or judging dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe use of any automated verification process or algorithmic QA for dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset, nor does it describe any quality assurance process for dataset annotation or validation. The datasets used (ADE20k, Cityscapes, COCO) are existing benchmark datasets, and no QA process is documented for dataset annotations in this paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1252-rubric-3",
    "token_usage": {
      "prompt_tokens": 12644,
      "completion_tokens": 327,
      "total_tokens": 12971
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 5.1 Implementation details; Section 5 Experiments",
          "reasoning": "The paper introduces MP-Former as a training method applied to existing image segmentation datasets (ADE20K, Cityscapes, COCO). The authors do not create new datasets but use existing ones with standard evaluation metrics. The method is a modification/training approach for Mask2Former models on existing datasets. Therefore, the data utilized is derived from existing sources with modifications applied in training methodology, but no new data collection or generation is mentioned."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources (ADE20K, Cityscapes, COCO) are clearly documented as standard datasets used in the experiments."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1252-rubric-4",
    "token_usage": {
      "prompt_tokens": 13162,
      "completion_tokens": 450,
      "total_tokens": 13612
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe or demonstrate the use of any newly introduced dataset for pre-training large models. It primarily focuses on training and evaluating models using existing datasets."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets mentioned (ADE20K, Cityscapes, COCO) are pre-existing datasets. The paper does not introduce any new dataset used for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is introduced and used for supervised fine-tuning of pre-trained models in the paper."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use reinforcement learning based post-training methods or datasets."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Existing datasets (ADE20K, Cityscapes, COCO) are used for evaluation; no new dataset is introduced for evaluation or benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper analyzes failure cases and metrics related to Mask2Former, it does not introduce any new dataset specifically for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is introduced or used as a knowledge base to augment models."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; all datasets used are pre-existing (ADE20K, Cityscapes, COCO). The described datasets serve standard roles in training or evaluation but are not newly proposed by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1252-rubric-5",
    "token_usage": {
      "prompt_tokens": 13885,
      "completion_tokens": 551,
      "total_tokens": 14436
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset containing language information; the datasets mentioned (ADE20K, Cityscapes, MS COCO) are image segmentation datasets without linguistic content. The paper text is in English, but this is not a dataset of English content. Therefore, no dataset with only English content is introduced."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper involves programming code implied for implementation, but it does not introduce a dataset of programming code entries or structured code data. The datasets discussed are image segmentation datasets, not code datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes mathematical notation and definitions (e.g., descriptions of metrics such as mIoU and Utilization), these mathematical notations describe model performance and metrics. The paper does not introduce a new dataset composed of mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention biological sequences or non-human communication systems in any new dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of constructed languages or datasets containing them."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset with unspecified or undocumented language is introduced."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new linguistic or language-based dataset. All datasets referred to (ADE20K, Cityscapes, MS COCO) are existing image segmentation datasets composed of images and their segmentation labels, which are not entries with human language or textual data. Thus, linguistic coverage does not apply to the proposed datasets since none are introduced."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1252-rubric-6",
    "token_usage": {
      "prompt_tokens": 11103,
      "completion_tokens": 177,
      "total_tokens": 11280
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section",
          "reasoning": "The paper mentions that the code of their method 'will be released at https://github.com/IDEA-Research/MP-Former', indicating code availability related to the method improvements, but it does not introduce any new dataset nor mention any code related to data collection, dataset construction, or preprocessing for a new dataset. All datasets used (ADE20K, Cityscapes, COCO) are public existing datasets with their own established code."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No dataset creation described",
          "reasoning": "The paper does not describe the creation process or construction of any new datasets; it uses existing public benchmark datasets (ADE20K, Cityscapes, COCO) for evaluation. Therefore, no documentation of dataset creation is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1293-rubric-0",
    "token_usage": {
      "prompt_tokens": 12360,
      "completion_tokens": 204,
      "total_tokens": 12564
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Experimental settings and Section 4 Experiments",
          "Reasoning": "The paper introduces new models trained and evaluated on four datasets, including the NeRF synthetic dataset, NSVF synthetic dataset, TanksAndTemples dataset, and LLFF dataset. The authors trained their proposed wavelet-based neural radiance field models on these image datasets consisting of real captured or synthetic images for novel view synthesis. Thus, the new dataset entries pertain to image data captured with human involvement (or synthetic images rendered for benchmarks). The paper's experiments rely on image data but does not introduce new raw data collection; rather, they introduce new models applied to existing datasets. Hence, strictly speaking, the paper does not introduce any new dataset explicitly with new source images, except their own derived grid representations (not raw data). Therefore, no new datasets with raw data are introduced by the authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1293-rubric-1",
    "token_usage": {
      "prompt_tokens": 13160,
      "completion_tokens": 238,
      "total_tokens": 13398
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Experimental settings",
            "reasoning": "The paper introduces a new dataset as part of the method evaluation using neural radiance fields, but does not describe any manual annotation process. Instead, the models are trained on existing datasets, and no mention of new human-annotated data is made. The method involves trainable masks and compression pipelines optimized automatically, indicating an automatic process rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not mention providing any annotation instructions for a new dataset or annotation task, as no manual annotations are performed."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There are no scoring rubrics described since no human annotation or scoring is involved for the proposed new dataset or method."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples are provided because the paper does not involve human annotation or labeling of new data; the data used are from existing datasets and the grid parameters and masks are learned automatically."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1293-rubric-2",
    "token_usage": {
      "prompt_tokens": 14350,
      "completion_tokens": 302,
      "total_tokens": 14652
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving a single human expert for validating dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance conducted by multiple human experts for the datasets introduced in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance by a single non-expert human annotator on any dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided that multiple non-expert human annotators performed quality assurance on any introduced dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention performing QA using AI models as judges for any dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper introduces a compression pipeline and training procedures, it does not describe an automatic verification process explicitly as quality assurance for dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces no new datasets. It uses four existing datasets (NeRF synthetic, NSVF, TanksAndTemples, and LLFF) for evaluation and does not describe any quality assurance process performed on dataset annotations or content. Therefore, QA is not applicable for new datasets in this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1293-rubric-3",
    "token_usage": {
      "prompt_tokens": 13960,
      "completion_tokens": 401,
      "total_tokens": 14361
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any new data was collected or created entirely from scratch by human contributors for the purpose of this work."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any new dataset purely via AI or machine learning models without reference to existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the paper indicates that data was produced by human translation of content from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about dataset generation via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used in experiments (NeRF synthetic, NSVF, TanksAndTemplets, LLFF) are pre-existing and used as benchmarks. The paper does not describe collating or aggregating new data from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper applies novel methods and compression techniques on existing public benchmark datasets for evaluation. It does not indicate creation of derived datasets by modification or transformation of existing data."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. All datasets referenced are standard benchmarks pre-existing in the community, and the origin or method of generation of any new dataset is not specified or documented because no new dataset is introduced."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1293-rubric-4",
    "token_usage": {
      "prompt_tokens": 14478,
      "completion_tokens": 479,
      "total_tokens": 14957
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any dataset introduced by the authors for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental settings",
          "reasoning": "The authors used several existing datasets (NeRF synthetic, NSVF, TanksAndTemples, LLFF) for training their models from scratch with the proposed wavelet transform and masking methods. While these datasets are pre-existing and not newly introduced by the authors, the paper does not introduce any new datasets. Since no new datasets are introduced, this use case is not applicable to a new dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe fine-tuning of any pre-trained model with a new dataset introduced by the authors."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning post-training using a new dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The evaluation is done on established datasets; no new dataset introduced by the authors is used exclusively for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a new dataset is used for analyzing trends, patterns, or characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset presented as a knowledge base to augment models is introduced."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. All datasets mentioned are existing ones used as benchmarks or for training. Therefore, no practical usage of a new dataset is described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1293-rubric-5",
    "token_usage": {
      "prompt_tokens": 15201,
      "completion_tokens": 609,
      "total_tokens": 15810
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset entries containing multiple human languages. The datasets used for experimentation are standard computer vision view synthesis datasets without any linguistic content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper exists about any dataset entries containing exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the text of the paper and code comments are in English, the proposed datasets for neural rendering are image-based datasets (NeRF synthetic, NSVF, TanksAndTemples, LLFF) consisting of images and 3D scene representations, not linguistic or textual content in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or indication that the datasets contain any non-English language content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses code (available at a github link) and algorithmic methods, but the proposed datasets themselves are not code or programming language datasets; they are neural radiance fields and image-based scene datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notation appears extensively in the paper describing methods, but this does not indicate that the datasets include entries with mathematical or logical symbolic representations. The datasets themselves comprise 3D scene data and images."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data is included in the proposed datasets."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed languages (e.g., fictional languages) are included in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The linguistic content (or lack thereof) of the datasets is explicitly understood as non-linguistic (image and 3D scene data), so the language coverage is not unknown."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced for evaluating compact neural radiance fields consist of 3D scenes and image data without any human or programming language content. There is no dataset containing entries with language content; therefore, the linguistic coverage metric is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1293-rubric-6",
    "token_usage": {
      "prompt_tokens": 12419,
      "completion_tokens": 174,
      "total_tokens": 12593
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and main text",
          "reasoning": "The paper provides a link to code for the proposed neural rendering method at https://github.com/daniel03c1/masked wavelet nerf. However, this code is related to the model implementation and training, not data collection or dataset construction. The paper uses standard existing datasets (e.g., NeRF synthetic, NSVF, TanksAndTemples, LLFF) and does not introduce or construct any new datasets. Consequently, there is no code for constructing a new dataset."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Not applicable",
          "reasoning": "The paper does not introduce or construct any new datasets; it uses well-known existing datasets for evaluation. Therefore, there is no documentation on dataset creation or construction in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-130-rubric-0",
    "token_usage": {
      "prompt_tokens": 11254,
      "completion_tokens": 362,
      "total_tokens": 11616
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.5 Setup, Pre-training & Fine-tuning paragraphs and Section 1 Introduction",
          "Reasoning": "The paper introduces the All-in-one Transformer pre-trained on large-scale video-text datasets WebVid2.5M and HowTo100M, which are video datasets containing human recorded videos paired with natural text. The datasets used for pre-training are large-scale video collections with paired textual annotations, indicative of human-generated video modality data. The paper explicitly states they pre-train on WebVid2.5M + HowTo100M, but these are noted as existing datasets, thus no new video dataset is introduced by the authors."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.5 Setup, Pre-training & Fine-tuning paragraphs and Section 1 Introduction",
          "Reasoning": "The paired texts paired with the video datasets WebVid2.5M and HowTo100M contain human-generated natural language descriptions or narrations. These text captions or narrations are from human creators describing the video content, hence text as a modality is human-generated. The authors use these paired text data for training but do not indicate introducing a new text dataset."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.4 Image Video Co-Training and 3.5 Setup",
          "Reasoning": "The authors use CC3M, a well-known image-text dataset created from human-generated image-text pairs, for co-training with video-text datasets. This is a pre-existing dataset, not a new dataset introduced by the authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-130-rubric-1",
    "token_usage": {
      "prompt_tokens": 12054,
      "completion_tokens": 203,
      "total_tokens": 12257
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.5.1 and 3.5.2",
            "reasoning": "The paper does not mention any human annotation for the new datasets used for pre-training such as Webvid2.5M and HowTo100M. These datasets are large-scale video-text pairs collected automatically or from existing sources; annotations for these datasets are not described as human-labeled within this paper."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No explicit mention of detailed annotation instructions for the new datasets is provided in the paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not describe any scoring rubrics related to data annotation for the new datasets."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples or detailed guidelines for the data annotations of the new datasets are provided."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-130-rubric-2",
    "token_usage": {
      "prompt_tokens": 13244,
      "completion_tokens": 403,
      "total_tokens": 13647
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset annotations or quality assurance conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description in the paper of multiple human experts performing quality assurance on dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide details about multiple non-expert annotators conducting quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of any AI model acting as a judge or quality assurance mechanism for dataset verification."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of automated verification or rule-based techniques used for quality assurance of dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets created or annotated by the authors, nor does it document any quality assurance process for new datasets. Instead, it relies on pre-existing datasets such as WebVid2.5M, HowTo100M, YT-Temporal 180M, and CC3M for pretraining and evaluation. Consequently, no quality assurance process for new datasets is described or applied."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-130-rubric-3",
    "token_usage": {
      "prompt_tokens": 12854,
      "completion_tokens": 387,
      "total_tokens": 13241
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the authors have created any original datasets from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper that any new dataset has been generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset generated through human translation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data generation by machine translation is mentioned in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.5 Setup",
          "reasoning": "The paper states that the authors use existing large-scale video-text datasets such as Webvid2.5M, HowTo100M, YT-Temporal 180M, and image dataset CC3M for pre-training and co-training. These datasets are pre-existing and collected from external sources, so the data used is aggregated from existing sources without indication of significant modification or newly collected data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data based on existing sources with modifications, transformations, or adaptations applied by the authors for a new dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset origins are specified as existing publicly available datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-130-rubric-4",
    "token_usage": {
      "prompt_tokens": 13372,
      "completion_tokens": 585,
      "total_tokens": 13957
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.3.1 Pre-training",
          "reasoning": "The authors explicitly state that they pre-train the All-in-one model using commonly used pre-training objectives on large-scale video-text datasets (WebVid2.5M, HowTo100M, YT-Temporal 180M). The datasets mentioned are used to learn general video-language representations in an unsupervised or self-supervised manner before fine-tuning."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training the model from randomly initialized parameters using any new datasets from scratch. It focuses on pre-training with specific objectives."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.6 Downstream Tasks Settings and Section 4 Main Results",
          "reasoning": "The paper uses the introduced datasets (combined video-text datasets) to fine-tune the pre-trained model on a variety of downstream supervised tasks such as text-to-video retrieval, video question answering, multiple-choice tasks, video captioning, and action recognition."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or description of reinforcement learning-based post-training (such as RLHF) with the introduced datasets is found in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Main Results",
          "reasoning": "The paper evaluates the effectiveness of the pre-trained All-in-one model on multiple downstream benchmarks and datasets across various tasks, demonstrating performance gains. These datasets are used for benchmarking and performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 Visualization and Ablation Studies (refer to supplementary material)",
          "reasoning": "The authors analyze internal representations of the model using the datasets, showing how the model attends to relevant video patches for masked tokens. They also perform ablations and detailed analysis to understand model components and contributions."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the introduced datasets being used as a knowledge base to augment models through retrieval-augmented generation or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the usage of the introduced datasets in pre-training, fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-130-rubric-5",
    "token_usage": {
      "prompt_tokens": 14095,
      "completion_tokens": 491,
      "total_tokens": 14586
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the introduction of any dataset that contains entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset introduced by the authors contains exactly two human languages as per the paper content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset with only English content; it mentions usage of several existing datasets, but no new datasets are introduced by the authors."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset introduced contains exactly one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset with programming or structured code-related content is introduced in the paper."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset with mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset with biological or non-human communication systems is introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset with constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of datasets introduced by the authors (if any) are specified; however, the paper does not introduce any new dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not propose or introduce any new dataset; it uses existing datasets for training and evaluation. Therefore, there are no entries with any human language introduced by the authors as new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-130-rubric-6",
    "token_usage": {
      "prompt_tokens": 11313,
      "completion_tokens": 155,
      "total_tokens": 11468
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention found",
          "reasoning": "The paper does not mention or provide any link to the code used for constructing any new dataset or for data preprocessing. The focus is on the model and training on existing publicly available datasets rather than releasing new datasets or associated code."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section or description found",
          "reasoning": "The paper does not introduce any new dataset, nor does it describe any dataset creation process. It uses existing datasets such as WebVid2.5M, HowTo100M, YT-Temporal 180M, and CC3M for pre-training and evaluation. Therefore, there is no documentation on new dataset creation."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1310-rubric-0",
    "token_usage": {
      "prompt_tokens": 17953,
      "completion_tokens": 217,
      "total_tokens": 18170
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.3, Section 4, and Tables 1-3",
          "Reasoning": "The paper introduces several new continual learning corruption datasets: split-CIFAR-10-C, split-CIFAR-100-C, and split-miniImageNet-C. These datasets are constructed by applying various corruption operations to standard image classification datasets (CIFAR-10, CIFAR-100, and miniImageNet), which are originally composed of human-captured and human-annotated images. The corruptions are algorithmically applied transformations simulating adverse conditions. Thus, the modality is image, and the original underlying images are human generated, while the corruptions are model (algorithm) generated transformations. However, since the datasets are based on original human-generated images with algorithmic corruptions applied by the authors, the dataset as a whole is considered human generated for the base data with model-generated modifications. No indication of unknown origin data is given."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1310-rubric-1",
    "token_usage": {
      "prompt_tokens": 18753,
      "completion_tokens": 322,
      "total_tokens": 19075
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 \"Construction of several new datasets, including split-CIFAR-10-C, split-CIFAR-100-C, and split-miniImageNet-C\"",
            "reasoning": "The paper describes constructing new corruption-robust continual learning datasets by applying corruption operations on existing datasets following standard protocols (e.g., corruptions applied only to test data). This dataset construction appears programmatic and based on established corruption procedure protocols without manual annotation or human labeling, indicating an automatic process for generating these corrupted datasets."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1 and throughout the paper",
            "reasoning": "The paper does not describe providing explicit annotation instructions or guidelines for human annotators, as the datasets are generated automatically by applying defined corruption operations from known sets to existing datasets' test splits."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "No scoring rubrics or annotation scoring criteria are mentioned because the labels are original dataset labels and the corruption operations are known transforms applied automatically; thus, no rubric needed or provided."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figure 1 and Table 5 in Appendix",
            "reasoning": "The paper provides visual examples of the corruptions applied on images (e.g., Figure 1 shows different corruptions on a Split-miniImageNet image, Table 5 in Appendix provides descriptions) which can be considered as example types of corruptions used to generate the new datasets."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1310-rubric-2",
    "token_usage": {
      "prompt_tokens": 19943,
      "completion_tokens": 320,
      "total_tokens": 20263
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process performed by a single human expert on the newly introduced datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper of quality assurance conducted by multiple human experts for the new datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any QA process involving a single non-expert human annotator for the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information about quality assurance by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report using an AI model to validate or judge the quality of the datasets introduced."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the datasets are constructed by applying corruptions to existing datasets at various severity levels, the paper does not describe any automated verification or algorithmic quality assurance process to validate these datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper proposes new continual learning corruption robustness benchmarks by applying known corruption operations at multiple severity levels to existing datasets. However, it does not document or describe any quality assurance procedure performed to validate the correctness, consistency, or annotation of these constructed corrupted datasets. Therefore, no quality assurance process is applied or documented in the paper for the new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1310-rubric-3",
    "token_usage": {
      "prompt_tokens": 19553,
      "completion_tokens": 433,
      "total_tokens": 19986
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly state that any new datasets are created entirely from scratch by human contributors unrelated to existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any new data is generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset creation involving human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of machine translation is present for the dataset construction."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe collecting or aggregating datasets from existing sources in an unmodified manner as a new dataset."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Problem Setup; Section 4 Dataset Construction (implied as part of 3.1 and 4 Experimental Setup sections)",
          "reasoning": "The authors construct new continual learning corrupted benchmarks\u2014split-CIFAR-10-C, split-CIFAR-100-C, and split-miniImageNet-C\u2014by applying various common corruption operations with several severity levels to the test data of established datasets CIFAR-10, CIFAR-100, and miniImageNet. These datasets are derived from existing well-known datasets by applying systematic corruptions to create new testing benchmarks for corruption robustness evaluation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources and generation methods are clearly documented as derived corruptions on existing datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1310-rubric-4",
    "token_usage": {
      "prompt_tokens": 20071,
      "completion_tokens": 223,
      "total_tokens": 20294
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 and Abstract",
          "reasoning": "The paper constructs several new datasets, including split-CIFAR-10-C, split-CIFAR-100-C, and split-miniImageNet-C, specifically to evaluate corruption robustness of continual learning models. These datasets are used exclusively for benchmarking and measuring model performance under various corruption conditions at testing time, as described in Section 4 and the Abstract."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1310-rubric-5",
    "token_usage": {
      "prompt_tokens": 20794,
      "completion_tokens": 548,
      "total_tokens": 21342
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets introduced (Split-CIFAR-10-C, Split-CIFAR-100-C, Split-miniImageNet-C) are based on image data with class labels and do not contain entries in multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not consist of entries in exactly two human languages; the data labels and descriptions are in English only."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Problem Setup and Dataset Description (Page 3-5)",
          "reasoning": "The proposed datasets (Split-CIFAR10-C, Split-CIFAR100-C, Split-miniImageNet-C) use class labels and dataset descriptions in English only, with no indication of other languages involved."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets use English for descriptions and labels; no non-English single language is involved."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are image datasets with class labels, not containing entries of programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical notation in its methodology and definitions, the datasets themselves do not contain entries consisting of mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are natural image datasets with class labels, not containing biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are involved in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset labels and documentation is explicitly English; there is no uncertainty."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain natural image data with English labels, hence include language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1310-rubric-6",
    "token_usage": {
      "prompt_tokens": 18012,
      "completion_tokens": 206,
      "total_tokens": 18218
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper",
          "reasoning": "The paper does not provide any link, URL, or repository information regarding the code used for constructing the new datasets (split-CIFAR-10-C, split-CIFAR-100-C, split-miniImageNet-C). There is no indication that the dataset construction code is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.3 \"Corruption CL Benchmark\" and throughout Section 4 (Experiments)",
          "reasoning": "The paper documents the dataset creation process by describing how they split standard datasets (CIFAR-10, CIFAR-100, miniImageNet) into disjoint tasks and applied various common corruption operations with different severity levels to the test data of each continual learning task. They specify the types and severities of corruptions, provide summary statistics, and explain the evaluation protocol clearly, which provides a transparent explanation of the dataset construction procedure."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1329-rubric-0",
    "token_usage": {
      "prompt_tokens": 10506,
      "completion_tokens": 149,
      "total_tokens": 10655
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets and Experimental Setups",
          "Reasoning": "The paper evaluates their proposed video generation model on four unconditional video generation benchmarks: FaceForensics 256, SkyTimelapse 256, RainbowJelly 256, and CelebV-HQ 256. However, the paper explicitly states that these are datasets used for evaluation and provides references to other works for these datasets (e.g., FaceForensics [20], CelebV-HQ [40]). There is no indication that the authors introduced any new dataset within the paper. They rely on existing datasets for evaluation."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1329-rubric-1",
    "token_usage": {
      "prompt_tokens": 11306,
      "completion_tokens": 278,
      "total_tokens": 11584
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 (Datasets) and overall methodology sections",
            "reasoning": "The paper introduces a novel video generation method trained on four existing public video datasets (FaceForensics, SkyTimelapse, RainbowJelly, CelebV-HQ). No new dataset is introduced by the paper itself. Instead, the method generates videos unconditionally; the data is from pre-existing datasets rather than newly annotated or created datasets. Hence, the annotations or labels are not collected or created via human experts or annotators, but the generation process is automatic and model-driven."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No section specifies annotation instructions for a new dataset.",
            "reasoning": "The paper does not present new data annotation guidelines or instructions because no new dataset is introduced or annotated by humans."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No section specifies annotation scoring rubrics for a new dataset.",
            "reasoning": "Since the paper does not introduce a new dataset or manual annotations, there are no annotation rubrics described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No section or appendix provides annotation examples for a new dataset.",
            "reasoning": "No examples for annotations are given as the experimental evaluation is on existing datasets and the model does not perform data annotation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1329-rubric-2",
    "token_usage": {
      "prompt_tokens": 12496,
      "completion_tokens": 258,
      "total_tokens": 12754
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about quality assurance being conducted by a single human expert on the datasets introduced."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no documented quality assurance by multiple human experts for any datasets in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of quality assurance performed by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of AI models as judges for quality assurance of datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated verification processes used as quality assurance for the datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets, only uses existing benchmark datasets for evaluation. Consequently, no quality assurance process for new dataset annotation or validation is described or performed."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1329-rubric-3",
    "token_usage": {
      "prompt_tokens": 12106,
      "completion_tokens": 315,
      "total_tokens": 12421
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset that is created entirely from scratch by human contributors. It uses existing unconditional video datasets for training and evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's focus is on model development and video generation, not on creating new datasets generated entirely by a model as data contribution. It uses existing datasets for experiments."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of data produced by machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe aggregating existing data sources to create a new dataset without modification. It only references standard datasets used for evaluation."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of data based on existing sources with modifications or transformations applied by the authors to create a dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. It uses four existing public datasets (FaceForensics, SkyTimelapse, RainbowJelly, CelebV-HQ) for evaluation purposes (Section 4.1). Since no new dataset is introduced or documented, the data origin is not applicable or specified."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1329-rubric-4",
    "token_usage": {
      "prompt_tokens": 12624,
      "completion_tokens": 274,
      "total_tokens": 12898
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental Setups; Section 4.2 Unconditional Video Generation",
          "reasoning": "The datasets (FaceForensics 256, SkyTimelapse 256, RainbowJelly 256, CelebV-HQ 256) are used to evaluate and benchmark the performance of the proposed video generation model, as described in the experimental setups and quantitative comparisons sections."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.4 Motion Diversity",
          "reasoning": "The authors analyze motion diversity and interpret the behavior of motion styles on the CelebV-HQ dataset by computing cosine similarities and attention scores, indicating use of the dataset primarily for analyzing motion patterns rather than training or evaluation exclusively."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1329-rubric-5",
    "token_usage": {
      "prompt_tokens": 13347,
      "completion_tokens": 623,
      "total_tokens": 13970
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset including more than two human languages. The datasets used for unconditional video generation are visual benchmarks (FaceForensics, SkyTimelapse, RainbowJelly, CelebV-HQ) and do not contain language data."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset described in the paper contains exactly two human languages. The video datasets used are visual and do not include bilingual textual or audio content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets composed solely of English language content. The datasets focus on video data without linguistic annotations or content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset is described as containing exactly one non-English language. The work focuses on video synthesis benchmarks that contain visual content only."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces new datasets used for training and evaluation of video generation models based on video frames. There is no dataset comprising programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical equations describing the model, these are part of the methodology description, not datasets. The datasets themselves do not include mathematical or logical notation entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets cited are video collections of human faces, sky timelapses, jellyfish, etc., focused on natural scenes. They are not datasets of biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any datasets involving constructed or fictional languages. The video benchmarks used do not contain such content."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language or linguistic content of the datasets is clearly non-applicable as the datasets consist of video frames without linguistic annotation; hence, the language is known to be absent rather than unknown."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced or used are all video datasets containing visual content only (frames of video footage). They do not contain any form of human language text or speech content but purely visual information."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1329-rubric-6",
    "token_usage": {
      "prompt_tokens": 10565,
      "completion_tokens": 217,
      "total_tokens": 10782
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 4.1",
          "reasoning": "The paper mentions that code and videos have been made available at a GitHub link (https://github.com/xiaoqian-shen/MoStGAN-V), but this code appears to pertain to the model implementation and training rather than code for dataset construction or dataset creation processes. The datasets used are existing public video benchmarks (FaceForensics, SkyTimelapse, RainbowJelly, CelebV-HQ), and there is no indication that the authors introduced or constructed any new datasets themselves or provided code related to dataset construction."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 4.1 and elsewhere",
          "reasoning": "The paper only uses existing benchmark datasets for evaluation and does not introduce any new datasets or provide documentation on dataset creation processes. Dataset details are stated to be in the supplementary material, but this concerns existing datasets. Therefore, there is no documentation within the paper regarding dataset creation as no new datasets were introduced."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1351-rubric-0",
    "token_usage": {
      "prompt_tokens": 12782,
      "completion_tokens": 115,
      "total_tokens": 12897
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 and throughout the paper",
          "Reasoning": "The new dataset introduced by the authors is the training data constructed from half the training set of existing MOT17 and MOT20 datasets, which consist of video sequences of pedestrians captured in real-world crowded scenes. This data modality is video as the task is multi-object tracking in video sequences, and the data is human generated as it comes from real-world recorded videos of pedestrian scenes."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1351-rubric-1",
    "token_usage": {
      "prompt_tokens": 13582,
      "completion_tokens": 230,
      "total_tokens": 13812
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 3.3 Interaction Module, Section 3.4 Refind Module, Section 3.5 Training",
            "reasoning": "The motion interaction and refind modules process tracklet and detection data algorithmically, using models such as self-attention, convolutional layers, and fully connected layers trained with loss functions, indicating annotation is performed by AI model processes."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.5 Training",
            "reasoning": "Training procedures include detailed loss functions and input-output definitions for the AI modules, indicating clear instructions for learning motion patterns and correlations."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.5 Training",
            "reasoning": "Loss functions such as IoU loss for Interaction Module and binary cross-entropy for Refind Module explicitly define desired outputs, serving as scoring rubrics guiding model training."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide explicit annotation examples or detailed annotated samples for the new dataset or the annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1351-rubric-2",
    "token_usage": {
      "prompt_tokens": 14772,
      "completion_tokens": 288,
      "total_tokens": 15060
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process conducted by a single human expert related to dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by multiple human experts or annotators for the datasets used or introduced."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description of quality assurance performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide information about multiple non-expert annotators conducting quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report using an AI model as a judge for quality assurance of dataset annotations or contents."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any automated verification or algorithmic rule-based QA process applied to dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; it uses existing datasets MOT17 and MOT20, and does not describe any quality assurance process for dataset annotations or content. Therefore, no QA process is applied or documented for datasets in this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1351-rubric-3",
    "token_usage": {
      "prompt_tokens": 14382,
      "completion_tokens": 364,
      "total_tokens": 14746
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 - Setting, Datasets",
          "reasoning": "The paper uses existing public datasets MOT17 and MOT20 for training and evaluation, which are collected from existing video sources. The authors apply a publicly available object detector YOLOX trained by ByteTrack on these datasets but do not introduce any new raw data themselves."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.5 - Training; Section 3 - Method",
          "reasoning": "The authors create new data representations and training samples derived from existing MOT17 and MOT20 datasets by extracting tracklets, constructing inputs for the Interaction Module and Refind Module, sampling data pairs, and generating labels from existing annotations. This constitutes derived data based on existing sources with modifications and transformations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies clearly that existing MOT benchmark datasets (MOT17 and MOT20) are used and how new training samples are constructed from them."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1351-rubric-4",
    "token_usage": {
      "prompt_tokens": 14900,
      "completion_tokens": 204,
      "total_tokens": 15104
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. It uses existing datasets (MOT17 and MOT20) for training, testing, and evaluation purposes, but no novel dataset is proposed by the authors. Therefore, no practical usage of a new dataset is described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1351-rubric-5",
    "token_usage": {
      "prompt_tokens": 15623,
      "completion_tokens": 552,
      "total_tokens": 16175
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not present any dataset entries containing more than two human languages. The datasets discussed (MOT17, MOT20) pertain to object tracking, consisting of tracking bounding boxes and trajectories without linguistic data."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that the dataset entries contain exactly two human languages. The tracking datasets involve images and associated bounding box annotations, not textual language data."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, including Abstract and Section 1 Introduction",
          "reasoning": "The introduced datasets and all annotations or descriptions are in English. The MOT17 and MOT20 datasets are standard benchmarks widely documented and labeled in English, and the paper text and dataset metadata are exclusively in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that the dataset or its annotations include content in a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset comprising programming or structured code-related content. It focuses on trajectory and detection data for MOT evaluated on existing benchmarks."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical formulas and notation to describe methods, the datasets themselves do not contain entries with mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries concern human pedestrian tracking and do not contain biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are involved in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is known and documented (English)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of annotations and metadata in English, so it does contain language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1351-rubric-6",
    "token_usage": {
      "prompt_tokens": 12841,
      "completion_tokens": 200,
      "total_tokens": 13041
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "The paper mentions code availability at https://github.com/qwomeng/MotionTrack, but no code release for dataset construction is specified.",
          "reasoning": "The paper provides a link to the repository for the implemented tracker code, but it does not introduce a new dataset, nor does it mention releasing code related to dataset construction or processing. The datasets used (MOT17 and MOT20) are pre-existing benchmarks, and the paper only uses half of their training sets for training modules. Therefore, code for dataset creation or processing beyond standard existing datasets is not provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No specific section or description related to dataset creation is provided, as the paper uses existing benchmark datasets.",
          "reasoning": "The paper uses existing datasets MOT17 and MOT20 for training and evaluation and does not introduce any new datasets. Consequently, no documentation on dataset creation or construction is present in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1354-rubric-0",
    "token_usage": {
      "prompt_tokens": 11936,
      "completion_tokens": 179,
      "total_tokens": 12115
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4, Experiments; Tables 1, 2, 3; and throughout the main text.",
          "Reasoning": "The new dataset introduced and used for evaluation is the ImageNet dataset, which consists of images collected and annotated by humans. The paper's experiments involve image classification tasks on ImageNet and CIFAR-10/100 datasets. Although CIFAR-10/100 are standard datasets, the experiments mention retraining and evaluation on ImageNet, including ImageNet-200 used during search and full ImageNet for final evaluation, indicating the use of human-generated image data. The dataset is employed as a benchmark without modifications implying model-generated or synthetic data; rather it is human collected and labeled image data used as input to their AutoML method."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1354-rubric-1",
    "token_usage": {
      "prompt_tokens": 12736,
      "completion_tokens": 231,
      "total_tokens": 12967
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.6 Training and Algorithm 1",
            "reasoning": "The joint optimization and performance evaluation for the introduced method MA2ML is performed through the described multi-agent reinforcement learning framework, with actions sampled from policies, pipeline training, reward calculation (top-1 accuracy), and then updating policies and centralized critic automatically. There is no indication that human annotators performed annotation tasks; rather, evaluations rely on automated model training and validation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not provide any instructions for human annotators or annotation guidelines; the optimization and evaluation procedures are automated and algorithmic."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "No rubric or scoring guidelines for human annotation are provided since evaluations are based on model performance metrics, not human judgment scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "No examples of annotation tasks or guidelines are provided; all data processing and evaluation are automated."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1354-rubric-2",
    "token_usage": {
      "prompt_tokens": 13926,
      "completion_tokens": 297,
      "total_tokens": 14223
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human expert performing quality assurance on datasets or annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about any involvement of multiple human experts in quality assurance processes for datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any single human non-expert performing quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of quality assurance by multiple non-experts is present in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses AI techniques in model training and evaluation, it does not describe using AI models specifically for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe an automated verification process for annotation quality assurance; it focuses on automated machine learning optimization rather than dataset QA."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets, nor does it discuss any quality assurance processes related to datasets or annotations. The experiments use existing datasets such as ImageNet and CIFAR-10/100, and there is no documented QA process for datasets within the paper. Therefore, QA is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1354-rubric-3",
    "token_usage": {
      "prompt_tokens": 13536,
      "completion_tokens": 379,
      "total_tokens": 13915
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any new dataset was created entirely from scratch by human contributors. The experiments use existing datasets such as ImageNet and CIFAR-10/100 for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset generated entirely by AI or machine learning models. It focuses on optimization methods rather than synthetic data generation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe collecting or aggregating data from existing sources as a new dataset introduced by the authors."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is described as derived from existing sources with some modifications, transformations, or adaptations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset, hence does not document the data source or method of generation for such data. All experiments are conducted on existing well-known datasets like ImageNet and CIFAR-10/100."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1354-rubric-4",
    "token_usage": {
      "prompt_tokens": 14054,
      "completion_tokens": 209,
      "total_tokens": 14263
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; it uses pre-existing datasets such as ImageNet, ImageNet-200, CIFAR-10, and CIFAR-100 for experiments and evaluations. There is no description or demonstration of a newly proposed dataset or its use within the machine learning pipeline in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1354-rubric-5",
    "token_usage": {
      "prompt_tokens": 14777,
      "completion_tokens": 558,
      "total_tokens": 15335
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset with entries in more than two human languages. The focus is on image classification tasks using datasets like ImageNet and CIFAR, which do not contain multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset comprising exactly two human languages is introduced. All datasets used or referenced are standard image classification datasets without bilingual textual content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly mention any dataset composed of only English language content. The datasets employed (ImageNet, CIFAR-10, CIFAR-100) consist of images and labels, not language data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets with only one non-English human language are introduced in the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains programming method descriptions and algorithms but does not introduce any dataset containing entries of programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper contains mathematical formulas and theoretical proofs, these are not parts of a proposed dataset but rather explanations and formulations of the method."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not present datasets including biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset with fictional or artificially created languages is introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are standard image classification datasets with well-known content; language information is either not applicable or clearly known."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The proposed datasets (e.g., ImageNet, CIFAR-10/100) are image classification datasets consisting of images and labels; they do not contain language entries. Therefore, they do not contain any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1354-rubric-6",
    "token_usage": {
      "prompt_tokens": 11995,
      "completion_tokens": 158,
      "total_tokens": 12153
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Not explicitly mentioned in the paper.",
          "reasoning": "The paper does not provide any explicit URL, repository link, or mention of code availability related to dataset construction or preprocessing. The main focus is on the AutoML method and its performance; no information is given about code for dataset preparation or dataset construction."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Dataset description section (Section 4 Experiments and other sections)",
          "reasoning": "The datasets used, such as ImageNet and CIFAR-10/100, are standard benchmark datasets widely known and publicly available. The paper does not provide new datasets or documentation related to dataset construction or creation process, merely uses standard datasets without detailing dataset creation or modification."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1364-rubric-0",
    "token_usage": {
      "prompt_tokens": 10776,
      "completion_tokens": 229,
      "total_tokens": 11005
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Experimental Setting",
          "Reasoning": "The new dataset consists of multi-view RGB images of tabletop scenes simulated in PyBullet. The authors use a subset of 20 YCB objects arranged randomly in scenes with 2 to 4 objects and a cylindrical pusher. The images represent observations from multiple camera viewpoints under varying and harsh lighting conditions, collected via simulation as described in Section 5.1, thus the images are generated by simulation but originally captured as images through simulated cameras, which involves human design of scenes and simulation setup."
        },
        {
          "Modality": "time series",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Experimental Setting",
          "Reasoning": "The authors collect 1000 simulated trajectories, where each trajectory records sequences of object poses and actions over time as the cylinder pushes objects. These pose and action sequences form time series data generated via the PyBullet physics simulator, not by humans directly, but simulated forward with planned random actions."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1364-rubric-1",
    "token_usage": {
      "prompt_tokens": 11576,
      "completion_tokens": 177,
      "total_tokens": 11753
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.2 Inverse Parameter Estimation",
            "reasoning": "The annotation involves estimation of object poses and light positions using covariance matrix adaptation (CMA), an optimization algorithm that aligns rendered images with observed images, indicating an automated estimation rather than manual human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No explicit description or mention of detailed annotation instructions for this annotation process is provided in the paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not provide any scoring rubrics or evaluation criteria for this annotation procedure."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples or guidelines illustrating the annotation process are provided in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1364-rubric-2",
    "token_usage": {
      "prompt_tokens": 12766,
      "completion_tokens": 354,
      "total_tokens": 13120
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance of dataset annotations or content was conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of multiple human experts performing quality assurance on dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The manuscript contains no information about multiple non-expert human annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of AI models to perform quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.2 Inverse Parameter Estimation and Algorithm 1: Visual MPC with OSFs and GNN dynamics",
          "reasoning": "The dataset used involves estimated object poses and lighting parameters derived via an automatic optimization process (e.g., covariance matrix adaptation - CMA) that inverses the rendering to estimate poses from images. This process uses algorithmic techniques for parameter estimation and thus constitutes an automated verification step for the object pose and lighting data. The paper transparently reports this procedure and uses it systematically in their method pipeline."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an automated procedure (inverse parameter estimation using CMA) to estimate scene parameters which acts as quality assurance. Therefore, the 'N/A' category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1364-rubric-3",
    "token_usage": {
      "prompt_tokens": 12376,
      "completion_tokens": 484,
      "total_tokens": 12860
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 5.1 Experimental Setting and Section 4 Methods",
          "reasoning": "The authors generate a new simulated dataset of multi-object manipulation scenarios with harsh and varying lighting conditions in PyBullet simulation, using a subset of 20 YCB objects. They collect 1000 object trajectories with randomized object configurations and actions to train their dynamics model and evaluate their method. This dataset is custom-created by the authors for their experiments and thus qualifies as newly created data by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the datasets themselves (e.g., images or trajectories) are generated entirely by AI or machine learning models. Instead, simulated data is generated by the PyBullet simulator, which is an environment, not a learned model."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data in the paper is created via translation from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data is produced by machine translation from other languages."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention assembling datasets by merely collecting or aggregating existing datasets without significant modification. The main dataset is a new simulated dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the authors use existing YCB object models, their dataset consists of new simulated trajectories generated by their own environment setup and novel lighting conditions. The paper does not indicate that the dataset is derived from or adaptations of existing datasets, only that YCB objects are used as models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies that the data used in experiments are newly simulated trajectories collected by the authors, thus the origin is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1364-rubric-4",
    "token_usage": {
      "prompt_tokens": 12894,
      "completion_tokens": 546,
      "total_tokens": 13440
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the introduced dataset being used exclusively for pre-training large models. Instead, it uses datasets primarily for training dynamics models and inverse parameter estimation in a task-specific manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5.1 and Section 5.4",
          "reasoning": "The paper describes generating simulated datasets with YCB objects under various lighting and configurations to train the dynamics model from scratch using supervised learning with ground-truth poses. Section 5.1 details using simulated object trajectories for training, and Section 5.4 uses this data for model predictive control."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any fine-tuning of pre-trained models on the introduced datasets; training is described as from scratch using supervised loss."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement-learning-based post-training, such as RLHF, is described using the datasets introduced in this work."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.3, Section 5.4, Section 5.5",
          "reasoning": "The introduced datasets (simulation scenarios with varying numbers of objects and lighting conditions) are extensively used to evaluate reconstruction quality, prediction performance, and model-predictive control success, benchmarking against existing methods."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.2, Section 5.6",
          "reasoning": "The datasets support analysis of reconstruction fidelity under harsh lighting (Section 5.2) and pose and lighting estimation on real-world images (Section 5.6), focusing on understanding model behavior."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as a knowledge base to augment models; instead, they are used for training, evaluation and analysis."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is clear practical usage of the introduced datasets for training, evaluation, and analysis described in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1364-rubric-5",
    "token_usage": {
      "prompt_tokens": 13617,
      "completion_tokens": 442,
      "total_tokens": 14059
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Entire paper",
          "reasoning": "The dataset introduced is composed of images and data used for visual robotic manipulation experiments with the text and explanation throughout the paper presented only in English. There is no indication of any other human language content or multilingual data in the new dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the method involves programming and algorithms, there is no indication that the proposed dataset itself contains code or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.2, Equations (1)-(8)",
          "reasoning": "The dataset includes entries with mathematical expressions and formal symbolic representations such as equations defining neural networks and rendering losses, which are integral to the dataset's modeling and inverse parameter estimation process."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes natural language descriptions, mathematical notation, and image-based data; hence it does contain language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1364-rubric-6",
    "token_usage": {
      "prompt_tokens": 10835,
      "completion_tokens": 176,
      "total_tokens": 11011
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper",
          "reasoning": "The paper does not provide any explicit link, URL, or mention of publicly available code repositories related to the dataset construction, data collection, or preprocessing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 5.1 (Experimental Setting), and Sections 4.1 and 4.2 (Methods)",
          "reasoning": "The paper documents the data generation process in simulation using PyBullet with a specific subset of YCB objects, describes the training of the OSF models from multi-view images with paired lighting pose information, and explains inverse parameter estimation for pose and lighting from images. Although exact details like dataset size and code are missing, the paper provides a clear description of how the simulated dataset is created and used for training and evaluation."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1369-rubric-0",
    "token_usage": {
      "prompt_tokens": 10899,
      "completion_tokens": 244,
      "total_tokens": 11143
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2 and Section 4.3",
          "Reasoning": "The paper introduces a novel multi-view azimuth stereo (MV AS) method that reconstructs 3D surfaces using calibrated multi-view azimuth maps obtained from various acquisition techniques. Specifically, in Section 4.2, the paper describes acquiring multi-view azimuth maps using a symmetric-light photometric stereo setup with multiple lights placed symmetrically and a rotating object on a turntable, capturing multiple images per view to compute azimuth angles. In Section 4.3, the paper uses azimuth maps from a snapshot polarization camera dataset. These azimuth maps, representing surface normal orientations in the image plane, are obtained through direct recordings from imaging hardware involving human operation (cameras and controlled lighting), thus human-generated sensor/camera data. This new dataset of multi-view azimuth maps is introduced and utilized by the authors for demonstrating their MV AS method. The data is not model generated or of unknown origin because it is explicitly captured via calibrated imaging devices and multi-light setups described by the authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1369-rubric-1",
    "token_usage": {
      "prompt_tokens": 11699,
      "completion_tokens": 229,
      "total_tokens": 11928
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3",
            "reasoning": "The paper describes the surface reconstruction process as an optimization of a neural implicit surface represented by a signed distance function (SDF) parameterized by an MLP, using a tangent space consistency loss computed from multi-view azimuth maps. This is a computational algorithmic process, not manual human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not describe any manual annotation guidelines or instructions for annotators, as annotation is not performed by humans but via an automatic process optimizing neural networks from azimuth maps."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics or grading criteria for manual annotation are described in the paper because the data processing is automatic and not based on annotator judgments."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No examples related to annotation guidelines are provided since the process is fully automated and does not involve human annotation with sample examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1369-rubric-2",
    "token_usage": {
      "prompt_tokens": 12889,
      "completion_tokens": 304,
      "total_tokens": 13193
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert for dataset annotation or validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts involved in quality assurance or validation of dataset annotations or content in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single human non-expert conducted quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is provided that multiple non-expert humans performed quality assurance on dataset annotations or content."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that an AI model was used for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses automated methods such as projection and neural implicit surface reconstruction with loss functions, it does not describe an automated quality assurance process for the dataset itself."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a novel method using inputs from azimuth maps derived from photometric stereo or polarization imaging datasets, but it does not introduce new datasets with annotated content, nor describe any quality assurance processes for dataset annotation or validation. Therefore, no quality assurance process for new datasets is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1369-rubric-3",
    "token_usage": {
      "prompt_tokens": 12499,
      "completion_tokens": 497,
      "total_tokens": 12996
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "The paper introduces new datasets created by the authors, including multi-view azimuth maps obtained from symmetric-light photometric stereo (Section 4.2) and multi-view polarization images captured using a polarization camera (Section 4.3). These datasets are collected by the authors through physical capture setups involving human coordination and are original contributions used to validate the proposed method."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated entirely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data in the paper was produced by translating content from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any data generated using machine translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The paper uses azimuth maps computed from normal maps of the DiLiGenT-MV benchmark dataset (Section 4.1) where the original normal maps are produced by external photometric stereo methods (SDPS). These azimuth maps are created by the authors through computational processing of an existing dataset, constituting collation or aggregation of existing data with minimal transformation."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The azimuth maps in Section 4.1 are derived from previously estimated normal maps by applying transformations (computing azimuth angles) to the existing DiLiGenT-MV data. Thus, the data is based on existing sources with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies the origin and generation method of all new datasets, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1369-rubric-4",
    "token_usage": {
      "prompt_tokens": 13017,
      "completion_tokens": 289,
      "total_tokens": 13306
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The paper uses the DiLiGenT-MV benchmark dataset with azimuth maps derived from photometric stereo methods to quantitatively evaluate and compare the proposed Multi-View Azimuth Stereo (MV AS) method against several existing multi-view photometric stereo methods for geometry and normal reconstruction. This demonstrates the dataset's use exclusively for evaluation purposes."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new datasets are introduced by the authors in the paper. Datasets mentioned such as DiLiGenT-MV and multi-view azimuth maps are pre-existing or obtained by other methods; the paper's focus is on proposing a reconstruction method rather than introducing a new dataset. Thus, there is no practical usage of a new dataset described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1369-rubric-5",
    "token_usage": {
      "prompt_tokens": 13740,
      "completion_tokens": 527,
      "total_tokens": 14267
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any new dataset entries contain more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of the new datasets containing exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Entire paper, including abstract, main text, and experiments",
          "reasoning": "The paper and its introduced datasets and experimental materials are exclusively described and presented in English. There is no indication of any other human language being used in the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset content in a non-English language is described or introduced in the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper references source code available externally but does not present a new dataset containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notation is extensively used throughout the paper to describe methods and formulations, but the dataset itself does not contain entries with mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains multi-view azimuth maps for 3D reconstruction but does not include biological or non-human communication system data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed, fictional, or artificial languages are mentioned in the dataset or paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of datasets is specified as English; there is no lack of documentation regarding language."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced contain human language content (English) in the form of labels, annotations, or descriptions accompanying the azimuth maps."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1369-rubric-6",
    "token_usage": {
      "prompt_tokens": 10958,
      "completion_tokens": 146,
      "total_tokens": 11104
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Footnote after author list",
          "reasoning": "The paper includes a GitHub link to the source code at the very beginning (https://github.com/xucao-42/mvas), indicating availability of code related to their method and possibly dataset construction or processing."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "None explicitly stated",
          "reasoning": "The paper does not explicitly introduce any new datasets constructed by the authors. They use existing benchmark datasets like DiLiGenT-MV and datasets from other methods for evaluation, but no dataset creation or data collection process by the authors is described or documented. Hence no documentation about dataset creation is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1461-rubric-0",
    "token_usage": {
      "prompt_tokens": 10701,
      "completion_tokens": 278,
      "total_tokens": 10979
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Experimental Setup - Datasets",
          "Reasoning": "The paper introduces a new experimental evaluation on sequences of RGB images from real-world indoor and outdoor scenes captured in the Tanks and Temples and ScanNet datasets, which are collections of images. Although these datasets are pre-existing, the paper does not explicitly mention introducing a new dataset. Instead, it uses these datasets for evaluation. However, since the user instruction is to list only newly introduced datasets, and the paper does not introduce any new dataset but uses existing ones, no new dataset with new images is introduced. Therefore, the paper does not introduce any new new-image dataset."
        },
        {
          "Modality": "other",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.3 Undistortion of Monocular Depth",
          "Reasoning": "The paper generates monocular depth maps from the input images using an off-the-shelf monocular depth estimation network (e.g., DPT). These monocular depth maps are model-generated data derived from the input images using AI depth estimation models and are used to compute undistorted multiview consistent depth maps. This constitutes a derived dataset generated by a model from original images, not directly human generated or collected."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1461-rubric-1",
    "token_usage": {
      "prompt_tokens": 11501,
      "completion_tokens": 252,
      "total_tokens": 11753
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3 and 3.4",
            "reasoning": "The paper describes the generation of monocular depth maps using an off-the-shelf monocular depth estimation network (e.g., DPT [28]) and the subsequent processing of these depth maps with an automated optimization process integrating scale and shift undistortion parameters and joint optimization with NeRF. No human annotators are involved in annotating or labeling the data; the process is automatic and algorithmic."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any instructions or guidelines for human annotators since the monocular depth maps are generated automatically by a network and processed by the optimization pipeline."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any scoring rubrics or criteria for evaluating human annotations; the evaluation metrics relate to pose accuracy and novel view synthesis but are not annotation rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No annotation examples or labeling examples are provided because no manual annotation is performed; the depth maps are generated and used automatically without human annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1461-rubric-2",
    "token_usage": {
      "prompt_tokens": 12691,
      "completion_tokens": 386,
      "total_tokens": 13077
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process involving a single human expert for dataset annotation or validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts were involved in quality assurance for dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe use of AI models as judges for quality assurance of datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or rule-based techniques for quality assurance of dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses existing datasets (Tanks and Temples and ScanNet) and does not introduce any new datasets. There is no mention of any quality assurance process being applied or documented for new datasets. The evaluation uses ground truth poses and depth maps provided by these datasets. As such, there is no documentation or application of a QA process for new datasets in this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1461-rubric-3",
    "token_usage": {
      "prompt_tokens": 12301,
      "completion_tokens": 475,
      "total_tokens": 12776
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the creation of any new datasets created entirely from human contributors from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper utilizes monocular depth maps generated by an existing monocular depth estimation network (DPT [28]), however, it does not introduce datasets that are entirely generated by AI or ML models from scratch."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data being produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation of data from other languages is discussed."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1, 'Experimental Setup', Datasets paragraph",
          "reasoning": "The authors use existing datasets, specifically Tanks and Temples and ScanNet, which are well-established and publicly available. They use these datasets for evaluation without specifying any significant modification, indicating that the data used is collated from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.3, 'Undistortion of Monocular Depth'; Section 3.4, 'Relative Pose Constraint'; Section 4.3, 'Comparing With COLMAP Assisted NeRF'",
          "reasoning": "While the base image datasets are pre-existing, the authors derive additional data such as undistorted monocular depth maps by applying scale and shift optimisations during training, as well as relative pose constraints through novel loss functions. These represent transformed and adapted data derived from the existing RGB image sequences and monocular depth estimates, thus constituting derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and processing are adequately described; hence, this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1461-rubric-4",
    "token_usage": {
      "prompt_tokens": 12819,
      "completion_tokens": 446,
      "total_tokens": 13265
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using any newly introduced dataset exclusively for pre-training large models. The datasets are used for joint training or evaluation purposes."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used (Tanks and Temples, ScanNet) are existing benchmarks and not newly introduced datasets. The paper does not introduce a new dataset used to train models from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description indicates that any new dataset is used to fine-tune pre-trained models in supervised manners."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention reinforcement learning or RL-based post-training methods using any dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used for evaluation in the paper are existing public datasets (Tanks and Temples, ScanNet), not newly introduced by the authors."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper analyses results on existing datasets, but does not introduce any new dataset primarily for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any novel dataset is used as a knowledge base for augmenting models."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. It uses only existing datasets (Tanks and Temples, ScanNet) for training and evaluation. Hence, no practical usage of a newly introduced dataset is described."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1461-rubric-5",
    "token_usage": {
      "prompt_tokens": 13542,
      "completion_tokens": 626,
      "total_tokens": 14168
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset entries containing more than two human languages. The datasets mentioned (Tanks and Temples, ScanNet) include images from scenes but no indication of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that datasets include exactly two human languages. The datasets comprise image sequences from real-world scenes without language content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper's text and annotations are in English, the datasets themselves consist of images from scenes, not textual data entries. There is no mention that dataset entries specifically contain English language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no content or description of dataset entries containing exactly one non-English human language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset itself is not code or structured programming content. The paper includes code references for implementations and uses mathematical formulas, but the datasets are image sequences. No code dataset entries are described."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3.1, 3.2, 3.3, 3.4 (Equations 1-11)",
          "reasoning": "The paper describes datasets and training processes using detailed mathematical notation such as equations for camera pose optimization, loss functions including photometric loss, depth loss, Chamfer distance, and volumetric rendering integrals. The datasets include inputs and outputs characterized and evaluated with these formal representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described are related to images of indoor and outdoor scenes and do not include any biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or inclusion of fictional or artificially created languages in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language or symbolic nature of dataset entries is clearly specified (images with associated camera poses and depth maps described with mathematics)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Datasets include images and associated annotations; even though not linguistic, they include symbolic mathematical representations and thus cannot be categorized as no language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1461-rubric-6",
    "token_usage": {
      "prompt_tokens": 10760,
      "completion_tokens": 218,
      "total_tokens": 10978
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention",
          "reasoning": "The paper does not provide any link, URL, or mention where code related to dataset construction, data collection, or preprocessing is made publicly available. While there is a project page link mentioned (https://nope-nerf.active.vision), the paper text does not explicitly state that code for dataset creation or processing is provided there."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Experimental Setup, Datasets",
          "reasoning": "The paper clearly describes the datasets used, i.e., Tanks and Temples and ScanNet, including how scenes were selected, frame sampling rates, image resolutions, and train/test splits. Pre-processing of images (e.g., downsampling, cropping dark borders) is also detailed. However, these datasets appear to be existing, publicly available datasets. There is no indication the authors introduce or construct any new dataset themselves. Nevertheless, the paper documents the processing and usage details of the datasets they use for evaluation."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1492-rubric-0",
    "token_usage": {
      "prompt_tokens": 15326,
      "completion_tokens": 132,
      "total_tokens": 15458
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Datasets",
          "Reasoning": "The paper proposes a new tracking method evaluated on several datasets (MOT17, MOT20, KITTI, DanceTrack, CroHD), but does not introduce any new datasets themselves. Instead, it introduces a new algorithm called OC-SORT that operates on existing datasets. The detection inputs used are from existing detectors such as YOLOX and PermaTrack. Therefore, no new dataset is introduced in the paper; only existing datasets are used to evaluate the method."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1492-rubric-1",
    "token_usage": {
      "prompt_tokens": 16126,
      "completion_tokens": 225,
      "total_tokens": 16351
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 5.1 Experimental Setup, Datasets and Method Descriptions",
            "reasoning": "The paper introduces a new multi-object tracking method called OC-SORT and evaluates it on multiple existing datasets. There is no mention of a new dataset being collected or introduced by the authors. Instead, the authors evaluate their method on publicly available datasets such as MOT17, MOT20, KITTI, DanceTrack, and CroHD. Therefore, no new dataset with associated data annotation process is introduced in this paper."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "Since there is no new dataset introduced in this paper, there are no annotation instructions described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No new dataset and thus no annotation scoring rubrics are mentioned in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples or guidelines are provided since the paper does not introduce a new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1492-rubric-2",
    "token_usage": {
      "prompt_tokens": 17316,
      "completion_tokens": 309,
      "total_tokens": 17625
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that a single human expert performed quality assurance on any new dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of multiple human experts performing quality assurance on any new dataset introduced by the authors."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance carried out by a single non-expert human annotator on any dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information suggesting that multiple non-expert human annotators performed quality assurance on any new dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the use of AI models to perform quality assurance on any new datasets introduced."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated processes used for quality assurance of dataset annotations or content for new datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets by the authors; it primarily uses existing datasets (MOT17, MOT20, KITTI, DanceTrack, CroHD) and relies on public detections provided by previous works. Thus, no quality assurance process related to new dataset annotation or content is described or applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1492-rubric-3",
    "token_usage": {
      "prompt_tokens": 16926,
      "completion_tokens": 336,
      "total_tokens": 17262
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention introducing any new dataset collected or created from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any novel dataset generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was produced via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any dataset generated through machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets collected or aggregated from existing sources without significant modification are introduced."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets derived or adapted from existing sources with modifications."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper evaluates the proposed tracking method on multiple existing datasets such as MOT17, MOT20, KITTI, DanceTrack, and CroHD but does not introduce any new datasets or specify any data creation for this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1492-rubric-4",
    "token_usage": {
      "prompt_tokens": 17444,
      "completion_tokens": 244,
      "total_tokens": 17688
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.2 Benchmark Results",
          "reasoning": "The new datasets introduced by the authors, such as the analysis on DanceTrack for non-linear motion and occlusion, are used exclusively for benchmarking the performance of the proposed OC-SORT method and comparing with existing trackers."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.2 Limitations of SORT and Section 5.3 Ablation Study",
          "reasoning": "The datasets are utilized for analyzing the limitations of existing methods and for ablation studies to evaluate the contributions of different components of OC-SORT."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1492-rubric-5",
    "token_usage": {
      "prompt_tokens": 18167,
      "completion_tokens": 624,
      "total_tokens": 18791
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset containing multiple human languages. The datasets referenced are for multi-object tracking in video sequences and contain visual data, not linguistic data."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets are described in the paper as containing exactly two human languages. The focus of datasets is on tracked objects in video frames rather than language."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets discussed (MOT17, MOT20, KITTI, DanceTrack, CroHD) are visual multi-object tracking datasets without indication of linguistic content in English or any language. They do not contain textual or language data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of datasets containing non-English language content. The datasets are visual tracking datasets without linguistic data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes pseudocode and programming code snippets (e.g., Algorithm 1 in Appendix), these are not datasets introduced by the paper. There is no new dataset of programming or structured code-related content introduced."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains extensive mathematical expressions, formulas, and symbolic notations used to describe the method, but these are not part of a dataset introduced by the paper. The datasets referenced are multi-object tracking datasets consisting of video frames and ground truth annotations, not collections of mathematical data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any datasets containing biological sequences or non-human communication data. All datasets consist of human visual object tracking data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets containing constructed or fictional languages in the paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced or referred to have clearly specified content related to visual object tracking data; no language content is unspecified or unknown."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset that contains language. The datasets are visual multi-object tracking datasets comprising video frame data and bounding box annotations, which do not contain any linguistic content."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1492-rubric-6",
    "token_usage": {
      "prompt_tokens": 15385,
      "completion_tokens": 189,
      "total_tokens": 15574
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section in the paper describes code for dataset construction.",
          "reasoning": "The paper mentions that the code and models for OC-SORT tracking method are available at a given GitHub repository (https://github.com/noahcao/OC_SORT), but this code pertains to the tracking method itself, not to dataset construction. The paper uses existing datasets such as MOT17, MOT20, KITTI, DanceTrack, and CroHD for evaluation but does not introduce any new datasets nor describe their creation or provide code for dataset construction."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section in the paper documents dataset creation process for any new dataset.",
          "reasoning": "The paper evaluates the proposed method on several existing datasets but does not introduce or construct any new datasets or detail any dataset creation process. Therefore, there is no documentation about dataset creation in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1497-rubric-0",
    "token_usage": {
      "prompt_tokens": 17699,
      "completion_tokens": 155,
      "total_tokens": 17854
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 3, and Dataset Size Table",
          "Reasoning": "The OMNI3D dataset is a large benchmark for 3D object detection composed of 234k images sourced from multiple existing datasets (e.g., SUN RGB-D, ARKitScenes, Hypersim, Objectron, KITTI, nuScenes). The images are captured by human-operated cameras or sensors in real-world environments, as the paper states that OMNI3D repurposes and combines existing datasets each targeting different applications and visual data forms such as videos and rig captures. Thus, the primary modality is image data captured in natural conditions by humans."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1497-rubric-1",
    "token_usage": {
      "prompt_tokens": 18499,
      "completion_tokens": 282,
      "total_tokens": 18781
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3 (Dataset Analysis) and Section 3 (Dataset Curation)",
            "reasoning": "The OMNI3D dataset is curated by re-purposing and combining existing datasets. The paper describes the process of re-processing raw visual data and annotations from multiple datasets, aligning them into a unified coordinate system and vocabulary. There is no mention of new manual human annotations being collected for OMNI3D; instead, annotations are unified and curated automatically from existing datasets."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit instructions described for OMNI3D annotation in the paper.",
            "reasoning": "Since OMNI3D re-purposes existing annotations and does not perform new manual annotations, there are no annotation instructions provided or required for the data annotation process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No rubrics are mentioned for annotation in the paper.",
            "reasoning": "As no new manual annotations are collected, there are no scoring rubrics or evaluation rubrics described in the paper related to annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples described in the paper or appendix.",
            "reasoning": "The paper does not provide or discuss annotation examples because the dataset is constructed via automatic curation of existing annotations, not via new manual annotation with guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1497-rubric-2",
    "token_usage": {
      "prompt_tokens": 19689,
      "completion_tokens": 516,
      "total_tokens": 20205
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert on the OMNI3D dataset annotations. There is no mention of expert annotation validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The OMNI3D dataset is constructed by re-purposing and combining annotations from multiple existing public datasets. The paper does not mention that multiple human experts performed quality assurance or validation on the merged or curated dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that a single non-expert human annotator conducted any explicit quality assurance over the OMNI3D dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention of multiple non-experts performing quality assurance. The dataset is formed by reusing and unifying annotations from prior datasets without reported manual QA by non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the dataset curation or quality assurance is described as being performed or judged by AI models."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3 (Dataset creation), Introduction",
          "reasoning": "The paper explains that OMNI3D is created by processing and unifying raw visual data and re-purposing annotations from multiple public 3D datasets. This involves transforming annotations into a unified 3D camera coordinate system and careful vocabulary curation. These operations imply use of automated data processing pipelines and algorithmic transformations but no explicit human QA is described. Given the scale and method of the benchmark creation, the main form of quality assurance appears to be automatic consistency checks and transformations rather than manual annotation validation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "While no explicit manual QA process is described, the paper discusses dataset curation involving re-purposing and unifying multiple datasets with automatic transformations. Therefore, it is not accurate to say no QA process is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1497-rubric-3",
    "token_usage": {
      "prompt_tokens": 19299,
      "completion_tokens": 341,
      "total_tokens": 19640
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3 Dataset Analysis and Section 3D Datasets",
          "reasoning": "OMNI3D is created by sourcing, unifying, and combining existing publicly released datasets (SUN RGB-D, ARKitScenes, Hypersim, Objectron, KITTI, nuScenes) without generating new raw data. The authors repurpose and aggregate these datasets to form a larger, more diverse unified benchmark."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3 Dataset Analysis and Appendix",
          "reasoning": "The dataset involves processing raw visual data, re-purposing annotations, unifying semantic vocabularies, and transforming annotations (e.g., extracting 3D cuboids in a unified coordinate system). These steps constitute modifications and adaptations of existing data before integration into OMNI3D."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1497-rubric-4",
    "token_usage": {
      "prompt_tokens": 19817,
      "completion_tokens": 502,
      "total_tokens": 20319
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 5.2, 'OMNI3D as a pre-training dataset'",
          "reasoning": "The paper states that OMNI3D is used to pre-train Cube R-CNN models which are then fine-tuned on smaller downstream 3D detection datasets like SUN RGB-D and KITTI, showing faster learning and higher performance with less data."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5.1, 'Model Performance' and Table 5 in Section 5.2",
          "reasoning": "Models, including Cube R-CNN, are trained from scratch on the OMNI3D dataset (and its subsets), demonstrating improved performance over single smaller datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.2, 'OMNI3D as a pre-training dataset'",
          "reasoning": "After pre-training on OMNI3D, models are fine-tuned in a supervised manner on smaller target datasets (SUN RGB-D, KITTI) to improve performance, indicating use of OMNI3D in a post-training fine-tuning capacity."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or evidence of OMNI3D being used for any reinforcement learning post-training methods like RLHF in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Abstract, Section 5 and Tables 1-5",
          "reasoning": "OMNI3D is used as a benchmark dataset to evaluate and compare 3D object detection methods and to assess generalization performance across different domains and datasets."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.1 'Dataset Analysis'",
          "reasoning": "The authors analyze OMNI3D's spatial distribution, category distribution, depth, size, and 2D/3D correlations to characterize its diversity and challenge level."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe usage of OMNI3D as a knowledge base for model augmentation or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "OMNI3D is actively used for training (from scratch and pre-training), fine-tuning, evaluation, and dataset analysis as described in multiple paper sections."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1497-rubric-5",
    "token_usage": {
      "prompt_tokens": 20540,
      "completion_tokens": 543,
      "total_tokens": 21083
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that dataset entries contain English language content. The dataset focuses on 3D object detection with images and annotations of object bounding boxes and categories, not linguistic content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed primarily of images with object annotations; no mention is made of entries using any non-English language exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses code-related implementations and algorithms (e.g., IoU computation, Cube R-CNN implemented with PyTorch), but these are not part of the dataset entries themselves."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Section 4, Method; Section 5, Experiments",
          "reasoning": "While the paper includes mathematical formulations to describe the model and loss functions, the dataset entries themselves consist of images and 3D box annotations without embedded mathematical or logical symbolic content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication or mention of biological data or non-human communication systems in the dataset entries."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain any entries involving fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language content is not specified because dataset entries are visual data with annotations, not textual data with unknown languages."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The proposed dataset (OMNI3D) contains images with 3D object annotations (bounding boxes, categories), but these annotations do not include any linguistic content such as textual labels in human language. Therefore, the dataset entries do not contain any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1497-rubric-6",
    "token_usage": {
      "prompt_tokens": 17758,
      "completion_tokens": 166,
      "total_tokens": 17924
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Conclusion",
          "reasoning": "The paper states in the Abstract and Conclusion that the OMNI3D benchmark and Cube R-CNN models are released at https://github.com/facebookresearch/omni3d. This indicates that the code related to data processing, benchmark creation, and model implementation is publicly available in an accessible repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 and Appendix",
          "reasoning": "Section 3 of the paper describes the curation process of OMNI3D, including re-purposing raw data and annotations from several publicly available datasets to build a cohesive benchmark. The paper mentions additional details about the benchmark creation are provided in the Appendix, showing transparent and complete documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1530-rubric-0",
    "token_usage": {
      "prompt_tokens": 15487,
      "completion_tokens": 141,
      "total_tokens": 15628
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Implementation Details, Training Details",
          "Reasoning": "The paper trains ODISE on the COCO dataset, which consists of human-generated images and associated annotations (panoptic mask annotations and captions). This dataset is explicitly mentioned as the training set used by the authors to train their model. The authors do not introduce any new dataset; rather, they use the COCO dataset for training and ADE20K, Pascal for testing. The COCO dataset images are human-generated photographs collected by manual capture and annotation, as described in the COCO dataset publication."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1530-rubric-1",
    "token_usage": {
      "prompt_tokens": 16287,
      "completion_tokens": 278,
      "total_tokens": 16565
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.1 Training Details, Section 3.1 Problem Definition",
            "reasoning": "The training data for the new dataset is the COCO dataset for open-vocabulary panoptic segmentation, which provides panoptic mask annotations and category labels or image captions. COCO annotations are produced by multiple human experts and annotators following standardized protocols."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "COCO dataset documentation; Section 4.1 Training Details",
            "reasoning": "COCO is a widely used human-annotated dataset with standardized annotation instructions to ensure high-quality panoptic segmentation masks. The paper uses these annotations as ground truth masks for training the mask generator, implying the presence of instructions for annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "COCO dataset documentation",
            "reasoning": "COCO's annotation process includes quality control and scoring rubrics to measure annotation consistency and correctness. Though not detailed in the paper, the standard COCO dataset annotation includes rubrics to maintain quality."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "COCO dataset documentation",
            "reasoning": "The COCO dataset was created with example images and annotated masks to illustrate annotation guidelines for annotators; the paper relies on these human-labeled masks, so examples are implied."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1530-rubric-2",
    "token_usage": {
      "prompt_tokens": 17477,
      "completion_tokens": 321,
      "total_tokens": 17798
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human annotator who is a subject matter expert or member of the target demographic for dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information or mention that multiple human experts performed quality assurance on dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process performed by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description is provided regarding multiple human non-expert annotators performing quality assurance for dataset content or annotations."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used for classification and feature extraction, the paper does not describe the use of an AI model specifically for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe an automated verification or algorithmic quality assurance process applied to dataset annotation validation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces no new datasets; instead, it uses existing datasets such as COCO and ADE20K for training and evaluation. There is no documentation of any quality assurance process related to dataset annotations conducted by the authors for new datasets, since none are introduced. Hence, no quality assurance process is applicable or described."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1530-rubric-3",
    "token_usage": {
      "prompt_tokens": 17087,
      "completion_tokens": 385,
      "total_tokens": 17472
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creating any original dataset entirely from scratch by human contributors. Instead, it uses existing datasets (e.g., COCO) for training and evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper leverages existing pretrained text-to-image diffusion models to extract internal features but does not generate new datasets purely from AI models for training or evaluation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was produced via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe machine translation being used to produce any dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation Details - Training Details",
          "reasoning": "The paper uses existing datasets, such as COCO for training and ADE20K for testing, collecting annotations and captions that already exist. The data is aggregated from these known sources without indication of modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that it modifies or derives new datasets from existing sources; rather, it uses existing datasets directly."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented; existing datasets are used and specified."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1530-rubric-4",
    "token_usage": {
      "prompt_tokens": 17605,
      "completion_tokens": 263,
      "total_tokens": 17868
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.4, 3.5, and 4.1",
          "reasoning": "The COCO dataset is used as a training set with panoptic mask annotations, and ODISE is trained on it using supervised fine-tuning to learn mask generation and classification modules for open-vocabulary panoptic segmentation."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The ADE20K and Pascal datasets are used at test time for benchmarking and evaluation of open-vocabulary panoptic and semantic segmentation performance. The paper reports performance metrics (PQ, mAP, mIoU) on these datasets."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1530-rubric-5",
    "token_usage": {
      "prompt_tokens": 18328,
      "completion_tokens": 568,
      "total_tokens": 18896
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the introduced dataset contains entries with more than two human languages. The dataset (COCO with annotations and captions) uses primarily English text only."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset content involves exactly two human languages. The captions and category names are English only."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation Details; Section 4 Experiments; Abstract",
          "reasoning": "The authors train and evaluate on COCO dataset with its panoptic mask annotations and captions provided in English. All category names used for classification and captions used for supervision are in English. The paper references COCO captions which are in English, and there is no mention of other languages used in their new dataset construction or annotation."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset used and introduced involves English captions and category names only; no non-English linguistic content is described."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes algorithmic descriptions and equations, the dataset itself does not contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not themselves contain mathematical or formal logical expressions; these appear only in the methodology description, not in the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication is included or described in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or constructed languages are mentioned or present in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset's language (English) is specified explicitly via the COCO captions and category labels."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset used contains language content (English captions and category names), so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1530-rubric-6",
    "token_usage": {
      "prompt_tokens": 15546,
      "completion_tokens": 200,
      "total_tokens": 15746
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Conclusion",
          "reasoning": "The paper mentions that code and models for the ODISE method are open-sourced at https://github.com/NVlabs/ODISE, which pertains to their segmentation model and approach. However, the paper does not introduce any new dataset that is constructed by the authors. The datasets used for training and evaluation (e.g., COCO, ADE20K, Pascal VOC) are all existing publicly available datasets. Therefore, no new dataset code or construction is presented or linked in the paper."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Not explicitly documented in the paper",
          "reasoning": "The paper does not introduce any new dataset created by the authors; instead, it uses existing datasets (COCO, ADE20K, Pascal VOC) for training and evaluation. Since no new dataset creation process is presented or documented, there is no documentation related to dataset construction within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-155-rubric-0",
    "token_usage": {
      "prompt_tokens": 13382,
      "completion_tokens": 97,
      "total_tokens": 13479
    },
    "response": {
      "sources": [
        {
          "Modality": "tabular",
          "Human Generated": false,
          "Model Generated": false,
          "Unknown Origin": true,
          "Reference": "Section 4 Experiments",
          "Reasoning": "The paper evaluates the proposed AstroNet method on three existing public datasets: ImageNet-1k, CIFAR, and MNIST. These datasets are pre-existing benchmark datasets commonly used in image classification tasks. The paper does not mention introducing any new dataset but rather uses these existing datasets for evaluation."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-155-rubric-1",
    "token_usage": {
      "prompt_tokens": 14182,
      "completion_tokens": 221,
      "total_tokens": 14403
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3 Training Scheme; Algorithm 1",
            "reasoning": "The paper describes an automatic bilevel learning scheme where the astrocyte network (AN) regulates the neural network (NN) connections via iterative computations and optimization on training and validation sets. Annotation or selection of sub-networks is therefore driven by an automatic optimization and pruning process rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit annotation instructions provided.",
            "reasoning": "The paper does not provide human annotation instructions; the selection of sub-networks is performed by the proposed learning algorithm automatically without human annotations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit scoring rubrics provided.",
            "reasoning": "The process is algorithmic and based on optimization objectives; no rubrics for human assessment are described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples provided.",
            "reasoning": "No examples of annotation or human labeling are provided, since the network optimization is automatic."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-155-rubric-2",
    "token_usage": {
      "prompt_tokens": 15372,
      "completion_tokens": 339,
      "total_tokens": 15711
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert for the new datasets or data used; no human expert annotation or review process is mentioned."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance on any datasets introduced by the authors, nor is there mention of expert annotation or validation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single non-expert performed any quality assurance on datasets; no such process is described."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description or evidence of multiple non-experts performing quality assurance on datasets is provided in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces a neural network architecture (AstroNet) that optimizes network structures, but it does not describe using AI models to perform quality assurance on dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses automated training and optimization processes for the neural network architecture, it does not state the use of automated verification or algorithmic quality assurance of dataset annotations or contents."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses three existing public datasets (ImageNet-1k, CIFAR, MNIST) and does not introduce any new datasets. Therefore, no dataset annotation or content quality assurance process is applied or described for new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-155-rubric-3",
    "token_usage": {
      "prompt_tokens": 14982,
      "completion_tokens": 244,
      "total_tokens": 15226
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4, Experiments",
          "reasoning": "The paper uses existing public datasets (ImageNet-1k, CIFAR, MNIST) for classification tasks to demonstrate the performance of the proposed method. The new data instances are not introduced; rather, the paper develops a new network model (AstroNet) for structure learning and optimizes fixed architectures. Hence, the datasets themselves are existing sources, and no mention is made of new data generation or human-created novel data samples. The evaluations and experiments are performed on existing data, so the data used is derived from existing sources with adaptations in the methodology but no new original data creation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-155-rubric-4",
    "token_usage": {
      "prompt_tokens": 15500,
      "completion_tokens": 204,
      "total_tokens": 15704
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce or describe any new datasets. It uses existing public datasets (e.g., CIFAR, ImageNet-1k, MNIST) for evaluation and experiments, but introduces no new datasets. Therefore, no practical usage of a new dataset is demonstrated or described."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-155-rubric-5",
    "token_usage": {
      "prompt_tokens": 16223,
      "completion_tokens": 548,
      "total_tokens": 16771
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Entire paper, including Abstract, Introduction, Methodology, and Experiments sections",
          "reasoning": "The paper is written entirely in English and the new dataset(s) introduced for evaluation are standard image datasets such as CIFAR10, CIFAR100, MNIST, and ImageNet-1k. These datasets primarily consist of images and associated labels with English annotations or class names. Hence, the dataset content is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the paper discusses implementing the proposed method (AstroNet) with PyTorch and mentions code-related details, there is no new dataset containing code or programming language entries introduced by the authors."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes mathematical equations describing the Astrocyte-Neuron model and related mechanisms. However, these are part of the model formulation and methodology rather than entries of a dataset. No dataset with mathematical or logical notation entries is introduced."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Biological concepts such as astrocytes are used metaphorically or in the inspiration for the model but there is no dataset introduced with biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any fictional or artificially created language datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly states the datasets used and their characteristics, so there is no unknown or unspecified language."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets used include language in the form of labels and class names; hence, they do contain language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-155-rubric-6",
    "token_usage": {
      "prompt_tokens": 13441,
      "completion_tokens": 159,
      "total_tokens": 13600
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4.1 Implementation Details",
          "reasoning": "The paper mentions that the code and models will be made available to facilitate reproducible research, but does not provide any link, URL, or repository location for the code related to dataset construction or processing. Therefore, based strictly on the paper content, the code is not publicly available within this document."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Not explicitly stated",
          "reasoning": "The paper does not introduce any new datasets; it uses three public datasets (ImageNet-1k, CIFAR, and MNIST) for evaluation. There is no description or documentation of a dataset creation process because no new dataset is introduced by the authors in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1599-rubric-0",
    "token_usage": {
      "prompt_tokens": 11838,
      "completion_tokens": 146,
      "total_tokens": 11984
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 and throughout the paper",
          "Reasoning": "The paper introduces PMTrans, a method for unsupervised domain adaptation using vision transformers that operates on image data. The intermediate domain is constructed by mixing image patches (visual data) sampled from source and target domains (which consist of images). The datasets used are established image datasets, and the new dataset introduced is the 'intermediate domain' created during training by mixing existing source and target domain images at the patch level. This intermediate domain is generated algorithmically through the PatchMix module but composed of patches from human-generated images."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1599-rubric-1",
    "token_usage": {
      "prompt_tokens": 12638,
      "completion_tokens": 233,
      "total_tokens": 12871
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2 PMTrans and PatchMix description",
            "reasoning": "The annotation corresponds to the generation of pseudo labels for the unlabeled target domain samples, which is performed via a k-means clustering method to produce target pseudo labels as described in Section 3.2. This process is algorithmic and automatic, not involving human annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Semi-supervised mixup loss",
            "reasoning": "The paper describes detailed methodology for generating pseudo labels using k-means clustering and discusses how these pseudo labels are utilized in training losses. This constitutes instructions on how labels are generated and used for annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No explicit scoring rubrics or criteria are described in the paper for evaluating or scoring annotations or pseudo label quality."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not provide annotation examples, sample label outputs, or illustrative annotation cases for the pseudo labels generated."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1599-rubric-2",
    "token_usage": {
      "prompt_tokens": 13828,
      "completion_tokens": 271,
      "total_tokens": 14099
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process conducted by a single human expert for dataset annotations or content validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance on the datasets introduced."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any quality assurance performed by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance performed by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe an AI model being used as a judge or for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automatic quality assurance process such as automated verification of dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets, nor does it document any quality assurance process related to dataset annotations or content. The datasets used are existing benchmarks, and no QA process for new datasets is described."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1599-rubric-3",
    "token_usage": {
      "prompt_tokens": 13438,
      "completion_tokens": 416,
      "total_tokens": 13854
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the creation of any new datasets collected or annotated from scratch by humans. It only utilizes existing benchmark datasets for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the paper generates any entirely new dataset synthetic or otherwise, generated purely by AI models without referencing existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention translation of datasets or data collection involving human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translated datasets or data samples are mentioned in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets and implementation",
          "reasoning": "The paper uses several existing well-known benchmark datasets for unsupervised domain adaptation such as Office-Home, Office-31, VisDA-2017, and DomainNet. These datasets are collected and compiled from existing sources without significant newly introduced modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper proposes novel data mixing techniques (PatchMix) to construct intermediate domain representations during model training, these are not introduced as new datasets independent of existing datasets; rather, they are transformations applied during learning and not separately released as datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and sources of all used datasets are described and well known; there is no lack of documentation about data source."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1599-rubric-4",
    "token_usage": {
      "prompt_tokens": 13956,
      "completion_tokens": 436,
      "total_tokens": 14392
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing public benchmark datasets (Office-Home, Office-31, VisDA-2017, DomainNet) for evaluation and training in unsupervised domain adaptation. No new dataset is proposed for pre-training large models."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are publicly available benchmark datasets for domain adaptation. The paper does not introduce any new dataset that is used to train a model from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No newly proposed dataset is used for supervised fine-tuning purposes in the paper."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe reinforcement learning-based post-training on any new dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper evaluates the proposed PMTrans method on several existing benchmark datasets, but does not propose any new dataset for the purpose of evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is introduced by the authors for the purpose of analysis or investigation of trends."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not propose or use a new dataset as a knowledge base to augment models."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. All datasets mentioned are existing benchmark datasets. Therefore, there is no practical usage of any newly proposed dataset."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1599-rubric-5",
    "token_usage": {
      "prompt_tokens": 14679,
      "completion_tokens": 563,
      "total_tokens": 15242
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset entries containing more than two human languages. The datasets used are standard UDA benchmarks for image classification, which are in a single language (English labels)."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not bilingual; they do not contain exactly two human languages. The labels and dataset descriptions are in English only."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets and implementation (Pages 13-14), as well as throughout the paper",
          "reasoning": "The newly introduced dataset aspect is the intermediate domain constructed by PatchMix from source and target domains, both using standard UDA benchmark datasets (Office-Home, Office-31, VisDA-2017, DomainNet) which are in English label space. The datasets and labels are English, and no other human languages are involved or mentioned."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of datasets with a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain images for UDA tasks, no programming or code data entries are described."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical formulations and logical descriptions of the method, the datasets themselves do not contain mathematical or formal logical data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data is involved in the presented datasets."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are part of the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of dataset entries are clearly described and standard, so unknown language status does not apply."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets clearly contain language in the form of English labels attached to image data."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1599-rubric-6",
    "token_usage": {
      "prompt_tokens": 11897,
      "completion_tokens": 105,
      "total_tokens": 12002
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not mention the availability of code related to dataset construction, nor does it provide any links or references to such code repositories."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "None",
          "reasoning": "The paper uses existing standard benchmark datasets (Office-31, Office-Home, VisDA-2017, DomainNet) for evaluation and does not introduce any new datasets nor document any dataset construction process."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1655-rubric-0",
    "token_usage": {
      "prompt_tokens": 8048,
      "completion_tokens": 88,
      "total_tokens": 8136
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Dataset, and throughout the paper",
          "Reasoning": "The paper uses the BAL dataset containing bundle adjustment problems including real-world 3D reconstructions from images captured by vehicles and from Flickr.com. The images originate from human photography, i.e., human-generated modality 'image'."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1655-rubric-1",
    "token_usage": {
      "prompt_tokens": 8848,
      "completion_tokens": 276,
      "total_tokens": 9124
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Overall paper content including Experimental Settings Section 5.1 and the Dataset description",
            "reasoning": "The paper describes the introduction of a new solver applied to the BAL dataset problems. The annotations for the dataset and evaluation were performed by experts in computer vision and bundle adjustment solvers, as indicated by the academic context and technical nature of the implementation and evaluation. There is no indication of annotation by non-experts, AI models, or automatic processes; the paper reflects expert evaluation of a new dataset subset."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No specific section or appendix describes detailed annotation instructions for data annotation.",
            "reasoning": "The paper does not mention providing instructions for annotators regarding the dataset; the BAL dataset is pre-existing, and the benchmarks involve evaluating solver performance rather than manual annotation tasks requiring instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention of annotation rubrics is made in the paper.",
            "reasoning": "The evaluation uses numerical performance metrics and solver convergence criteria rather than subjective scoring requiring rubric guidelines."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples provided for annotation guidance in the paper.",
            "reasoning": "No example annotations or exemplary labeling cases are discussed; the paper focuses on algorithmic evaluation rather than human annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1655-rubric-2",
    "token_usage": {
      "prompt_tokens": 10038,
      "completion_tokens": 275,
      "total_tokens": 10313
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process involving a single human expert for dataset validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple human experts performing quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any single non-expert human performing quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of multiple non-expert human annotators conducting quality assurance is found in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of AI models as judges or for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper performs extensive algorithmic evaluation and theoretical proofs, it does not describe automated verification processes as quality assurance for dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses the publicly available BAL dataset for evaluation but does not introduce any new dataset or describe any quality assurance process for datasets. Hence, there is no QA process applied or documented for any new dataset."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1655-rubric-3",
    "token_usage": {
      "prompt_tokens": 9648,
      "completion_tokens": 382,
      "total_tokens": 10030
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creation of any new dataset contributed by humans from scratch. Instead, it uses existing benchmark datasets to evaluate the proposed methods."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data is generated artificially by AI or machine learning models in this work. The paper focuses on algorithmic developments and evaluations on existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of machine translated data used or introduced."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.1 Experimental Settings (Dataset)",
          "reasoning": "The authors use the publicly available BAL dataset, which is a collection of bundle adjustment problems from previous sources like internet photo collections. They perform extensive evaluation on all 97 existing problems from BAL dataset which are aggregated from real-world sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe creating new datasets derived or transformed from existing data, but rather evaluates an algorithm using existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is documented as the BAL dataset, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1655-rubric-4",
    "token_usage": {
      "prompt_tokens": 10166,
      "completion_tokens": 261,
      "total_tokens": 10427
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.2 Analysis",
          "reasoning": "The BAL dataset is used exhaustively to evaluate and benchmark the proposed Power Bundle Adjustment solver, including performance profiles on all 97 BAL problems to compare runtime, accuracy, and memory consumption against state-of-the-art solvers."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.2 Analysis",
          "reasoning": "The authors perform extensive analysis using the BAL dataset to study solver speed, accuracy, convergence behavior, and memory consumption trends across many problems, providing detailed insights into algorithm characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The BAL dataset is actively used in the paper for evaluation and analysis of the proposed solvers, demonstrating its practical utility."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1655-rubric-5",
    "token_usage": {
      "prompt_tokens": 10889,
      "completion_tokens": 600,
      "total_tokens": 11489
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset entries containing more than two human languages. The evaluation dataset is the BAL dataset, related to 3D reconstruction problems, not linguistic data."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset contains exactly two human languages. The dataset is from computer vision bundle adjustment problems."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper (e.g., Abstract, Introduction, Sections 4 and 5)",
          "reasoning": "The text and dataset descriptions are entirely in English. The dataset used (BAL) is publicly available with annotations and documentation solely in English. The paper's explanations, evaluation, and code references are in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication or evidence that the dataset contains entries in a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses implementations and algorithms in C++, and mentions releasing code, the new dataset itself does not contain programming or structured code-related content. The dataset consists of bundle adjustment problems (numerical inputs) rather than code snippets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 4 (Power Bundle Adjustment), Proposition 1, Lemma 1 and related mathematical formulas.",
          "reasoning": "The dataset and method descriptions contain extensive mathematical and formal logical expressions, including matrices, power series expansions, eigenvalues, and convergence proofs relevant to the bundle adjustment problems."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include biological sequences or any non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset has no mention of constructed or fictional languages such as Klingon or Esperanto."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of the dataset are clearly specified and are in English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains entries with language: English text and mathematical notation, so it is not the case that the dataset contains no languages."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1655-rubric-6",
    "token_usage": {
      "prompt_tokens": 8107,
      "completion_tokens": 178,
      "total_tokens": 8285
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 5 Implementation, last paragraph",
          "reasoning": "The paper states that the Power Bundle Adjustment (PoBA) solver is released as open source with a provided URL (https://github.com/simonwebertum/poba), which implements the solver rather than code for dataset construction. The paper uses the publicly available BAL dataset; it does not introduce a new dataset nor provide code for creating or processing new datasets. Therefore, there is no code released related to constructing a new dataset."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets. It evaluates on the existing BAL dataset and describes implementation and solver methodology, but does not provide documentation related to dataset creation since no new dataset is created or described. Hence, no documentation on dataset creation is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-171-rubric-0",
    "token_usage": {
      "prompt_tokens": 11222,
      "completion_tokens": 208,
      "total_tokens": 11430
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Implementation Details; Section 4.2 Main Results",
          "Reasoning": "The new dataset introduced by the authors is a training set assembled from existing publicly available tracking video datasets (e.g., GOT-10k, LaSOT, TrackingNet) used to train their model ARTrack. Although these datasets are originally human captured videos, the authors use them in their new sequential training framework. There is no indication of them generating new synthetic or model-generated data; the data is human-generated video sequences (video modality). The authors do not introduce a new standalone dataset but use these video datasets for sequential training. Based on the instruction, only new datasets introduced by the authors should be listed. Since the paper does not present any new dataset but only uses publicly available datasets for training, there is no new dataset introduced by the authors. Therefore, no data source entries meet the instruction criteria."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-171-rubric-1",
    "token_usage": {
      "prompt_tokens": 12022,
      "completion_tokens": 255,
      "total_tokens": 12277
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 4.1 Implementation Details - Training strategy",
            "reasoning": "The paper states that the training set consists of publicly available datasets (GOT-10k, LaSOT, TrackingNet) for training the ARTrack model. These datasets are manually annotated with bounding boxes by multiple annotators, usually non-expert crowdworkers or annotators, to provide ground truth tracking data. The annotations are not created by experts, nor automatically generated by models or automatic processes, indicating multiple human non-experts performed the annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "--",
            "reasoning": "The paper does not provide or reference any specific annotation instructions or guidelines used for creating ground truth annotations in the datasets introduced or used."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "--",
            "reasoning": "No scoring rubrics or evaluation criteria for annotators are described in the paper regarding the annotation process of the new dataset; thus, no rubrics are evident."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "--",
            "reasoning": "The paper does not present annotation examples or annotation guideline supplements for annotators related to the new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-171-rubric-2",
    "token_usage": {
      "prompt_tokens": 13212,
      "completion_tokens": 299,
      "total_tokens": 13511
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts were involved in quality assurance of dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA process done by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description of QA by multiple non-expert human annotators is provided in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using any AI model as a judge for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper describes automated training and evaluation processes, it does not describe any automated quality assurance process for dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper describes the use of existing benchmark datasets (GOT-10k, TrackingNet, LaSOT, etc.) but does not introduce any new dataset nor describe any quality assurance process for datasets. Therefore, no quality assurance process is applied or documented in relation to new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-171-rubric-3",
    "token_usage": {
      "prompt_tokens": 12822,
      "completion_tokens": 408,
      "total_tokens": 13230
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the authors created any new dataset entirely from scratch by human annotation or creation; instead, they use existing benchmark datasets such as GOT-10k, LaSOT, TrackingNet, etc., for training and evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that any dataset was generated by AI or machine learning models exclusively without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of any datasets produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of any datasets generated through machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that the authors collected or aggregated existing datasets to form a new dataset without significant modifications."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new dataset derived from existing datasets by applying modifications or transformations; their work focuses on a new tracking framework and training models on existing datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new dataset introduced by the authors is described in the paper. The authors utilize existing public datasets (e.g., GOT-10k, LaSOT, TrackingNet) for training and testing but do not propose or introduce any new dataset or data collection."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-171-rubric-4",
    "token_usage": {
      "prompt_tokens": 13340,
      "completion_tokens": 319,
      "total_tokens": 13659
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 (Main Results), Section 4.3 (Analysis of the Autoregressive Model)",
          "reasoning": "The paper extensively evaluates the performance of the ARTrack model on existing benchmark datasets such as GOT-10k, TrackingNet, LaSOT, and others to demonstrate its effectiveness. However, no new datasets are introduced by the authors for evaluation only."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3 (Analysis of the Autoregressive Model)",
          "reasoning": "The paper uses the benchmark datasets to analyze the effect of the autoregressive order, bin quantization, loss functions, and other model properties, which indicates that datasets are used for analysis of trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. All datasets referenced and used for training, evaluation, or analysis are existing, publicly available datasets. Therefore, there is no practical usage of a new proposed dataset described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-171-rubric-5",
    "token_usage": {
      "prompt_tokens": 14063,
      "completion_tokens": 542,
      "total_tokens": 14605
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset containing entries with more than two human languages. The datasets used are visual object tracking video datasets without linguistic content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the newly introduced datasets contain exactly two human languages. The datasets comprise video sequences with visual data, not multilingual text."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper and references are in English, the datasets themselves are video datasets used for object tracking and do not contain English textual content as part of the data entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any dataset with entries in a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are standard visual tracking video datasets, which consist of image frames and bounding box coordinates, not programming or structured code data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses mathematical notation to define the modeling framework and loss functions, the datasets themselves do not contain mathematical or logical symbolic expressions as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are visual tracking datasets and do not include biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of any fictional or constructed languages included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are clearly visual data with associated bounding box coordinates, no linguistic content that is unspecified or undocumented."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The proposed datasets contain purely visual data - video frames and bounding box coordinates for object tracking - and do not contain any human or programming language as data entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-171-rubric-6",
    "token_usage": {
      "prompt_tokens": 11281,
      "completion_tokens": 152,
      "total_tokens": 11433
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract",
          "reasoning": "The paper only provides a link to the source code of the tracker implementation (https://github.com/MIV-XJTU/ARTrack) but does not include or mention any code for dataset construction, collection, or preprocessing. The datasets used are existing public benchmarks and no new dataset is introduced by the authors, hence no dataset construction code is provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "N/A",
          "reasoning": "The paper does not introduce any new datasets; it uses publicly available benchmarks such as GOT-10k, TrackingNet, LaSOT and others. Therefore, there is no section or description documenting any dataset creation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1713-rubric-0",
    "token_usage": {
      "prompt_tokens": 15222,
      "completion_tokens": 521,
      "total_tokens": 15743
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2 Knowledge Sources; Table 1",
          "Reasoning": "The paper introduces four new knowledge corpora used as sources for constructing the memory for retrieval-augmented pre-training and evaluation, specifically: WIT dataset which contains images from Wikipedia with alt-text captions and contextualized text passages; these images originate from human-generated Wikipedia content and are not model-generated or web-crawled without clear provenance."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2 Knowledge Sources; Table 1",
          "Reasoning": "The WIT dataset used as one of the four knowledge sources contains Wikipedia passages and alt-text captions accompanying images, which are human-created text content curated on Wikipedia, therefore human-generated with known origin."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2 Knowledge Sources; Table 1",
          "Reasoning": "The Conceptual Captions (CC12M) dataset is included as a knowledge source, which consists of web images paired with alt-text captions. As web images are captured or uploaded by humans, this data is considered human generated with known provenance (not model-generated)."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": false,
          "Unknown Origin": true,
          "Reference": "Section 4.2 Knowledge Sources; Table 1",
          "Reasoning": "The CC12M text data consists of alt-text captions collected from the web, which typically comes from web crawl and noisy sources, hence the exact origin is unknown or not explicitly specified (Unknown Origin), and not model-generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2 Knowledge Sources; Table 1",
          "Reasoning": "The VQA-v2 dataset, used as one of the knowledge sources, provides human-written question-answer pairs per image, merged into passages for knowledge memory. This data is human generated with clear annotation provenance."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2 Knowledge Sources; Table 1",
          "Reasoning": "WikiData knowledge graph triplets are used as one knowledge source, linearized into text passages. These triplets are human-curated structured knowledge from the open knowledge base Wikidata, considered human generated with clear provenance."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1713-rubric-1",
    "token_usage": {
      "prompt_tokens": 16022,
      "completion_tokens": 262,
      "total_tokens": 16284
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2 - Memory; Section 4.1 - Pre-Training Objective",
            "reasoning": "The annotation involves construction and encoding of the knowledge memory from large-scale datasets in an automated manner via predefined pipelines (e.g., knowledge items are encoded with neural encoders and indexed; retrieval ground-truth is pseudo-constructed from Wikipedia Image-Text data). The paper does not mention human annotators performing the annotation, rather it uses computational/algorithmic methods to generate and update annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not provide any annotation instructions for humans regarding the annotation of the new dataset or knowledge items; the process is automated without manual annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics or explicit evaluation criteria for annotations are described, because annotations are based on automated encoding and retrieval processes rather than manual labeling."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No examples of manual annotation guidance or annotated samples are provided; the knowledge sources are pre-existing or processed automatically, so no such annotation examples exist in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1713-rubric-2",
    "token_usage": {
      "prompt_tokens": 17212,
      "completion_tokens": 383,
      "total_tokens": 17595
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the introduced datasets or knowledge sources."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts conducted quality assurance on the introduced knowledge corpora; the datasets used are existing large-scale knowledge sources."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of quality assurance by a single non-expert annotator for the dataset annotations or content."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that multiple non-expert human annotators performed quality assurance on any dataset introduced by the authors."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance of the datasets is not stated to be conducted by AI models acting as judges; the paper discusses model training and retrieval but not dataset QA by AI models."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 Memory; Section 4 Pre-Training Data and Objective",
          "reasoning": "The authors utilize large-scale existing knowledge sources (e.g., Wikipedia passages, Web image-text pairs, knowledge graph triplets) which have been collected and filtered using automated processes. The paper describes filtering (e.g., removing captions shorter than 50 characters) and uses automatic contrastive pre-training and retrieval methods, but does not describe manual or human-led quality assurance. Therefore, the quality assurance can be considered to be through automatic or algorithmic verification and filtering."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is documentation of dataset collection and filtering, though no explicit manual quality assurance, so N/A does not fully apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1713-rubric-3",
    "token_usage": {
      "prompt_tokens": 16822,
      "completion_tokens": 504,
      "total_tokens": 17326
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset or knowledge corpus created entirely from scratch by human contributors. Instead, it utilizes existing datasets such as Wikipedia-Image-Text (WIT), Conceptual Captions (CC12M), VQA-v2, and WikiData, which are established external knowledge sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset or knowledge source used was generated solely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state any use of data produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data generated by machine translation systems is mentioned in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.2 (Memory); Section 4.2 (Knowledge Sources)",
          "reasoning": "The authors collect and aggregate existing multimodal knowledge sources such as Wikipedia passages, Wikimedia images and captions, web image-text pairs, visual question answering datasets, and WikiData knowledge graph triplets into a unified large-scale memory. These are existing datasets collated together without indication of original generation by the authors."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 (Memory); Section 4.2 (Knowledge Sources)",
          "reasoning": "Some data are transformed or adapted from existing sources, for example, WikiData knowledge graph triplets are linearized into textual passages, and multiple question-answer pairs per image in VQA-v2 are merged into single passages. The authors also apply compression and encoding methods (Perceiver model) to process data, which counts as derivation and transformation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies the origins of all data sources used and their processing methods; therefore, this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1713-rubric-4",
    "token_usage": {
      "prompt_tokens": 17340,
      "completion_tokens": 590,
      "total_tokens": 17930
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.3",
          "reasoning": "The authors pre-train their model REVEL on a massive Web-Image-Text dataset (1.3 billion image-caption pairs) using a text generation objective (prefix language modeling). This pre-training is performed to learn general multimodal patterns and to train all model components end-to-end, as described in Sections 4.1 and 4.3."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The model is warm-started from pre-trained ViT and T5 checkpoints and does not appear to be trained completely from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.1, 5.2",
          "reasoning": "The pre-trained REVEL model is fine-tuned on specific downstream tasks such as knowledge-based Visual Question Answering (OK-VQA, A-OKVQA) and image captioning datasets (MSCOCO, NoCaps) using supervised objectives, as described in Sections 5.1 and 5.2."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention or describe any use of reinforcement learning-based post-training methods such as RLHF."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets introduced by the authors are described as being used solely for evaluation; existing standard benchmarks are used for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets are integrated into the model pipeline rather than used primarily for analysis or studying trends."
        },
        "Knowledge Base": {
          "is_applicable": true,
          "reference": "Section 3.2, 4.2, 5.3",
          "reasoning": "The new datasets constructed from multiple multimodal knowledge sources (e.g., Wikipedia passages, Web image-text pairs, knowledge graph triplets) collectively form a large-scale knowledge memory which augments the model via retrieval, serving as a knowledge base as per Section 3.2, 4.2 and the analysis in 5.3."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The authors document multiple usages of the new datasets in pre-training, fine-tuning, and as a knowledge base."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1713-rubric-5",
    "token_usage": {
      "prompt_tokens": 18063,
      "completion_tokens": 568,
      "total_tokens": 18631
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not mention the datasets containing entries with more than two human languages. The knowledge sources used consist mainly of English text passages and captions."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not describe any dataset entries that contain exactly two human languages. The datasets referenced and used appear to be monolingual."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.2 (Knowledge Sources), Table 1, and throughout the paper",
          "reasoning": "The knowledge sources used for the memory include Wikipedia passages, web image alt-text captions, VQA question-answer pairs, and WikiData triplets, all described or linearized as textual passages in English. The paper refers to Wikipedia and web captions that are primarily in English, given no mention of multilingual text content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention or indication that the datasets include exclusively non-English language content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not state that any dataset entries contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Although the model and paper involve mathematical formulations, the proposed datasets themselves (knowledge corpora) do not contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No biological sequences or non-human communication data is described as part of the datasets."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention of fictional or constructed languages in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The languages of dataset entries are specified explicitly as English, not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain natural language text, so language is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1713-rubric-6",
    "token_usage": {
      "prompt_tokens": 15281,
      "completion_tokens": 191,
      "total_tokens": 15472
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or link provided for dataset construction code",
          "reasoning": "The paper does not mention providing public access to the code for constructing or processing the introduced datasets. There is no explicit reference to a code repository related to dataset creation or preprocessing, only a project page URL for the model."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.2 (Knowledge Sources), Section 4.3 (Implementation Details)",
          "reasoning": "The paper provides detailed descriptions of the new datasets and knowledge sources used (e.g., Wikipedia-Image-Text (WIT), Conceptual (CC12M), VQA-v2 merged question-answer pairs, WikiData linearized triplets). It documents the sources, preprocessing (e.g., filtering captions shorter than 50 characters), and data statistics (Table 1). These details qualify as dataset creation process documentation."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1715-rubric-0",
    "token_usage": {
      "prompt_tokens": 16237,
      "completion_tokens": 94,
      "total_tokens": 16331
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5, Experiments, Dataset paragraph",
          "Reasoning": "The paper states that the experiments are conducted on the ScanNet-V2 dataset, which consists of RGBD images captured from real-world indoor scenes using scanning devices operated by humans. The dataset is pre-processed but originally human-generated by capturing RGBD images through scanning."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1715-rubric-1",
    "token_usage": {
      "prompt_tokens": 17037,
      "completion_tokens": 249,
      "total_tokens": 17286
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 4.2 RGBD Diffusion for Rendered View Inpainting",
            "reasoning": "The paper describes the use of an RGBD diffusion model implemented as a UNet for inpainting missing regions of RGBD views. This process is automated and based on a trained neural network (diffusion model), indicating the annotations (inpainting) are performed by an AI model rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not provide any mention of human annotation instructions or detailed guidelines related to annotating or inpainting data; the inpainting is performed via a learned diffusion model."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics or evaluation guidelines for annotation are described; evaluations focus on metrics like PSNR, SSIM, LPIPS, but these are for model performance, not annotation guidelines."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples or sample annotations are presented as guidelines; the inpainting is model-driven and no human annotation examples are given."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1715-rubric-2",
    "token_usage": {
      "prompt_tokens": 18227,
      "completion_tokens": 371,
      "total_tokens": 18598
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any single non-expert human annotator performing quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance done by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper utilizes an AI model (diffusion model) for inpainting but does not describe the diffusion model or any other AI model as performing quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of an automatic verification process or algorithmic quality assurance for validating dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document any quality assurance process for the new dataset derived from ScanNet RGBD inputs; the dataset is preprocessed (e.g., removing redundant frames), but no explicit QA of annotations or data quality is described."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1715-rubric-3",
    "token_usage": {
      "prompt_tokens": 17837,
      "completion_tokens": 494,
      "total_tokens": 18331
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the creation of any new dataset constructed entirely from scratch by human contributors; instead, it uses existing datasets for training and evaluation."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 4.2 and Figure 2; Algorithm 1",
          "reasoning": "The proposed RGBD diffusion model generates new RGBD views through inpainting of partially rendered novel views created by the model's rendering from an intermediate mesh. These generated RGBD images are synthesized entirely by the machine learning model to complete missing parts of the scene, constituting new data originating directly from the model."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translating data from another language using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that data was translated from another language by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5, Dataset; Section 4 (Methodology)",
          "reasoning": "The training and evaluation data are derived from the ScanNet-V2 dataset, which is an existing, pre-processed set of RGBD scenes collected from various sources. The authors collate and aggregate multiple posed RGBD images from this dataset to form the sparse multi-view inputs for their experiments."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4, 4.2 RGBD Diffusion for Rendered View Inpainting",
          "reasoning": "The data used at test time includes intermediate meshes and novel RGBD views that are derived by rendering and back-projecting the original sparse ScanNet inputs, combined with inpainting via the diffusion model. Thus, new RGBD views and 3D meshes are generated through transformations and modifications of existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin and processing of the data used."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1715-rubric-4",
    "token_usage": {
      "prompt_tokens": 18355,
      "completion_tokens": 311,
      "total_tokens": 18666
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5, Experiments; Implementation Details",
          "reasoning": "The ScanNet dataset is used for training the proposed RGBD diffusion model from scratch as stated in Section 5 Experiments. The model is trained on the first 1,293 scenes of ScanNet, indicating the dataset is used for training from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5, Experiments",
          "reasoning": "The ScanNet dataset is used as a benchmark for quantitative and qualitative evaluation of the proposed method, as detailed in Section 5, including metrics such as PSNR, SSIM, LPIPS, and others, demonstrating its role in evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.1, Ablation Studies",
          "reasoning": "The dataset is used in ablation studies to analyze the effects of conditioning, inpainting, and guidance scale on results, as reported in Section 5.1, indicating an analysis use."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1715-rubric-5",
    "token_usage": {
      "prompt_tokens": 19078,
      "completion_tokens": 549,
      "total_tokens": 19627
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes the use of RGBD images and associated data for scene synthesis, but there is no mention of the dataset containing multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset used (ScanNet-V2) and the data introduced involve RGBD images and meshes without any indication of containing two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5 (Experiments) and throughout the paper",
          "reasoning": "The proposed dataset consists of RGBD images of indoor scenes taken from the ScanNet-V2 dataset, which was collected in English-speaking contexts and the paper's descriptions, documentation, and annotations are presented in English. There is no indication of the dataset containing other languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence or mention of dataset entries being in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains pseudocode (Algorithm 1) and mathematical formulas, it does not propose a dataset consisting of code or programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes mathematical expressions and formulas to describe model operations and algorithms, but these are part of the methodology and not entries in the proposed dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or use of biological or non-human communication data in the proposed dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the dataset contains constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is clearly English and well-documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries consist of RGBD images and associated annotations, which contain human language information in English."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1715-rubric-6",
    "token_usage": {
      "prompt_tokens": 16296,
      "completion_tokens": 178,
      "total_tokens": 16474
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "The paper does not provide any explicit link or mention of code availability for the dataset creation or related processing.",
          "reasoning": "The paper mentions a project page for the method (https://jblei.site/proj/rgbd-diffusion) but does not reference any code release or repository for dataset preparation or processing. No sections specify availability of dataset construction code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 5. Experiments, Dataset subsection.",
          "reasoning": "The paper documents the dataset used (ScanNet-V2) along with pre-processing steps such as removal of redundant frames and training/test split details. These descriptions provide some level of dataset preparation transparency, although the dataset itself is pre-existing rather than newly introduced by the authors (the paper introduces a method rather than a new dataset)."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1721-rubric-0",
    "token_usage": {
      "prompt_tokens": 12056,
      "completion_tokens": 249,
      "total_tokens": 12305
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4, Dataset details",
          "Reasoning": "The paper evaluates their model on the VQA-CP v2 dataset, which is a reorganization of the VQA v2 dataset. The VQA-CP v2 dataset consists of images and questions reorganized from VQA v2 dataset, containing approximately 121k images and 438k questions for training. The authors do not state they created new data but only use this reorganized dataset. Since the dataset images originate from human-captured photographs (MS-COCO) and human-generated questions, this data is human generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4, Dataset details",
          "Reasoning": "As with the images, the text data in VQA-CP v2, i.e., questions and answers, are human generated. VQA-CP v2 was created by reorganizing the training and validation sets of the human-generated VQA v2 dataset. The authors use it for evaluation but do not introduce any new dataset or text data."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1721-rubric-1",
    "token_usage": {
      "prompt_tokens": 12856,
      "completion_tokens": 337,
      "total_tokens": 13193
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4, 'Dataset details' and throughout experiments",
            "reasoning": "The paper evaluates the model on the newly introduced VQA-CP v2 dataset, which is a reorganization of the VQA v2 dataset to create a benchmark with distribution changes. The dataset itself is not manually annotated anew but created through reorganization; thus, no human annotation of labels is conducted by the authors. Instead, the annotations (image-question-answer triplets) come from pre-existing datasets. Therefore, annotations are essentially reused from existing datasets, and no new human annotations are introduced or described by the authors themselves in this paper. The evaluation and instance-specific margins are computed automatically via the model and training process, not via manual annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention of annotation instructions for new data in the paper",
            "reasoning": "The paper does not provide any instructions or guidelines for annotators related to VQA-CP v2 or any new dataset. The dataset is a reorganization of existing data; thus no new annotation instructions exist."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No description of scoring rubrics for annotations in the paper",
            "reasoning": "No scoring rubrics or guidelines for annotating or evaluating annotations in the newly introduced data are mentioned, as the paper does not introduce new manual annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples provided related to new dataset annotations",
            "reasoning": "Since no annotation instructions or procedures are described, no examples of such annotations or annotation guidelines are provided for the dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1721-rubric-2",
    "token_usage": {
      "prompt_tokens": 14046,
      "completion_tokens": 304,
      "total_tokens": 14350
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process conducted by a single human expert for any dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process conducted by multiple human experts for any dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model was used to perform quality assurance on dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes the use of automated methods and losses to train the model, but does not describe any automated verification or validation of dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses existing datasets (VQA-CP v2 and VQA v2) and does not introduce any new dataset. Hence, there is no new dataset with annotations or content introduced by the authors that would require a quality assurance process. Consequently, no quality assurance process is described or applied for new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1721-rubric-3",
    "token_usage": {
      "prompt_tokens": 13656,
      "completion_tokens": 465,
      "total_tokens": 14121
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset created from scratch by human contributors. It exclusively uses existing benchmark datasets such as VQA v2 and VQA-CP v2 for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that new data samples are generated entirely by AI or machine learning models. The work focuses on method development and evaluation on existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), Dataset details",
          "reasoning": "The datasets used for evaluation, namely VQA-CP v2 and VQA v2, are existing benchmark datasets. VQA-CP v2 is created by reorganizing the training and validation sets of VQA v2, which itself is built upon MS-COCO images. Therefore, the data is aggregated and rearranged from existing sources, constituting a collated dataset rather than newly created."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), Dataset details",
          "reasoning": "VQA-CP v2 is explicitly described as a dataset derived from VQA v2 by re-splitting training and validation sets to change answer distributions. Hence, it is derived data involving modification of existing data splits and answer distributions."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origins and nature of the datasets used are clearly described; hence, N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1721-rubric-4",
    "token_usage": {
      "prompt_tokens": 14174,
      "completion_tokens": 258,
      "total_tokens": 14432
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 'Experiments' and 'Dataset details'",
          "reasoning": "The paper evaluates its model on the VQA-CP v2 and VQA v2 validation sets, which are used as benchmarks for testing performance, indicating that these datasets serve as evaluation datasets in the study."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 'Ablation studies' and Figure 3 and Figure 4",
          "reasoning": "The datasets are used for ablation studies and analysis of model components, including t-SNE visualization and Spearman's rank correlation of margins, to understand the effect of various training components."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1721-rubric-5",
    "token_usage": {
      "prompt_tokens": 14897,
      "completion_tokens": 263,
      "total_tokens": 15160
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4: Experiments, Dataset details; the datasets VQA-CP v2 and VQA v2 are based on English language questions.",
          "reasoning": "The paper evaluates their method on established datasets VQA-CP v2 and VQA v2, which contain questions and answers only in English. No other languages are mentioned or used in the dataset; hence, the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1721-rubric-6",
    "token_usage": {
      "prompt_tokens": 12115,
      "completion_tokens": 172,
      "total_tokens": 12287
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 1 (Introduction)",
          "reasoning": "The paper states that the code, hyperparameter analyses, and results on multiple datasets are shared as supplementary material, but there is no explicit mention or link provided for code related to dataset construction or dataset generation. Moreover, the datasets used (VQA-CP v2 and VQA v2) are existing benchmark datasets, and no new dataset is introduced by the authors in this paper."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "N/A",
          "reasoning": "The paper does not introduce any new datasets or provide documentation or details about dataset creation process. The datasets used (VQA-CP v2 and VQA v2) are pre-existing benchmark datasets, and the paper only references their existing details without further documentation."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1736-rubric-0",
    "token_usage": {
      "prompt_tokens": 17117,
      "completion_tokens": 146,
      "total_tokens": 17263
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4. Experimental Results, Table 5, and Section 4.4 Few-shot generation",
          "Reasoning": "The paper evaluates their Re-GAN method on few-shot datasets such as Obama, Grumpy Cat, Panda, Animal Face, Oxford Flowers, Cat, Dog, and FFHQ at high resolutions. These datasets are used for training and evaluation in their experiments, indicating that the images come from human-generated data sources (e.g., photographs) and are not generated by models or unknown provenance. The paper explicitly states training on these datasets, indicating their origin as human-generated images."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1736-rubric-1",
    "token_usage": {
      "prompt_tokens": 17917,
      "completion_tokens": 269,
      "total_tokens": 18186
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.4 (Few-shot generation), Section 4 (Experimental Results)",
            "reasoning": "The annotations correspond to training and evaluation on multiple newly introduced datasets such as few-shot datasets (e.g., 100 training images, Animal-Face Dog and Cat, 100-Shot-Obama, Panda, Grumpy-cat, Oxford-flowers, and FFHQ subsets). The paper references training of these datasets and evaluation metrics, which implies annotation was done by human experimenters knowledgeable about GAN training and evaluation, likely experts in computer vision and GANs."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper documents training and evaluation experiments but does not describe any formal annotation instructions given to annotators or experimenters regarding handling or labeling data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "There is no description of scoring rubrics or formal grading criteria for annotations or dataset labelling in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not provide illustrative annotation examples or labelled data samples showing expected annotations or labels from annotators."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1736-rubric-2",
    "token_usage": {
      "prompt_tokens": 19107,
      "completion_tokens": 311,
      "total_tokens": 19418
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving a single human expert annotator for any new dataset introduced."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance carried out by multiple human experts on dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper of quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about quality assurance by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the work involves AI models (GANs), there is no indication that AI models were used specifically for quality assurance or validation of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any automated verification, algorithmic, or rule-based quality assurance of dataset content or annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any new dataset annotation or content quality assurance process or protocol. It mainly uses existing datasets (e.g., CIFAR-10, Tiny-ImageNet, FFHQ, few-shot datasets) without mention of new dataset creation or associated quality control. Therefore, no QA process is applied or documented for datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1736-rubric-3",
    "token_usage": {
      "prompt_tokens": 18717,
      "completion_tokens": 388,
      "total_tokens": 19105
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate creation of any new dataset entirely from scratch by human contributors; all datasets used are standard benchmarks or publicly available datasets."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 4. Experimental Results",
          "reasoning": "The paper introduces Re-GAN, a method to train GAN models capable of generating new synthetic images; these generated images constitute new data produced by AI models rather than collected or translated from existing sources."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of any datasets produced via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets in the paper are generated by machine translation from other languages."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing benchmark datasets (e.g., CIFAR-10, Tiny-ImageNet, FFHQ), but does not introduce new datasets by collating existing data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe producing a new dataset derived or adapted from existing ones; rather, it focuses on generating synthetic images via trained GANs."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data (generated images) is clearly documented as being produced by the proposed generative models."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1736-rubric-4",
    "token_usage": {
      "prompt_tokens": 19235,
      "completion_tokens": 537,
      "total_tokens": 19772
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset introduced by the authors used exclusively for pre-training large models. The datasets mentioned (e.g., CIFAR-10, Tiny-ImageNet, FFHQ) are known existing datasets and used for benchmark training and evaluation."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.4 Few-shot generation; Experiments in Section 4 including Sections 4.2 and 4.4",
          "reasoning": "The paper uses datasets (including few-shot datasets) to train GAN models from scratch without any pre-training. Specifically, it states that Re-GAN can train models from scratch with limited data (e.g., only 100 training images) and improves performance in such scenarios."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset introduced by the authors is used for fine-tuning pre-trained models with supervised methods."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of any RL-based post-training techniques such as RLHF, nor the usage of datasets for such purposes."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses standard, existing datasets (CIFAR-10, Tiny-ImageNet, few-shot datasets etc.) for evaluation and benchmarking, but does not introduce any new datasets intended solely for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper performs analysis and ablation studies on existing datasets; however, it does not introduce new datasets meant only for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use any datasets introduced as a knowledge base for model augmentation or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. All datasets mentioned are pre-existing public datasets used for training or evaluation in the experimental studies."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1736-rubric-5",
    "token_usage": {
      "prompt_tokens": 19958,
      "completion_tokens": 526,
      "total_tokens": 20484
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any proposed dataset containing entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any proposed dataset containing entries with exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4 Experimental Results and throughout the paper",
          "reasoning": "The paper uses and introduces model training on existing benchmark image datasets such as CIFAR-10 and Tiny-ImageNet, which are standard datasets with images and English language labels or metadata. The paper does not indicate any other languages for entries in these datasets, so the dataset entries can be assumed to be monolingual English in terms of associated language content or labeling."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe data in exactly one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no proposed dataset containing programming or code-related content; the code repository shared is for model implementation and not a dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notation appears in the methodology and theoretical explanation but not as part of a dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include entries of constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper specifies the datasets used; no language uncertainty is indicated for dataset entries."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets used include natural image datasets with human language labeling, so language content is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1736-rubric-6",
    "token_usage": {
      "prompt_tokens": 17176,
      "completion_tokens": 214,
      "total_tokens": 17390
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "The paper abstract and main text mention 'Code can be found at this link: https://github.com/IntellicentAI-Lab/Re-GAN' but this code repository is for the code of the GAN training method, not for dataset generation or construction.",
          "reasoning": "The paper provides a GitHub link to the code for the proposed GAN training methodology (Re-GAN) but does not introduce or construct any new datasets themselves. All datasets used are pre-existing standard datasets like CIFAR-10, Tiny-ImageNet, FFHQ, etc. Therefore, there is no code related to new dataset construction provided in the paper."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Throughout the paper, descriptions focus on the method and experiments on existing datasets but do not describe any new dataset creation or construction process.",
          "reasoning": "The paper does not introduce or create any new dataset. It only uses existing datasets for evaluation. Hence, no documentation regarding dataset construction is provided or necessary."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1748-rubric-0",
    "token_usage": {
      "prompt_tokens": 13285,
      "completion_tokens": 115,
      "total_tokens": 13400
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Data Collection",
          "Reasoning": "The MPEblink dataset consists of 686 untrimmed video clips collected from 86 different unconstrained movies, capturing real-world multi-person scenarios. The videos are human recorded as they are extracted from movies which inherently involve human creation and recording. Moreover, extensive manual annotation was performed for face bounding boxes, facial landmarks, and eyeblink events, confirming human involvement in dataset creation."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1748-rubric-1",
    "token_usage": {
      "prompt_tokens": 14085,
      "completion_tokens": 206,
      "total_tokens": 14291
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.3 Data Annotation",
            "reasoning": "Section 3.3 states that human annotators carefully check and correct the annotations generated by the semi-supervised process and define the start and end frames of eyeblink events. It does not specify that the annotators are experts, so multiple human non-experts is the most appropriate category."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.3 Data Annotation",
            "reasoning": "The paper does not mention any detailed annotation instructions provided to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.3 Data Annotation",
            "reasoning": "There is no mention of scoring rubrics or formal evaluation criteria provided in the annotation guidelines."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.3 Data Annotation",
            "reasoning": "The paper does not indicate that annotation examples were provided in the guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1748-rubric-2",
    "token_usage": {
      "prompt_tokens": 15275,
      "completion_tokens": 396,
      "total_tokens": 15671
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that quality assurance was conducted by a single human annotator who is a subject matter expert or a member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that multiple human annotators with subject matter expertise or belonging to the target demographic performed quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert human annotator was responsible for quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.3 Data Annotation",
          "reasoning": "The data annotation process involved human annotators who carefully checked the annotation quality and corrected the errors generated by the automatic method (InsightFace). However, the paper does not specify that these annotators were experts; thus, it is assumed they are non-expert humans who performed quality assurance in a team."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although an AI model (InsightFace) was used to generate initial face bounding boxes and landmarks, this is part of the data annotation pipeline, not a quality assurance judge. There is no indication AI was used to verify annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3 Data Annotation",
          "reasoning": "The initial face bounding boxes and landmark positions were obtained via the automatic face analysis engine InsightFace, and a matching strategy for tracking instances was designed based on bounding box IoU and deep features. These constitute automated processes involved in QA to reduce human workload and ensure correctness before manual checking."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a quality assurance process involving human checking and correction of annotations generated by AI models and automatic matching strategies; thus, QA is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1748-rubric-3",
    "token_usage": {
      "prompt_tokens": 14885,
      "completion_tokens": 460,
      "total_tokens": 15345
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3, 'The MPEblink Benchmark', particularly sections 3.2 'Data Collection' and 3.3 'Data Annotation'",
          "reasoning": "The MPEblink dataset is collected from 686 untrimmed video clips sourced from 86 different unconstrained movies to represent 'in the wild' characteristics. The videos were annotated by human annotators who exhaustively labeled face bounding boxes, facial landmarks, and eyeblink events for multiple persons across the full span of the videos. This process indicates original content created by human contributors, rather than sourced or adapted from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the dataset content (videos or annotations) was generated by AI or machine learning models independently."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that data was produced by translation from another language by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any part of the data was produced via machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.2 'Data Collection'",
          "reasoning": "The dataset videos are collected from existing sources, namely 86 unconstrained movies, without significant modification mentioned. The paper emphasizes the use of diverse unconstrained movies to collect realistic scenarios which indicates aggregation of existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention in the paper of modifying or adapting existing datasets or data sources to create the dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1748-rubric-4",
    "token_usage": {
      "prompt_tokens": 15403,
      "completion_tokens": 341,
      "total_tokens": 15744
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5.1, Section 4 (Method)",
          "reasoning": "The paper describes using the proposed MPEblink dataset to train their InstBlink model from scratch with supervised learning. Specifically, Section 4 details the model training procedure using MPEblink, and Section 5.1 reports experimental results of training methods on this dataset, showing its usage for training the model from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1 (Benchmark Results on MPEblink Dataset)",
          "reasoning": "The authors conduct benchmark experiments on the MPEblink dataset to evaluate the effectiveness and efficiency of different eyeblink detection methods, including their own InstBlink, as detailed in Section 5.1."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.3 (Ablation Study) and Section 5.4 (Qualitative Analysis)",
          "reasoning": "The dataset is used to perform thorough analysis including ablation studies on model components, spatial and temporal modeling, inference speed, and qualitative visualization of model outputs, demonstrating the dataset's role in analyzing challenges and model behavior."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1748-rubric-5",
    "token_usage": {
      "prompt_tokens": 16126,
      "completion_tokens": 612,
      "total_tokens": 16738
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the MPEblink dataset contains samples in multiple human languages. The dataset is derived from unconstrained movies but no specific information about language diversity or multiple human languages per entry is provided."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset includes exactly two human languages per entry or that it is bilingual in nature."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the dataset content is only in English or that it is limited to English language data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that the dataset is composed of entries with a single non-English language content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The MPEblink dataset comprises untrimmed videos of people eyeblinking for computer vision tasks. There is no indication that programming code or structured code-related content are included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper contains mathematical definitions and notations describing the task and model, the dataset itself (untrimmed videos and annotations of eyeblinks) does not consist of mathematical or logical symbolic content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human videos with annotated eyeblink events, which are human biological signals, but the entries themselves are video data and annotations, not biological sequences like DNA or animal communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or artificially created languages being present in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly describes the dataset content as untrimmed video clips from unconstrained movies with annotated human eyeblink events. There is no indication that the language content is unspecified or undocumented."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The dataset contains video data of people and their eyeblink events, which are visual and temporal annotations without spoken or written language content. Therefore, the dataset entries do not contain any human language, so this category applies."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1748-rubric-6",
    "token_usage": {
      "prompt_tokens": 13344,
      "completion_tokens": 242,
      "total_tokens": 13586
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention of code availability related to dataset construction in the paper.",
          "reasoning": "The paper provides a GitHub link (https://github.com/wenzhengzeng/MPEblink) for the proposed multi-person eyeblink detection method implementation but does not explicitly state or link any code for the dataset collection, annotation, or construction process. Therefore, it cannot be inferred that the code used for constructing the dataset is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (The MPEblink Benchmark), specifically Sections 3.1 to 3.4",
          "reasoning": "The paper details the dataset creation process comprehensively in Section 3, including the task formulation (3.1), data collection procedure (3.2) describing source (86 unconstrained movies), statistics, and scenario diversity, annotation method (3.3) describing the semi-supervised annotation assisted by the InsightFace engine and manual correction for face bounding boxes, landmarks, and eyeblink events, and dataset statistics (3.4). This thorough documentation supports reproducibility and downstream usage, even though code is not provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1751-rubric-0",
    "token_usage": {
      "prompt_tokens": 11058,
      "completion_tokens": 287,
      "total_tokens": 11345
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 4 (Hardware Setup & Data Collection Pipeline)",
          "Reasoning": "The dataset contains 150,000 real impact sound recordings of 50 everyday objects, collected by physically striking real objects with an impact hammer and recording the resulting sounds with calibrated microphones in an acoustically treated room, as described in the abstract and detailed in Section 4."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 4.1 and 4.3 (Hardware Setup & Data Collection Pipeline)",
          "Reasoning": "Alongside audio recordings, synchronized RGBD images of the objects are captured from multiple positions using a RealSense D415 camera aligned with the microphone array, indicating the images are human-captured data, not generated, as described in the abstract and data collection pipeline."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 4.1 (Hardware Setup)",
          "Reasoning": "The dataset includes contact force profiles from the impact hammer's calibrated force transducer, which measures the contact force during each impact at the same sampling rate as audio, representing sensor data collected through human-instrumented measurement setups described in the hardware setup."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1751-rubric-1",
    "token_usage": {
      "prompt_tokens": 11858,
      "completion_tokens": 263,
      "total_tokens": 12121
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.3 Data Collection Pipeline",
            "reasoning": "The annotations such as impact locations, microphone locations, contact force profiles, material labels, and RGBD images are recorded automatically by the hardware setup and gantry system as described in Section 4.3. The impact points are selected as vertices on a virtual mesh, microphone positions are controlled by the gantry, and force profiles are measured by the impact hammer's calibrated force transducer. There is no mention of human annotators performing manual annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not mention any detailed annotation instructions provided to annotators, as the annotations are acquired by an automated measurement system rather than by human annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No scoring rubrics are mentioned in the paper for the annotations because the annotations are automatically recorded measurements rather than subjective or scored evaluations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No annotation examples or illustrative examples are provided for annotations since annotations consist of objective measurements (impact points, microphone poses, force measurements, labels) captured by the automated hardware setup."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1751-rubric-2",
    "token_usage": {
      "prompt_tokens": 13048,
      "completion_tokens": 305,
      "total_tokens": 13353
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating multiple human experts were involved in quality assurance of the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert human annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset annotations or content are not described as being validated by an AI model acting as a judge."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While some signal processing and deconvolution methods are applied to the audio signals during processing, these do not constitute a quality assurance process for validating dataset annotations or content. The paper does not describe any automated verification methods used specifically for quality assurance of the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any explicit quality assurance process conducted to validate the dataset annotations or content beyond the reported repeatability tests of measurement equipment and procedures. No human or automated QA procedures aimed at verifying annotation correctness or data consistency are documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1751-rubric-3",
    "token_usage": {
      "prompt_tokens": 12658,
      "completion_tokens": 383,
      "total_tokens": 13041
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4 (The REALIMPACT Dataset), throughout",
          "reasoning": "The dataset consists of 150,000 real-world recordings of impact sounds from 50 everyday objects collected using a novel, automated hardware setup involving human design and physical collection in a controlled environment. The data were generated by physically striking objects with an impact hammer and recording the resulting sounds and annotations in a lab, thus constituting original data created from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not generated by AI or machine learning models; rather, it consists of real physical recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involved translation from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence of machine translation used in producing the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not aggregated from existing sources without modification; it is new data collected specifically for this paper."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data do not result from adapting or transforming existing datasets; they are original recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is explicitly documented and well-specified."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1751-rubric-4",
    "token_usage": {
      "prompt_tokens": 13176,
      "completion_tokens": 486,
      "total_tokens": 13662
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the REALIMPACT dataset for pre-training models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report training any models from scratch using the REALIMPACT dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.2 and 5.3",
          "reasoning": "The dataset is used to train supervised learning models such as a ResNet-18 for listener location classification (Section 5.2) and an audio-visual network for visual acoustic matching (Section 5.3), which involve supervised fine-tuning on the labeled dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or demonstration of reinforcement learning based post-training methods using the REALIMPACT dataset is made in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1",
          "reasoning": "The dataset serves as a benchmark for evaluating existing physics-based sound simulation methods by comparing simulated sounds against the real recorded impact sounds."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 4.5 and 5.1",
          "reasoning": "The dataset is analyzed to understand spatial sampling effects, measurement repeatability, and discrepancies between simulation and real recordings, demonstrating its use for analyzing trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that REALIMPACT serves as a knowledge base for augmenting models such as retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents clear, practical usages of the dataset in supervised fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1751-rubric-5",
    "token_usage": {
      "prompt_tokens": 13899,
      "completion_tokens": 473,
      "total_tokens": 14372
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains recordings of impact sounds of objects and their annotations. There is no mention of multiple human languages being present in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain exactly two human languages; it primarily contains audio recordings of impact sounds and associated metadata, not human language data."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as containing English text or language data; it contains audio recordings of object sounds and metadata, but no English language content is indicated."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English human language content is described as part of the dataset entries."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio recordings and associated annotations such as impact location, microphone location, force profiles, images, and material labels. There is no indication that the dataset includes any programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical equations describing physics-based sound synthesis, the dataset itself does not contain entries with mathematical or logical expressions. The dataset consists of real-world recorded sounds and metadata."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on sounds made by physical impacts on everyday objects, not biological sequences or non-human communication such as animal vocalizations or DNA sequences."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or artificially constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) of dataset entries are explicit; however, the dataset consists of audio impact sounds and annotations not involving human languages, so this category is not applicable."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The dataset consists of audio recordings of physical object impact sounds and related metadata such as impact location and force signals. It does not contain any human language or linguistic content in its entries. Therefore, the dataset does not contain any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1751-rubric-6",
    "token_usage": {
      "prompt_tokens": 11117,
      "completion_tokens": 162,
      "total_tokens": 11279
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "-",
          "reasoning": "The paper does not include any mention or link to publicly available code or repositories related to the data collection, preprocessing, or generation of the REALIMPACT dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 (The REALIMPACT Dataset), subsections 4.1 (Hardware Setup), 4.3 (Data Collection Pipeline), and 4.4 (Processing)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including the hardware setup, the automated striking mechanism, the gantry system for precise microphone placement, the objects selected, and the data collection and processing pipeline. This extensive description allows for understanding and potential replication of the data collection methodology."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1764-rubric-0",
    "token_usage": {
      "prompt_tokens": 9381,
      "completion_tokens": 108,
      "total_tokens": 9489
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2 Datasets",
          "Reasoning": "The paper describes evaluation using three standard datasets (Synthetic, LLFF, and Tanks and Temples) which are pre-existing datasets. The paper does not mention that the authors introduced any new dataset themselves. No new dataset is introduced or created by the authors in the paper. Therefore, there are no new data sources introduced by the authors to list."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1764-rubric-1",
    "token_usage": {
      "prompt_tokens": 10181,
      "completion_tokens": 254,
      "total_tokens": 10435
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.2 Datasets",
            "reasoning": "The paper introduces experiments conducted on three standard datasets: Synthetic [28], LLFF [27], and Tanks and Temples (T&T) [21]. These datasets are pre-existing and publicly available, as indicated by the citations and standard usage. There is no mention in the paper of creation or annotation of a new dataset introduced by the authors. The paper focuses on proposing a novel method (Ref-NPR) and evaluating it on existing datasets without describing any new dataset collection or annotation process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.2 Datasets",
            "reasoning": "No new dataset is introduced in the paper, and therefore no annotation instructions are provided or discussed."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.2 Datasets",
            "reasoning": "Since no new dataset is introduced nor annotated in the work, there are no scoring rubrics mentioned."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.2 Datasets",
            "reasoning": "No annotation examples are described or provided in the context of a new dataset in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1764-rubric-2",
    "token_usage": {
      "prompt_tokens": 11371,
      "completion_tokens": 306,
      "total_tokens": 11677
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance being performed by a single human expert for the introduced datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance involving multiple human experts for the introduced datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating quality assurance was conducted by a single non-expert annotator for the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by multiple non-expert human annotators for the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that AI models were used as judges to conduct quality assurance on the new datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic quality assurance process applied to the new datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces stylized radiance fields and uses existing standard datasets (Synthetic, LLFF, Tanks and Temples) for evaluation. However, it does not document or discuss any explicit quality assurance process related to the introduced stylized dataset or the generated stylized views. Therefore, no quality assurance process is applied or documented for the new datasets or stylizations."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1764-rubric-3",
    "token_usage": {
      "prompt_tokens": 10981,
      "completion_tokens": 319,
      "total_tokens": 11300
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The datasets used for evaluation (Synthetic [28], LLFF [27], and Tanks and Temples [21]) are pre-existing datasets collected and aggregated from existing sources. The paper employs these datasets without indication of significant modification, as stated in Section 4.2: 'We evaluate the performance of Ref-NPR on three standard datasets... We follow the official training and testing splits for all datasets.' Therefore, the data can be considered collated."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper does not introduce any new datasets; it uses pre-existing datasets for evaluation, so the data origin is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1764-rubric-4",
    "token_usage": {
      "prompt_tokens": 11499,
      "completion_tokens": 262,
      "total_tokens": 11761
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "The datasets Synthetic [28], LLFF [27], and Tanks and Temples (T&T) [21] are used in the experiments section exclusively for evaluation and benchmarking the performance of the proposed Ref-NPR method. The paper explicitly reports quantitative and qualitative results on these datasets, but none of them are introduced by the authors as new datasets."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new datasets are introduced by the authors in the paper; only existing datasets are used for evaluation purposes. Therefore, no practical usage of new datasets described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1764-rubric-5",
    "token_usage": {
      "prompt_tokens": 12222,
      "completion_tokens": 303,
      "total_tokens": 12525
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, including Abstract and Sections 1 and 4.2",
          "reasoning": "The dataset consists primarily of images and stylized views with English language used exclusively for all accompanying text, explanations, and descriptions. No other human languages are mentioned or used in the dataset descriptions or annotations."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3.1 'Preliminary: Radiance Field Rendering' and Section 3.3 'Template-Based Semantic Correspondence'",
          "reasoning": "The paper includes mathematical formulas describing the radiance field rendering equations, feature correspondence functions, and optimization losses, which form part of the dataset's formal description for ray registration and style transfer."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1764-rubric-6",
    "token_usage": {
      "prompt_tokens": 9440,
      "completion_tokens": 154,
      "total_tokens": 9594
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 4.1",
          "reasoning": "The abstract states that the code and data are publicly available on the project page (https://ref-npr.github.io). In Section 4.1, the implementation details mention the use of the ARF codebase and Plenoxels, implying the code for the method and datasets is accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.2",
          "reasoning": "Section 4.2 provides clear documentation of the datasets used, describing the Synthetic, LLFF, and Tanks and Temples datasets, their characteristics, and the preprocessing steps applied for the experiments, indicating transparent dataset information and construction process."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1771-rubric-0",
    "token_usage": {
      "prompt_tokens": 10829,
      "completion_tokens": 271,
      "total_tokens": 11100
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.2, Table 3",
          "Reasoning": "The paper introduces new datasets in the form of image recognition benchmarks used to validate their proposed models R-PolyNets and D-PolyNets. These datasets are well-known image datasets such as CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet, ImageNet, and Oxford 102 Flowers. The data consists of images collected with human involvement (photographed natural images) and annotated for classification tasks. Since the paper evaluates on these datasets to demonstrate their models' performance and these datasets are not created by the authors but rather standard benchmarks, they do not constitute new datasets introduced by the authors."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.4, Table 7",
          "Reasoning": "The paper evaluates the proposed models on the Speech Commands dataset for audio classification to showcase model generalization to audio domain. The dataset consists of audio recordings of human voice commands recorded by humans. As it is an established dataset, it is not newly introduced by the authors. The paper does not state that the authors collected or created any new audio datasets."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1771-rubric-1",
    "token_usage": {
      "prompt_tokens": 11629,
      "completion_tokens": 277,
      "total_tokens": 11906
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 5 (Experiments), especially 5.1 Ablation Study, and 5.2 Image Classification",
            "reasoning": "The paper's experimental section describes training and evaluation of the proposed R-PolyNets and D-PolyNets on multiple datasets, e.g., Cifar-10, Cifar-100, STL-10, Tiny ImageNet, Speech Commands, and Oxford 102 Flowers. These datasets (and their annotations) are pre-existing standard benchmarks. There is no indication in the paper of a newly introduced dataset specifically created or annotated by the authors for this work. The authors perform training and evaluation on existing datasets rather than collecting or annotating a new dataset. Hence, no new dataset annotation is performed or described by any annotator."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "Since no new dataset annotation is introduced, no annotation instructions are provided in the paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "There are no rubrics or criteria described for annotation scoring since no new dataset annotation is introduced."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No annotation examples are provided as the paper does not introduce a new annotated dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1771-rubric-2",
    "token_usage": {
      "prompt_tokens": 12819,
      "completion_tokens": 288,
      "total_tokens": 13107
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on any dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any quality assurance process involving multiple human experts for dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper of quality assurance by a single non-expert on dataset annotations or content."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving multiple non-expert annotators for dataset annotations or content."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that quality assurance was performed by an AI model judging the dataset quality or annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses algorithmic methods and models for training and evaluation, it does not describe any automated process specifically for quality assurance of dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses standard, widely known public datasets for evaluation and does not introduce any new dataset. Hence, no quality assurance process for dataset annotations or content is applied or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1771-rubric-3",
    "token_usage": {
      "prompt_tokens": 12429,
      "completion_tokens": 368,
      "total_tokens": 12797
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets created entirely from scratch by human contributors. Instead, it uses existing benchmark datasets for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets generated entirely by AI or machine learning models are introduced in the paper; all datasets used are pre-existing."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset was generated through machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets employed are well-known public datasets used for benchmarking; the paper does not mention collating or aggregating data from existing sources to form new datasets."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper reports experiments on existing standard image and audio recognition datasets; no new derived datasets created by modifying or adapting existing data are presented."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. It only uses pre-existing public datasets for benchmarking, without documentation of any dataset creation or modification."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1771-rubric-4",
    "token_usage": {
      "prompt_tokens": 12947,
      "completion_tokens": 466,
      "total_tokens": 13413
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention or describe the use of any new dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper evaluates polynomial networks and their variants using existing benchmark datasets; there is no mention of introducing and using a newly introduced dataset for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any new dataset introduced by the authors is used specifically for fine-tuning pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss reinforcement learning post-training techniques or use of any dataset for such purpose."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses multiple well-known existing datasets for evaluation and benchmarking, but does not introduce any new dataset solely for evaluation purposes."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper performs ablation studies and analyzes performance trends using standard datasets rather than any new dataset introduced for the purpose of analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is introduced to serve as a knowledge base for retrieval or augmentation tasks."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new datasets are introduced by the authors in the paper. All datasets mentioned (e.g., Cifar-10, Cifar-100, ImageNet, STL-10, Tiny ImageNet, Speech Commands, Oxford Flowers) are pre-existing benchmark datasets. Therefore, no practical usage of new datasets is applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1771-rubric-5",
    "token_usage": {
      "prompt_tokens": 13670,
      "completion_tokens": 568,
      "total_tokens": 14238
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes datasets used for image and audio recognition, none of which are described as containing entries in multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any proposed dataset contains exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5 (Experiments generally), and implied through dataset descriptions such as ImageNet, CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet, Speech Commands",
          "reasoning": "The proposed datasets are derived from standard vision and audio benchmarks like ImageNet, CIFAR, Speech Commands, Oxford 102 Flowers, which contain images/audio data labeled/classified with English-language categories or labels. The text and annotations, such as dataset or class names, are in English. No other human languages are mentioned for the datasets."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset is described as containing data in any non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses polynomial networks, mathematical equations, and implementation details, it does not propose or include any dataset entries containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notation is used extensively to describe models, but the datasets themselves do not contain mathematical or formal logical expressions as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets involving biological sequences or non-human communication are introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets containing constructed or fictional languages are introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of the datasets are clear and specified as English; no ambiguity is presented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Datasets consist of images or audio with labels in English, so they do contain language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1771-rubric-6",
    "token_usage": {
      "prompt_tokens": 10888,
      "completion_tokens": 201,
      "total_tokens": 11089
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and main text",
          "reasoning": "The paper provides a link to the source code of their polynomial networks model at https://github.com/grigorisg9gr/regularized_polynomials for reproducing their model and training. However, the paper does not introduce any new datasets themselves; they use existing public benchmark datasets such as CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet, ImageNet, Speech Commands, and Oxford Flowers 102. Since no new datasets are introduced by the authors, there is no code related to dataset construction or generation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Throughout the paper",
          "reasoning": "The paper does not describe or document the process of constructing or collecting any new datasets. It only evaluates on existing established public datasets with standard splits and references. Since no new datasets are introduced, there is no dataset creation process to document in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-181-rubric-0",
    "token_usage": {
      "prompt_tokens": 11824,
      "completion_tokens": 107,
      "total_tokens": 11931
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4 Experiments, Dataset paragraph",
          "Reasoning": "The paper states that the experiments are conducted on the nuScenes dataset, which is a large-scale outdoor dataset collected with diverse sensors including cameras, Lidar, and Radar. The data consists of sensor signals collected from real-world driving scenarios, thus human-generated in the sense of recorded sensor data from human-driven vehicles during data collection."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-181-rubric-1",
    "token_usage": {
      "prompt_tokens": 12624,
      "completion_tokens": 229,
      "total_tokens": 12853
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4, Dataset subsection",
            "reasoning": "The ground-truth BEV semantic and velocity labels are generated from the map annotation, bounding boxes, and sensor calibration matrices provided by the nuScenes dataset. The paper describes a process to project bounding boxes onto BEV maps and assign labels at pixel-level automatically, indicating an automatic process rather than manual human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4, Dataset subsection",
            "reasoning": "The paper does not mention any human annotators or instructions provided for manual annotation. Instead, the annotations are derived from existing annotations and are generated automatically."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4, Dataset subsection",
            "reasoning": "There is no mention of any scoring rubrics used or provided for annotation. The automatic annotation process uses existing dataset annotations and not subjective evaluation criteria."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire Paper",
            "reasoning": "No annotation examples or guidelines for human annotators are described or included in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-181-rubric-2",
    "token_usage": {
      "prompt_tokens": 13814,
      "completion_tokens": 389,
      "total_tokens": 14203
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided indicating that multiple human experts performed quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance performed by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of quality assurance performed by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using an AI model for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an automated process or algorithmic verification was used for dataset quality assurance; the paper does not describe any such processes."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses the publicly available nuScenes dataset for experiments and generates ground-truth BEV semantic and velocity labels using dataset annotations, bounding boxes, maps, and calibration matrices; however, the authors do not describe performing any quality assurance or validation of these annotations. No new datasets with annotations or manual labeling are introduced by the authors that require quality assurance."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-181-rubric-3",
    "token_usage": {
      "prompt_tokens": 13424,
      "completion_tokens": 376,
      "total_tokens": 13800
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4, Dataset",
          "reasoning": "The paper uses the nuScenes dataset, which is a large-scale autonomous driving dataset collected under various conditions. The dataset contains images, Radar point clouds, and Lidar scans with annotations, and the authors generate ground-truth BEV semantic and velocity labels from these existing annotations and sensor calibration data. Thus, the data is collected from an existing source without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4, Dataset and Evaluation",
          "reasoning": "The authors generate derived ground-truth BEV semantic segmentation and velocity flow labels by processing existing annotated bounding boxes, map annotations, and sensor calibration data from the nuScenes dataset. This involves transforming 3D bounding boxes and velocity information into BEV representations and pixel-wise velocity maps. This process involves modifications and adaptations of existing annotated data, thus constituting derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is specified clearly via the use and processing of the nuScenes dataset."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-181-rubric-4",
    "token_usage": {
      "prompt_tokens": 13942,
      "completion_tokens": 387,
      "total_tokens": 14329
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments) and subsections 4.1, 4.2, and 4.3",
          "reasoning": "The paper uses the nuScenes dataset for evaluation of the proposed BEVGuide method on various tasks including BEV semantic segmentation, velocity estimation, and 3D detection. The ground-truth BEV semantic and velocity labels are generated from the dataset annotations and used to benchmark and compare performance of BEVGuide against baselines. The dataset is not introduced by the authors, so not considered here."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3 (Ablation and Analysis) and 4.1, 4.2, 4.3",
          "reasoning": "The authors analyze performance trends in different sensor combinations, weather, lighting, and sensor failure scenarios using the nuScenes dataset. These analyses provide insights into robustness and efficiency of the BEVGuide model. However, the nuScenes dataset is not new, so these analyses are not on a new dataset introduced by this paper."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new dataset is introduced by the authors in this paper. The paper exclusively uses the pre-existing nuScenes dataset to train and evaluate their BEVGuide model. Therefore, there is no practical usage to assess for a new dataset."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-181-rubric-5",
    "token_usage": {
      "prompt_tokens": 14665,
      "completion_tokens": 525,
      "total_tokens": 15190
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only discusses the nuScenes dataset and data in the context of autonomous driving perception tasks, without mentioning any human language content or multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the dataset entries contain exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4 Experiments, Dataset description",
          "reasoning": "The dataset used, nuScenes, is an autonomous driving dataset that contains sensor data (camera images, lidar, radar) collected with English annotations and labels. The paper does not mention any language other than English in annotations or content, thus it can be considered monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any non-English textual data or annotation."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset described consists of sensor data and annotations relevant to autonomous driving but does not contain programming or code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains formulas and mathematical notation describing the method, the dataset itself does not contain mathematical or logical symbolic expressions as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains sensor data related to autonomous driving and does not include biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset annotations is known and English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain linguistic content in English (annotations, labels), so it is not devoid of language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-181-rubric-6",
    "token_usage": {
      "prompt_tokens": 11883,
      "completion_tokens": 232,
      "total_tokens": 12115
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not provide any link or mention of publicly available code related to dataset construction, data preprocessing, or generation for the BEV semantic or velocity labels derived from the nuScenes dataset. The project page link provided (https://yunzeman.github.io/BEVGuide) appears to be for the model code and results, but there is no indication that code for dataset construction or label generation is made available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4, Dataset subsection",
          "reasoning": "The paper documents the dataset used for experiments as the nuScenes dataset, specifying the number of annotated samples, sensor types, and the training/validation splits. It further details how ground-truth BEV semantic and velocity labels are generated from map annotations, bounding boxes, and sensor calibration matrices provided by nuScenes. The description of the label generation process for BEV semantic segmentation and velocity flow estimation is detailed in Section 4, under 'Dataset' and 'Evaluation' subsections, indicating transparency and sufficient documentation of the data creation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-183-rubric-0",
    "token_usage": {
      "prompt_tokens": 9943,
      "completion_tokens": 166,
      "total_tokens": 10109
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset",
          "Reasoning": "The paper introduces evaluation on the nuScenes dataset which contains surround-view camera images collected at 1600x900 resolution. These camera images are captured by human-operated sensors as part of real autonomous driving data collection."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset",
          "Reasoning": "The nuScenes dataset also includes LiDAR data collected by sensors mounted on vehicles. LiDAR point clouds are human-operated sensor data capturing 3D environment information, used for the LiDAR-guided sampling strategy of local BEV slices in the proposed method."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-183-rubric-1",
    "token_usage": {
      "prompt_tokens": 10743,
      "completion_tokens": 236,
      "total_tokens": 10979
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1 Dataset",
            "reasoning": "The paper utilizes the nuScenes dataset for evaluation, which is a pre-existing dataset. The paper does not mention introducing or collecting any new datasets, nor any new annotations. Thus, annotations are from the nuScenes dataset, which is publicly available and annotated by experts as part of nuScenes. Hence, the annotated data is from an existing dataset."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not describe any annotation instructions provided for the data annotators, as it uses an existing dataset (nuScenes). No new annotation process or instructions were introduced by this work."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "Since there is no new annotation process or scoring introduced by the paper, rubrics for annotation do not apply here."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No examples of annotation guidelines or annotated samples are provided in the paper, as no new dataset is introduced or annotated."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-183-rubric-2",
    "token_usage": {
      "prompt_tokens": 11933,
      "completion_tokens": 264,
      "total_tokens": 12197
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human annotator expert on the dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by multiple human experts on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model was used as a judge for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification process or algorithmic/rule-based techniques applied to quality assure the dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses the publicly available nuScenes dataset for evaluation and does not introduce any new dataset. Therefore, no quality assurance process related to dataset annotation is described or applied by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-183-rubric-3",
    "token_usage": {
      "prompt_tokens": 11543,
      "completion_tokens": 362,
      "total_tokens": 11905
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report creation of any entirely new dataset created from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated entirely by AI or machine learning models for the purpose of training or evaluation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data was produced by translating content from another language via human translators as per the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data was generated by machine translation methods as stated in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper uses the nuScenes dataset, which is an existing large-scale autonomous driving dataset collected from sensors including cameras and LiDAR. The authors did not create a new dataset but conducted experiments on this pre-existing compiled dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creation of new data derived by modification or transformation of existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the dataset used is clearly specified as nuScenes, an existing dataset."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-183-rubric-4",
    "token_usage": {
      "prompt_tokens": 12061,
      "completion_tokens": 228,
      "total_tokens": 12289
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper extensively uses the nuScenes dataset for evaluation and analysis of the proposed BEV Slice Attention Network (BEV-SAN) method, but it does not introduce any new dataset of its own. All references to datasets are to pre-existing datasets, particularly nuScenes. Thus, there is no new dataset introduced or used for any training, fine-tuning, evaluation, or analysis described in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-183-rubric-5",
    "token_usage": {
      "prompt_tokens": 12784,
      "completion_tokens": 508,
      "total_tokens": 13292
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset; Throughout the paper",
          "reasoning": "The dataset used is the nuScenes dataset, which is described in English and all documentation, annotations, metrics, and training details are presented solely in English throughout the paper. There is no mention of any other human language in the dataset or annotations."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the method involves code-related components such as algorithms and implementation details, the paper does not introduce any new dataset entries containing programming or structured code-related content per se."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3. Methods, e.g., Sections 3.1 and 3.2",
          "reasoning": "The dataset inputs and features include numerical tensors and mathematical notations describing feature dimensions, convolutional feature shapes, and operations such as attention mechanisms. These are mathematical expressions present in the description of the method and data representation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset and paper content do not contain any biological or non-human communication system data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are present in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as English in all documentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language (English) as part of its annotations and documentation."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-183-rubric-6",
    "token_usage": {
      "prompt_tokens": 10002,
      "completion_tokens": 145,
      "total_tokens": 10147
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and main text",
          "reasoning": "The paper mentions that code will be released, however, there is no explicit link or reference to a publicly accessible repository for code related to dataset construction or preprocessing. Also, the code referenced appears to relate to the model rather than dataset construction."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 4.1 Dataset and throughout the paper",
          "reasoning": "The paper uses the nuScenes dataset, which is a pre-existing dataset; no new datasets are introduced by the authors. The authors do not describe creating or collecting any new dataset; hence there is no dataset creation documentation provided in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1856-rubric-0",
    "token_usage": {
      "prompt_tokens": 12095,
      "completion_tokens": 152,
      "total_tokens": 12247
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data Distribution, Section 3.3 Self-supervised Learning Stage, and throughout the Method and Experiment sections",
          "Reasoning": "The paper introduces a new dataset setting consisting of a few labeled dynamic and static Low Dynamic Range (LDR) images and unlabeled dynamic LDR images. These datasets consist of multiple LDR images captured with different exposure levels (-2, 0, +2) and their corresponding HDR ground truth for labeled samples. The data originates from real-world photography sensors capturing dynamic scenes, involving human operation of cameras, thus the data modality is 'image' and is human-generated."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1856-rubric-1",
    "token_usage": {
      "prompt_tokens": 12895,
      "completion_tokens": 263,
      "total_tokens": 13158
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3 and 3.2",
            "reasoning": "The paper describes the Saturated Mask AutoEncoder (SMAE) and the sample-quality-based iterative semi-supervised learning framework for training the model using self-supervision and pseudo-labels generated by the model itself. Therefore, the data labeling (annotation) process for the new dataset is done automatically via the model and its training procedure, rather than by humans manually annotating the data."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit section; implicit from overall training procedure.",
            "reasoning": "There are no detailed annotation instructions described for human annotators, as the annotations (pseudo-labels) are generated by models automatically."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit section; no rubric described.",
            "reasoning": "The paper does not mention any scoring rubrics provided for annotation or pseudo-label selection; it only describes an adaptive pseudo-label selection strategy algorithmically."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No mention in the text or appendix.",
            "reasoning": "The paper does not include explicit annotation guideline examples as the pseudo-labels are produced by the model during training rather than by human annotation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1856-rubric-2",
    "token_usage": {
      "prompt_tokens": 14085,
      "completion_tokens": 318,
      "total_tokens": 14403
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving a single human expert for the new dataset. There is no mention of human expert annotation or validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance for the dataset. No such expert validation process is described."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human non-expert annotator for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of quality assurance by multiple non-expert annotators in the paper for the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The QA process does not involve an AI model serving as judge or validator of annotations or dataset quality."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe automated verification or rule-based QA processes for dataset annotation or validation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces datasets used for training and evaluation but does not introduce new datasets or describe any quality assurance processes applied to any new datasets. The datasets mentioned (Kalantari\u2019s, Hu\u2019s, Tursun\u2019s, Prabhakar\u2019s) are pre-existing public datasets. No QA details for new datasets are provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1856-rubric-3",
    "token_usage": {
      "prompt_tokens": 13695,
      "completion_tokens": 476,
      "total_tokens": 14171
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing public datasets (e.g., Kalantari\u2019s, Hu\u2019s, Tursun\u2019s, Prabhakar\u2019s) for evaluation, but does not describe creating any new datasets or new data collected entirely from scratch by humans."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors generate pseudo-labels for unlabeled data using their model, these pseudo-labels are generated as part of training but do not constitute a new dataset introduced by the authors."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data generated by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data produced via machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1, Section 3.1, and throughout the paper discussion on datasets",
          "reasoning": "The authors use existing public HDR imaging datasets (Kalantari\u2019s, Hu\u2019s, Tursun\u2019s, Prabhakar\u2019s) which are publicly available and are aggregated from previous works without modification. They use these datasets as-is for training, validation and evaluation."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Data Distribution and Section 3.3 Self-supervised Learning Stage",
          "reasoning": "The paper describes transforming existing LDR images from the datasets by exposure adjustment and gamma correction to produce input data for their self-supervised training. Additionally, pseudo-labels are generated from unlabeled data during semi-supervised training, which represents derived data based on transformations of existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources and generation methods are sufficiently documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1856-rubric-4",
    "token_usage": {
      "prompt_tokens": 14213,
      "completion_tokens": 499,
      "total_tokens": 14712
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The paper introduces the Saturated Mask AutoEncoder (SMAE) as a self-supervised pre-training model to learn a robust representation and reconstruct non-saturated HDR images from masked LDR input patches without using ground truth HDR labels, thus exclusively used for pre-training."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training models from random initialization on any newly introduced dataset; instead, it uses SMAE pre-training followed by fine-tuning."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 and 3.3",
          "reasoning": "The paper describes fine-tuning the pretrained SMAE model using a few labeled HDR samples in a supervised manner during the second stage of the proposed semi-supervised learning framework."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or description of reinforcement learning or RL-based post-training methods like RLHF using the introduced datasets."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are used primarily for training and fine-tuning purposes rather than exclusively for evaluation; although evaluation is performed, the datasets themselves are not introduced as new datasets solely for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using any newly introduced datasets for analysis apart from their role in training or fine-tuning."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as knowledge bases for retrieval-augmented generation or other augmentation techniques."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes the use of the new dataset(s) for pre-training and fine-tuning stages, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1856-rubric-5",
    "token_usage": {
      "prompt_tokens": 14936,
      "completion_tokens": 319,
      "total_tokens": 15255
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, e.g., Abstract, Introduction, and Sections 3 and 4",
          "reasoning": "The paper and the description of the datasets and methods are exclusively in English, with no mention of any other human languages being part of the dataset entries. The datasets consist of images and related metadata with annotations but no textual data in other languages. All dataset descriptions and experimental discussions are in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The primary data in the proposed new datasets introduced, which are sets of multi-exposure images with ground truths, do not contain any human language text. They are photographic image data and numerical metadata without any textual linguistic content. Therefore, the dataset entries themselves do not contain language, only image data."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1856-rubric-6",
    "token_usage": {
      "prompt_tokens": 12154,
      "completion_tokens": 168,
      "total_tokens": 12322
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not mention providing or linking to any code repository or code files related to data collection, preprocessing, or dataset generation. The focus is on the method and training using existing datasets, and no explicit statement or link for the dataset construction code is given."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not describe the creation process of any new dataset introduced by the authors. The datasets used for training and evaluation are existing public datasets (Kalantari's, Hu's, Tursun\u2019s, and Prabhakar\u2019s), and the paper does not present or document any novel dataset creation or curation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1861-rubric-0",
    "token_usage": {
      "prompt_tokens": 11598,
      "completion_tokens": 126,
      "total_tokens": 11724
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5 Experiments Dataset",
          "Reasoning": "The paper introduces a new dataset comprised of challenging real-world scenes containing views of the same scene with and without a target object for the purpose of evaluating 3D scene inpainting methods. These images were captured and annotated with human involvement, including manual object mask annotations, as explicitly stated in Section 5 'we introduce a dataset containing 10 real-world forward-facing scenes with human annotated object masks', and the images are real captures of scenes."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1861-rubric-1",
    "token_usage": {
      "prompt_tokens": 12398,
      "completion_tokens": 208,
      "total_tokens": 12606
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 5, Dataset, Paragraph 1",
            "reasoning": "The paper introduces a new dataset comprising 10 real-world forward-facing scenes with human annotated object masks. The use of 'human annotated' and plural scenes implies multiple human experts performed the annotations for those object masks."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 5, Dataset",
            "reasoning": "The paper mentions human annotated object masks but does not describe or reference any annotation instructions provided to annotators for this new dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 5, Dataset",
            "reasoning": "There is no mention of scoring rubrics or evaluation guidelines provided to annotators related to the annotation of the new dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 5, Dataset",
            "reasoning": "No examples of annotation instructions or sample annotated data are provided or discussed in the paper for the new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1861-rubric-2",
    "token_usage": {
      "prompt_tokens": 13588,
      "completion_tokens": 306,
      "total_tokens": 13894
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any quality assurance by a single human expert for the dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by multiple human experts for the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of quality assurance conducted by a single human non-expert in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 5, Dataset subsection",
          "reasoning": "The paper states that they introduce a new dataset with human annotated object masks for 10 real-world forward-facing scenes. The presence of 'human annotated object masks' suggests that multiple human annotators (non-experts) were involved in producing these annotations for quality assurance, although the paper does not provide detailed information about the annotators' expertise level or number."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance of the dataset annotations is not reported to be conducted by AI models."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or algorithmic quality assurance method for the dataset annotations is described in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents that human annotation was used for the dataset, so quality assurance process is at least partially applied."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1861-rubric-3",
    "token_usage": {
      "prompt_tokens": 13198,
      "completion_tokens": 420,
      "total_tokens": 13618
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 5 Experiments, Dataset paragraph",
          "reasoning": "The paper introduces a new dataset containing 10 real-world forward-facing scenes with human annotated object masks. This dataset includes 60 training images with the object and 40 test images without the object per scene. The dataset is described as newly introduced for evaluating 3D scene inpainting methods with corresponding object-free ground-truth, demonstrating original data collected and annotated by humans entirely from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset generated completely by AI or ML models without reference or transformation of existing data. The 3D scene inpainting outputs and segmentations are results generated by the proposed model but not introduced as datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of dataset creation via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset generated via machine translation from other languages."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset is newly collected real-world scenes with annotations; it is not described as aggregated from existing sources without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as derived by modification or adaptation of existing data sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset source and creation methodology are explicitly described, thus N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1861-rubric-4",
    "token_usage": {
      "prompt_tokens": 13716,
      "completion_tokens": 297,
      "total_tokens": 14013
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 Experiments - Dataset and Metrics, Tables 1, 2, 3, and associated text",
          "reasoning": "The newly introduced dataset comprises 10 real-world forward-facing scenes with human annotated object masks, which contain views of the same scene with and without a target object, enabling principled benchmarking of the 3D inpainting task. The dataset is used for evaluation, specifically to measure inpainting and segmentation performance quantitatively using metrics such as LPIPS and FID, as well as segmentation accuracy and IoU. It is not used for training the model but rather to evaluate and compare against baselines."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes use of the new dataset for evaluation and benchmarking of 3D scene inpainting, thus N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1861-rubric-5",
    "token_usage": {
      "prompt_tokens": 14439,
      "completion_tokens": 566,
      "total_tokens": 15005
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, including Abstract, Introduction, Method, and Experiments sections.",
          "reasoning": "The newly introduced dataset consists of images and corresponding object masks for 3D scene inpainting tasks. The paper is written in English, and no indication suggests that the dataset entries contain any language other than English. The dataset is described in terms of images with annotations (object masks), and no mention is made of content in multiple languages or non-English languages. Therefore, the dataset content is considered monolingual (English) in terms of metadata and annotations."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses programming methods and techniques, it does not present a dataset comprising code or programming languages. The dataset introduced is composed of images with object masks, not code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes mathematical expressions describing methods and losses, but these are part of the methodology and not the dataset entries themselves. The dataset entries do not contain mathematical or logical notations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains images of real-world scenes with annotated objects, not biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the dataset includes constructed languages or fictional/artificial languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly English, as annotations and metadata are presented in English; thus, language is specified and known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains images and associated annotations, thus it contains language metadata in English (e.g., object mask annotations), so it does contain language entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1861-rubric-6",
    "token_usage": {
      "prompt_tokens": 11657,
      "completion_tokens": 207,
      "total_tokens": 11864
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper for code release or dataset generation code",
          "reasoning": "The paper does not include any link or reference to a public code repository for constructing the introduced dataset. There is no explicit indication that the code for data collection, annotation, and dataset preparation is available to the public."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 5 (Experiments, Dataset subsection) and Section 4.1",
          "reasoning": "The paper describes the dataset composition in Section 5, detailing that it includes 10 real-world forward-facing scenes with human annotated object masks, specifying the number of images per scene and the presence of corresponding object-free ground truth test images. The data acquisition and annotation process is discussed sufficiently to understand what the dataset contains and its purpose for evaluation of 3D inpainting. However, no detailed methodology on how exactly data was collected or annotation procedures beyond human-annotated masks are described in depth."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1893-rubric-0",
    "token_usage": {
      "prompt_tokens": 9161,
      "completion_tokens": 191,
      "total_tokens": 9352
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset, Real Dataset paragraph",
          "Reasoning": "The paper states that the real dataset consists of 10 scenes of insect specimens captured by the authors. The images are collected using a photography light tent with a camera capturing multiple viewpoints and manual labeling of the transparent box geometry and masks. Hence, this data is human generated image data captured manually."
        },
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset, Synthetic Dataset paragraph",
          "Reasoning": "The synthetic dataset was generated by the authors using the Blender software with a physical-based renderer (Cycles) to render photorealistic images of 13 objects within a transparent box. This process is algorithmic and simulated, thus the image data is model generated synthetic data."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1893-rubric-1",
    "token_usage": {
      "prompt_tokens": 9961,
      "completion_tokens": 201,
      "total_tokens": 10162
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1 Dataset",
            "reasoning": "The real dataset involves manually labeling the transparent box by edges in the images and calculating its dimensions and pose. This manual labeling and calibration imply that expert human annotators did the annotation work."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1 Dataset",
            "reasoning": "There is no mention or indication of any detailed annotation instructions or guidelines provided for the annotators who labeled the dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1 Dataset",
            "reasoning": "The paper does not describe any scoring rubrics or criteria for the manual annotation of the transparent box or dataset images."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.1 Dataset",
            "reasoning": "No examples or sample annotations are presented or described in the paper related to the annotation process for the new datasets."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1893-rubric-2",
    "token_usage": {
      "prompt_tokens": 11151,
      "completion_tokens": 367,
      "total_tokens": 11518
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts conducted quality assurance on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any QA process done by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the involvement of multiple human non-expert annotators for dataset quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of an AI model as a judge or for quality assurance of the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or rule-based algorithmic QA process for dataset annotation or content is described in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces two new datasets (synthetic and real), but does not document any quality assurance process applied to the datasets. There is no description of verification, validation, or annotation checking. Thus, no QA is considered present."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1893-rubric-3",
    "token_usage": {
      "prompt_tokens": 10761,
      "completion_tokens": 457,
      "total_tokens": 11218
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper introduces a new real dataset consisting of 10 scenes containing real insect specimens enclosed in a transparent glass container. The dataset was captured using a photography light tent setup with manual calibration of camera intrinsics, poses, and transparent box dimensions. This collection was created entirely by the authors and involved manual labelling and capture processes, constituting original human-generated data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset was generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of datasets produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication exists in the paper that machine translation was used to generate datasets."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The introduced datasets (synthetic and real) were not collected or aggregated from existing sources; instead, they were generated or captured by the authors."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset (Synthetic Dataset)",
          "reasoning": "The synthetic dataset is generated by rendering 3D models of objects with Blender and a physically based renderer. The objects are existing 3D models chosen by the authors and placed inside synthetic transparent boxes. Thus, the dataset is derived from pre-existing 3D models, modified by rendering them in a new setting with specific transparent enclosure configuration."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides clear information about the origin and generation of the introduced datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1893-rubric-4",
    "token_usage": {
      "prompt_tokens": 11279,
      "completion_tokens": 475,
      "total_tokens": 11754
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset and Section 4.3 Implementation Details",
          "reasoning": "The introduced synthetic and real datasets are used for training the ReNeuS model from randomly initialized parameters. Specifically, the parameters of the ReNeuS network are optimized solely by minimizing the photometric loss between the rendered images and the ground truth images from the datasets, as described in Section 3.4 Loss Function. Section 4.3 mentions implementation details for training on the datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or indication that the introduced datasets are used for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of reinforcement learning or RL-based methods being applied to the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset and Section 4.4 Results",
          "reasoning": "The newly introduced synthetic and real datasets are used for quantitative and qualitative evaluation and benchmarking of the proposed method versus baseline methods, as well as for ablation study analysis to evaluate reconstruction quality."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used primarily for analyzing trends or characteristics but rather for training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not serve as a knowledge base for retrieval-augmented generation or similar."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets are clearly used for both training a new model from scratch and for evaluation, thus this option is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1893-rubric-5",
    "token_usage": {
      "prompt_tokens": 12002,
      "completion_tokens": 584,
      "total_tokens": 12586
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces two new datasets (synthetic and real) used for 3D reconstruction of objects inside transparent enclosures. There is no mention of multiple human languages in the dataset entries; the datasets consist of multi-view images of objects."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the proposed datasets include exactly two human languages. The dataset is image-based, not language-based."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced contain images and calibrations, not textual data in English or any language. The paper itself is in English, but this metric concerns dataset content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English linguistic content is indicated in the dataset description."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are composed of synthetic/rendered or real images and camera poses. No programming or structured code-related entries are reported as part of the datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical formulas and notations are present in the paper describing methods, but the datasets themselves consist of images and related calibration data. The datasets do not contain mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Section 4.1 Dataset",
          "reasoning": "Though the real dataset involves insect specimens enclosed in transparent containers, the dataset entries themselves are images, not biological sequences or non-human communication content."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages or textual content of the dataset are clearly not linguistic and therefore are documented as images and calibration data."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The datasets introduced consist of multi-view 2D images and corresponding calibration parameters without any linguistic content. Therefore, they do not contain entries with any human or programming language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1893-rubric-6",
    "token_usage": {
      "prompt_tokens": 9220,
      "completion_tokens": 202,
      "total_tokens": 9422
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 4.1 Dataset",
          "reasoning": "The abstract states that \"Codes and data will be available at https://github.com/hirotong/ReNeuS\", indicating public availability of the code, presumably including that related to data collection and generation. Section 4.1 describes the datasets and refers also to supplementary material for more details, supporting that code for dataset generation or capture is available or will be available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "Section 4.1 provides a detailed description of the synthetic dataset creation process (using Blender with the Cycles rendering engine, object placement, material settings, camera poses) and the real dataset capture process (photography light tent, ChArUco board for calibration, pose estimation, and mask generation). The paper also references supplementary material for more details. This shows clear documentation of dataset creation."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1914-rubric-0",
    "token_usage": {
      "prompt_tokens": 9470,
      "completion_tokens": 258,
      "total_tokens": 9728
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets",
          "Reasoning": "The new dataset used by the authors includes multi-view RGB images of indoor scenes from ScanNet and 7-Scenes, which are human captured datasets. The paper does not introduce a new collection of raw images; instead, it introduces a new method applied to existing datasets, thus no new human generated image data sets created by the authors. Therefore, no new image dataset is introduced by the authors in this paper."
        },
        {
          "Modality": "other",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Sections 3.2 and 3.5",
          "Reasoning": "The paper introduces a novel neural 3D reconstruction framework that generates super-plane segmentation masks in a self-supervised manner from the model's predicted geometry and normal maps. These super-plane masks are generated algorithmically during training without additional ground truth supervision or manual annotation, thus they are model generated artifacts serving as unsupervised segmentation results and plane segmentations. This super-plane segmentation data created during training is new and introduced by the authors as part of their method for self-supervised plane reconstruction and automatic labeling."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1914-rubric-1",
    "token_usage": {
      "prompt_tokens": 10270,
      "completion_tokens": 302,
      "total_tokens": 10572
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2 and 3.3",
            "reasoning": "The paper explicitly describes that the super-plane segmentation and plane reconstruction are performed in an unsupervised, self-supervised manner without ground truth labels or human annotation. Pixels are grouped into super-planes by clustering predicted surface normals using a Mean-shift clustering method (a deterministic clustering algorithm), and filtering is performed via automated normal difference thresholds and edge detection. This constitutes an automatic process rather than human annotation or AI model prediction trained on labels."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No instructions described",
            "reasoning": "The paper does not mention any annotation instructions or guidelines given to humans or models for producing the super-plane segmentation or plane reconstruction. The segmentation is generated by an unsupervised clustering and filtering process automatically, so no annotation instructions are applicable."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No rubrics described",
            "reasoning": "No scoring rubrics or evaluation criteria for annotation were provided because the super-plane masks are automatically generated by clustering and filtering with no human annotation involved."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples shown for annotation",
            "reasoning": "The paper presents qualitative visualizations of super-plane segmentations (e.g., Figure 3) generated automatically, but does not provide examples or demonstrations of annotation instructions or labeled data examples since it is an automated unsupervised process."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1914-rubric-2",
    "token_usage": {
      "prompt_tokens": 11460,
      "completion_tokens": 311,
      "total_tokens": 11771
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the dataset or annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance involving multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of quality assurance performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no indication of dataset quality assurance by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance in the dataset or annotations is described as performed by AI models acting as judges."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3 (Super-plane Segmentation Refinement)",
          "reasoning": "The paper describes an auto-filtering strategy to filter out noisy or outlier pixel assignments in super-plane segmentation based on comparing normals and detecting edge regions using color and geometry discontinuity measurement. This constitutes an automatic verification and refinement process of the unsupervised segmentation data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The authors introduce a self-supervised unsupervised super-plane segmentation method and perform automatic filtering but do not mention any lack of QA or not performing quality assurance. Thus, N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1914-rubric-3",
    "token_usage": {
      "prompt_tokens": 11070,
      "completion_tokens": 386,
      "total_tokens": 11456
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 (Datasets)",
          "reasoning": "The paper uses existing datasets such as ScanNet and 7-Scenes for training and evaluation, which are large-scale RGB-D datasets with ground-truth camera parameters and surface reconstructions. These datasets are collected and aggregated from prior collection efforts and are not newly created by the authors."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Sections 3.2, 3.3, 3.5 (Proposed Method), and 4.2 (Ablation Study)",
          "reasoning": "The authors generate new super-plane segmentation masks derived from the predicted surface normals during training via self-supervised clustering methods (mean-shift clustering on normals), along with auto-filtering and masking strategies to refine these segmentations. These super-plane segmentations and plane reconstructions are derived data based on existing multi-view RGB images and the neural implicit reconstruction process, representing modifications and transformations of existing data (input images and predicted geometry) rather than new raw data creation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1914-rubric-4",
    "token_usage": {
      "prompt_tokens": 11588,
      "completion_tokens": 296,
      "total_tokens": 11884
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments, Evaluation Metrics and Comparisons)",
          "reasoning": "The paper evaluates the performance of both 3D reconstruction and plane segmentation using standard metrics on existing datasets such as ScanNet and 7-Scenes, using the authors' proposed unsupervised plane reconstruction results for benchmarking. This indicates that the segmentation and plane reconstruction derived from their method are used primarily for evaluation to demonstrate effectiveness."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 (Ablation Study and Analysis)",
          "reasoning": "The paper conducts ablation studies to analyze the impact of their super-plane constraint and related methods on reconstruction quality, examining effects such as noise reduction, comparison with ground truth segmentation, and effectiveness of filtering strategies. This use of the dataset for examining trends and method attributes classifies as analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1914-rubric-5",
    "token_usage": {
      "prompt_tokens": 12311,
      "completion_tokens": 458,
      "total_tokens": 12769
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention or describe any dataset containing entries with more than two human languages. The datasets used are image datasets with no language content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of datasets containing exactly two human languages in the paper. The datasets are image-based and do not contain language data."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 and throughout the paper",
          "reasoning": "The datasets referenced (ScanNet and 7-Scenes) are typical computer vision datasets where annotations and documentation are in English, and the paper's content and dataset references are in English. There is no mention of other languages. Hence, the dataset entries related to labels, descriptions, and annotations are English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or evidence of datasets involving non-English languages exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are not described as containing code or programming language content; the code presented is for method description and implementation, not dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notation appears in the paper text for explaining algorithms and methodology, but the datasets themselves do not contain mathematical or logical symbolic content as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no involvement of biological or non-human communication data in the datasets introduced in this work."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any fictional or constructed languages in relation to its datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) of the dataset entries are clearly associated with English annotations and descriptions; thus, not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain human language content in terms of annotations and labels, so it is not applicable to label them as having no language content."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1914-rubric-6",
    "token_usage": {
      "prompt_tokens": 9529,
      "completion_tokens": 222,
      "total_tokens": 9751
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract; Section 4.1 Implementation Details",
          "reasoning": "The paper explicitly mentions in the abstract that 'The code and models are available at https://github.com/botaoye/S3PRecon', indicating that the code related to dataset generation or processing is publicly available. Additionally, Section 4.1 on Implementation Details discusses dataset usage and training implementation, supporting that code for processing is provided."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3.2 and 3.3 (Self-supervised Super-plane Constraint and Super-plane Segmentation Refinement) and Section 4.1 Implementation Details",
          "reasoning": "The paper describes in detail the process of generating the super-plane segmentation (an unsupervised clustering approach using predicted normals) and the refinement steps (auto-filtering and edge detection) used to create the dataset and plane segmentation results. Furthermore, Section 4.1 gives implementation details that pertain to dataset usage and processing. This constitutes sufficient documentation of the dataset creation process within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1933-rubric-0",
    "token_usage": {
      "prompt_tokens": 9691,
      "completion_tokens": 96,
      "total_tokens": 9787
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 6.2",
          "Reasoning": "The paper introduces experiments on the Reichstag dataset, which consists of 75 views of roughly 18k 3D points with ground truth correspondences estimated by structure from motion. The images and corresponding 2D observations originate from real camera captures in this dataset, thus are human generated data."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1933-rubric-1",
    "token_usage": {
      "prompt_tokens": 10491,
      "completion_tokens": 259,
      "total_tokens": 10750
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 5 and Section 6",
            "reasoning": "The paper introduces two new semidefinite relaxation formulations for robust multiview triangulation (Eqs. (RT) and (RTF)). These are algorithmic mathematical formulations solved by convex optimization solvers, i.e., automatic computational processes, not human or expert annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention of annotation instructions in Sections 5 or 6 or elsewhere",
            "reasoning": "The paper does not describe any manual annotation instructions or guidelines since the 'annotations' correspond to computational iterative optimization processes rather than labeled data by humans."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit scoring rubric described for annotation in the entire paper",
            "reasoning": "There are no scoring rubrics provided; the evaluation metrics discussed are related to tightness of relaxation and estimation error, not human annotation rubric."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples provided as this is a mathematical optimization method, not an annotation task",
            "reasoning": "No example annotation cases are shown since the process is a convex optimization relaxation solved algorithmically rather than human-labeled data requiring examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-1933-rubric-2",
    "token_usage": {
      "prompt_tokens": 11681,
      "completion_tokens": 284,
      "total_tokens": 11965
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human annotators or expert involvement in quality assurance of datasets; therefore, this label does not apply."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or documentation of multiple human experts performing quality assurance on any dataset in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of a single non-expert human annotator performing quality assurance; thus, not applicable."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of AI models being used as judges for quality assurance on the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper describes algorithmic methods for solving triangulation problems and validating relaxation tightness, it does not describe using automated processes specifically for quality assurance of dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets nor describe any quality assurance process or validation for dataset annotations or content. Therefore, quality assurance is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1933-rubric-3",
    "token_usage": {
      "prompt_tokens": 11291,
      "completion_tokens": 413,
      "total_tokens": 11704
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any new data was created entirely from scratch by human contributors. It focuses on methods and relaxations for multiview triangulation and uses existing datasets (e.g., Reichstag dataset) for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any original dataset entirely by AI or machine learning models. It develops optimization approaches but does not create new data from models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any translation of data from another language by human translators in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any machine translation of data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 6.2 (Reichstag dataset)",
          "reasoning": "The Reichstag dataset used for evaluation is an existing real-world dataset sourced from prior work [12]. The paper uses ground truth correspondences estimated by structure from motion from that dataset without significant modification, representing collated data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper applies transformations and methods on existing data, no new datasets derived from existing datasets with modifications are explicitly introduced by the authors."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes usage of existing datasets (Reichstag) and simulations for experiments, so the data origin is known and documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1933-rubric-4",
    "token_usage": {
      "prompt_tokens": 11809,
      "completion_tokens": 230,
      "total_tokens": 12039
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 6.2 and 6.3",
          "reasoning": "The Reichstag dataset is used to validate the proposed relaxations by measuring relaxation tightness and estimation errors under various noise and outlier settings (Section 6.2). Additionally, this dataset is used to compare the proposed relaxation methods against RANSAC performance for benchmarking (Section 6.3). Hence, the dataset serves exclusively for evaluation and benchmarking purposes."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1933-rubric-5",
    "token_usage": {
      "prompt_tokens": 12532,
      "completion_tokens": 595,
      "total_tokens": 13127
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset entries containing more than two human languages. There is no indication of multilingual text data in the proposed dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of entries with exactly two human languages in the new dataset introduced by the authors."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 6.2 (Reichstag dataset) and throughout the paper",
          "reasoning": "The paper uses the Reichstag dataset containing image correspondences and triangulation tasks. The dataset documentation, experimental descriptions, and all textual content in the paper are exclusively in English. There is no evidence of other languages being used in the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the presence of datasets in any non-English languages."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses algorithms and convex relaxations and provides a GitHub link for implementation, it does not present a new dataset consisting of programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Sections 3, 4, 5",
          "reasoning": "The paper introduces two new semidefinite relaxation formulations for robust triangulation problems with detailed mathematical expressions, polynomial optimization problems, constraints and cost functions. These formulations are part of the new dataset's problem definition and solution framework."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention of biological sequences or non-human communication systems in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of fictional or artificially created languages is made in relation to the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) used in the dataset are clearly English and mathematically represented; thus, language is specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset introduced involves human language in the form of English text and mathematical notation; therefore, it is not devoid of language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-1933-rubric-6",
    "token_usage": {
      "prompt_tokens": 9750,
      "completion_tokens": 188,
      "total_tokens": 9938
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Implementation statement in first page, Abstract and Introduction",
          "reasoning": "The paper mentions an implementation repository URL (github.com/linusnie/robust-triangulation-relaxations) for the optimization methods, but does not indicate the release or provision of new dataset code. The datasets used (simulated experiments and Reichstag dataset from [12]) appear to be standard or pre-existing, and no new datasets are introduced by the authors nor is any dataset creation code provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "The paper does not provide any section or description documenting dataset creation.",
          "reasoning": "The datasets used are either simulated with parameters described in experiments or an existing Reichstag dataset cited from prior work [12]. There is no mention of any new dataset created by the authors, nor any detailed documentation on dataset construction processes relevant to new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2044-rubric-0",
    "token_usage": {
      "prompt_tokens": 13159,
      "completion_tokens": 179,
      "total_tokens": 13338
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4 (Experiments) and related descriptions",
          "Reasoning": "The paper uses the 50Salads dataset for online action recognition, which includes video data of human activities performed and recorded. Although this is a known dataset used for evaluation, the authors do not introduce it as a new dataset; thus, it is not considered a new dataset introduced by the authors."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4 (Experiments) and related descriptions",
          "Reasoning": "The paper uses the Sub-JHMDB dataset for online pose estimation, which consists of labeled video frames of human pose. Similarly, this is a pre-existing dataset, not newly introduced by the authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2044-rubric-1",
    "token_usage": {
      "prompt_tokens": 13959,
      "completion_tokens": 259,
      "total_tokens": 14218
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1, 3.2",
            "reasoning": "The paper describes that the agent module \u03c0 uses reinforcement learning (an AI model-based approach) to automatically generate policies for frame processing, and the main network M processes frames dynamically based on these policies without human annotators. No mention of human annotators labeling data for training is provided; rather, the system is trained via RL and self-supervised adaptation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not describe any manual annotation process that would require detailed instructions; training is done via automated RL methods and self-supervision, so no annotation instructions are provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "No scoring rubrics or evaluation guidelines used by human annotators are discussed; evaluation metrics like accuracy and delay are used for training and testing, but these are not human annotation rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "No example annotations or annotation guideline examples for human annotators are provided; the paper focuses on training AI models in a self-supervised manner."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2044-rubric-2",
    "token_usage": {
      "prompt_tokens": 15149,
      "completion_tokens": 285,
      "total_tokens": 15434
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert annotator for the introduced datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by multiple human expert annotators for the introduced datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single non-expert human annotator for the introduced datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by multiple non-expert human annotators for the introduced datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model acting as a judge for dataset quality assurance."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automatic or algorithmic verification process used for dataset quality assurance."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; it uses existing datasets (50Salads and Sub-JHMDB) without describing any new dataset annotation or quality assurance process. Therefore, no quality assurance process for new dataset annotations is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2044-rubric-3",
    "token_usage": {
      "prompt_tokens": 14759,
      "completion_tokens": 358,
      "total_tokens": 15117
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the authors created any new dataset entirely from scratch by human annotation or contribution."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any new data was generated solely by AI or machine learning models as original content."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data being produced by translating content from other languages via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence or statement in the paper about use of machine translation for the data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments)",
          "reasoning": "The paper applies its method on standard public datasets such as 50Salads and Sub-JHMDB, which are pre-existing datasets from other sources. These datasets are used directly without indication of modification, thus they are collated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new datasets created by transforming or modifying existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources used are clearly specified in the Experiments section."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2044-rubric-4",
    "token_usage": {
      "prompt_tokens": 15277,
      "completion_tokens": 442,
      "total_tokens": 15719
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using any new dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset used for training models from scratch; rather, it uses existing datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No newly introduced dataset is used for supervised fine-tuning of pre-trained models in this paper."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the method involves reinforcement learning (RL) for policy learning, the datasets used are existing ones and no new dataset is introduced for RL-based post-training."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets mentioned (50Salads, Sub-JHMDB) are pre-existing datasets and used for evaluation and training, but the paper does not introduce any new dataset for evaluation or benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper analyzes online video understanding tasks using existing datasets but does not introduce new datasets primarily for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is introduced to serve as a knowledge base or for model augmentation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; all datasets used are pre-existing benchmarks. Therefore, no applicable usage of a new dataset within the machine learning pipeline is described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2044-rubric-5",
    "token_usage": {
      "prompt_tokens": 16000,
      "completion_tokens": 614,
      "total_tokens": 16614
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset containing entries with more than two human languages. The datasets used (50Salads and Sub-JHMDB) are video datasets for action recognition and pose estimation, with no mention of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing exactly two human languages. The datasets used are not language-based textual datasets but video datasets labeled for visual tasks."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset containing English text data exclusively. The datasets are video datasets without emphasis on language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced or used in the paper are video-based datasets without any indication they contain data in a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper involves programming code for implementation of models but does not introduce a new dataset containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Section 3.1, 3.2",
          "reasoning": "Although the paper contains mathematical expressions to describe the method, it does not introduce any new dataset containing mathematical or logical notation as data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce datasets related to biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset involves fictitious or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper explicitly uses known datasets (50Salads and Sub-JHMDB) for evaluation and does not introduce a new dataset with unspecified or undocumented languages."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new dataset introduced by the authors consists chiefly of visual video streams used for online video understanding tasks (action recognition and pose estimation). These datasets do not consist of any linguistic content, human language entries, or textual data, but consist of video frames and associated labels for action categories or joint positions. Therefore, the dataset does not contain any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2044-rubric-6",
    "token_usage": {
      "prompt_tokens": 13218,
      "completion_tokens": 150,
      "total_tokens": 13368
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper",
          "reasoning": "The paper does not mention the availability or release of code related to the construction, collection, or preprocessing of any dataset. It mainly focuses on the model architecture and adaptation methods, and uses existing datasets such as 50Salads and Sub-JHMDB without discussion of new dataset creation or associated code."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No explicit section or content on dataset creation",
          "reasoning": "The paper utilizes pre-existing public datasets (50Salads and Sub-JHMDB) for evaluation and does not introduce any new datasets. Therefore, there is no documentation regarding dataset creation presented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2056-rubric-0",
    "token_usage": {
      "prompt_tokens": 10997,
      "completion_tokens": 120,
      "total_tokens": 11117
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1.1, Section 4 Experiment",
          "Reasoning": "The paper describes the use of real-world data captured from aerial drones and automobiles to evaluate their proposed TEGBP method. The normal flow measurements input to their method are computed from asynchronous events of event-based cameras, which are physical sensors capturing data in real time. This data is thus sensor data obtained via human-operated devices or experimental capture setups, implying human involvement in data acquisition."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2056-rubric-1",
    "token_usage": {
      "prompt_tokens": 11797,
      "completion_tokens": 222,
      "total_tokens": 12019
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4 (Experiment) and Section 5 (Conclusion)",
            "reasoning": "The paper describes the introduction of a novel dataset application for event-based optical flow estimation and evaluates their proposed TEGBP method on existing real-world datasets such as MVSEC and DSEC. However, there is no indication that a new dataset was introduced or annotated in this paper. Instead, existing datasets were used for evaluation. Therefore, no new dataset annotation was performed by annotators or models in this work."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "Since no new dataset was introduced or annotated, there are no annotation instructions provided in the paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No rubrics are described for data annotation because no new dataset annotations are introduced or performed."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples exist because no new dataset annotation occurs in this work."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2056-rubric-2",
    "token_usage": {
      "prompt_tokens": 12987,
      "completion_tokens": 285,
      "total_tokens": 13272
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process conducted by a single human expert for dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts were involved in quality assurance for any dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance processes performed by a single non-expert annotator on datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance by multiple human non-expert annotators on datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any AI model being used for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe an automated verification or algorithmic/rule-based QA process for dataset validation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses existing public datasets (e.g., MVSEC, DSEC, ESIM simulation) for evaluation but does not introduce any new dataset. Therefore, no quality assurance processes for new dataset annotation or content are documented or applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2056-rubric-3",
    "token_usage": {
      "prompt_tokens": 12597,
      "completion_tokens": 362,
      "total_tokens": 12959
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4. Experiment (4.1.1 Normal flow measurement) and Section 4.1.2 Full flow estimation",
          "reasoning": "The paper utilizes normal flow measurements computed using a naive least-squares plane fitting method applied to events from existing real-world datasets such as MVSEC, DSEC, and DVS. The authors perform incremental full flow estimation using their proposed TEGBP on these measurements. The new data involved in the experiments are processed and transformed forms (i.e., derived) of existing event data. No indication is given that entirely new datasets were created from scratch; rather, the paper uses publicly available datasets with additional processing and new flow estimation outputs derived from them."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly states usage of existing datasets and shows derivation of data through their proposed method, so the data origin is documented and known."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2056-rubric-4",
    "token_usage": {
      "prompt_tokens": 13115,
      "completion_tokens": 337,
      "total_tokens": 13452
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experiment), specifically Sections 4.2 (ESIM-bricks), 4.3 (DVS-stripes), 4.4 (MVSEC), and 4.5 (DSEC)",
          "reasoning": "The authors applied their proposed Tangentially Elongated Gaussian BP (TEGBP) algorithm on various existing real-world and synthetic datasets to measure and benchmark the accuracy of their method against state-of-the-art incremental flow estimation algorithms. The datasets mentioned, such as MVSEC and DSEC, are used exclusively for evaluation purposes to compare TEGBP with others, confirming that the datasets serve an evaluation role in demonstrating the algorithm's performance."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new dataset is introduced in this paper. The paper leverages existing datasets such as ESIM, DVS, MVSEC, and DSEC for evaluation, but does not propose or introduce any new datasets. Therefore, no practical usage of a newly introduced dataset is described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2056-rubric-5",
    "token_usage": {
      "prompt_tokens": 13838,
      "completion_tokens": 525,
      "total_tokens": 14363
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Entire paper, e.g., Abstract and Section 4 Experiment",
          "reasoning": "The paper and its newly introduced datasets are described entirely in English. The datasets (e.g., event streams, optical flow measurements) pertain to numerical and sensor data but the accompanying documentation, explanations, and annotations are in English. There is no indication of any other human language included in the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "No specific code dataset introduced.",
          "reasoning": "While the paper discusses algorithms and methods that may be implemented in code, it does not introduce any dataset containing entries with programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "The paper contains mathematical notation but no dataset with such entries is introduced.",
          "reasoning": "The paper includes mathematical formulas and symbolic notation to describe the methods and algorithms, but these are part of the methodological exposition and not entries within a dataset introduced in the paper."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper focuses on event-based camera data and optical flow estimation and does not include datasets containing biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or inclusion of constructed or fictional languages in the proposed datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language is clearly documented as English; no ambiguity or lack of specification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries include human language content (English documentation and description), so not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2056-rubric-6",
    "token_usage": {
      "prompt_tokens": 11056,
      "completion_tokens": 136,
      "total_tokens": 11192
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention",
          "reasoning": "The paper does not include any link, URL, or mention to code repositories or supplementary code materials related to dataset construction or processing. The methods and experiments are described, but no code is shared or referred to."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No explicit section or mention",
          "reasoning": "The paper uses several existing datasets (e.g., MVSEC, DSEC) for evaluation, but it does not introduce or construct any new dataset by the authors themselves. Therefore, there is no documentation on dataset creation process presented within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2111-rubric-0",
    "token_usage": {
      "prompt_tokens": 11772,
      "completion_tokens": 121,
      "total_tokens": 11893
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Experiment Setup",
          "Reasoning": "The paper uses several benchmark hyperspectral image datasets (CAVE, Harvard, Chikusei, WorldView2) as training and testing data. These datasets consist of hyperspectral images (HSIs) captured from real-world scenes using hyperspectral imaging sensors, involving human operation or manual acquisition, hence the data are human generated images. These datasets are not introduced newly by the authors but are pre-existing benchmarks."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2111-rubric-1",
    "token_usage": {
      "prompt_tokens": 12572,
      "completion_tokens": 214,
      "total_tokens": 12786
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4 (Optimization), Section 5 (Experimental Results)",
            "reasoning": "The paper describes a computational Monte Carlo Expectation-Maximization algorithm and alternative optimization strategy for training and fine-tuning the proposed model. There is no mention of human annotators performing manual annotation or labeling of data; instead, degradation estimation and fusion are performed by mathematical and algorithmic processes."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not provide annotation instructions or guidelines for human annotators, as the data processing is done algorithmically."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There is no mention of scoring rubrics or evaluation criteria given as annotation guidelines for human annotators."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No examples or sample annotations are provided for annotation guidelines because annotation is not done by humans but by algorithmic procedures."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2111-rubric-2",
    "token_usage": {
      "prompt_tokens": 13762,
      "completion_tokens": 293,
      "total_tokens": 14055
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the new datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts were involved in quality assurance of the new datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance conducted by a single non-expert human annotator for the new datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert human annotators being involved in quality assurance for the new datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of AI models to perform quality assurance on the new datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or rule-based quality assurance procedure on dataset annotations or content is described in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document any quality assurance process applied to the new datasets. The datasets referenced appear to be existing known benchmarks (CAVE, Harvard, Chikusei, Worldview2) with no description of new datasets introduced by the authors or quality assurance steps thereof."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2111-rubric-3",
    "token_usage": {
      "prompt_tokens": 13372,
      "completion_tokens": 428,
      "total_tokens": 13800
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state or imply the creation of any new dataset entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the authors generated entirely novel datasets solely by AI or machine learning models without using or modifying existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or evidence is provided about data generated via machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.1 Experiment Setup",
          "reasoning": "The paper uses existing datasets for experimentation, such as CA VE, Harvard, Chikusei, and Worldview2 hyperspectral datasets. These datasets are publicly available and aggregated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 5.1 Experiment Setup and Section 5.2 Experiments on Synthetic Benchmarks",
          "reasoning": "The paper generates new synthetic degraded datasets by applying various simulated degradations (e.g., Gaussian blur kernels as PSFs and spectral response functions) to existing hyperspectral datasets to create LR-HSI and HR-MSI pairs for training/testing. This derivation involves modifications of existing data via transformations or adaptations but not creation of entirely new data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is documented through references to existing datasets and the described degradation synthesis process."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2111-rubric-4",
    "token_usage": {
      "prompt_tokens": 13890,
      "completion_tokens": 547,
      "total_tokens": 14437
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the new dataset being used for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention that the introduced dataset is used to train a model from randomly initialized parameters (from scratch). Training is discussed but mostly uses the dataset for fine-tuning or evaluation."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.2 (Fast Fine-tuning), Section 5.1 (Experiment Setup), Section 5.4 (Algorithmic Analysis)",
          "reasoning": "The paper describes fast fine-tuning strategy using the datasets. The new datasets (processed splits from existing benchmarks) are used for supervised fine-tuning of the fusion module, adapting the pre-trained model to new inputs efficiently during the testing stage."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss reinforcement learning or RL-based post-training techniques involving the new datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1 (Experiment Setup), Section 5.2 (Experiments on Synthetic Benchmarks), Section 5.3 (Real Scenario Experiments), Tables 1 and 2",
          "reasoning": "The paper uses the new datasets (synthetic splits of CA VE, Harvard, Chikusei, and WorldView2) extensively for benchmarking and quantitative evaluation of their method's performance compared to other state-of-the-art methods."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not primarily use the new datasets for trends, pattern or characteristic analysis separate from training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the new datasets serve as a knowledge base for model augmentation or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The new datasets are actively used for supervised fine-tuning and evaluation, thus this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2111-rubric-5",
    "token_usage": {
      "prompt_tokens": 14613,
      "completion_tokens": 591,
      "total_tokens": 15204
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset containing entries in more than two human languages. The datasets referenced are related to hyperspectral imaging data with no mention of language content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets containing exactly two human languages. The data are hyperspectral images and associated multispectral images, which are image data without language annotations."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5.1 Experiment Setup",
          "reasoning": "The new dataset introduced is the training and testing data derived from synthetic and real hyperspectral datasets (e.g., CAVE, Harvard, Chikusei, Worldview2). All associated text, instructions, and annotations in the paper are in English, indicating the dataset and accompanying materials are in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication the datasets include a single non-English language. The paper and dataset materials are presented in English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper implements methods in PyTorch and shares code via GitHub, the datasets themselves do not contain programming code entries as data. The datasets are hyperspectral images rather than code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains mathematical formulations to describe models, but the datasets introduced do not contain mathematical or logical expressions as part of the data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are hyperspectral imaging data of natural scenes and materials, not biological sequence data or animal communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are involved in the dataset entries."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language or linguistic status of the datasets is clearly English; it is not unspecified or undocumented."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The datasets introduced primarily consist of hyperspectral image data, which do not contain linguistic or language-based content."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2111-rubric-6",
    "token_usage": {
      "prompt_tokens": 11831,
      "completion_tokens": 237,
      "total_tokens": 12068
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 5.1 Experiment Setup",
          "reasoning": "While the authors mention that code is available at https://github.com/WenjinGuo/DAEM for their proposed method, there is no indication in the paper that code for dataset construction or dataset generation is provided. The datasets used (e.g., CA VE, Harvard, Chikusei, Worldview2) are existing public datasets obtained from external sources and not constructed by the authors. Therefore, code related to dataset creation is not applicable or not provided."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 5.1 Experiment Setup and throughout Section 5 Experiments",
          "reasoning": "The paper documents the usage, selection, and preprocessing of multiple publicly available hyperspectral datasets (CA VE, Harvard, Chikusei, Worldview2) with clear references to where these datasets are obtained and how they are used or cropped; thus, the dataset usage and preprocessing process is documented. However, the datasets themselves are not newly created by the authors but are well described concerning how they are applied in the experiments."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2126-rubric-0",
    "token_usage": {
      "prompt_tokens": 11474,
      "completion_tokens": 156,
      "total_tokens": 11630
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1, Dataset and Evaluation Metrics",
          "Reasoning": "The new dataset used in the paper is the COCO 2017 dataset for object detection and human pose estimation. The dataset consists of images with annotated objects, and these images are human-captured natural images. The paper uses COCO 2017 as the benchmark for training and evaluation but does not introduce a new dataset. Therefore, no new dataset is introduced by the authors. This paper only uses the existing COCO dataset for experiments, and no new datasets are created or introduced. The paper's contribution is a new method for object detection, not a new dataset."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2126-rubric-1",
    "token_usage": {
      "prompt_tokens": 12274,
      "completion_tokens": 212,
      "total_tokens": 12486
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.3 Sparse Feature Sampling and Aggregation",
            "reasoning": "The multi-scale feature sampling is performed automatically by the algorithm as described in Section 4.3. It involves selecting promising regions based on high-confidence predictions and sampling features at keypoints using computations by the model pipeline rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not mention any human annotation instructions or guidelines related to the annotation process for the multi-scale features. The process is automated within the network architecture."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "There is no description of scoring rubrics or evaluation criteria for human annotation in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "There are no annotation examples provided for this process since the multi-scale feature extraction is automated as part of the model's design."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2126-rubric-2",
    "token_usage": {
      "prompt_tokens": 13464,
      "completion_tokens": 289,
      "total_tokens": 13753
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance process involving a single human expert annotator for dataset annotations or content validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by multiple human experts for dataset annotations or validation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance performed by a single human non-expert annotator in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information on quality assurance conducted by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of an AI model as a judge for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated or algorithmic quality assurance process concerning dataset annotation or content is described in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a method evaluated on pre-existing datasets (e.g., COCO 2017) without describing any new datasets or any quality assurance process related to datasets. Therefore, no quality assurance process is applied or documented for dataset annotation in this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2126-rubric-3",
    "token_usage": {
      "prompt_tokens": 13074,
      "completion_tokens": 373,
      "total_tokens": 13447
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets created entirely from scratch by human contributors. It focuses on proposing a method (IMFA) to leverage multi-scale features for existing object detection models."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not generate any datasets entirely by AI or machine learning models. It deals with model architecture and training on established datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of any data produced by human translation in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any machine-translated data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing datasets such as COCO 2017 for training and evaluation but does not collate or aggregate data from multiple sources to create a new dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset derived or adapted from existing sources is introduced in this work."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. All experiments are conducted on pre-existing benchmark datasets such as COCO 2017. Therefore, the data novelty metric is not applicable here."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2126-rubric-4",
    "token_usage": {
      "prompt_tokens": 13592,
      "completion_tokens": 205,
      "total_tokens": 13797
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. It uses the pre-existing COCO 2017 dataset solely for evaluation and demonstration purposes. No new datasets are proposed, described, or utilized for training, pre-training, fine-tuning, reinforcement learning, analysis, or as a knowledge base."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2126-rubric-5",
    "token_usage": {
      "prompt_tokens": 14315,
      "completion_tokens": 539,
      "total_tokens": 14854
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing multiple human languages as part of its contributions."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset with exactly two languages is introduced or described in the paper."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5.1 Experiment Setup - Dataset and Evaluation Metrics",
          "reasoning": "The only new dataset mentioned is the COCO 2017 dataset, which is a standard computer vision dataset annotated with English text labels for object categories. There is no indication of other languages used in annotations or data. The paper explicitly uses COCO for training and evaluation, which is in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset with a single non-English language is introduced or described in the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes discussion of code and algorithmic procedures, there is no new dataset composed of programming language code introduced."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains mathematical notations to describe models and methods but does not introduce any dataset containing mathematical or logical expressions as data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper is focused on object detection in images and does not involve biological or non-human communication datasets."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets containing constructed or fictional languages are mentioned."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) of the dataset used (COCO) is known and documented as English; the paper does not mention datasets with unspecified languages."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset used contains language in the form of annotations and labels, thus not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2126-rubric-6",
    "token_usage": {
      "prompt_tokens": 11533,
      "completion_tokens": 142,
      "total_tokens": 11675
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not mention or provide any link to code repositories related to dataset construction, data preprocessing, or generation. It mainly focuses on model design, implementation, and evaluation using existing datasets such as COCO 2017, without introducing any new dataset or dataset creation code."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "None",
          "reasoning": "The paper does not introduce any new dataset; therefore, there is no documentation regarding dataset creation or collection process. All experiments are conducted on publicly available standard datasets like COCO 2017, and the paper does not describe any novel dataset construction process."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2140-rubric-0",
    "token_usage": {
      "prompt_tokens": 11133,
      "completion_tokens": 128,
      "total_tokens": 11261
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Experimental Setup, Datasets",
          "Reasoning": "The paper introduces usage of target class datasets comprising real images from ImageNet and Paintings datasets, which are human-generated image datasets; these datasets are used as the source and target domains for the training and evaluation of the proposed TTAA method. No new dataset created by authors is explicitly mentioned, but the target class datasets are formed by sampling images from existing datasets rather than generating new data, indicating human-generated image modality data are utilized."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2140-rubric-1",
    "token_usage": {
      "prompt_tokens": 11933,
      "completion_tokens": 258,
      "total_tokens": 12191
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4 (especially 4.4 Generative Adversarial Training) and Section 5.1 Experimental Setup",
            "reasoning": "The paper describes that adversarial example generation is performed by a generator network (a deep neural network) trained via a generative adversarial training process. The evaluation of transferability and the generation of perturbations are done via an iterative optimization process involving networks, which is an automated model-driven process, not involving human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not mention any human annotation guidelines, instructions, or protocols given to human annotators for data labeling or annotation since no manual annotation process is involved."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "No scoring rubrics or evaluation criteria for human annotation are described, because no human annotation procedure was part of the new dataset creation or evaluation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not present annotation examples or exemplary annotated data, as the adversarial examples are generated through automated generative models rather than human annotation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2140-rubric-2",
    "token_usage": {
      "prompt_tokens": 13123,
      "completion_tokens": 326,
      "total_tokens": 13449
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process involving a single human expert annotator for dataset validation or content verification."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description of multiple human experts performing quality assurance on any dataset introduced by the authors."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance being conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no documentation or indication of multiple non-expert human annotators performing quality assurance on datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models (label discriminator and feature discriminator) are used in the proposed generative adversarial training framework, their role is to guide adversarial example generation rather than quality assurance of dataset annotation or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report using any automated verification process specifically for quality assurance of dataset annotations or content. The automatic processes described relate to training models, not dataset QA."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces datasets compiled from ImageNet and Paintings datasets, but does not document any quality assurance process applied to these datasets or new annotations. The focus is on adversarial example generation, and no QA procedures are described for dataset validation or content checking."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2140-rubric-3",
    "token_usage": {
      "prompt_tokens": 12733,
      "completion_tokens": 446,
      "total_tokens": 13179
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the creation of any new datasets entirely from scratch by human contributors. Instead, it uses existing datasets such as ImageNet and the Paintings dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The adversarial examples generated by the proposed method are model outputs based on existing data inputs rather than standalone datasets. The paper does not claim creation of a novel dataset purely generated by AI without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any dataset produced via manual translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.1 Experimental Setup, Datasets",
          "reasoning": "The paper uses existing datasets such as ImageNet and Paintings dataset, aggregating and selecting subsets (e.g., target class images, training and testing sets). This process aligns with collating data from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Overview of TTAA; Section 5.1 Experimental Setup, Datasets",
          "reasoning": "The paper derives adversarial example datasets by applying perturbations generated by their model onto existing images from the ImageNet or Paintings datasets to produce targeted adversarial examples. This constitutes data derived from existing sources with modifications (perturbations)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies the data origins and generation methods sufficiently; hence, N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2140-rubric-4",
    "token_usage": {
      "prompt_tokens": 13251,
      "completion_tokens": 519,
      "total_tokens": 13770
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe or demonstrate that any newly introduced dataset is used exclusively for pre-training models on general patterns in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any newly introduced dataset is used for training a model from random initialization (from scratch). The datasets used are either existing datasets (ImageNet, Paintings) or derived subsets; no new dataset is introduced for training a model from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report using any new dataset to fine-tune a pre-trained model using supervised methods. Existing datasets and established models are used for evaluation and experiments."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or description of using any newly introduced dataset for reinforcement learning based methods such as RLHF."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used for evaluation (ImageNet, Paintings) are pre-existing datasets. The authors sample subsets from these datasets, but do not introduce any new dataset primarily for evaluation purposes. Hence, no new dataset is exclusively used for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not present any newly introduced dataset intended primarily for analysis of trends, patterns, or characteristics. The conducted analyses are based on existing datasets."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not present any new dataset serving as a knowledge base to augment models through retrieval or similar mechanisms."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; instead, it uses existing datasets (ImageNet and Paintings) or subsets thereof. Therefore, no practical usage of a new dataset is described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2140-rubric-5",
    "token_usage": {
      "prompt_tokens": 13974,
      "completion_tokens": 518,
      "total_tokens": 14492
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced in the paper uses image datasets (ImageNet and Paintings) without any mention of textual content in multiple human languages. There is no indication of any multilingual linguistic component in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence or description in the paper suggests that the dataset contains exactly two human languages. The dataset content is image data without linguistic annotations."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper is written in English, the dataset itself does not contain English textual entries. It consists of image data without described English-language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of image data from ImageNet and Paintings datasets, with no indication that they contain non-English language content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper presents a method that involves programming code and algorithmic descriptions, but the dataset itself is image data and does not include programming or code-related content as entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3 and 4 (e.g., Equation (1), Eq. (3), Eq. (4), Algorithm 1)",
          "reasoning": "The paper includes mathematical notation and formal expressions to describe the optimization objectives, loss functions, and algorithms involved in generating adversarial examples. These formulas are part of the dataset documentation and the approach description, reflecting that the dataset involves mathematical aspects for defining attacks and metrics."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset includes biological sequences or non-human communication systems. The dataset consists exclusively of image data derived from image recognition datasets."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or evidence that the dataset contains entries in fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset languages or data types are clearly specified as image data from well-known datasets. No ambiguity or unknown language conditions are described."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The dataset introduced in the paper consists of image data only, without inclusion of language content (human language or code) as dataset entries. Therefore, they contain no linguistic content per se."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2140-rubric-6",
    "token_usage": {
      "prompt_tokens": 11192,
      "completion_tokens": 180,
      "total_tokens": 11372
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No mention of code availability in the paper.",
          "reasoning": "The paper does not provide any link, repository, or statement indicating that the code related to data collection, preprocessing, or generation is publicly available. The focus of the paper is on the proposed method and evaluation, but there is no explicit mention of releasing source code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 5.1 Experimental Setup, Datasets paragraph",
          "reasoning": "The paper documents the dataset creation process by detailing how the target class datasets are constructed: sampling 1300 images for each of three target classes from ImageNet, selecting non-target images, and specifying training/testing splits and source/target domain configurations. This level of documentation clearly describes how the datasets for experiments are formed and used, ensuring transparency about the dataset creation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2147-rubric-0",
    "token_usage": {
      "prompt_tokens": 13008,
      "completion_tokens": 212,
      "total_tokens": 13220
    },
    "response": {
      "sources": [
        {
          "Modality": "time series",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4 Experiments, Datasets, and details on ORCA dataset; Section 4.1 and Table 1",
          "Reasoning": "The ORCA dataset is described as containing synthetic trajectory data generated using the ORCA crowd simulator with scenes of agents and obstacles created procedurally. This indicates the trajectories are model generated through simulation, not recorded from humans or real-world sensors."
        },
        {
          "Modality": "time series",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4 Experiments, Datasets, and details on ORCA dataset; Section 4.1 and Table 1",
          "Reasoning": "Within the ORCA dataset, there are two subsets: ORCA-Maps with obstacles and fewer agents, and ORCA-Interact with many agents but no obstacles. Both subsets are synthetic, generated from the ORCA crowd simulator, thus the data is model generated."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2147-rubric-1",
    "token_usage": {
      "prompt_tokens": 13808,
      "completion_tokens": 240,
      "total_tokens": 14048
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2 and Section 4",
            "reasoning": "The paper describes the annotation of motion data and simulation parameters used primarily for training the physics-based Pedestrian Animation Controller (PACER). The data used for training is from motion databases such as AMASS and simulated terrains, body types, and environments. The annotation of these is performed via automatic/deterministic processes such as physics simulation, procedural terrain generation, and body parameter sampling, not human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "No detailed instructions for human annotators annotating the new datasets are mentioned; the data is generated or sampled automatically or from existing databases."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "There is no mention of scoring rubrics or evaluation rubrics used by annotators in the annotation process for the new dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "No examples of annotations or annotation guidelines are provided in the paper for the new dataset annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2147-rubric-2",
    "token_usage": {
      "prompt_tokens": 14998,
      "completion_tokens": 286,
      "total_tokens": 15284
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the new datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance on the new datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided regarding quality assurance conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance performed by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model used as a judge for quality assurance of the new datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any automated verification or rule-based quality assurance applied to the new datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces new synthetic dataset(s) from the ORCA crowd simulator and annotations converted from nuScenes and ETH/UCY datasets but does not document any explicit quality assurance process (human or automated) for these data. There is no mention of annotation verification or validation procedures for the datasets or their annotations."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2147-rubric-3",
    "token_usage": {
      "prompt_tokens": 14608,
      "completion_tokens": 425,
      "total_tokens": 15033
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly state that the authors collected any new human-generated dataset entirely from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the authors generate trajectories using their diffusion model as part of their method, these generated datasets are not newly introduced datasets but outputs from their model used for evaluation, thus not constituting new original datasets created entirely by AI for data contribution."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation of data from other languages via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation applied on data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4. Experiments - Datasets (page 6)",
          "reasoning": "The authors train and evaluate on existing datasets such as ORCA synthetic pedestrian data, ETH/UCY real-world pedestrian data, and nuScenes with annotated semantic maps. These datasets are used as is or combined for training and evaluation. There is no indication the authors created new data but rather aggregated multiple existing datasets, sometimes mixing them for enhanced training."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the authors use data from existing datasets and apply normalization and produce simulation outputs, the paper does not state that they create a substantially new dataset derived through transformations or adaptations of existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of training data is clearly documented in the experiments section."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2147-rubric-4",
    "token_usage": {
      "prompt_tokens": 15126,
      "completion_tokens": 605,
      "total_tokens": 15731
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1.1 and 4.1",
          "reasoning": "The TRACE trajectory diffusion model is trained from scratch to predict pedestrian trajectories conditioned on past motion and environment context. The training process uses ground truth future trajectories (Equation 3 and descriptions in Section 3.1.1). This indicates that the datasets used for training TRACE are employed for training models from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the proposed datasets for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": true,
          "reference": "Section 3.2 and 4.3",
          "reasoning": "The physics-based pedestrian animation controller (PACER) is trained using reinforcement learning with procedural and dataset-based trajectories as training data. In Section 3.2, it is explained that the policy is trained via Proximal Policy Optimization (PPO) and conditioned on trajectories, body type, and environment state. Section 4.3 demonstrates evaluation of the system involving the RL-trained controller, indicating the datasets are used in an RL context after initial training."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "Datasets such as ORCA-Maps, ORCA-Interact (synthetic), ETH/UCY, and nuScenes are used for evaluation of controllability, realism, and guidance performance as shown in Sections 4.1 and 4.2. Metrics such as collision rates, guidance error, Earth Mover's Distance to ground truth data distributions, and trajectory smoothness are computed on these datasets to benchmark the performance of TRACE and PACER."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the newly introduced datasets are used primarily for data analysis of trends or patterns separate from training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the datasets are used as knowledge bases to augment models such as via retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced are clearly used for model training, RL training, and evaluation purposes as detailed above."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2147-rubric-5",
    "token_usage": {
      "prompt_tokens": 15849,
      "completion_tokens": 578,
      "total_tokens": 16427
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced or used in the paper do not mention containing multiple human languages. The data consists of pedestrian trajectories and related motion data without reference to multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset entries contain exactly two human languages. The focus is on pedestrian trajectories and motion rather than linguistic content."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Across the paper, including Abstract, Introduction, and Dataset descriptions in Sections 4.1 and 4.2",
          "reasoning": "All textual content in the datasets and annotations are in English, as is typical for datasets used in autonomous driving, pedestrian forecasting, and computer vision domains. The paper exclusively uses English for dataset descriptions and labels."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or use of datasets containing only a non-English language is made, nor are there any instances of non-English language data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper describes algorithmic models and uses mathematical notation, there is no new dataset comprising programming or structured code-related content introduced."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes mathematical notation to describe models and methods, the new datasets introduced consist of pedestrian trajectories and associated semantic maps, not datasets containing mathematical or logical notation as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets focus on human pedestrian trajectories and do not include biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not contain any fictional or artificial languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) of the datasets are clearly described and identifiable as English, so this label does not apply."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets do contain language, primarily English textual annotations and labels, so this label does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2147-rubric-6",
    "token_usage": {
      "prompt_tokens": 13067,
      "completion_tokens": 210,
      "total_tokens": 13277
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention",
          "reasoning": "The paper does not mention providing a link to code repositories or sharing code related to the dataset creation, data processing, or simulation. There is no explicit statement about releasing the code for the synthetic ORCA datasets or procedures for processing real-world datasets."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4. Experiments - Datasets description and Section 3.1 Method description",
          "reasoning": "The paper documents the datasets used: synthetic ORCA dataset subsets (ORCA-Maps and ORCA-Interact) are described as procedurally generated using the ORCA crowd simulator with details on scene configurations, and the real-world datasets (ETH/UCY and nuScenes) are described with annotation procedures (e.g., semantic maps in nuScenes, pedestrian bounding box conversion). Additionally, the method section details the use of these datasets for training and testing, suggesting a reasonable level of dataset creation/documentation transparency."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2176-rubric-0",
    "token_usage": {
      "prompt_tokens": 10751,
      "completion_tokens": 152,
      "total_tokens": 10903
    },
    "response": {
      "sources": [
        {
          "Modality": "graph",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Setup, Datasets and GNN models",
          "Reasoning": "The paper evaluates attacks on benchmark graph datasets: Cora, Citeseer, and BlogCatalogs. These are widely known citation and social network graphs collected from real-world data with human involvement (citation networks, social interactions). The paper does not explicitly state that the authors introduced new datasets but uses these existing benchmark datasets for evaluation. Since the instruction requests only datasets that are newly introduced by the authors, and no new datasets are presented or introduced by the authors in the paper, this dataset is pre-existing and therefore should not be considered."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2176-rubric-1",
    "token_usage": {
      "prompt_tokens": 11551,
      "completion_tokens": 200,
      "total_tokens": 11751
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 5.1 Dataset and GNN models",
            "reasoning": "The paper introduces a new attack framework applied to existing graph datasets (Cora, Citeseer, BlogCatalogs) but does not introduce a new dataset; thus, no annotation process on a new dataset is described. The research is methodological, focusing on attacks leveraging certified robustness, not dataset creation or annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "Since no new dataset is introduced, the paper does not provide annotation instructions related to any new dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No rubrics are described because no new dataset annotation is performed or documented."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No examples of annotations are provided as the paper does not annotate a new dataset; it only uses existing datasets for evaluation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2176-rubric-2",
    "token_usage": {
      "prompt_tokens": 12741,
      "completion_tokens": 296,
      "total_tokens": 13037
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human annotators involved in quality assurance processes for any dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple expert humans performed quality assurance on the datasets or annotations used."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any single non-expert human performing quality assurance is present in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any involvement of multiple non-expert humans in quality assurance tasks."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No use of AI models as judges or verifiers for quality assurance processes on datasets is described."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated verification or rule-based algorithmic quality assurance process applied to dataset annotations, though it does describe automated computations for attacks and certified robustness."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. It uses pre-existing benchmark datasets such as Cora, Citeseer, and BlogCatalogs without describing any quality assurance process related to annotation or dataset content. Therefore, no quality assurance process is applicable or documented for new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2176-rubric-3",
    "token_usage": {
      "prompt_tokens": 12351,
      "completion_tokens": 348,
      "total_tokens": 12699
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.1 Setup",
          "reasoning": "The paper uses benchmark graph datasets such as Cora, Citeseer, and BlogCatalogs for evaluation, which are established datasets collected from existing sources without significant modification. These datasets are standard benchmarks widely used in graph learning research."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "The paper derives new node weights based on the certified perturbation sizes computed from existing graph data and modifies existing attack loss functions accordingly. This data is based on existing benchmark graphs but transformed by computing certified robustness metrics and applying those in derivation of weighted loss functions and attacks. Therefore, the data used in their certified robustness inspired attack framework is derived from existing datasets with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources are specified and documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2176-rubric-4",
    "token_usage": {
      "prompt_tokens": 12869,
      "completion_tokens": 405,
      "total_tokens": 13274
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any new dataset being used for pre-training models."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training models from scratch using any new dataset introduced by the authors."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss supervised fine-tuning of pre-trained models using any newly proposed dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning post-training methods or related dataset usage are described with respect to new datasets."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used for evaluation (e.g., Cora, Citeseer, BlogCatalog) are well-known benchmark datasets, not newly introduced by the authors."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although analysis is performed, it is based on existing benchmark datasets rather than any new dataset created by the authors."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of a new dataset serving as a knowledge base to augment models is found in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. All datasets mentioned are pre-existing benchmark datasets used for evaluation of the proposed attack framework."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2176-rubric-5",
    "token_usage": {
      "prompt_tokens": 13592,
      "completion_tokens": 585,
      "total_tokens": 14177
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset with entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5.1 Setup, Datasets and GNN models",
          "reasoning": "The paper uses benchmark graph datasets such as Cora, Citeseer, and BlogCatalog, which are citation and social network datasets commonly in English-language research context. The entire paper is in English, and it refers to datasets and tasks in English without any mention of other human languages. Thus, the dataset entries are considered monolingual (English)."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset in a non-English language is described."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses attack implementations (e.g., PGD, Minmax attacks) and mentions implementing attacks in PyTorch, there is no description of any dataset entries consisting of programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notation is used extensively in the description of attacks and models, but this is part of the paper's exposition and methodology, not the content of a novel dataset introduced by the authors. There is no indication that a new dataset contains mathematical or logical expressions as data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication systems are part of the datasets presented or introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset involving constructed or artificial languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) of the datasets are clear and documented; no ambiguity exists."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries clearly consist of textual and graph data representing human language and related annotations; thus, language is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2176-rubric-6",
    "token_usage": {
      "prompt_tokens": 10810,
      "completion_tokens": 110,
      "total_tokens": 10920
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not mention any code availability or provide a link to a repository for dataset construction or data preprocessing related to new datasets."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets, nor does it describe the dataset construction process or provide documentation about any new dataset creation. The experiments use benchmark datasets such as Cora, Citeseer, and BlogCatalogs, which are pre-existing datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2182-rubric-0",
    "token_usage": {
      "prompt_tokens": 12577,
      "completion_tokens": 116,
      "total_tokens": 12693
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.6 Implementation Details",
          "Reasoning": "The authors train their FSNet model on image pairs from the ScanNet and MegaDepth datasets introduced by previous work. However, the paper does not indicate that the authors introduce a new dataset themselves; rather, they generate training and validation sets by extracting correspondences and hypotheses from these established datasets. There is no explicit mention of a new dataset collected or introduced by the authors in their work."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2182-rubric-1",
    "token_usage": {
      "prompt_tokens": 13377,
      "completion_tokens": 254,
      "total_tokens": 13631
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.6 Implementation Details",
            "reasoning": "The training and evaluation datasets are generated by automated processes involving feature extraction (SuperPoint-SuperGlue correspondences), minimal subset random sampling, fundamental matrix hypothesis generation, and computation of ground truth pose errors. There is no indication of human annotators performing manual annotations for the new dataset."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.6 Implementation Details",
            "reasoning": "No detailed annotation instructions or guidelines for human annotators are provided since the data generation involves an automatic process using available datasets and automated sampling methods."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4.5 Loss Function",
            "reasoning": "The scoring and supervision of hypotheses use quantitative criteria: angular rotation and translation errors computed against ground truth, used as a supervised loss signal for training (e.g., soft-clamped L1 loss on pose errors). These act as rubrics defining how to evaluate hypothesis quality."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not provided",
            "reasoning": "The paper does not present specific annotation examples or exemplar annotations illustrating the scoring or labeling process for individual data samples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2182-rubric-2",
    "token_usage": {
      "prompt_tokens": 14567,
      "completion_tokens": 358,
      "total_tokens": 14925
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving a single human expert annotator for dataset validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any involvement of multiple human experts performing quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description in the paper of a single non-expert human annotator performing quality assurance for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper proposes an AI model (FSNet) to score two-view geometry hypotheses, but it does not use an AI model to perform quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.6 Implementation Details and Section 3 Analysis",
          "reasoning": "The dataset annotations are obtained automatically by generating fundamental matrix hypotheses from correspondences extracted by the SuperPoint-SuperGlue model, and ground truth camera poses and depth maps are used to calculate angular rotation and translation errors automatically. There is no description of manual annotation or human involvement in quality assurance; instead, automated verification processes based on ground-truth pose and depth information are used to ensure the correctness of annotations and training labels."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an automated process using ground truth data for labeling and verification; therefore, a quality assurance process is present and documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2182-rubric-3",
    "token_usage": {
      "prompt_tokens": 14177,
      "completion_tokens": 476,
      "total_tokens": 14653
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing datasets such as ScanNet [18] and MegaDepth [35] for training and evaluation. There is no indication that the authors created new datasets from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the method generates fundamental matrix hypotheses using models, the datasets themselves are not newly generated data by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of any dataset being produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was involved in dataset creation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.6 Implementation Details and throughout the paper",
          "reasoning": "The authors use existing public datasets such as ScanNet [18], MegaDepth [35], and PhotosphereTourism for training and evaluation. They extract correspondences with established methods (e.g., SuperPoint-SuperGlue) and generate fundamental matrix hypotheses using existing frameworks (USAC). The data used for training and evaluation is collected and aggregated from these existing sources without indication of creation of new original datasets."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.6 Implementation Details and Section 3 Analysis",
          "reasoning": "The authors derive training data by generating sets of fundamental matrix hypotheses from existing correspondences extracted from established datasets. They compute angular translation and rotation errors using ground truth camera poses and intrinsic parameters. Thus, the data used for training (hypotheses, pose errors) is derived by transformations and processing of existing raw datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is sufficiently documented as existing public datasets and their processed derivatives."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2182-rubric-4",
    "token_usage": {
      "prompt_tokens": 14695,
      "completion_tokens": 430,
      "total_tokens": 15125
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.6 Implementation Details",
          "reasoning": "The paper describes training FSNet models on ScanNet and MegaDepth datasets (Sections 4.6 and 5). These datasets are used to train FSNet from scratch by supervising it with relative pose error regression."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention fine-tuning a pre-trained model on the proposed datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning based post-training or RLHF usage."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 5.1 and 5.2",
          "reasoning": "The paper uses ScanNet, MegaDepth, and PhotosphereTourism datasets to evaluate fundamental and essential matrix estimation accuracy, benchmarking FSNet against other methods."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 Analysis",
          "reasoning": "ScanNet and MegaDepth validation splits are mined and analyzed in Section 3 to study failures and properties influencing fundamental matrix estimation, such as number of correspondences."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as a knowledge base for retrieval or augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced and used in the paper are clearly employed for training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2182-rubric-5",
    "token_usage": {
      "prompt_tokens": 15418,
      "completion_tokens": 523,
      "total_tokens": 15941
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the introduction of any dataset containing entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is introduced in the paper containing exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset with exclusively English language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset introduced contains entries with exactly one non-English language according to the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions code repositories and software implementations but does not introduce a new dataset containing entries in programming or structured code-related languages."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical expressions and notation in the text for describing the method and models, it does not introduce a dataset whose entries contain mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not include any dataset entries involving biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are involved in any newly introduced dataset in the paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset with unspecified or undocumented language content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. Instead, it uses established datasets such as ScanNet and MegaDepth as training and evaluation data. The content consists of image pairs, feature correspondences, and geometric hypotheses, which do not involve human languages. Therefore, the entries do not contain any linguistic data."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2182-rubric-6",
    "token_usage": {
      "prompt_tokens": 12636,
      "completion_tokens": 201,
      "total_tokens": 12837
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or link describing code for dataset construction is provided in the paper.",
          "reasoning": "The paper mentions code at the GitHub repository for the main method (https://www.github.com/nianticlabs/scoring-without-correspondences) but does not indicate that code for dataset construction or generation is provided or made publicly available. The work uses existing datasets (ScanNet, MegaDepth, PhotoTourism) for training and evaluation, but no new datasets are introduced or constructed by the authors."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No explicit description of dataset creation or construction is present in the paper.",
          "reasoning": "The paper employs publicly available datasets (ScanNet, MegaDepth, PhotoTourism) and generates fundamental matrix hypotheses using existing correspondences and minimal sampling methods. There is no description or documentation about any new dataset creation process because the authors do not introduce any new datasets in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2210-rubric-0",
    "token_usage": {
      "prompt_tokens": 13256,
      "completion_tokens": 4,
      "total_tokens": 13260
    },
    "response": {
      "sources": []
    }
  },
  {
    "id": "CVPR-2023-2210-rubric-1",
    "token_usage": {
      "prompt_tokens": 14056,
      "completion_tokens": 241,
      "total_tokens": 14297
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3 Hierarchical Mask Calibration, Figure 2",
            "reasoning": "The paper describes that pseudo-label masks for the target domain are generated by the momentum model Gm and calibrated by the Hierarchical Mask Calibration (HMC) process automatically and online during training. This indicates that the annotations (pseudo masks) for the unlabeled target domain are produced by an algorithmic model pipeline, not by human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not describe any human annotation instructions given to annotators for the new dataset. The target domain data are unlabeled; pseudo labels are generated automatically by the model and calibrated online."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No mention of scoring rubrics or grading criteria for human annotation in the new dataset in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No annotation examples or human labeling guidelines are provided for the new dataset. The paper focuses on automated pseudo-labeling and calibration in domain adaptation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2210-rubric-2",
    "token_usage": {
      "prompt_tokens": 15246,
      "completion_tokens": 408,
      "total_tokens": 15654
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance involving a single human expert for dataset annotation validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description of quality assurance performed by multiple human experts for the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of quality assurance by a single non-expert human annotator is present in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process conducted by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses models for pseudo-label generation and mask calibration, it does not describe the use of an AI model as a quality assurance judge of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper presents automated processes for pseudo-label calibration and hierarchical mask calibration internally for training, but these do not constitute quality assurance procedures applied to dataset annotations themselves."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset with annotation or dataset content that undergoes a quality assurance process. The introduced method adapts existing public datasets (SYNTHIA, Cityscapes, VIPER, Foggy Cityscapes), and no quality assurance process related to annotation or dataset content is described for any newly introduced datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2210-rubric-3",
    "token_usage": {
      "prompt_tokens": 14856,
      "completion_tokens": 264,
      "total_tokens": 15120
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper uses existing datasets SYNTHIA, Cityscapes, Foggy Cityscapes, and VIPER for domain adaptation tasks. These datasets are widely used benchmarks, and no indication is given of dataset creation by the authors, implying that data are collected or aggregated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2210-rubric-4",
    "token_usage": {
      "prompt_tokens": 15374,
      "completion_tokens": 476,
      "total_tokens": 15850
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe or demonstrate the use of any newly introduced dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used (SYNTHIA, Cityscapes, Foggy Cityscapes, VIPER) are existing public datasets; no new datasets are introduced or used to train a model from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not present any new datasets used specifically for supervised fine-tuning of pre-trained models. The training is performed on existing source domain datasets for supervised learning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention reinforcement learning post-training techniques or use any datasets for such methods."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing public datasets (SYNTHIA, Cityscapes, Foggy Cityscapes, VIPER) for evaluation but does not introduce any new datasets solely for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets are introduced for the purpose of analyzing trends, patterns, or characteristics. Analysis is performed on existing datasets."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset that serves as a knowledge base to augment models."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new datasets are introduced by the authors in this paper; the paper uses existing public datasets for training, adaptation, and evaluation. Therefore, there is no documented practical usage or utility of new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2210-rubric-5",
    "token_usage": {
      "prompt_tokens": 16097,
      "completion_tokens": 546,
      "total_tokens": 16643
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only references visual datasets for domain adaptive panoptic segmentation but all datasets are image-based with no mention of multiple human languages being present in the datasets."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets mentioned do not contain exactly two human languages; there is no language content described at all."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's datasets consist purely of images for computer vision tasks with no textual language data, so they do not contain entries with only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that datasets contain any language content, English or non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the model described includes mathematical formulations and code-like expressions in the methodology, the datasets themselves are visual segmentation datasets and do not include programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notations are present in the paper describing model and loss functions but the dataset itself does not contain mathematical or logical expressions as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described consist of visual images from urban scenes and synthetic environments; these do not involve biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets contain entries with fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are clearly described as visual domain adaptation datasets; their content is known (images) without any undocumented languages."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The datasets introduced consist entirely of images representing urban scenes and synthetic data for panoptic segmentation tasks. They do not contain any human language entries or linguistic content."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2210-rubric-6",
    "token_usage": {
      "prompt_tokens": 13315,
      "completion_tokens": 151,
      "total_tokens": 13466
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "The paper does not mention any code release or repository related to dataset construction.",
          "reasoning": "The paper uses existing public datasets (e.g., SYNTHIA, Cityscapes, Foggy Cityscapes, VIPER) and does not introduce any new dataset. There is no information about any code release for data collection, preprocessing, or generation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "The paper does not provide any description of a dataset creation process.",
          "reasoning": "Since the paper does not introduce any new dataset, there is no documentation related to dataset creation or construction within the paper. All datasets used are well-known public benchmarks referenced without detailed creation information."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2212-rubric-0",
    "token_usage": {
      "prompt_tokens": 12592,
      "completion_tokens": 133,
      "total_tokens": 12725
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 4 (Evaluation), and throughout the paper",
          "Reasoning": "The paper introduces the UniDistill framework evaluated on nuScenes dataset, which is a well-known autonomous driving dataset containing LiDAR point clouds (signal/sensor data). The dataset is human-generated from real-world sensor recordings in autonomous driving scenarios as stated when mentioning evaluations and experiments on nuScenes. The paper does not indicate introducing any new data but uses existing nuScenes for evaluation\u2014thus, no new dataset is introduced with new data sources."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2212-rubric-1",
    "token_usage": {
      "prompt_tokens": 13392,
      "completion_tokens": 249,
      "total_tokens": 13641
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 1 Introduction, Section 3 Methodology, Section 4 Experiments",
            "reasoning": "The dataset in this paper is primarily the nuScenes dataset used for evaluation and experiments. The paper focuses on proposing a knowledge distillation framework rather than introducing or annotating a new dataset. The nuScenes dataset is a well-known existing dataset for 3D object detection and no new dataset is introduced or annotated by human annotators within this paper."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No section in paper introduces data annotation instructions for a new dataset.",
            "reasoning": "The paper does not describe any data annotation guidelines, instructions, or processes since it does not introduce or annotate any new dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No section in paper introduces scoring rubrics for data annotation.",
            "reasoning": "No scoring rubrics or evaluation criteria for annotation are described because no new dataset annotation is presented."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of annotation are provided anywhere in the paper.",
            "reasoning": "No annotation examples are present as there is no new dataset or annotation described."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2212-rubric-2",
    "token_usage": {
      "prompt_tokens": 14582,
      "completion_tokens": 316,
      "total_tokens": 14898
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert or annotator. There is no description of manual annotation or validation of datasets or content by a subject matter expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information or mention of multiple human experts conducting quality assurance of dataset annotations or content in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any single non-expert human performing quality assurance on dataset annotations or content."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that multiple non-expert humans performed quality assurance or validation of datasets or annotations."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that an AI model was used as a judge or for quality assurance of dataset annotations, labels, or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any automated verification or algorithmic/rule-based quality assurance process applied to dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset and does not describe any quality assurance process for dataset annotation or content; it primarily proposes a knowledge distillation framework evaluated on the pre-existing nuScenes dataset. Therefore, no quality assurance process is applied or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2212-rubric-3",
    "token_usage": {
      "prompt_tokens": 14192,
      "completion_tokens": 356,
      "total_tokens": 14548
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the authors created any original dataset from scratch. Instead, it uses the nuScenes dataset for experiments."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating new data entirely by AI or models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the data being produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of machine translation applied to data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the collection or aggregation of datasets from existing sources by the authors as a new dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors propose a framework that works with existing datasets (nuScenes) but do not describe creating any new dataset derived from existing sources."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. It uses the publicly available nuScenes dataset for evaluation. Therefore, there is no specification or documentation of any new data source introduced by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2212-rubric-4",
    "token_usage": {
      "prompt_tokens": 14710,
      "completion_tokens": 412,
      "total_tokens": 15122
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 Methodology, Section 4.1 Implementation details, Section 4.2 Comparison with State-of-the-Arts",
          "reasoning": "The dataset (nuScenes) is used to train 3D object detectors from scratch or with knowledge distillation framework applied, as described in Section 3 and demonstrated in Section 4. The training batch size and epochs are specified, and the detectors are trained with and without UniDistill, indicating dataset use in training from randomly initialized models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 UniDistill, Section 4 Experiments",
          "reasoning": "The proposed UniDistill framework uses the dataset to fine-tune (or train) student models based on a teacher, with supervised distillation losses, improving performance without inference cost, indicating usage for supervised fine-tuning on the dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 and Table 1 Reporting performance of student detectors on nuScenes test dataset",
          "reasoning": "The nuScenes dataset is employed for evaluation and benchmarking of the proposed UniDistill method and student detectors\u2019 performance, as reported with metrics such as mAP and NDS on the test set."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3 Ablation Studies and Section 4.4 Visualization",
          "reasoning": "The dataset is used to analyze the effects of different distillation losses and design choices by ablative experiments and visualize detection results, indicating use for detailed analysis of trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2212-rubric-5",
    "token_usage": {
      "prompt_tokens": 15433,
      "completion_tokens": 570,
      "total_tokens": 16003
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces a dataset focused on 3D object detection for autonomous driving, using sensor data modalities such as LiDAR and camera images. There is no indication that the dataset contains entries in different human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of dataset entries containing exactly two human languages. The dataset is about sensor data and object annotations, not linguistic content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper is written in English and the annotations and labels might be in English, the dataset itself does not consist of natural language entries but rather sensor data and annotations for object detection."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence of dataset entries containing only a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes code and methods but does not introduce a new dataset containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains mathematical formulas to describe their methods, but the dataset itself does not contain entries in mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset pertains to 3D object detection in autonomous driving and involves sensor data; no biological or non-human communication data is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language or linguistic content of dataset entries is well specified: they are sensor data and object labels, not natural language entries, so this category does not apply."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The dataset introduced is a 3D object detection dataset consisting of sensor modalities such as LiDAR point clouds and camera images along with object annotations. These do not contain any linguistic content or human language entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2212-rubric-6",
    "token_usage": {
      "prompt_tokens": 12651,
      "completion_tokens": 147,
      "total_tokens": 12798
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section mentions code release for dataset construction",
          "reasoning": "The paper does not mention any new dataset creation or introduce code related to data collection, preprocessing, or dataset generation. It primarily uses the existing nuScenes dataset and focuses on knowledge distillation methods rather than data generation. Therefore, there is no code provided for dataset construction."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section describes dataset creation",
          "reasoning": "The paper does not introduce any new dataset or discuss the dataset creation process. It only utilizes existing datasets (nuScenes) for evaluation and experimentation. Hence, there is no documentation related to dataset creation process provided in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2238-rubric-0",
    "token_usage": {
      "prompt_tokens": 15686,
      "completion_tokens": 125,
      "total_tokens": 15811
    },
    "response": {
      "sources": [
        {
          "Modality": "time series",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section Dataset: ETH-UCY [8] and 4.1 Experimental Setup",
          "Reasoning": "The ETH-UCY dataset, utilized for evaluation, consists of pedestrian trajectories represented as sequences of world-coordinate positions over time, which constitutes time series data. These trajectories were captured in real-world environments, implying human involvement in data collection. The paper explicitly states using the ETH-UCY dataset as the test benchmark but does not introduce any new dataset generated by the authors themselves."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2238-rubric-1",
    "token_usage": {
      "prompt_tokens": 16486,
      "completion_tokens": 259,
      "total_tokens": 16745
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Experimental Setup and Dataset descriptions",
            "reasoning": "The paper introduces BOsampler, an unsupervised sampling promoting method that integrates with existing pre-trained stochastic trajectory prediction models without retraining or manual annotation. The evaluation is conducted on publicly available ETH-UCY datasets with processed trajectories and exception subsets identified by an off-the-shelf Kalman filter method, indicating an automatic process for subset selection and scoring rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "The paper does not provide any annotation instructions or guidelines for human annotators related to new data annotation on the new dataset; rather, the exception subset is selected using an automatic Kalman filter-based method."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "No scoring rubrics are presented for human annotators; metric evaluation is performed algorithmically using minADE and minFDE computed against ground truth data."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "There are no provided annotation examples or instructions for manual data annotation processes; example trajectories and visualization relate to model outputs, not annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2238-rubric-2",
    "token_usage": {
      "prompt_tokens": 17676,
      "completion_tokens": 272,
      "total_tokens": 17948
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts were involved in quality assurance of the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided about a single non-expert performing quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The QA is not described as performed by an AI model judging dataset quality."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the approach uses algorithmic and Bayesian optimization techniques for sampling, there is no explicit indication that quality assurance of the dataset annotations or content is done through automated verification processes."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses the ETH-UCY dataset, an existing public benchmark dataset, but does not introduce any new datasets; quality assurance processes for the dataset are not described or documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2238-rubric-3",
    "token_usage": {
      "prompt_tokens": 17286,
      "completion_tokens": 398,
      "total_tokens": 17684
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the creation of any new datasets collected entirely by human contributors. It uses existing benchmark datasets such as ETH-UCY for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets are generated purely by AI or machine learning models from scratch. The authors propose a method to improve the sampling process of existing models rather than generating new datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by machine translation from other languages."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 'Dataset: ETH-UCY, 4.1 Experimental Setup'",
          "reasoning": "The authors use the ETH-UCY dataset, a well-known public benchmark dataset that collates pedestrian trajectories collected from multiple scenes and existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe creating new datasets derived from existing data by applying notable transformations or adaptations; it rather proposes a method to refine sampling on existing trained models using existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source is specified and documented (ETH-UCY dataset), so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2238-rubric-4",
    "token_usage": {
      "prompt_tokens": 17804,
      "completion_tokens": 458,
      "total_tokens": 18262
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the ETH-UCY dataset or any other new dataset introduced by the authors for pre-training large models."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The ETH-UCY dataset used is a standard benchmark dataset and is not introduced by the authors as a new dataset. The paper does not mention training from scratch on a newly introduced dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No supervised fine-tuning on a newly introduced dataset is described in the paper."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning method or related post-training on a newly introduced dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper evaluates the method using the ETH-UCY dataset, a pre-existing public dataset. There is no new dataset introduced by the authors that is used exclusively for evaluation or benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper analyses sampling methods and their distributions but not on any new dataset introduced by the authors."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention or describe a knowledge base dataset introduced by the authors used for model augmentation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. The datasets mentioned (e.g., ETH-UCY, ETH, UCY) are existing benchmark datasets. Therefore, no practical usage of new datasets is described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2238-rubric-5",
    "token_usage": {
      "prompt_tokens": 18527,
      "completion_tokens": 554,
      "total_tokens": 19081
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset used is ETH-UCY, which consists of pedestrian trajectories represented in world coordinates. There is no mention of the dataset including multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that the dataset includes exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4. Dataset: ETH-UCY and Experimental Setup",
          "reasoning": "The dataset ETH-UCY contains pedestrian trajectory data which is numerical and coordinate-based, and all textual content and annotations in the paper are in English only. The dataset contains no indication of other human languages, and the paper text and annotations are entirely in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of any non-English language content in the new dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the paper discusses algorithms, Bayesian optimization, and sampling techniques, it does not introduce a new dataset with programming language or code entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains mathematical equations and formulae to describe the method, but these are not entries within the dataset itself."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data is included in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional language data is present in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset entries is not unknown; it is numerical coordinate data with English language textual annotations in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains numerical trajectory data representing human pedestrian movement, which is not non-linguistic content; also the paper and dataset annotations are in English, so language is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2238-rubric-6",
    "token_usage": {
      "prompt_tokens": 15745,
      "completion_tokens": 129,
      "total_tokens": 15874
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention regarding dataset construction code availability",
          "reasoning": "The paper mentions the usage of the ETH-UCY dataset but does not introduce any new dataset nor provide code related to dataset construction or preprocessing. The source code released link pertains to their method code, not to dataset construction."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section describes dataset construction process",
          "reasoning": "The paper uses existing datasets (ETH-UCY) and does not introduce any new dataset or describe dataset construction process. Therefore, no dataset creation documentation is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2239-rubric-0",
    "token_usage": {
      "prompt_tokens": 11245,
      "completion_tokens": 118,
      "total_tokens": 11363
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.3 Implementation - paragraph on dataset and training details; Section 5.1 Datasets",
          "Reasoning": "The authors train their unsupervised 3D motion segmentation network on the training set of DAVIS2016 (DA VIS2016) dataset, which consists of 50 videos filmed and annotated by humans. The paper explicitly states they use this dataset's training set for training, indicating the data is human generated video sequences."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2239-rubric-1",
    "token_usage": {
      "prompt_tokens": 12045,
      "completion_tokens": 209,
      "total_tokens": 12254
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.3 Data augmentation and network training",
            "reasoning": "The method is fully unsupervised and does not use any manual annotation. The training of the 3D motion segmentation network is done without any human annotators involved. Thus, the motion segmentation masks are produced automatically by the network without human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation instructions are provided since no human annotators are used; annotation is not performed manually but generated in an unsupervised manner by the model itself."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics are described or necessary for annotation because there is no human annotation process involved."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples are provided, as labels are not manually created but produced automatically in an unsupervised way."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2239-rubric-2",
    "token_usage": {
      "prompt_tokens": 13235,
      "completion_tokens": 303,
      "total_tokens": 13538
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance of the datasets conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance steps involving multiple human experts on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of QA conducted by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information about QA by multiple human non-expert annotators for any dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of an AI model as a judge for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the method involves automated procedures in network training and loss functions, there is no description that these are used as quality assurance processes for the datasets themselves."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses pre-existing datasets (DAVIS2016, SegTrackV2, FBMS59) and does not introduce any new datasets. Therefore, no quality assurance of new dataset annotations or content is described or applied. The focus is on method development and evaluation rather than dataset creation."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2239-rubric-3",
    "token_usage": {
      "prompt_tokens": 12845,
      "completion_tokens": 432,
      "total_tokens": 13277
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any new dataset was created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any new dataset was generated purely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any dataset produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of datasets generated via machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.1",
          "reasoning": "The method is trained and evaluated on existing VOS benchmark datasets such as DAVIS2016, SegTrackV2, and FBMS59, which are publicly available datasets openly used for evaluation. Since these datasets were not created by the authors, and no new data creation is described, the data is effectively collated from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.2, 5.1",
          "reasoning": "The method uses optical flow fields computed from the existing video datasets (DAVIS2016 train set) using the RAFT model, with additional augmentation applied such as adding global flow and corrupting inputs during training. Hence, the input data to the network is derived from existing datasets with modifications like normalization, augmentations, and transformation to flow fields."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data is clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2239-rubric-4",
    "token_usage": {
      "prompt_tokens": 13363,
      "completion_tokens": 294,
      "total_tokens": 13657
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.3 (Data augmentation and network training)",
          "reasoning": "The authors train their 3D motion segmentation network using the DAVIS2016 training set from scratch with randomly initialized parameters in a fully unsupervised manner, without any manual annotation."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1 (Datasets) and Section 5.3 (Quantitative and comparative evaluation)",
          "reasoning": "The DAVIS2016, SegTrackV2 and FBMS59 datasets are used as benchmarks to evaluate and compare the performance of the proposed method quantitatively and qualitatively."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes the use of existing datasets (DAVIS2016, SegTrackV2, FBMS59) for training from scratch and for evaluation. No new dataset is introduced by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2239-rubric-5",
    "token_usage": {
      "prompt_tokens": 14086,
      "completion_tokens": 575,
      "total_tokens": 14661
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset containing entries in more than two human languages. The datasets used (DAVIS2016, SegTrackV2, FBMS59) are existing video segmentation datasets and contain video data without linguistic content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset with exactly two human languages is introduced. The datasets are visual video datasets not involving language."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper is written in English and references datasets with English descriptions, no new dataset introduced consists of only English textual content. The datasets are video data without language entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no new dataset introduced that contains entries in exactly one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets mentioned and proposed are video datasets for motion segmentation. The authors mention their code implementations are available, but the dataset itself does not contain code or programming language as data entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical formulae describing their models and loss functions, these are part of the paper's method description, not of any new dataset entries. The new datasets are composed of video data, not mathematical or logical symbolic data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data are introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificial languages are included in any new datasets introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language properties of datasets are either not applicable or are clearly not involving language."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset containing language content; rather it uses or refers to video datasets composed of visual data (video frames and optical flow). Therefore, the datasets contain no language entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2239-rubric-6",
    "token_usage": {
      "prompt_tokens": 11304,
      "completion_tokens": 189,
      "total_tokens": 11493
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 4.1 (Network coding)",
          "reasoning": "The paper states in Section 4.1 that the authors have developed an original and flexible implementation of a U-net for motion segmentation and that they will make the code available in an open source repository, providing a GitHub link (https://github.com/Etienne-Meunier-Inria/GeneralUnet). This indicates that the code relevant to data preprocessing and model implementation for their dataset processing is publicly available."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "N/A",
          "reasoning": "The paper does not introduce or create any new datasets; it utilizes existing benchmark datasets (DAVIS2016, SegTrackV2, FBMS59) and provides no documentation or description regarding the creation of a new dataset in the paper. Therefore, there is no documentation about dataset creation to evaluate."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-224-rubric-0",
    "token_usage": {
      "prompt_tokens": 11006,
      "completion_tokens": 254,
      "total_tokens": 11260
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5, Datasets paragraph; Figure 1; Section 6 Experiments",
          "Reasoning": "The paper introduces BiasBed as a new evaluation framework that includes multiple datasets; five are texture-bias datasets including Stylized ImageNet, Cue-Conflict, Edges, Sketches, and Silhouettes. Stylized ImageNet and Cue-Conflict are stylized versions of ImageNet images, created by augmenting natural images. Edges, Sketches, and Silhouettes are derived datasets containing pure shape information, created from existing images. These datasets are fundamentally image-based and the source images originate from human-generated acquisitions (photographs or human-created sketches/labels). Though some images are stylized or augmented (e.g., stylized ImageNet), this stylization is a procedural transformation, but the raw content is human-generated images. Therefore, the modality is image, and the origin is human generated (original photos and human-created datasets). The paper explicitly distinguishes these as new datasets included in BiasBed, indicating that these datasets come from human-generated images and are used within the new BiasBed framework introduced by the authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-224-rubric-1",
    "token_usage": {
      "prompt_tokens": 11806,
      "completion_tokens": 295,
      "total_tokens": 12101
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 5, 'Datasets' and 'BiasBed framework'",
            "reasoning": "The new datasets included in BiasBed are added as dataloaders integrated into the BiasBed testbed framework. The paper describes that these include Stylized ImageNet, Cue-Conflict, Edges, Sketches, Silhouettes, along with robustness datasets. These datasets are publicly available or stylized variants of existing datasets, loaded automatically for algorithm evaluation. The annotation process is thus automated via dataset loaders rather than manual human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "The paper does not mention providing any manual annotation instructions or guidelines for annotators for the new datasets because the datasets are stylized or derived from existing datasets, with automatic transformations; no manual human annotation is described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "No scoring rubrics related to annotation guidelines are described since the data labeling comes from established dataset ground truths or automated transformations; thus, no rubric for annotation scoring applies."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "No annotation examples or example labels beyond the original datasets are described. Since the new datasets are modified versions or derived automatically, no separate annotation examples are given."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-224-rubric-2",
    "token_usage": {
      "prompt_tokens": 12996,
      "completion_tokens": 347,
      "total_tokens": 13343
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process for datasets conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance involving multiple human experts for the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of quality assurance by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that an AI model was used to perform quality assurance on dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper describes automated training, evaluation, and statistical hypothesis testing, it does not document an automated quality assurance process for dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces BiasBed as a testbed framework incorporating existing datasets (e.g., Stylized ImageNet, Cue-Conflict, Edges, Sketches, Silhouettes, ImageNet1K, ImageNet-A, ImageNet-R, DeepAug datasets) but does not describe any quality assurance process applied to these datasets. The datasets themselves are pre-existing and BiasBed includes data loaders for them; no new dataset introduced by the authors is described as having undergone a quality assurance process. Hence, no quality assurance process is applied or documented for the new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-224-rubric-3",
    "token_usage": {
      "prompt_tokens": 12606,
      "completion_tokens": 443,
      "total_tokens": 13049
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets created entirely from scratch by human contributors. The datasets used are existing ones or derived versions, not newly created human data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new datasets generated entirely by AI or machine learning models. While stylized datasets exist, they are pre-existing; the paper does not create new model-generated datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of datasets produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any datasets produced by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing datasets but does not explicitly mention collecting or aggregating them anew without modification as a contribution."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 5: Datasets",
          "reasoning": "The paper integrates several datasets including stylized versions of ImageNet (e.g., Stylized ImageNet and Cue-Conflict) and shape-based datasets like Edges, Sketches, Silhouettes, which are derived or transformed from existing sources. Additionally, robustness datasets like ImageNet-A and ImageNet-R are included. All these datasets are existing datasets or existing datasets modified with style or texture transformations, thus they are derived data. The paper does not introduce new datasets, but includes these derived versions."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is well documented and discussed in the paper as derived from existing sources with transformation."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-224-rubric-4",
    "token_usage": {
      "prompt_tokens": 13124,
      "completion_tokens": 415,
      "total_tokens": 13539
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe or demonstrate use of any new datasets for pre-training purposes."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new datasets introduced for training models from scratch. All datasets mentioned are existing datasets integrated into the BiasBed framework."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets are introduced or used by the authors for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention reinforcement learning post-training techniques or usage of new datasets for such methods."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 (BiasBed framework), Section 6 (Experiments)",
          "reasoning": "The paper introduces BiasBed, a testbed that includes several datasets, some existing and some stylized or transformed variants, for rigorous evaluation of texture bias algorithms. These datasets are used explicitly for benchmarking and performance evaluation in a statistically rigorous manner. However, no evidence is found that the authors introduce entirely new original datasets; rather, they assemble and extend existing datasets. The use of datasets in BiasBed is for evaluation of algorithms."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 6 (Experiments), Section 7 (Discussion)",
          "reasoning": "The datasets are used to analyze algorithm performance regarding texture bias, including trends and behavior under domain shifts. The authors analyze fluctuations, model selection effects, and statistical testing results, indicating analysis usage of the datasets."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or demonstration of using any dataset as a knowledge base to augment models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets assembled and used in BiasBed are clearly employed for evaluation and analysis tasks."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-224-rubric-5",
    "token_usage": {
      "prompt_tokens": 13847,
      "completion_tokens": 498,
      "total_tokens": 14345
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are image datasets (e.g., Stylized ImageNet, Cue-Conflict, Edges, Sketches, Silhouettes, etc.) and robustness datasets for vision tasks; the paper does not present datasets comprising programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3 (Biased learning)",
          "reasoning": "The paper presents mathematical formalism describing their data and methods, including definitions of statistical distributions, feature maps, and formulas for style transfer. While these are part of the methodological description, they also appear in their datasets' handling and augmentation, indicating that mathematical/logical notation is inherent to the dataset construction and evaluation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets with biological sequences or non-human communication systems are introduced or described in the paper."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional/artificial languages are mentioned or included in the introduced datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages or data types of the datasets introduced are fully documented and specified as image data."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The datasets introduced are image datasets for texture bias evaluation and robustness testing with no linguistic content; thus, they do not contain any human language or linguistic text content."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-224-rubric-6",
    "token_usage": {
      "prompt_tokens": 11065,
      "completion_tokens": 216,
      "total_tokens": 11281
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 5 BiasBed framework",
          "reasoning": "The paper explicitly states that BiasBed is released openly with code available at https://github.com/D1noFuzi/BiasBed. This includes data loaders for multiple datasets, including six texture-bias datasets and four robustness datasets, implying that the code related to dataset construction and preprocessing is publicly available as part of this framework."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 5 BiasBed framework and Section 6 Experiments",
          "reasoning": "The paper provides details about the included datasets in BiasBed, listing six texture-bias datasets and four robustness datasets along with their characteristics, origins, and usage (e.g., Stylized ImageNet, Cue-Conflict, Edges, Sketches, Silhouettes, ImageNet-A, ImageNet-R, DeepAug variants). Also, the paper explains how these data loaders implement these datasets and the evaluation protocols, ensuring transparency about dataset creation and usage within the BiasBed framework."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2276-rubric-0",
    "token_usage": {
      "prompt_tokens": 13830,
      "completion_tokens": 168,
      "total_tokens": 13998
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Step 0: Starting Ingredients, Experimental Setup",
          "Reasoning": "The authors use WebVid-2M dataset as initial pretraining data, which consists of video data. It is a publicly available video dataset whose videos originate from human-created sources and recorded content, not generated by models."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Step 4: Pretraining Data",
          "Reasoning": "The authors use the CC3M dataset, a dataset of 3 million image-text pairs collected from human-generated web images with text captions. The images are human-generated in the sense of being captured or created by humans, not synthesized by models."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2276-rubric-1",
    "token_usage": {
      "prompt_tokens": 14630,
      "completion_tokens": 385,
      "total_tokens": 15015
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3 (Step 0 to Step 6) and Experimental Setup",
            "reasoning": "The paper describes the pretraining of video-language models using standard datasets like WebVid-2M and CC3M. These datasets are publicly available and the annotation process is not described as involving human annotators; rather, the datasets were collected or constructed prior to this work, or are existing datasets. The study builds models using these datasets and evaluates them. The model training and evaluation, including verification of paired video and text data, is likely via automatic processes. Moreover, the paper discusses WebVid-2M and CC3M as pretraining data, and explicitly states the use of existing datasets like MSR-VTT, DiDeMo, ActivityNet, and others for evaluation. There is no mention of a newly introduced dataset or any new human annotation efforts detailed in the paper. Therefore, annotations are via automatic process or existing datasets, and no new manual annotations are introduced or discussed in the paper."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No section in paper describing annotation instructions for new dataset",
            "reasoning": "The paper does not introduce a new dataset or describe any annotation instructions or protocols associated with new data annotation. The study uses existing datasets and describes model training and evaluation only."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No section in paper describing rubric usage for new dataset annotation",
            "reasoning": "No annotation rubrics or scoring criteria are presented or discussed in relation to any newly introduced dataset, since no new dataset or new annotation effort is described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No section or appendix describing annotation examples",
            "reasoning": "The paper does not provide annotation examples or guidelines because it does not present or release any new dataset requiring manual annotations or human labeling."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2276-rubric-2",
    "token_usage": {
      "prompt_tokens": 15820,
      "completion_tokens": 294,
      "total_tokens": 16114
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset annotation or content quality assurance performed by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about any dataset quality assurance done by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using AI models as judges for quality assurance of datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any automated or algorithmic verification process used as quality assurance for dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper primarily focuses on the development and evaluation of video-language models using pre-existing datasets (e.g., MSR-VTT, DiDeMo, ActivityNet, WebVid-2M, CC3M). No new datasets are introduced by the authors, and no quality assurance processes for dataset annotations are discussed or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2276-rubric-3",
    "token_usage": {
      "prompt_tokens": 15430,
      "completion_tokens": 440,
      "total_tokens": 15870
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe collecting or creating any brand new dataset from scratch by human contributors. Instead, it uses existing datasets such as WebVid-2M, CC3M, CC12M, and WebVid10M for pretraining."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any original data entirely by AI or machine learning models as dataset content. The focus is on training models, not creating new datasets from model-generated data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper of translating datasets or data from one language to another using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using machine translation to create or augment the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.4 Pretraining Data (pages 5-6)",
          "reasoning": "The datasets used for pretraining (e.g., WebVid-2M, CC3M, CC12M, WebVid10M) are existing datasets collected from external sources without significant modifications. The paper uses these aggregated datasets to form combined pretraining corpora but does not report creating new datasets."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe creating datasets derived from existing datasets with transformations or adaptations. Instead, it uses original existing datasets directly."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the datasets used and their source, so the data origin is not undocumented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2276-rubric-4",
    "token_usage": {
      "prompt_tokens": 15948,
      "completion_tokens": 615,
      "total_tokens": 16563
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3 (Recipe for Video-Language Pretraining), Step 0, Step 1, Step 2, Step 3, Step 4, Step 6",
          "reasoning": "The paper uses the introduced datasets such as WebVid-2M, CC3M, CC12M, WebVid10M primarily for pre-training the video-and-language model in an (mainly) self-supervised or supervised manner to learn general video-text representations before fine-tuning."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention training models from randomly initialized parameters without pretraining for the introduced datasets; it focuses on pretraining and fine-tuning based on pretrained models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3, Step 5 (Finetuning & Inference); Section 4 (Experimental Results)",
          "reasoning": "The introduced datasets used for fine-tuning and evaluation on downstream VidL tasks include MSR-VTT, DiDeMo, ActivityNet-Captions, etc., where the pretrained models are fine-tuned with supervised learning methods for tasks such as text-to-video retrieval and video question-answering."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or description of reinforcement learning based post-training techniques such as RLHF using any introduced dataset is present in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experimental Results)",
          "reasoning": "Datasets such as MSR-VTT, DiDeMo, ActivityNet-Captions, ActivityNet-QA, MSRVTT-QA, MSRVTT-MC, TVQA, and SSv2 variants are used for evaluation and benchmarking the pretrained and fine-tuned models on text-to-video retrieval and video question-answering tasks."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper\u2019s analyses are conducted on existing datasets to study design choices rather than introducing new datasets solely for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any introduced dataset is used as a knowledge base to augment models via retrieval or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The introduced datasets are actively used for pre-training, supervised fine-tuning, and evaluation as detailed in multiple sections of the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2276-rubric-5",
    "token_usage": {
      "prompt_tokens": 16671,
      "completion_tokens": 266,
      "total_tokens": 16937
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract and Section 3 (Step 0 and Step 4)",
          "reasoning": "The datasets used for pretraining and fine-tuning (WebVid-2M, MSR-VTT, DiDeMo, ActivityNet-Captions, CC3M) predominantly contain English textual descriptions or captions. There is no mention of multiple human languages or use of non-English languages, indicating the dataset entries contain only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2276-rubric-6",
    "token_usage": {
      "prompt_tokens": 13889,
      "completion_tokens": 205,
      "total_tokens": 14094
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and conclusion",
          "reasoning": "The paper mentions that their code and pretrained models are publicly available at a GitHub link for the VidL framework and model implementations, but it does not mention or provide code related to the construction or collection of any new datasets. The datasets used are existing ones (e.g., WebVid-2M, CC3M, CC12M, WebVid10M). Thus, no new dataset or corresponding data construction code is introduced or provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Entire paper",
          "reasoning": "The paper discusses multiple existing datasets used for pretraining and evaluation such as WebVid-2M, CC3M, CC12M, and WebVid10M, but does not introduce any new dataset. Therefore, it does not provide documentation of dataset creation processes for any new datasets. The dataset descriptions are only brief mentions of those pre-existing datasets without details of data construction."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2336-rubric-0",
    "token_usage": {
      "prompt_tokens": 10858,
      "completion_tokens": 121,
      "total_tokens": 10979
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 (Implementation Details)",
          "Reasoning": "The authors state they train their model on the LOL dataset containing 500 pairs of real-captured images. This LOL dataset is cited as [31], and since it is real-captured data, it involves human generation as the images are captured by cameras operated by humans or manually collected. The paper mentions no new dataset introduced by the authors themselves; the LOL dataset is an existing dataset used for training."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2336-rubric-1",
    "token_usage": {
      "prompt_tokens": 11658,
      "completion_tokens": 289,
      "total_tokens": 11947
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.1 Implementation Details",
            "reasoning": "The paper states that the model is trained on the LOL dataset using standard deep learning optimization procedures. No indication of crowdsourced or multiple annotators is given; the annotation (labeling) was presumably performed by a single expert or a controlled team as part of dataset preparation or in previous work, but the paper itself does not mention new human annotations directly by multiple persons. The annotation for the new dataset introduced is not explicitly described, but the paper relies on the LOL dataset for training, which is an existing dataset, so this indicates no new explicit manual data annotation by multiple annotators is discussed."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not describe annotation instructions or guidelines given to annotators for any new dataset, since no new dataset is introduced with manual annotations in the paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "There is no description of scoring rubrics or grading criteria for annotations, consistent with lack of new manual annotations or evaluation by human annotators."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not provide annotation examples or exemplar annotations for annotators because no new annotation effort with guidelines is described."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-2336-rubric-2",
    "token_usage": {
      "prompt_tokens": 12848,
      "completion_tokens": 357,
      "total_tokens": 13205
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on the new dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information about multiple expert human annotators performing quality assurance on the new dataset is provided in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert human annotators performing quality assurance for the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of an AI model being used as a judge for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automatic or rule-based verification or quality assurance applied to the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper utilizes the LOL dataset for training and evaluation, which is a pre-existing dataset. The paper does not introduce any new dataset. Therefore, no quality assurance process on new datasets is described or applied."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2336-rubric-3",
    "token_usage": {
      "prompt_tokens": 12458,
      "completion_tokens": 397,
      "total_tokens": 12855
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Implementation Details",
          "reasoning": "The training data used in the paper is the LOL dataset, which contains 500 pairs of real-captured images. These images are collected and created by human photographers under different lighting conditions, constituting new data collected and created by human contributors rather than synthetic or model-generated data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state or indicate that any datasets introduced are generated entirely by AI or machine learning models. The datasets used are real-captured images."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any datasets are produced through human translation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any datasets are produced through machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention collating existing data from multiple sources without significant modification to create new datasets; it only uses the LOL dataset for training."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of datasets derived from existing ones with modifications; the LOL dataset is used as provided."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is specified as the LOL dataset, which is a real-captured image dataset explicitly named and documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2336-rubric-4",
    "token_usage": {
      "prompt_tokens": 12976,
      "completion_tokens": 397,
      "total_tokens": 13373
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1 Implementation Details, Section 2 Methodology",
          "reasoning": "The LOL dataset is used to train the proposed Retinex-based model from scratch. The model is trained on pairs of low-light and normal-light images without pre-training mentioned, as the network is optimized end-to-end using the LOL dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.4 Overall Architecture, Section 3.1 Implementation Details",
          "reasoning": "After training the reflectance module, the entire network is fine-tuned, implying supervised fine-tuning using the dataset. The fine-tuning phase uses paired low-light and normal-light images to optimize reconstruction loss."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 Experiments, Tables 1, 2, 3, and Section 3.3 Qualitative Evaluation",
          "reasoning": "The paper uses several datasets such as LOL, VE-LOL and unpaired datasets for evaluation to benchmark and measure the performance of the proposed model compared to state-of-the-art methods."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.3 Qualitative Evaluation, Section 3.4 User Study",
          "reasoning": "The dataset (LOL) is used for analyzing visual quality, Retinex decomposition components, and conducting a user study to analyze subjective image quality and visual characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes multiple practical uses of the datasets introduced, including training, fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2336-rubric-5",
    "token_usage": {
      "prompt_tokens": 13699,
      "completion_tokens": 515,
      "total_tokens": 14214
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any proposed dataset containing entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the proposed dataset contains exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Implementation Details; Section 3 Experiments",
          "reasoning": "The paper uses datasets and presents experiments entirely in English, and the description, explanations, and data annotations (image pairs with English labels) are in English. There is no mention of any other language being present in the datasets."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset or dataset entries are described as containing a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains methodological descriptions and formulas but does not introduce a dataset with programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper contains mathematical expressions describing the model and loss functions, these expressions describe the methodology, not the content or entries of a proposed dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not contain datasets with biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset languages are known and described; therefore, this category does not apply."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains images and associated annotations with language labels in English, so it is not void of language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-2336-rubric-6",
    "token_usage": {
      "prompt_tokens": 10917,
      "completion_tokens": 131,
      "total_tokens": 11048
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Not mentioned in the paper",
          "reasoning": "The paper does not provide any links, URLs, or references to publicly available code repositories related to data collection, preprocessing, or dataset generation. It only mentions training on existing datasets but does not mention sharing code."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Not mentioned in the paper",
          "reasoning": "The paper uses existing datasets (e.g., LOL dataset) for training and evaluation but does not introduce any new datasets or provide documentation on dataset creation or construction processes. Therefore, there is no documentation provided regarding dataset creation."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-265-rubric-0",
    "token_usage": {
      "prompt_tokens": 9659,
      "completion_tokens": 183,
      "total_tokens": 9842
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 (Datasets), Section 3.1 (SHNet input description)",
          "Reasoning": "The paper introduces InterWild which uses entire human images from the MSCOCO dataset for training, specifically using cropped single-hand images obtained from these human images as input. MSCOCO is an existing public dataset, so not new. However, the DetectNet component and the process of cropping single-hand images and flipping left hand images as a data preparation strategy integrate human captured images actively for training the model. The paper does not mention any new dataset collected or introduced by the authors; rather, it uses existing datasets (IH2.6M and MSCOCO) for training and testing. Therefore, no new dataset is introduced by the authors with any modality."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-265-rubric-1",
    "token_usage": {
      "prompt_tokens": 10459,
      "completion_tokens": 296,
      "total_tokens": 10755
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 (Datasets) and throughout the paper",
            "reasoning": "The new dataset is not explicitly introduced in this paper as a new dataset. The paper uses existing datasets like IH2.6M (a MoCap dataset) and MSCOCO (in-the-wild 2D single hand data). The Hands In Action dataset (HIC) is mentioned only as a test set, not as newly introduced. Therefore, the paper does not explicitly introduce any new dataset requiring human labeling or annotation processes that would involve human annotators or models. The paper focuses on a framework and training procedure using existing datasets."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No annotation instructions described for new datasets in the paper",
            "reasoning": "The paper does not describe any annotation guidelines, instructions, or processes related to new datasets, as no new human-annotated datasets are introduced."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No annotation rubrics described for new datasets in the paper",
            "reasoning": "Without annotation guidelines for new datasets, no scoring rubrics are described or referenced."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples described for new datasets in the paper",
            "reasoning": "The paper does not provide any examples of annotation tasks or labels for any new dataset, since no new dataset annotation process is introduced."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-265-rubric-2",
    "token_usage": {
      "prompt_tokens": 11649,
      "completion_tokens": 410,
      "total_tokens": 12059
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating quality assurance was conducted by a single human expert on dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by multiple human experts for the datasets introduced by the authors."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper about single non-expert human annotators performing quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any multiple non-expert human annotators overseeing quality assurance for the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the system uses AI models for 3D hand recovery, the paper does not state that an AI model was used for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any automated verification or algorithmic rule-based quality assurance processes applied to dataset annotation verification."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset but rather uses existing datasets (InterHand2.6M and MSCOCO) and describes training and testing strategies. While the paper discusses bringing domains together and training approaches, it does not describe any quality assurance process for dataset annotations as no new dataset is introduced by the authors. Therefore, quality assurance is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-265-rubric-3",
    "token_usage": {
      "prompt_tokens": 11259,
      "completion_tokens": 416,
      "total_tokens": 11675
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the authors created any original datasets entirely from scratch by human contributors. They use existing datasets such as IH2.6M (a MoCap dataset) and MSCOCO (an in-the-wild dataset). No new human-collected dataset is introduced."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creating any datasets generated entirely by AI or machine learning models. The datasets used for training and evaluation are existing datasets collected from real-world or MoCap sources."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the dataset being produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets generated via machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets - Train sets; Sections throughout describing use of IH2.6M and MSCOCO",
          "reasoning": "The paper uses existing datasets (IH2.6M and MSCOCO) for training, and refers to their combination as the training data. No new dataset is introduced; rather, the approach collates samples from these existing datasets, using them jointly in mixed-batch training. Thus, the data used is collated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe deriving new datasets by modifying or adapting existing datasets to create a new corpus. The approach modifies the inputs to the models (e.g., cropping and flipping images), but does not generate or produce a derived dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origins of the datasets used (IH2.6M, MSCOCO, HIC) are clearly documented. The paper does not introduce new datasets whose origins are unspecified."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-265-rubric-4",
    "token_usage": {
      "prompt_tokens": 11777,
      "completion_tokens": 303,
      "total_tokens": 12080
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3 (Ablation study), Section 4.4 (Comparison with state-of-the-art methods)",
          "reasoning": "The paper uses in-the-wild (ITW) datasets such as MSCOCO for analyzing and demonstrating the effectiveness of their InterWild framework, including ablation studies and comparisons, but does not introduce any new dataset that is used for model training or evaluation directly. The existing datasets (IH2.6M, MSCOCO, HIC) are used for training and testing, but the paper does not introduce any new datasets."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; it uses pre-existing datasets such as InterHand2.6M, MSCOCO, and HIC. Therefore, no new dataset's practical usage is described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-265-rubric-5",
    "token_usage": {
      "prompt_tokens": 12500,
      "completion_tokens": 515,
      "total_tokens": 13015
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the creation or introduction of any dataset containing multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new dataset containing exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset with English language content. The datasets used (IH2.6M, MSCOCO, HIC) are existing datasets. The paper focuses on 3D hand pose and mesh data, not linguistic data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets introduced by the authors contain non-English language content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset consisting of programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical expressions for network architecture and loss definitions, it does not introduce any dataset entries containing mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's datasets concern visual data of human hands, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional language is involved in any dataset introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets with unspecified or undocumented language types are introduced by the authors."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The authors do not introduce any new datasets containing language content. The focus is on 3D hand pose and mesh data derived from images. Therefore, the dataset entries do not contain linguistic content."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-265-rubric-6",
    "token_usage": {
      "prompt_tokens": 9718,
      "completion_tokens": 175,
      "total_tokens": 9893
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 5 Conclusion",
          "reasoning": "The paper explicitly mentions that the code is publicly available for their method InterWild, but it does not mention releasing any code related to dataset construction or dataset creation processes. The paper uses existing datasets such as InterHand2.6M and MSCOCO for training and Hands In Action (HIC) for testing. They do not introduce any new datasets, nor do they describe any new dataset construction code."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Entire paper",
          "reasoning": "Since the authors do not introduce any new dataset, there is no documentation on dataset creation process provided in the paper. The paper focuses on a method to reduce the domain gap using existing datasets rather than constructing new datasets. Hence, no dataset documentation is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-295-rubric-0",
    "token_usage": {
      "prompt_tokens": 14303,
      "completion_tokens": 118,
      "total_tokens": 14421
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.3 and throughout the paper",
          "Reasoning": "The new dataset is not explicitly named or described in the paper. The paper introduces the CLIPPING method which uses existing datasets like MSR-VTT for training and evaluation. No new dataset is introduced by the authors; rather, their work focuses on knowledge distillation from existing pre-trained models. Therefore, no new data source with video modality and origin can be asserted as introduced by authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-295-rubric-1",
    "token_usage": {
      "prompt_tokens": 15103,
      "completion_tokens": 255,
      "total_tokens": 15358
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3 and 4",
            "reasoning": "The paper presents a new dataset corresponding to the MSR-VTT benchmark used to evaluate their method CLIPPING. The knowledge distillation is carried out from a large pre-trained teacher model (Clip4clip) to a small student model (MobileViT-v2) fine-tuned on MSR-VTT. Although the MSR-VTT dataset is previously published (Xu et al. 2016), this paper does not introduce any new dataset but rather a new distilled model evaluated on existing benchmarks. Therefore, no new data annotations by human annotators are explicitly introduced in this paper."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any new annotation process or guidelines for new data collection or annotation; accordingly, no instructions for annotators are described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "Since no new annotation process or label scoring is defined for a new dataset, rubric information is not present."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No annotation examples or annotation guidelines related to a new dataset are provided."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-295-rubric-2",
    "token_usage": {
      "prompt_tokens": 16293,
      "completion_tokens": 287,
      "total_tokens": 16580
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset annotation conducted or quality assurance performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts were involved in quality assurance of any dataset; no such process is described."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about quality assurance performed by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe use of an AI model acting as a judge for dataset quality assurance or annotation validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper describes automated processes for knowledge distillation and model training, it does not mention any automated quality assurance process for dataset content or annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets nor describe any quality assurance process for dataset construction or annotation. All datasets mentioned are existing benchmarks. Therefore, no quality assurance process for new datasets is applicable or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-295-rubric-3",
    "token_usage": {
      "prompt_tokens": 15903,
      "completion_tokens": 407,
      "total_tokens": 16310
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention collecting or creating any new dataset from humans. The focus is on knowledge distillation methods applied to existing pre-trained models and benchmarks."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new data is generated by AI or models in this work; rather, the paper focuses on distilling knowledge from existing large models to smaller student models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any translation process or generation of data via human translation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation or generating data by this means."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset created by aggregating or collecting from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the work uses existing datasets (e.g., MSR-VTT, LSMDC, MSVD) for evaluation and utilizes pre-trained models, there is no indication that the authors created derived datasets through modification or transformation. They focus on training models using existing data."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document or specify any new data creation or dataset generation process. The experimental work is conducted using existing publicly available datasets and pre-trained models. Therefore, the origin of new data is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-295-rubric-4",
    "token_usage": {
      "prompt_tokens": 16421,
      "completion_tokens": 260,
      "total_tokens": 16681
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Abstract, Sections 1 and 3",
          "reasoning": "The new dataset introduced (the distillation training data using the CLIPPED method with MobileViT-v2 as student model and Clip4clip as teacher) is used exclusively at the fine-tuning stage to transfer knowledge from a large pre-trained vision-language model (Clip4clip) to a smaller model without additional pre-training. The paper repeatedly emphasizes knowledge distillation at the fine-tuning stage rather than at pre-training. This indicates that the dataset's primary utility is for supervised fine-tuning of the student model."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-295-rubric-5",
    "token_usage": {
      "prompt_tokens": 17144,
      "completion_tokens": 540,
      "total_tokens": 17684
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any introduced datasets containing entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the new dataset introduced contains exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.2 Preliminaries; Dataset descriptions in experiment sections",
          "reasoning": "The new dataset described for fine-tuning the student model uses video-caption pairs where captions are in English. References to MSR-VTT, LSMDC, and MSVD datasets, which contain English content, are mentioned and used for experiments. The paper focuses on English video-to-text retrieval tasks without mentioning other human languages, indicating monolingual English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce datasets with content in a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not include programming or structured code-related contents. The paper is focused on video and text data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although mathematical equations are presented in the methodology to explain knowledge distillation, they are not part of dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data are introduced in the datasets described."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain any constructed, fictional, or artificial languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages present in the dataset entries are specified as English; they are not unspecified or undocumented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain human language, specifically English captions associated with videos, so they are not language-free."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-295-rubric-6",
    "token_usage": {
      "prompt_tokens": 14362,
      "completion_tokens": 135,
      "total_tokens": 14497
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention",
          "reasoning": "The paper does not provide any reference, URLs, or mention about publicly available code or repositories for the dataset creation code or the CLIPPING method implementations. There is no indication that code is released for reproducibility."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section or mention",
          "reasoning": "The paper does not introduce a new dataset. It only uses existing datasets such as MSR-VTT, LSMDC, and MSVD for evaluation purposes. Since no new datasets are introduced, there is no dataset creation process documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-322-rubric-0",
    "token_usage": {
      "prompt_tokens": 10918,
      "completion_tokens": 151,
      "total_tokens": 11069
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1 Introduction, Section 3 Our Method, Section 4 Integration to Off-the-shelf HOI Detectors",
          "Reasoning": "The paper introduces a novel human-object interaction (HOI) classification method that is integrated with existing HOI datasets and baselines. However, it does not introduce any new dataset; instead, it uses existing datasets such as HICO-DET and V-COCO. The data underlying these datasets are images, which are human-generated via human photography and annotation. There is no mention of any new dataset introduced by the authors, nor generation of data by models or unknown origin."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-322-rubric-1",
    "token_usage": {
      "prompt_tokens": 11718,
      "completion_tokens": 238,
      "total_tokens": 11956
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper introduces a method that integrates with existing HOI detectors and does not describe collecting or annotating a new dataset; instead it focuses on category query learning applied to existing datasets such as HICO-DET and V-COCO. Therefore, no human annotators or annotation procedures are described for a new dataset; the annotations are essentially those of pre-existing datasets."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No relevant section describing annotation instructions for a new dataset",
            "reasoning": "The paper does not describe creating or annotating a new dataset, so there are no instructions provided for annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No relevant section describing annotation rubrics for a new dataset",
            "reasoning": "Since no new dataset is introduced or annotated by humans, rubric guidelines are not present."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No relevant section or appendix providing annotation examples",
            "reasoning": "No examples related to annotation instructions or guidelines are provided, as no new annotation process is described."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-322-rubric-2",
    "token_usage": {
      "prompt_tokens": 12908,
      "completion_tokens": 263,
      "total_tokens": 13171
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for any newly introduced dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information provided in the paper about quality assurance performed by multiple human experts for any new dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single human non-expert for any dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of quality assurance by multiple non-expert annotators for dataset quality control is found in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report using AI models as judges for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification or algorithmic quality assurance processes applied to dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets, and thus there is no documented quality assurance process related to new dataset annotation or validation."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-322-rubric-3",
    "token_usage": {
      "prompt_tokens": 12518,
      "completion_tokens": 321,
      "total_tokens": 12839
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets created from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not generate any dataset purely from AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by human translation in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating or using any machine-translated data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors do not report collecting or aggregating data from existing sources as a new dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is presented as derived or adapted from an existing dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses existing datasets (HICO-DET and V-COCO) for evaluation and does not introduce or describe any new datasets introduced by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-322-rubric-4",
    "token_usage": {
      "prompt_tokens": 13036,
      "completion_tokens": 449,
      "total_tokens": 13485
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any newly introduced dataset used exclusively for pre-training large models; no pre-training-only usage of a novel dataset is reported."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset introduced by the authors is used for training a model from scratch. The paper focuses on methods applied on existing HOI benchmarks."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using any novel dataset for supervised fine-tuning of pre-trained models on HOI tasks."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of using any newly created dataset for reinforcement learning or RL-based post-training."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used for evaluation in the paper, such as HICO-DET and V-COCO, are pre-existing, not newly introduced by the authors."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper performs analysis of method effectiveness, it does not introduce new datasets for trend or pattern analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No novel dataset is introduced or used as a knowledge base to augment models."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; all datasets referenced (e.g., HICO-DET, V-COCO) are pre-existing and used as benchmarks. Thus, there is no practical usage of any newly proposed dataset."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-322-rubric-5",
    "token_usage": {
      "prompt_tokens": 13759,
      "completion_tokens": 544,
      "total_tokens": 14303
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce a new dataset containing entries in multiple human languages. The work focuses on human-object interaction datasets/images but does not mention any multilingual linguistic content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of a dataset with exactly two human languages in the dataset introduced by the authors."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new dataset introduced by the authors that contains only English text entries. Instead, the focus is on visual data (images) with annotations related to human-object interactions."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets introduced by the authors contain only non-English language entries."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset entries related to programming code or structured code content. The content is exclusively about images and their interaction labels."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains formulas and symbolic representations to describe their method, this does not correspond to a dataset containing such notation. The introduced dataset is visual with annotations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The proposed dataset does not contain biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of fictional or constructed languages used or included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages present in the dataset are not unspecified as the dataset itself is visual with interaction category labels rather than linguistic entries."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The dataset introduced in the paper focuses on images with human-object interaction annotations and does not contain entries with any linguistic content, as per the provided description."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-322-rubric-6",
    "token_usage": {
      "prompt_tokens": 10977,
      "completion_tokens": 221,
      "total_tokens": 11198
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention of dataset code availability in the paper",
          "reasoning": "The paper explicitly states that code will be available at https://github.com/charles-xie/CQL, which pertains to the code implementing their method for human-object interaction detection. However, there is no mention or indication that code related to dataset construction or dataset generation is provided. Moreover, the datasets used (HICO-DET, V-COCO) are pre-existing benchmarks, and the paper does not introduce any new datasets or describe any dataset construction process for which code would be relevant."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No sections or descriptions related to dataset creation or construction",
          "reasoning": "The paper does not present or introduce any new datasets. It extensively uses existing human-object interaction detection datasets such as HICO-DET and V-COCO. There is no description or documentation of a dataset creation process, data collection, labeling, or preprocessing steps relevant to dataset construction. Thus, no documentation on dataset creation is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-338-rubric-0",
    "token_usage": {
      "prompt_tokens": 11790,
      "completion_tokens": 171,
      "total_tokens": 11961
    },
    "response": {
      "sources": [
        {
          "Modality": "other",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1. Introduction; Section 4. Experiments; Section 5. Conclusions",
          "Reasoning": "The paper introduces a new high-quality scan dataset named THuman-CloSET which contains more than 2,000 scans of humans in real-world clothing, capturing diverse outfits and poses with high-quality scans via a dense camera rig. The dataset consists of real scan data captured with human involvement, i.e., humans wearing real clothes scanned in various poses. This is explicitly stated as new real-world scanned data, not synthetic or simulated. Thus, the data is of modality 'other', referring to 3D scan point clouds or meshes, and is human generated as it involves real-world human scans from cameras."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-338-rubric-1",
    "token_usage": {
      "prompt_tokens": 12590,
      "completion_tokens": 223,
      "total_tokens": 12813
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4, Dataset paragraph",
            "reasoning": "The new dataset THuman-CloSET is introduced with detailed real-world human scans and body model fittings. The dataset curation involves fitting SMPL-X models with shape parameters and pose guidance to subjects, indicating expert involvement rather than automated or non-expert annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit section describing annotation instructions",
            "reasoning": "The paper does not describe providing any annotation instructions for THuman-CloSET dataset; it focuses on scan acquisition and body model fitting without detailing annotation protocols."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit section describing annotation rubrics",
            "reasoning": "There is no mention of scoring rubrics or measures used to guide data annotation in the dataset creation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No appendix or section with annotation examples",
            "reasoning": "The paper does not provide any annotation examples regarding the THuman-CloSET dataset preparation or labeling procedures."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-338-rubric-2",
    "token_usage": {
      "prompt_tokens": 13780,
      "completion_tokens": 290,
      "total_tokens": 14070
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process for the introduced dataset being conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper indicating that multiple human experts performed quality assurance on the introduced dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single non-expert on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any quality assurance conducted by multiple non-expert humans on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report the use of AI models as judges in quality assurance for the new dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated verification processes or algorithmic quality assurance for the annotations or content of the new dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new real-world scan dataset of clothed humans (THuman-CloSET) but does not detail any quality assurance process applied to validate the dataset annotations or content. No QA procedure, human or automatic, is described in the text."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-338-rubric-3",
    "token_usage": {
      "prompt_tokens": 13390,
      "completion_tokens": 426,
      "total_tokens": 13816
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4 (Datasets), and Abstract",
          "reasoning": "The authors introduce a new high-quality scan dataset called THuman-CloSET containing over 2,000 scans of humans in diverse real-world clothing styles. This dataset was collected by capturing real humans in various poses and outfits, including diverse clothing types such as T-shirts, pants, skirts, dresses, jackets, and coats. This is original content created from scratch by human contributors and is not described as derived or synthesized from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any new data was generated purely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data being produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of machine-translated data in the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The newly introduced dataset is described as newly captured scans and not collected or aggregated from existing datasets."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset uses fitted body models (SMPL-X) for pose and shape information, the scan data itself is original capture data, not described as derived or adapted from existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset origin is explicitly stated in the paper as newly introduced scan data; therefore, the origin is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-338-rubric-4",
    "token_usage": {
      "prompt_tokens": 13908,
      "completion_tokens": 493,
      "total_tokens": 14401
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the introduced dataset for pre-training large models or for unsupervised/self-supervised learning."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 Experiments; Datasets paragraph; Section 4.1 Comparison with the State-of-the-art Methods",
          "reasoning": "The newly introduced THuman-CloSET dataset is used as training data to train models from scratch to learn pose-dependent clothing deformations. This is explicitly mentioned where the authors train their method and compare with baselines on THuman-CloSET."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of the new dataset being used specifically for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning or related post-training methods are described using the new dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Comparison with the State-of-the-art Methods; Table 2; Figure 4",
          "reasoning": "The THuman-CloSET dataset is used for quantitative and qualitative evaluation of the proposed method and competing methods on unseen poses. Metrics such as Chamfer Distance and normal error are reported on it."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the dataset primarily for analysis of trends or characteristics beyond its role in training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or retrieval augmentation according to the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is clearly used for training from scratch and evaluation as described, so N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-338-rubric-5",
    "token_usage": {
      "prompt_tokens": 14631,
      "completion_tokens": 519,
      "total_tokens": 15150
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4 (Datasets) and throughout the paper",
          "reasoning": "The newly introduced dataset, THuman-CloSET, is a high-quality scan dataset of humans in real-world clothing. The documentation and descriptions provided in the paper are exclusively in English. All methodology, metrics, and experimental results, including dataset descriptions, are presented in English, indicating that the dataset's accompanying textual content is monolingual in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper and project include code, the dataset itself is described as composed of 3D human scans, which are geometric data rather than programming or structured code entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical expressions and notation are present in the paper for method explanation and loss functions, but the dataset entries themselves do not contain mathematical or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of 3D human scans of clothed humans, with no biological sequences or non-human communication data involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of fictional or constructed languages in the dataset entries or descriptions."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset descriptions clearly specify the language used (English), so the language origin is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains linguistic data in the form of English language descriptions and metadata associated with the scans."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-338-rubric-6",
    "token_usage": {
      "prompt_tokens": 11849,
      "completion_tokens": 183,
      "total_tokens": 12032
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and conclusion",
          "reasoning": "The paper explicitly states in both the abstract and the conclusion that the project page includes code and dataset, providing a URL (https://www.liuyebin.com/closet). This indicates the code related to the dataset construction is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 (Experiments) and Introduction",
          "reasoning": "The paper provides detailed documentation of the dataset creation in multiple sections, including the introduction where the new dataset THuman-CloSET is described with its characteristics, size, capture setup, and pose guidance. Section 4 also provides detailed information about the dataset, including preprocessing such as body model fitting, pose selection for training and evaluation, and characteristics distinguishing it from existing datasets. Therefore, the dataset creation process is well documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-342-rubric-0",
    "token_usage": {
      "prompt_tokens": 13015,
      "completion_tokens": 282,
      "total_tokens": 13297
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Pre-training datasets",
          "Reasoning": "The paper states that the video dataset WebVid2M with 2.5M video-text pairs is used for pre-training. WebVid2M is a video dataset collected from web video sources (YouTube) with human-generated video content. Thus, the video modality originates from human-generated content recorded by human-operated devices."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Pre-training datasets",
          "Reasoning": "The accompanying text for the video dataset WebVid2M consists of natural language captions or descriptions, which are provided or derived from human-generated textual content associated with the videos. Hence, the text modality is human generated."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Pre-training datasets",
          "Reasoning": "The authors also use the Google Conceptual Captions (CC3M) dataset, which contains image-text pairs. Images are natural photographs captured by humans, thus human generated. During pre-training, the images are treated as one-frame videos, but the original image modality is present. The associated captions are human generated text."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-342-rubric-1",
    "token_usage": {
      "prompt_tokens": 13815,
      "completion_tokens": 230,
      "total_tokens": 14045
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1, Pre-training datasets",
            "reasoning": "The paper explicitly states that the pre-training data is from large-scale video-text pairs datasets (WebVid2M with 2.5M video-text pairs and CC3M with 2.8M image-text pairs treated as one frame videos). The annotation process is not described as involving human annotators but is collected at scale with automatic or web-crawled methods, consistent with automatic processes."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not described",
            "reasoning": "The paper does not mention any annotation instruction guidelines provided to annotators, as the data is collected from existing datasets without manual annotation or crowdsourced labeling instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not described",
            "reasoning": "No scoring rubrics or evaluation guidelines are described for annotation in the paper for these datasets."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not described",
            "reasoning": "No examples of annotations or annotation guidelines are provided or discussed in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-342-rubric-2",
    "token_usage": {
      "prompt_tokens": 15005,
      "completion_tokens": 322,
      "total_tokens": 15327
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human expert annotations or human expert quality assurance for the new dataset used for pretraining (WebVid2M and CC3M are existing datasets, not introduced by the authors)."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper about multiple human expert annotators performing quality assurance or validation on any new dataset introduced by the authors."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance conducted by a single non-expert human annotator for any new datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information provided about quality assurance by multiple non-expert annotators on any newly introduced dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of AI models for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification process for dataset quality assurance is described in the paper for any introduced datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; it uses existing datasets (WebVid2M, CC3M) for pretraining, and does not describe any quality assurance process for dataset annotations or content. Therefore, no QA process is applied or documented for datasets introduced by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-342-rubric-3",
    "token_usage": {
      "prompt_tokens": 14615,
      "completion_tokens": 355,
      "total_tokens": 14970
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not present any original content generated entirely by AI or machine learning models as new data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any dataset was produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset in the paper is described as being generated by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing large-scale datasets such as WebVid2M and CC3M for pre-training, but does not collate or aggregate existing data to construct a new dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new dataset derived by modifying, transforming, or adapting existing sources."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; it only uses pre-existing datasets for training and evaluation. Thus, no new dataset or data generation process is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-342-rubric-4",
    "token_usage": {
      "prompt_tokens": 15133,
      "completion_tokens": 420,
      "total_tokens": 15553
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 4.1, 'Pre-training datasets' and Section 3.3, 'Training Objective'",
          "reasoning": "The paper clearly states that Clover is pre-trained on large-scale video-text datasets WebVid2M and Google Conceptual Captions (CC3M) using semi-supervised/contrastive and masked language modeling objectives, indicating that these datasets are used exclusively for pre-training the model to learn general video-language representations."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any usage of the dataset for training a model from scratch without pre-training; the focus is on pre-training followed by fine-tuning."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets mentioned (WebVid2M, CC3M) are not used for supervised fine-tuning in the paper; instead, downstream tasks use other benchmark datasets for evaluation and fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of the datasets being used for reinforcement learning based post-training techniques such as RLHF."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced or used for pre-training are not exclusively for evaluation; evaluation is done on standard downstream benchmark datasets, but these pre-training datasets are not used for evaluation or benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No analysis or pattern study is conducted directly on the proposed datasets; analysis focuses on model performance and components."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not described as serving as a knowledge base for retrieval-augmented generation or similar augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets are utilized practically for pre-training large video-language models; usage is well documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-342-rubric-5",
    "token_usage": {
      "prompt_tokens": 15856,
      "completion_tokens": 487,
      "total_tokens": 16343
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Pre-training datasets",
          "reasoning": "The pre-training video-text datasets WebVid2M and Google Conceptual Captions 3M contain video or image-text pairs with language data. The text data is described in English throughout the paper, with no mention of other languages or multilingual entries. The example text snippets, descriptions, questions, and answers provided in the paper are all in English. The downstream tasks also use datasets with English content, indicating the dataset entries contain only English language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3.2 Tri-Modal Alignment and 3.3 Training Objective",
          "reasoning": "The paper contains mathematical notation and formulas describing the proposed tri-modal alignment loss (Equation 2, 3), pair-wise ranking loss (Equation 4), and MLM loss (Equation 5). These mathematical expressions are part of the dataset's formal description and are included as symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain human language text paired with video data; thus, the dataset does contain language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-342-rubric-6",
    "token_usage": {
      "prompt_tokens": 13074,
      "completion_tokens": 143,
      "total_tokens": 13217
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 1 Introduction",
          "reasoning": "The abstract explicitly states that 'Codes and pre-trained models will be released at https://github.com/LeeYN-43/Clover,' indicating that the code related to the dataset construction, preprocessing, and training will be made publicly available."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets created by the authors. Instead, it uses existing public datasets such as WebVid2M and CC3M for pre-training. There is no description or documentation of a novel dataset creation process, so documentation about dataset construction is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-40-rubric-0",
    "token_usage": {
      "prompt_tokens": 13253,
      "completion_tokens": 97,
      "total_tokens": 13350
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4. Dataset",
          "Reasoning": "The Pathologic Vessel Image Dataset (PVID) is newly collected by the authors from whole slide pathological images of liver cancer patients. These are physical medical images that were manually captured and processed, thus the data modality is 'image' and the origin is human generated through human involvement and institutional ethics approval."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-40-rubric-1",
    "token_usage": {
      "prompt_tokens": 14053,
      "completion_tokens": 230,
      "total_tokens": 14283
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4 (Dataset)",
            "reasoning": "Section 4 states that all annotations were labeled by an experienced pathologist of the cooperative institution. No mention of multiple annotators or crowd annotations, indicating a single expert pathologist performed the annotations."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4 (Dataset)",
            "reasoning": "The section states the annotations were performed by an experienced pathologist, implying domain expertise. While explicit instructions are not quoted, medical annotation by an expert pathologist typically follows clinical protocols and diagnostic standards, strongly implying instructions were understood and followed."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not describe any scoring rubrics or grading scales used for the annotation process; only category labels and centroid locations are mentioned."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "No examples or annotation guidelines including sample annotated images or instruction manuals are provided or referenced for the annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-40-rubric-2",
    "token_usage": {
      "prompt_tokens": 15243,
      "completion_tokens": 350,
      "total_tokens": 15593
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 4 Dataset",
          "reasoning": "The dataset annotations, including image-level category labels and cell centroid annotations for test data, were all labeled by an experienced pathologist from the cooperative institution, indicating quality assurance by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper specifies annotations were done by an experienced pathologist but does not mention multiple experts involved in QA."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Annotations were performed by an experienced pathologist, a subject matter expert, rather than a non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that multiple non-expert annotators were involved."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of AI models used for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification processes for annotation quality assurance are described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly states that annotations were performed by an experienced pathologist, indicating that QA processes were applied and documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-40-rubric-3",
    "token_usage": {
      "prompt_tokens": 14853,
      "completion_tokens": 411,
      "total_tokens": 15264
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4 Dataset",
          "reasoning": "The paper explicitly states that the authors collected the first Pathologic Vessel Image Dataset (PVID) containing vessel samples (healthy and MVI) from pathological images of 103 liver cancer patients. The samples were obtained by randomly cropping from whole slide pathological images with annotations labeled by experienced pathologists, indicating original data creation by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was collected from human pathological images; there is no indication that the dataset itself was generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of dataset translation from other languages by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of dataset translation using machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as a collection or aggregation of existing datasets or sources without modification; rather, it is newly collected from patient pathological slides."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the dataset uses pseudo masks generated from correlation filters to supervise cell locating, the base dataset itself is not derived from existing datasets with modifications; it is an original collection."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source is clearly specified and well documented as newly collected pathological vessel images labeled by pathologists."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-40-rubric-4",
    "token_usage": {
      "prompt_tokens": 15371,
      "completion_tokens": 526,
      "total_tokens": 15897
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 Method and Section 5 Experiments",
          "reasoning": "The collected Pathologic Vessel Image Dataset (PVID) is used to train the Loopback Network (LoopNet) from scratch with image-level category annotation supervision, as described in Section 3 and Section 5. The training uses the PVID training set to learn both the binary vessel classification and cell locating branches, with no mention of initializing the model from pre-trained weights for this task."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.4 Performance Comparison of Cell Locating",
          "reasoning": "The paper mentions a 'Fully LoopNet' variant where cancerous cell centroid annotations are used to fine-tune the cell locating branch of LoopNet after training on the image classification branch, which implies supervised fine-tuning on the proposed PVID dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or evidence in the paper describing the use of reinforcement learning or RL-based post-training methods with the proposed dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Dataset and Section 5 Experiments",
          "reasoning": "The test split of the PVID dataset is utilized exclusively to evaluate the classification and cell locating performance of LoopNet and compare it against other methods, as described in Section 4 and Section 5."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe primarily analyzing trends or characteristics of the dataset for purposes other than training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base to augment models or retrieval-based generation methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is clearly used in training, evaluation, and supervised fine-tuning stages, so N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-40-rubric-5",
    "token_usage": {
      "prompt_tokens": 16094,
      "completion_tokens": 647,
      "total_tokens": 16741
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The Pathologic Vessel Image Dataset (PVID) introduced in this paper contains image data of pathological samples, specifically liver cancer vessel images, and does not mention any human language content. There is no indication of data entries containing multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset PVID consists of pathological images and does not include entries with exactly two human languages. No bilingual language content is described."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper's text is in English, the dataset itself comprises pathological vessel images for MVI classification and cell locating. It does not contain entries consisting of English language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are images of pathological vessels and do not contain non-English language text as dataset entries."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset described (PVID) contains pathological images of blood vessels; it does not include or consist of programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical expressions for loss functions and related notation describing the models, these are not part of the dataset entries themselves. The dataset comprises image samples, not mathematical or logical notation entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": true,
          "reference": "Section 4 (Dataset) and Introduction",
          "reasoning": "The Pathologic Vessel Image Dataset (PVID) consists of images of blood vessels from liver cancer patients showing microvascular invasion, which are biological structures. Although not sequence data like DNA, these images represent biological specimens rather than human language, thus qualifying as biological data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include entries of constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of dataset entries is clearly non-linguistic image data representing biological vessels. Language characteristics are either absent or clearly identified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain images that are biological data; while not language, the paper does not specify the dataset as devoid of language-like content but rather non-linguistic image data. Therefore, 'N/A' is not applicable because biological communication is a category related to the dataset."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-40-rubric-6",
    "token_usage": {
      "prompt_tokens": 13312,
      "completion_tokens": 236,
      "total_tokens": 13548
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention",
          "reasoning": "The paper does not mention or provide any link, URL, or location for code related to data collection, dataset construction, preprocessing, or dataset generation. There is no explicit indication that the code for constructing the Pathologic Vessel Image Dataset (PVID) is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 Dataset (page 6)",
          "reasoning": "The paper provides detailed documentation about the new dataset Pathologic Vessel Image Dataset (PVID). It describes the source of data (100 whole slide pathological images of liver cancer patients), the size of images, the method of cropping vessel images, the size of the dataset (5,000 vessel samples with breakdown of healthy and MVI vessels), the data splitting strategy (train, validation, test splits ensuring vessel images from the same slide are in the same set), annotation details (image-level category labels and a dedicated annotated subset of 130 MVI vessels with cancerous cell centroids by experienced pathologists), and ethical approval. This constitutes comprehensive documentation of dataset creation and characteristics."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-465-rubric-0",
    "token_usage": {
      "prompt_tokens": 11675,
      "completion_tokens": 194,
      "total_tokens": 11869
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1 (Estimating the Input Distribution), Section 4.3 (Qualitative Results)",
          "Reasoning": "The paper introduces estimator networks g_p and g_s that reconstruct photo and sketch samples approximating the training distributions of their respective classifiers. These estimators generate photo and sketch images (image modality) used to train encoders for retrieval. The reconstructed sketches are described as realistic and resembling human amateur sketches, implying some human-like characteristics, but they are generated by estimator networks trained via adversarial optimization, so the data is model generated. The estimators produce modality-specific reconstructions that reflect the manifold of the original human-generated photo and sketch train data. Hence, the modality is image, with origins both human generated (original data manifold learned by classifiers) and model generated (reconstructions synthesized by estimators)."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-465-rubric-1",
    "token_usage": {
      "prompt_tokens": 12475,
      "completion_tokens": 263,
      "total_tokens": 12738
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1 and 3.2",
            "reasoning": "The paper describes trainable estimator networks (g) that reconstruct the train set distributions of classifiers and are optimized with multiple loss functions (e.g., semantic consistency, class alignment, modality guidance, adversarial losses) to generate pseudo-samples used to train encoders. This is a deterministic learning-based reconstruction process rather than human annotation. No mention of any human annotators creating data exists."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not provide any annotation instructions for humans. The 'annotation' is a trainable, data-free reconstruction process from classifiers, not human annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "No scoring rubrics or criteria for human annotations are described, as the reconstructed dataset is generated by learned estimators rather than by human annotation."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 4.3 Figure 3",
            "reasoning": "The paper shows example reconstructed photos and sketches generated by the estimator networks in Figure 3, serving as examples of the constructed new 'dataset' from estimators."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-465-rubric-2",
    "token_usage": {
      "prompt_tokens": 13665,
      "completion_tokens": 327,
      "total_tokens": 13992
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert on the new datasets introduced. No mention of human expert annotation or verification is made."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance performed by multiple human experts. There is no documentation of expert annotators validating the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models (pre-trained classifiers) are used to generate and estimate data distributions for training encoders, these models are not used as a quality assurance judge to validate annotations or dataset content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification processes for data annotations or dataset content quality. The estimators generate sample approximations, but no automated QA processes are described."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces new datasets implicitly through estimated train set distributions reconstructed by estimator networks. However, there is no explicit discussion or documentation of any quality assurance processes conducted to validate these reconstructed datasets or their annotations. Therefore, no QA process is applied or reported."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-465-rubric-3",
    "token_usage": {
      "prompt_tokens": 13275,
      "completion_tokens": 461,
      "total_tokens": 13736
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report creating any new datasets manually created by humans from scratch. Instead, it uses existing benchmark datasets (Sketchy, TU-Berlin, QuickDraw) for evaluation."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.1 Estimating the Input Distribution; Section 4.3 Qualitative Results",
          "reasoning": "The authors introduce estimator networks (generators) that reconstruct approximations of the training data distributions from the pre-trained classifiers. These estimators generate synthetic photo and sketch samples entirely by generative modeling, without access to real training data. This generated data is novel data from models created via the proposed data-free knowledge distillation framework."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of any dataset generated by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of any dataset generated by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing datasets for evaluation but does not create new data by collating existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Estimating the Input Distribution; Section 4.3 Qualitative Results",
          "reasoning": "The synthetic data generated by the estimator networks are derived from existing pre-trained classifiers (photo and sketch models) through inversion and optimization processes. Therefore, the new synthetic data is derived by transformation and adaptation of existing model knowledge rather than purely new creation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and generation process of the synthetic data is explicitly described and documented; thus, N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-465-rubric-4",
    "token_usage": {
      "prompt_tokens": 13793,
      "completion_tokens": 218,
      "total_tokens": 14011
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 \"Training the Encoders\" and Section 4.2 \"Comparison with the Data-Dependent Setting\"",
          "reasoning": "The estimated reconstructed datasets are used to train (fine-tune) photo and sketch encoders via supervised metric learning objectives such as the queue-based InfoNCE loss, enabling cross-modal retrieval without access to original training data."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-465-rubric-5",
    "token_usage": {
      "prompt_tokens": 14516,
      "completion_tokens": 498,
      "total_tokens": 15014
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset entries containing more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the proposed dataset includes exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4, Experiments - Datasets",
          "reasoning": "The datasets (Sketchy, TU-Berlin, QuickDraw) used are standard image and sketch datasets typically annotated with English class labels. All dataset-related text such as class labels, paper language, and experimental descriptions are in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any dataset content involving a non-English language exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets introduced contain code or programming language content; the paper uses code for methodology but does not introduce datasets of code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notation is used in the paper to describe methodology, but no dataset entries contain mathematical or logical symbolic data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset entries involve biological or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no reference to constructed languages in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets' language specifications are clear and standard (English labels and captions)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced involve human language labels (class names in English), so language content is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-465-rubric-6",
    "token_usage": {
      "prompt_tokens": 11734,
      "completion_tokens": 179,
      "total_tokens": 11913
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 1",
          "reasoning": "The paper provides a direct URL to the implementation of the method: 'https://github.com/abhrac/data-free-sbir' mentioned in the Abstract. This indicates that code related to data generation and training is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 and Section 4",
          "reasoning": "The paper extensively describes the data estimation process using estimator networks in Section 3, outlining the methodology for reconstructing class-aligned, modality-specific datasets without original training data. Section 4 provides experimental details on the datasets, their statistics, and train-test splits (noted as being in the supplementary), explaining the dataset creation assumptions and evaluation protocol. Thus, sufficient documentation on the dataset construction process is provided within the paper and supplement."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-472-rubric-0",
    "token_usage": {
      "prompt_tokens": 14978,
      "completion_tokens": 126,
      "total_tokens": 15104
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.3 Dataset, Section 4 Experiments",
          "Reasoning": "The paper introduces a new dataset setup based on the ImageNet dataset with samples redistributed into private and public subsets across multiple clients for decentralized learning. ImageNet consists of images originally collected and labeled by humans, thus the data is image modality and human generated. The authors simulate different heterogeneity scenarios by partitioning ImageNet samples among clients but do not create new datasets beyond this redistribution of an existing human-generated image dataset."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-472-rubric-1",
    "token_usage": {
      "prompt_tokens": 15778,
      "completion_tokens": 205,
      "total_tokens": 15983
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3 and Section 4.1",
            "reasoning": "The paper describes the data distribution for the experiments as a simulation process assigning ImageNet samples across clients with specified label partitions and skewness parameters. This process is algorithmic and automated rather than involving human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "There are no mentions of annotation instructions provided to human annotators, since data assignment is performed synthetically."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No scoring rubrics are described for annotation since no human labeling or quality scoring is involved; dataset labels come from ImageNet existing labels."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No annotation examples are given because the dataset is a distributed subset of an existing labeled dataset (ImageNet), and no new labels or annotations are manually made."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-472-rubric-2",
    "token_usage": {
      "prompt_tokens": 16968,
      "completion_tokens": 402,
      "total_tokens": 17370
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any quality assurance process conducted by a single human expert for the dataset introduced. The datasets used, including the constructed distributed versions of ImageNet, rely on algorithmic distribution and selection."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on the dataset. The paper does not mention expert annotation or validation specifically."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of single non-expert human annotators performing QA is provided in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple non-expert humans involved in QA processes for the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used to perform quality assurance or judgement on the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3 Dataset and Section 4 Experiments",
          "reasoning": "The dataset used is ImageNet, which is a pre-existing dataset. The paper introduces a method to distribute ImageNet among multiple clients to simulate heterogeneous data distributions by algorithmically assigning labels and samples using defined parameters such as dataset skewness and primary label assignment strategies. The construction and partitioning of data is thus done automatically without manual intervention or human annotation. There is no additional quality assurance process described; the correctness of data labels and distributions rely on the original dataset's quality and the deterministic algorithmic distribution performed by the authors."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "While no explicit QA process is described, the use of an established dataset (ImageNet) and automatic programmatic division of data suggests that automated procedures constitute the 'QA' process. Therefore, marking N/A is not appropriate."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-472-rubric-3",
    "token_usage": {
      "prompt_tokens": 16578,
      "completion_tokens": 415,
      "total_tokens": 16993
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any new dataset created entirely from scratch by humans is introduced by the authors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset generated entirely by AI or machine learning models as part of the work."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of dataset creation via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any machine-translated dataset for use in their experiments."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.3 Dataset",
          "reasoning": "The authors simulate data heterogeneity scenarios by partitioning and distributing an existing labeled ImageNet dataset into private and public subsets and distributing samples with different assignment probabilities to clients. This process aggregates and redistributes existing ImageNet data without creating new data from scratch, representing data collated or aggregated from an existing source without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors partition the ImageNet data, they do not indicate modifications or transformations to the data itself. The data remains original samples from ImageNet with only distribution altered. Thus, it is not considered derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and processing method is clearly documented (e.g., usage and partitioning of ImageNet), so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-472-rubric-4",
    "token_usage": {
      "prompt_tokens": 17096,
      "completion_tokens": 366,
      "total_tokens": 17462
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and throughout experiments",
          "reasoning": "The paper explicitly states that they perform training of individual client models on their private subsets of data derived from a labeled dataset (ImageNet) assigned across clients. The models (e.g., ResNet-18, ResNet-34) are trained with SGD from scratch on their designated private data."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2.1, 4.3",
          "reasoning": "The constructed dataset splits are used for measuring the performance of the client models on private tasks and on a shared dataset with uniform label distribution, serving as evaluation to analyze methods' effectiveness."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2, 4.4, 4.5",
          "reasoning": "The dataset is used to analyze effects of data heterogeneity, communication topology, auxiliary heads, and model architecture heterogeneity to understand learning dynamics and knowledge transfer."
        },
        "Knowledge Base": {
          "is_applicable": true,
          "reference": "Section 3.1 (Setup), 3.3 (Dataset), 4.1",
          "reasoning": "A fraction (~10%) of the underlying dataset is treated as a public unlabeled dataset to support distillation and knowledge exchange between clients, effectively serving as a knowledge base for augmenting training."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-472-rubric-5",
    "token_usage": {
      "prompt_tokens": 17819,
      "completion_tokens": 559,
      "total_tokens": 18378
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset entries containing multiple (>2) human languages. The datasets used are image datasets without linguistic content in multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset that contains exactly two human languages. The datasets are visual image datasets (e.g., ImageNet, CIFAR-100) without linguistic content involving two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce new datasets with entries containing English text or language content. The datasets referenced (ImageNet, CIFAR-100) are visual datasets without textual language entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets containing entries with a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets with programming or structured code content are introduced; the paper focuses on image classification datasets and distillation techniques."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical expressions to describe the method, the datasets themselves do not include entries with mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets containing biological sequences or non-human communication data were introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce datasets containing constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the datasets is clear; they are visual datasets without linguistic entries or with known natural language labels."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced and used in the paper are based on ImageNet and CIFAR-100 image classification datasets, which consist of images labeled with categories but do not contain any human language entries. Therefore, the dataset entries do not contain any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-472-rubric-6",
    "token_usage": {
      "prompt_tokens": 15037,
      "completion_tokens": 207,
      "total_tokens": 15244
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not provide any link or mention of publicly available code related to dataset construction, distribution, or preprocessing for reproducing their new data partitioning or simulation setups. Although they describe their data partitioning methods in Section 3.3 and experimental details in Section 4.1, no explicit code repository or artifacts are indicated."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.3 (Dataset) and Section 4.1 (Experimental Framework)",
          "reasoning": "The paper describes in detail the process of dataset preparation including how they partition the ImageNet dataset into private and public subsets, the label assignment strategies (even and random), the distribution skewness parameter s, and the process by which data samples are assigned to clients. These details offer transparency on how the new dataset partitions (simulations of heterogeneous data across clients) are constructed, allowing readers to understand and potentially replicate the setup from the textual description."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-481-rubric-0",
    "token_usage": {
      "prompt_tokens": 12678,
      "completion_tokens": 101,
      "total_tokens": 12779
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Experimental Settings and Section 5.2 Comparison with SOTA",
          "Reasoning": "The authors report experiments on standard in-distribution datasets CIFAR-10, CIFAR-100, and ImageNet, which are human-generated image datasets widely used for training and evaluating models. These datasets are not newly introduced by the authors but used as benchmarks."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-481-rubric-1",
    "token_usage": {
      "prompt_tokens": 13478,
      "completion_tokens": 298,
      "total_tokens": 13776
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 5.1 Experimental Settings: In-distribution datasets and Out-of-distribution datasets",
            "reasoning": "The paper introduces OOD detection evaluation using widely recognized standard datasets like CIFAR-10, CIFAR-100, and ImageNet as in-distribution data, and several known benchmarks such as Textures, SVHN, LSUN, iSUN, Places365, iNaturalist, SUN as OOD datasets. These datasets are externally established and are not created by manual annotation within this work. The evaluation on these datasets involves automatic scoring using trained models rather than human annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No sections provide annotation instructions for these datasets",
            "reasoning": "No description or mention of annotation instructions for any new dataset is found in the paper. The datasets are standard public datasets without new annotation processes introduced."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No sections provide annotation rubrics for these datasets",
            "reasoning": "The paper neither introduces nor discusses any annotation scoring rubrics. Evaluation metrics are standard metrics like AUROC and FPR95 for OOD detection, not annotation rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of annotation are provided",
            "reasoning": "No annotation examples or manual labels are mentioned or described since the datasets are standard and their evaluation is automatic through metrics rather than manual annotation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-481-rubric-2",
    "token_usage": {
      "prompt_tokens": 14668,
      "completion_tokens": 332,
      "total_tokens": 15000
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of single human expert for quality assurance of datasets. It relies on existing datasets and does not document any expert annotation or QA."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts conducting quality assurance on the datasets introduced. The paper does not introduce new annotated datasets requiring expert validation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human non-expert on any new dataset. No new annotated data requiring such QA is described."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide details of multiple non-experts performing QA on any new dataset. No such process is documented or implied."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of using an AI model to perform quality assurance on dataset annotations or content of any new dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic quality assurance process for dataset annotations or contents of new datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. It uses existing well-known datasets such as CIFAR-10, CIFAR-100, and ImageNet for evaluation. Therefore, no quality assurance process for dataset annotations or content is applicable or documented for new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-481-rubric-3",
    "token_usage": {
      "prompt_tokens": 14278,
      "completion_tokens": 391,
      "total_tokens": 14669
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any creation of new datasets by human contributors. It primarily uses public benchmark datasets (CIFAR-10, CIFAR-100, ImageNet) and public OOD benchmark datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any datasets generated by models or AI. The experiments use existing datasets and do not produce new data by generative models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by translation from other languages via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine-translated data or datasets generated through machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only uses pre-existing standard datasets as in-distribution and out-of-distribution benchmarks. There is no mention of collecting or aggregating new data from several sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets derived by modification, transformation, or adaptation of existing data are introduced or discussed in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. All datasets used are publicly available standard datasets used as benchmarks. There is no specification or documentation of any novel dataset creation."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-481-rubric-4",
    "token_usage": {
      "prompt_tokens": 14796,
      "completion_tokens": 366,
      "total_tokens": 15162
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets mentioned such as CIFAR-10, CIFAR-100, ImageNet, and various OOD datasets (Textures, SVHN, LSUN-Crop/Resize, iSUN, Places365, iNaturalist, SUN) are all pre-existing datasets used for evaluation purposes. The paper does not introduce any new datasets for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No newly created datasets are presented for the purpose of analyzing trends or characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset is used or proposed as a knowledge base to augment models."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. It uses existing benchmark datasets for training, evaluation, and comparisons. Hence, no practical usage of new datasets is described."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-481-rubric-5",
    "token_usage": {
      "prompt_tokens": 15519,
      "completion_tokens": 521,
      "total_tokens": 16040
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not propose any new dataset containing entries with multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset with exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper makes use of existing benchmark datasets (e.g., CIFAR-10, CIFAR-100, ImageNet) which are primarily image datasets and do not contain language content. No new dataset of English language content is introduced."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper includes equations and algorithms, it does not propose a new dataset consisting of programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes mathematical notations in the context of methodology and analysis but does not introduce any new dataset containing such notations as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not involve any biological or non-human communication data in any dataset introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset involving constructed or fictional languages is introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "All datasets referenced are existing well-known benchmarks; no new dataset with unspecified language content is introduced."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. Instead, it uses existing image classification datasets (CIFAR-10, CIFAR-100, ImageNet) which are image datasets without any language entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-481-rubric-6",
    "token_usage": {
      "prompt_tokens": 12737,
      "completion_tokens": 141,
      "total_tokens": 12878
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None found in the paper",
          "reasoning": "The paper does not mention any code release or provide a link or reference to code related to dataset construction or preprocessing. No supplementary material or repository is indicated for dataset creation code."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "None found in the paper",
          "reasoning": "The paper does not introduce any new dataset created by the authors. It uses existing benchmark datasets (CIFAR-10, CIFAR-100, ImageNet, and various OOD datasets) without describing any process of dataset collection, construction or annotation. Therefore, there is no documentation on dataset creation."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-484-rubric-0",
    "token_usage": {
      "prompt_tokens": 15779,
      "completion_tokens": 109,
      "total_tokens": 15888
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1: Datasets, Backbones, and Comparison Methods; Table 2",
          "Reasoning": "The paper describes using datasets such as MNIST, AnimeFaces, ILSVRC, CelebA-HQ, and LSUN Car for training GANs and evaluating the proposed method, but these datasets are publicly available datasets introduced previously by others and not new datasets created by the authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-484-rubric-1",
    "token_usage": {
      "prompt_tokens": 16579,
      "completion_tokens": 261,
      "total_tokens": 16840
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.3, 5.1",
            "reasoning": "The annotations correspond to regressor models (reconstructor R) that automatically predict the edited attribute index and amount, training in an unsupervised framework without human intervention."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper of annotation instructions for human annotators; the labeling is done via the reconstructor neural network as part of the training procedure.",
            "reasoning": "The paper describes an automatic training of a neural network to regress attributes and their change amounts, without mentioning separate annotation instructions for humans."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No description of rubrics for annotation scoring or guidelines, because the label regression is automated and based on the model's internal loss functions.",
            "reasoning": "The annotation target is the supervised signal for the reconstructor network; no scoring or rubric guidance for human annotators is mentioned."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "The paper does not provide annotation examples or example label data since the annotations arise from automatic label regression via model training.",
            "reasoning": "The paper focuses on a fully automatic annotation process through model training without human-generated example annotations."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-484-rubric-2",
    "token_usage": {
      "prompt_tokens": 17769,
      "completion_tokens": 307,
      "total_tokens": 18076
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on the datasets introduced."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any involvement of multiple human experts performing quality assurance on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert human performed quality assurance on the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance conducted by multiple non-expert humans."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 5.1 Experimental Setting, Evaluation Metrics",
          "reasoning": "The paper evaluates the quality of edited images using pre-trained AI-based attribute predictors (e.g., FairFace, CelebA-HQ attribute classifiers, ArcFace for identity) to measure attributes and assess editing quality quantitatively. This constitutes quality assurance by AI models."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of automated verification processes for dataset annotation quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Although the datasets used are pre-existing, the paper introduces no new dataset annotations or datasets that require manual quality assurance described in the paper. The evaluation relies on AI models rather than manual QA."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-484-rubric-3",
    "token_usage": {
      "prompt_tokens": 17379,
      "completion_tokens": 340,
      "total_tokens": 17719
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any new datasets created entirely from scratch by human contributors are introduced by the authors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used in the paper are either existing public datasets or consist of images generated from pretrained generative models; however, the paper does not claim to have generated entirely new datasets as original contributions from models beyond using pretrained models for editing tasks."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data produced via human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data generated by machine translation systems is mentioned."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing public datasets such as MNIST [42], AnimeFaces [31], ILSVRC [14], CelebA-HQ [47], and LSUN Car [40] without mention of curating or aggregating new collections by the authors."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors use pretrained generative models to produce edited images, these edits are applied in experiments and are not presented as new datasets created by the authors. The paper does not introduce datasets derived from existing ones with modifications as a dataset contribution."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. The experiments utilize existing datasets and pretrained models; thus, no new dataset origin or generation method is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-484-rubric-4",
    "token_usage": {
      "prompt_tokens": 17897,
      "completion_tokens": 240,
      "total_tokens": 18137
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper references several datasets including MNIST, AnimeFaces, ILSVRC, CelebA-HQ, and LSUN Car, but these are all existing, pre-existing datasets and not newly introduced by the authors. The paper does not introduce any new datasets authored by the paper's contributors. Thus, there is no new dataset introduced in the paper to assess utility for. Consequently, no practical usage of any new dataset is described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-484-rubric-5",
    "token_usage": {
      "prompt_tokens": 18620,
      "completion_tokens": 554,
      "total_tokens": 19174
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets that contain entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets that contain exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets mentioned (MNIST, AnimeFaces, ILSVRC, CelebA-HQ, LSUN Car) are image datasets and do not consist of English language content as entries; thus, they are not monolingual English datasets."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced or used do not contain textual contents in a non-English language as entries; they are image datasets."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses programming methods and models but does not introduce datasets containing code or programming language content as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains many mathematical expressions and theoretical explanations; however, these are used to describe the methods, models, and theory, not as entries in a new dataset introduced by the authors."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data are introduced or discussed as new datasets."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss datasets containing constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "All datasets mentioned are existing well-known image datasets with clearly known content; no unknown language datasets are introduced."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces no new datasets but discusses image generation and editing on existing image datasets. These datasets consist of images which do not contain any human or programming language as entries. Therefore, the dataset entries do not contain any language content."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-484-rubric-6",
    "token_usage": {
      "prompt_tokens": 15838,
      "completion_tokens": 141,
      "total_tokens": 15979
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper.",
          "reasoning": "The paper does not contain any links, references, or mentions to publicly available code repositories associated with the datasets introduced or used."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No dedicated section or explicit description found in the paper.",
          "reasoning": "The paper does not describe any new dataset creation process or provide detailed documentation about constructing any new dataset. It uses existing datasets (e.g., MNIST, AnimeFaces, CelebA-HQ, ILSVRC, LSUN Car) and pre-trained models, but does not introduce new datasets nor document their construction."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-49-rubric-0",
    "token_usage": {
      "prompt_tokens": 12868,
      "completion_tokens": 192,
      "total_tokens": 13060
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Benchmark Datasets and Section 5 Experiments",
          "Reasoning": "The paper uses several image classification benchmark datasets (CIFAR10-to-CIFAR10C, CIFAR100-to-CIFAR100C, ImageNet-to-ImageNetC, and ImageNet-to-Imagenet3DCC) for evaluating lifelong test-time adaptation methods. These datasets consist of original image datasets with different corruptions applied. The base datasets (e.g., CIFAR10, CIFAR100, ImageNet) are human-generated images collected originally by humans. The corrupted versions used are derived by applying algorithmic corruption processes to these human-generated images. The paper does not introduce any new dataset but uses these benchmark datasets for evaluation, implying the images are originally human-generated, not model-generated or unknown origin."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-49-rubric-1",
    "token_usage": {
      "prompt_tokens": 13668,
      "completion_tokens": 299,
      "total_tokens": 13967
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3, Algorithm 1, and Section 5 Experiments",
            "reasoning": "The new data used for adaptation consists of corrupted versions of existing datasets (CIFAR10C, CIFAR100C, ImageNetC, ImageNet3DCC) which are benchmark datasets derived by applying corruptions to original datasets; these test inputs are unlabeled and the adaptation and labeling (pseudo-labeling) is performed automatically by the model during test-time adaptation. The paper discusses adaptation methods applied online on the incoming test data without human annotation during this process, indicating an automatic process rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No section specifies instructions for manual annotation of new dataset.",
            "reasoning": "Since the adaptation process on unlabeled corrupted test data is automatic and does not involve human annotators, there are no annotation instructions provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention of scoring rubrics for annotations in the adaptation process.",
            "reasoning": "No rubrics for human annotation scoring are described, given that no human annotation is involved in the test-time adaptation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples presented for the datasets or tasks.",
            "reasoning": "The paper does not provide examples of annotation, as the data used is unlabeled corrupted test data used for automatic model adaptation, without human labeling."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-49-rubric-2",
    "token_usage": {
      "prompt_tokens": 14858,
      "completion_tokens": 301,
      "total_tokens": 15159
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process conducted by a single human expert for dataset annotation or validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by multiple human experts for dataset annotation or validation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information provided in the paper about quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss quality assurance by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of AI models for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper involves automated methods for model adaptation and evaluation, it does not describe automated verification or rule-based algorithmic quality assurance processes for dataset quality or annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses pre-existing benchmark datasets (CIFAR-10C, CIFAR-100C, ImageNetC, ImageNet3DCC) rather than introducing new datasets. There is no description of any quality assurance processes applied or documented for dataset annotation or validation within this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-49-rubric-3",
    "token_usage": {
      "prompt_tokens": 14468,
      "completion_tokens": 456,
      "total_tokens": 14924
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creation of any new dataset completely original from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of novel datasets generated entirely by AI or machine learning models without reference or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss producing any data by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of datasets generated via machine translation is found in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.1 Benchmark Datasets",
          "reasoning": "The paper uses existing benchmark datasets such as CIFAR10C, CIFAR100C, ImageNetC, and ImageNet3DCC, which are constructed by applying corruptions to original datasets. The new evaluation protocol involves using these pre-existing corrupted datasets in an online lifelong test-time adaptation setting but does not create new raw data. Hence, the data for evaluation is collated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 5.1 Benchmark Datasets",
          "reasoning": "Datasets like CIFAR10C, CIFAR100C, ImageNetC, and ImageNet3DCC are derived datasets created by applying corruptions and transformations to original datasets. The paper uses these existing derived datasets for evaluation, which are modifications of base datasets. This implies the data used is derived from existing datasets with transformations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies sources and nature of all datasets used, so data origin is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-49-rubric-4",
    "token_usage": {
      "prompt_tokens": 14986,
      "completion_tokens": 286,
      "total_tokens": 15272
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1 and Section 5.5 to 5.8",
          "reasoning": "The paper uses the CIFAR10C, CIFAR100C, ImageNetC, and ImageNet3DCC datasets primarily as benchmark datasets for evaluation of the proposed PETAL method on lifelong test-time adaptation. The datasets are used to measure classification error, uncertainty estimation metrics like NLL and Brier score, and to compare against baselines and other methods. The evaluation is done on standard benchmark datasets rather than newly created datasets."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new datasets are introduced by the authors in the paper; only existing benchmark datasets are used for evaluation. Therefore, there is no practical usage of any new dataset within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-49-rubric-5",
    "token_usage": {
      "prompt_tokens": 15709,
      "completion_tokens": 655,
      "total_tokens": 16364
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention of any dataset entries containing more than two human languages. The datasets involved (CIFAR10, CIFAR100, ImageNet and their corrupted versions) are visual image classification benchmarks and do not have multilingual text content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence or mention of any dataset entries containing exactly two human languages. The datasets are standard visual classification datasets with image inputs and labels in English only."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5.1 (Benchmark Datasets), throughout the paper",
          "reasoning": "The new datasets introduced by the authors are the setup for lifelong test-time adaptation on existing benchmark datasets: CIFAR10C, CIFAR100C, ImageNetC, and ImageNet3DCC. All these datasets involve image classification with class labels and corruption types described in English. The paper is written in English and all descriptions of classes and corruptions are in English. No other human language content is present in the datasets."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any dataset is in a non-English language. All datasets and labels are presented in English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of images and associated labels for classification tasks; there is no dataset entry with programming language code or structured code-related content described as part of the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper contains mathematical notation describing the proposed methods, these are not part of the dataset entries. The datasets themselves do not contain mathematical or formal logic expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used and introduced in the paper are image-based classification datasets corrupted in various ways, and do not include biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of any constructed or fictional languages present in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of dataset entries is known and specified as English in the dataset documentation and throughout the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain labeled images with labels in English language and thus contain language data, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-49-rubric-6",
    "token_usage": {
      "prompt_tokens": 12927,
      "completion_tokens": 177,
      "total_tokens": 13104
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 5 (Experiments)",
          "reasoning": "The paper provides a URL to the source code for the PETAL method implementation (https://github.com/dhanajitb/petal) but does not provide or introduce any new datasets or dataset construction code. The datasets used (CIFAR-10C, CIFAR-100C, ImageNetC, ImageNet3DCC) are existing benchmark datasets introduced by prior works, and no new dataset collection or generation code is provided or mentioned."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Not applicable",
          "reasoning": "The paper does not introduce any new datasets nor document any process for dataset construction. All discussed datasets are pre-existing benchmarks cited from prior work, so there is no dataset creation documentation provided in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-50-rubric-0",
    "token_usage": {
      "prompt_tokens": 11381,
      "completion_tokens": 187,
      "total_tokens": 11568
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 (Simulation Experiments) and Section 5.2 (Real Experiments)",
          "Reasoning": "The paper introduces simulations of IMU and camera measurements forming visual-inertial datasets with synthetic trajectories and noise modeled explicitly (Section 5.1). However, these simulated datasets are generated by the authors' simulation environment to verify their method, i.e., generated by algorithmic systems. The real dataset segment obtained from their experiments with cameras and IMUs (possibly the EuRoC dataset segments) are pre-existing and thus excluded. Their simulation data consists of IMU signals and image frames (indirectly) synthesized by simulation thus, the 'signal/sensor' modality applies, and is model generated because it is artificially simulated rather than recorded from real sensors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-50-rubric-1",
    "token_usage": {
      "prompt_tokens": 12181,
      "completion_tokens": 276,
      "total_tokens": 12457
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 5: Experiments (5.2 Real Experiments)",
            "reasoning": "The paper presents experiments conducted by the authors themselves on a new initialization method for visual-inertial odometry. The dataset applications and evaluations were performed by the research team proposing the method, indicating expert human annotation rather than AI or automatic annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit description of annotation instructions found in the paper.",
            "reasoning": "The paper does not provide any detailed instructions issued to annotators or labelers. The evaluation studies refer to algorithmic performance metrics and do not describe human annotators or specific instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention of scoring rubrics or evaluation rubrics in the paper.",
            "reasoning": "Performance evaluation metrics are quantitative measurements (e.g., error rates, RMSE) computed by the authors' implementations, not rubric-based human scores. No rubric or scoring system is documented."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of data annotation guidelines or annotated samples provided in the paper or appendices.",
            "reasoning": "The paper contains no example annotations or guides; the datasets used for evaluation are publicly available or simulated and the method operates algorithmically without human annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-50-rubric-2",
    "token_usage": {
      "prompt_tokens": 13371,
      "completion_tokens": 388,
      "total_tokens": 13759
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving a single human expert for dataset annotation or content validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of multiple human experts performing quality assurance on the introduced dataset or data segments."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about multiple non-expert human annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of AI models for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 5 (Experiments), especially 5.2 (Real Experiments) and 5.2.3 (Running time evaluation)",
          "reasoning": "While the paper does not explicitly describe a manual or human-in-the-loop quality assurance process for the dataset annotations, it extensively evaluates the proposed initialization method on publicly available datasets (EuRoC dataset). The evaluation includes automated metrics such as gyroscope bias error, velocity error, gravity direction error, and scale error, computed in a rigorous, quantitative manner. The experimental evaluation and performance metrics serve as automated verification of the data and annotations used in experiments. Thus, the quality assurance is conducted through automated verification of data consistency and algorithm correctness using standard metrics and comprehensive experimentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper includes a thorough experimental evaluation section with rigorous metrics and benchmarks, indicating that some form of quality assurance (automated) is present rather than none at all."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-50-rubric-3",
    "token_usage": {
      "prompt_tokens": 12981,
      "completion_tokens": 527,
      "total_tokens": 13508
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 5 and 5.2 Experimental parts describing simulation and real datasets",
          "reasoning": "The paper introduces new experimental datasets created by the authors through simulation of camera motion and IMU measurements (Section 5.1), as well as data segments extracted from a publicly available dataset (EuRoC), which are processed by the authors for evaluation. The simulation data is generated by the authors from scratch based on analytic parametric trajectories and controlled noise, thus constituting new data created entirely by human contributors. Furthermore, the processed real datasets segments used for evaluation are selected and curated by the authors, which involves human effort to create the evaluation dataset specific to their method."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state or imply that any datasets were generated wholly by AI or machine learning models without human design or intervention."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translating datasets from another language by human translators in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to produce any dataset in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.2 Real Experiments",
          "reasoning": "The paper uses the EuRoC dataset, a publicly available micro aerial vehicle dataset, which the authors have divided into multiple data segments for experimentation. This is an example of data collected from an existing source with minimal modification (segmenting the sequences). Thus, it can be regarded as collated data according to the rubric."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe creating datasets by modifying, transforming, or adapting existing data sources beyond simple segmentation or selection. No derivation or transformation of existing data is documented to a degree that would classify as derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources are explicitly documented, including simulation data generated by the authors and use of the EuRoC public dataset."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-50-rubric-4",
    "token_usage": {
      "prompt_tokens": 13499,
      "completion_tokens": 242,
      "total_tokens": 13741
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5",
          "reasoning": "The paper uses the EuRoC dataset and simulation data segments for extensive accuracy, robustness, and time efficiency evaluations of the proposed VIO initialization method. These evaluations serve to benchmark and measure the performance of their algorithm compared to existing methods."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new dataset is introduced by the authors in the paper. All experiments are conducted on existing public datasets (e.g., EuRoC). Therefore, the paper does not describe or demonstrate any practical usage of a new dataset."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-50-rubric-5",
    "token_usage": {
      "prompt_tokens": 14222,
      "completion_tokens": 536,
      "total_tokens": 14758
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Entire paper, including abstract and experiments sections",
          "reasoning": "The paper and the datasets introduced contain entries with only English content. All textual explanations, labels, and annotations related to the dataset are in English. There is no mention or presence of any other human language in the dataset descriptions or documentation."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses algorithmic methods and refers to code repositories for implementation, the new dataset introduced as part of the paper's experimental evaluation is described in terms of sensor data (images and IMU measurements). There is no indication that the dataset entries contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains extensive use of mathematical and logical notation to describe methods and algorithms; however, this notation is used for explanation and is not part of the dataset entries. The dataset itself consists of sensor data, not mathematical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of visual and inertial measurements from cameras and IMUs, with no indication of biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or evidence of fictional or artificially created languages included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is clearly specified as English; there is no ambiguity."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language content (English) in annotations, labels, and documentation, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-50-rubric-6",
    "token_usage": {
      "prompt_tokens": 11440,
      "completion_tokens": 197,
      "total_tokens": 11637
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 6 Conclusion",
          "reasoning": "The paper provides a link to the source code for their visual-inertial odometry initialization method (https://github.com/boxuLibrary/drt-vio-init), which is related to the implementation of their algorithm. However, there is no indication in the paper that new datasets are introduced by the authors or that code for dataset construction, collection, or generation is provided. The datasets used (e.g., EuRoC) are publicly available benchmark datasets, not newly created by the authors."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "None",
          "reasoning": "The paper does not introduce any new datasets created by the authors. Therefore, there is no dataset creation or construction process documented in the paper. The datasets used in experiments are existing public datasets (such as EuRoC), and no description of data collection or generation procedures is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-506-rubric-0",
    "token_usage": {
      "prompt_tokens": 11965,
      "completion_tokens": 158,
      "total_tokens": 12123
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5 Experiments - Datasets",
          "Reasoning": "The paper introduces their experiments on the ImageNet-100 dataset, which is a subset of ImageNet containing 127K training images and 5K validation images from 100 classes. The dataset consists of natural images captured and labeled by humans as part of the ImageNet project, thus human generated. The dataset is used as the primary data source for training and evaluation, including applying patch-based trigger backdoor poisons and subsequent cleaning. The authors do not introduce any new dataset themselves but exclusively rely on this existing dataset. Hence, no new dataset is introduced by the authors in the paper."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-506-rubric-1",
    "token_usage": {
      "prompt_tokens": 12765,
      "completion_tokens": 263,
      "total_tokens": 13028
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4, 'Scoring an image', 'Iterative search', and 'Poison classification' paragraphs",
            "reasoning": "The paper describes PatchSearch as an algorithmic defense pipeline that identifies poisoned samples using methods such as clustering, Grad-CAM, and training a poison classifier without human annotator involvement. The approach is an automated process without manual labeling or human annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit reference",
            "reasoning": "The paper does not describe any annotation instructions given to annotators because the poison detection and classification are performed through algorithmic methods, not human annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit reference",
            "reasoning": "Scoring rubrics are not provided as the classification is performed by an automated poison classifier trained within the method; no manual scoring or rubric guidelines are mentioned."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figure 2 and Section 5.1, 'Poison detection results of PatchSearch'",
            "reasoning": "The paper provides visualization of detected poisonous and non-poisonous images found by PatchSearch (Figure 2), serving as examples illustrating the annotation results of the automatic poison detection method."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-506-rubric-2",
    "token_usage": {
      "prompt_tokens": 13955,
      "completion_tokens": 392,
      "total_tokens": 14347
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of quality assurance by multiple human experts for the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any QA conducted by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance is not performed by an AI model as a judge according to the paper."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 5 (Experiments) and Section 4 (PatchSearch methodology)",
          "reasoning": "The defense method called PatchSearch involves automated procedures to identify and filter out poisoned samples from the training dataset. It uses algorithmic techniques such as k-means clustering on model representations, Grad-CAM computed via cluster centers as classifier weights, and iterative search strategies to automatically score and remove poisoned samples. The process does not rely on human annotation or verification but on automated, rule-based or algorithmic verification of images based on learned features and poison scores. This constitutes an automatic quality assurance process on the dataset to identify and remove poisons."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper introduces the ImageNet-100 dataset as the main dataset used, which is a subset of ImageNet, a well-known dataset. While the authors do not mention any additional human or manual quality assurance on their modifications, the PatchSearch defense process applies an automatic algorithmic QA process to the training data to identify and remove poisoned samples."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-506-rubric-3",
    "token_usage": {
      "prompt_tokens": 13565,
      "completion_tokens": 342,
      "total_tokens": 13907
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5 Experiments - Datasets",
          "reasoning": "The paper uses established datasets, specifically ImageNet-100, which is a subset of ImageNet, an existing dataset. The authors use the data as is, without creating new data or significant modification. The dataset is thus collected from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 5 Experiments - Attack and Poisoning",
          "reasoning": "The authors generate poisoned data by modifying existing images in ImageNet-100, specifically by pasting trigger patches on images to create poisoned samples. This is a derivative process that transforms existing images to create new poisoned variants used for training and evaluation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are documented: existing datasets are used, and poisoned data is created via modification of these data."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-506-rubric-4",
    "token_usage": {
      "prompt_tokens": 14083,
      "completion_tokens": 327,
      "total_tokens": 14410
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.1, Table 1, Figure 2",
          "reasoning": "The paper introduces a method called PatchSearch to identify poisoned samples in the training dataset and uses this to analyze poisoning characteristics and the effectiveness of defenses. The dataset (ImageNet-100) and the poisoned variants are primarily used to analyze detection and defense performance via precision, recall, and visualizations of poisonous vs. non-poisonous images, rather than directly for training or evaluation of models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-506-rubric-5",
    "token_usage": {
      "prompt_tokens": 14806,
      "completion_tokens": 519,
      "total_tokens": 15325
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only describes datasets consisting of images from ImageNet-100 and related image datasets, with no mention of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains exactly two human languages; the dataset is image-based with no linguistic content described."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper is written in English, the dataset described (ImageNet-100) is an image dataset and does not contain English textual content as dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains no textual data in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes code references and mentions implementation details, but the dataset described is an image dataset (ImageNet-100) without code or programming language data entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical notation to describe methods, the dataset itself does not contain mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains image data; biological or non-human communication system data are not included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or constructed languages are included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language or type are explicitly known and described; no indication of unknown language status."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The dataset introduced (ImageNet-100, subsets of ImageNet) consists solely of images and does not contain any human language or textual language as dataset entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-506-rubric-6",
    "token_usage": {
      "prompt_tokens": 12024,
      "completion_tokens": 220,
      "total_tokens": 12244
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 5 Experiments, paragraph titled 'Datasets' and 'Implementation details'",
          "reasoning": "The paper uses pre-existing datasets (e.g., ImageNet-100) and does not mention introducing or constructing any new datasets. It also does not provide any link or mention availability of code related to dataset construction or poisoning sample generation. The code link provided (https://github.com/UCDvision/PatchSearch) appears to be for the defense method implementation, but there is no explicit indication that code for dataset construction or poisoning sample generation is publicly available."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 5 Experiments, specifically the 'Datasets' paragraph",
          "reasoning": "The paper uses the ImageNet-100 dataset, which is a known, publicly available dataset with no new dataset introduced by the authors. The paper describes the poisoning process referencing prior work [1, 44], but does not detail a new dataset creation procedure or provide documentation on dataset construction since no new datasets are introduced."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-522-rubric-0",
    "token_usage": {
      "prompt_tokens": 12888,
      "completion_tokens": 201,
      "total_tokens": 13089
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5 (Implementation Details), Dataset description",
          "Reasoning": "The dataset used is the nuScenes outdoor driving dataset collected with a car equipped with cameras, which are human-operated sensors capturing real-world street scenes. The paper uses these images as inputs and does not create or simulate new image data itself. Hence the image data is human generated."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5 (Implementation Details), Dataset description and Section 3, detailed description of mmWave radar point cloud generation",
          "Reasoning": "The radar point clouds are collected from mmWave radars mounted on the car in the nuScenes dataset. These radars generate sparse and noisy point clouds from real-world reflections, recorded via human-operated platforms. Thus, the radar signal/sensor data is human generated."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-522-rubric-1",
    "token_usage": {
      "prompt_tokens": 13688,
      "completion_tokens": 217,
      "total_tokens": 13905
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1, Section 5",
            "reasoning": "The annotation labels y_gt for training RadarNet are constructed automatically using accumulated lidar depth maps d_acc by setting pixels within 0.4m of the radar point depth to positive class, and negative otherwise; this is a deterministic label generation method using preprocessed lidar data and scaffolding, not manual human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not describe any detailed annotation instructions provided to human annotators, as labels are automatically generated from lidar data and scaffolding."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No scoring rubrics or grading schemes are mentioned for annotation; only automatic label generation is described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not provide annotation examples or detailed annotated samples; the label construction is a binary thresholding on accumulated lidar depth maps."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-522-rubric-2",
    "token_usage": {
      "prompt_tokens": 14878,
      "completion_tokens": 367,
      "total_tokens": 15245
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information about any quality assurance done by multiple human experts on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper of quality assurance conducted by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance done by an AI model acting as a judge on the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 5 (Implementation Details), specifically Data Preprocessing",
          "reasoning": "The paper states that for supervision of RadarNet, accumulated lidar frames are reprojected and used to construct labels and depth maps. The point clouds and depth maps are processed via algorithmic reprojection, accumulation, scaffolding (interpolation), and thresholding (e.g., pixels within 0.4m of radar point depth set positive) to create ground-truth labels. This constitutes an automated verification process that does not involve manual annotation but relies on algorithmic processing of sensor data for quality assurance of the labels used in training."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes an automated preprocessing and label generation pipeline used for training data preparation which suffices as a quality assurance process; hence, 'N/A' is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-522-rubric-3",
    "token_usage": {
      "prompt_tokens": 14488,
      "completion_tokens": 582,
      "total_tokens": 15070
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 5: Implementation Details - Dataset and Data Preprocessing",
          "reasoning": "The paper uses the nuScenes dataset, which is collected by driving a car fitted with sensors such as lidar, radar, and camera around Boston and Singapore. This dataset is created by human contributors collecting real-world sensor data. The training labels for RadarNet and FusionNet are constructed using accumulated lidar point clouds and scaffolding methods devised by humans. Therefore, the dataset used for training and evaluation is originally collected and annotated by humans, constituting new data from human efforts."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data generated using machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5: Implementation Details - Data Preprocessing",
          "reasoning": "The authors accumulate multiple past and future lidar frames by projecting lidar point clouds from different time steps to the reference frame of the given frame to produce an accumulated depth map d_acc. They remove dynamic objects before projection and then perform scaffolding to interpolate a denser depth map for supervision. This process aggregates existing lidar frames without fundamentally modifying the underlying data, hence this is collated data from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1: Learning Radar to Image Correspondence and Section 5: Implementation Details",
          "reasoning": "The accumulated depth maps d_acc are processed by scaffolding (interpolating) to create semi-dense labels y_gt for training RadarNet. The ground truth lidar depth maps d_gt and accumulated depth maps d_acc, along with interpolated depth maps, are used for supervising FusionNet. Thus, the training data is derived by applying transformations and adaptations on the original lidar and radar point cloud data, generating labels and semi-dense maps used for training the proposed networks."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes the data sources, their collection process, and pre-processing steps; hence, the origin of the data is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-522-rubric-4",
    "token_usage": {
      "prompt_tokens": 15006,
      "completion_tokens": 360,
      "total_tokens": 15366
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The nuScenes dataset is used to train the proposed RadarNet and FusionNet networks from scratch. RadarNet learns radar to image correspondence with supervision from accumulated lidar depth maps, while FusionNet is trained to fuse radar and image inputs to produce dense depth maps. The models are trained using the provided nuScenes training split, starting from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 6",
          "reasoning": "The nuScenes dataset's test split is used exclusively for benchmarking and performance evaluation of the proposed method against baselines by computing error metrics such as MAE and RMSE on its lidar ground truth depth maps."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-522-rubric-5",
    "token_usage": {
      "prompt_tokens": 15729,
      "completion_tokens": 475,
      "total_tokens": 16204
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, e.g., Abstract, Sections 1-8",
          "reasoning": "The dataset used is the nuScenes dataset which is a real-world driving dataset collected in Boston and Singapore, USA and Singapore English-speaking countries, and the paper's entire description and annotations are in English without mention of any other human language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes code implementations and methods but does not introduce a dataset containing programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although mathematical formulas are used in describing the method and loss calculations, the dataset itself does not contain entries with mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains radar and camera data from real-world driving scenes and does not include biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset is specified and clearly English as described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains natural language annotations and data collected from real-world scenes, so it contains language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-522-rubric-6",
    "token_usage": {
      "prompt_tokens": 12947,
      "completion_tokens": 138,
      "total_tokens": 13085
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract",
          "reasoning": "The paper explicitly provides a URL to the code repository for their method: 'https://github.com/nesl/radar-camera-fusion-depth', indicating that code related to their dataset processing and model implementation is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 5: Implementation Details",
          "reasoning": "The paper documents the dataset used (nuScenes), data preprocessing details such as accumulation and scaffolding of lidar depth maps, ROI sizes, label construction, training details, and hyperparameters. This indicates the dataset creation process and usage are well documented within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-530-rubric-0",
    "token_usage": {
      "prompt_tokens": 10692,
      "completion_tokens": 309,
      "total_tokens": 11001
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Implementation Detail",
          "Reasoning": "The paper details training and evaluation on three popular object detection benchmark datasets: COCO, Object365, and Visual-Genome. These datasets contain images collected and annotated by humans. For example, COCO contains 118,000 web images with human annotations; Object365 contains around 740,000 images densely annotated by human labelers; Visual-Genome contains 108,077 images, with the note that boxes are generated by algorithms leading to noisier annotations. Despite Visual-Genome's annotation being algorithmic, the images themselves are human-generated photos. These three datasets are used jointly in experiments, but the paper does not state that they introduced these datasets; they are pre-existing datasets."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.6 Effectiveness on Extreme Varied Datasets",
          "Reasoning": "The paper evaluates the method on UODB, described as a combination of 11 varied datasets (COCO, KITTI, WiderFace, VOC, LISA, DOTA, Watercolor, Clipart, Comic, Kitchen, and DeepLesion). The paper uses UODB as an existing benchmark dataset for evaluation, not as a newly introduced dataset. The images come from various sources spanning natural images, aerial images, and medical images, all human-generated images."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-530-rubric-1",
    "token_usage": {
      "prompt_tokens": 11492,
      "completion_tokens": 381,
      "total_tokens": 11873
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.1 Implementation Detail; Section 4.6 Effectiveness on Extreme Varied Datasets",
            "reasoning": "The newly introduced dataset in this paper is UODB, which combines 11 varied datasets including COCO, KITTI, WiderFace, VOC, LISA, DOTA, Watercolor, Clipart, Comic, Kitchen, and DeepLesion. The description states that Object365 is densely annotated by human labelers ensuring quality, and Visual-Genome annotations are obtained by algorithms, but for UODB no direct annotation process is described by the authors; however, since UODB is a combination of existing public datasets, and annotations in these datasets were performed by multiple human experts originally, it implies multiple human experts performed the annotation for the constituent datasets. Therefore, the annotations reflect multiple expert annotators' efforts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit annotation instructions for UODB dataset are provided in the paper.",
            "reasoning": "The paper does not discuss or provide annotation instructions specific to UODB or any newly collected dataset. It only mentions combining existing datasets for evaluation. Hence, no instructions for annotators are described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No rubrics for annotations described for the new dataset in this paper.",
            "reasoning": "Similarly, the paper does not mention any scoring rubrics or standardized assessment criteria being provided or used by annotators for the new dataset or combined dataset. Thus, no rubric information is present."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples or guidelines with examples are given in the paper.",
            "reasoning": "The paper lacks annotation guidelines or example annotations for the new combined dataset (UODB). There is no appendix or section with annotation examples or illustrative instructions for annotators."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-530-rubric-2",
    "token_usage": {
      "prompt_tokens": 12682,
      "completion_tokens": 307,
      "total_tokens": 12989
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance of any new dataset introduced or used was performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of quality assurance being carried out by multiple human experts for any dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe QA by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance performed by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of AI models as judges for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the paper describes automated processes like query adaptation and detection modeling, it does not describe any automated quality assurance process applied to dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets, only uses existing public datasets (COCO, Object365, Visual-Genome, UODB) without describing any quality assurance process for their annotations or content. There is no documentation or description of any quality assurance procedures performed by the authors on these datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-530-rubric-3",
    "token_usage": {
      "prompt_tokens": 12292,
      "completion_tokens": 367,
      "total_tokens": 12659
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets created entirely from scratch by human annotators; it utilizes existing datasets for joint training."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new data is generated entirely by AI models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any data generated through machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although multiple existing datasets are used together, the authors do not introduce a new dataset simply by collating existing data; their contribution is a method for joint training, not a new collated dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe creation of a new dataset derived from existing data with modifications or transformations; it focuses on methodology to unify and train across existing datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; therefore, the origin or method of generation of new data is not specified or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-530-rubric-4",
    "token_usage": {
      "prompt_tokens": 12810,
      "completion_tokens": 507,
      "total_tokens": 13317
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 4.2",
          "reasoning": "The paper describes training object detectors from ImageNet-1k pre-trained backbones but explicitly training on multi-dataset object detection data from scratch or standard schedules. The datasets introduced as a merged training collection (UODB), formed from multiple existing datasets, are used to train the Detection Hub model in end-to-end supervised learning for object detection, specifically from scratch when referring to train schedules and ablations."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the datasets as fine-tuning datasets for supervised post-training. Instead, it focuses on joint training schemes on combined datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the datasets are used for reinforcement learning based post-training such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.6 and various tables such as Table 8 and the discussion in Section 4",
          "reasoning": "The combined dataset UODB and the three large datasets (COCO, Object365, Visual-Genome) are used for evaluation and benchmarking the Detection Hub's performance after training, e.g., measuring AP scores per dataset and on the unified benchmark, demonstrating the utility for performance measurement."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not focus on analyzing the new dataset beyond its use in training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as a knowledge base for retrieval-augmented generation or similar purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents usage of the datasets for training and evaluation purposes."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-530-rubric-5",
    "token_usage": {
      "prompt_tokens": 13533,
      "completion_tokens": 400,
      "total_tokens": 13933
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only discusses standard object detection datasets that are in English; no mention is made of multiple human languages in the dataset entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description or evidence in the paper indicates the dataset contains exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation Detail and dataset descriptions throughout the paper",
          "reasoning": "The datasets used and introduced such as COCO, Object365, Visual-Genome, UODB, and their combined dataset contain category names and annotations in English only. The language embedding is derived from English category names, and no mention of other human languages is present."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets containing only non-English languages."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Despite method descriptions involving programming concepts and algorithms, the datasets themselves do not contain programming or code entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical expressions to describe method components, the dataset entries themselves do not include mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data are included in the datasets."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are present in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets and their languages (English) are explicitly stated and known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain human language entries in the form of English category names and annotations; thus this label does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-530-rubric-6",
    "token_usage": {
      "prompt_tokens": 10751,
      "completion_tokens": 177,
      "total_tokens": 10928
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention",
          "reasoning": "The paper does not mention or provide any link, URL, or repository for code associated with constructing the datasets or for the new dataset unification method described. The datasets used (COCO, Object365, Visual-Genome, UODB) are pre-existing datasets, but the paper does not provide code for combining or pre-processing these datasets or any new dataset code."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No specific section or mention",
          "reasoning": "The paper describes the method for unifying multiple existing object detection datasets via query adaptation and language embedding, but it does not introduce a new dataset or provide documentation for dataset creation. All datasets discussed are existing ones, and no new dataset creation process or data collection is described or documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-550-rubric-0",
    "token_usage": {
      "prompt_tokens": 19385,
      "completion_tokens": 185,
      "total_tokens": 19570
    },
    "response": {
      "sources": [
        {
          "Modality": "other",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Dataset and Section 3. Methodology",
          "Reasoning": "The authors introduce a new dataset derived by converting meshes from the ShapeNet dataset (version 1) into voxel-shaped truncated signed distance fields (T-SDF) using the Mesh2SDF tool. This converted data is used as training and evaluation data for their method, constituting a new data source introduced by the authors in the form of voxel-shaped SDF representations. Since the original ShapeNet meshes are human created (explicitly models generated by humans), and the conversion process to SDF voxels is a procedural transformation performed by the authors, the resulting voxel SDF data is human generated data (from original human meshes), modified by computational transformation, but not model generated/generated via AI models themselves."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-550-rubric-1",
    "token_usage": {
      "prompt_tokens": 20185,
      "completion_tokens": 213,
      "total_tokens": 20398
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 'Dataset'",
            "reasoning": "The paper indicates that data preprocessing for the new dataset involved converting ShapeNet meshes into voxel-shaped SDFs using existing software (Mesh2SDF) and automated voxel extraction processes, implying the annotation was performed by an automatic process rather than humans."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 'Dataset'",
            "reasoning": "The paper does not mention any detailed instructions given to annotators or any human annotation process related to the creation of the SDF data; the dataset preparation is described as an automated conversion process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 'Dataset'",
            "reasoning": "No scoring rubrics or evaluation criteria for annotation are discussed in the context of dataset preparation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 'Dataset'",
            "reasoning": "No examples or annotation guidelines are provided related to dataset preparation; the section describes only the automated processing pipeline."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-550-rubric-2",
    "token_usage": {
      "prompt_tokens": 21375,
      "completion_tokens": 320,
      "total_tokens": 21695
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple human experts performing quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that a single non-expert conducted quality assurance for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information about multiple non-expert annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used as a judge for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3. Methodology, Dataset section in Section 3 and 4",
          "reasoning": "The dataset used is ShapeNet meshes converted into voxel-shaped Truncated Signed Distance Fields (T-SDF) using Mesh2SDF, an automated software tool. The paper describes the normalization and sampling procedures for generating T-SDF voxels and clipping methods to decrease redundant information. These steps are automated processes without human intervention and thus quality assurance is conducted via automated verification through code and algorithmic processing."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Since an automatic, code-driven conversion process is described for dataset preparation, stating no QA process is applied is inaccurate."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-550-rubric-3",
    "token_usage": {
      "prompt_tokens": 20985,
      "completion_tokens": 356,
      "total_tokens": 21341
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 'Dataset'",
          "reasoning": "The paper uses the ShapeNet dataset (version 1), which is a pre-existing dataset of 3D mesh shapes. The authors convert these meshes into voxel-shaped signed distance fields (SDF) using existing methods, such as Mesh2SDF. Thus, the data used to train and evaluate the model is collected from an existing source with little to no significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 'Dataset'",
          "reasoning": "The authors derive the voxel-shaped truncated signed distance fields (T-SDF) from the original ShapeNet mesh data by applying transformations including mesh normalization, SDF computation, clipping, and voxelization. This processing alters the original data into a form suitable for their method, indicating the data is derived from existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset origin and processing are clearly specified in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-550-rubric-4",
    "token_usage": {
      "prompt_tokens": 21503,
      "completion_tokens": 304,
      "total_tokens": 21807
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3, Methodology and Section 4, Experiments",
          "reasoning": "The authors use the newly introduced voxel-shaped Signed Distance Field (SDF) representations of 3D shapes, converted from meshes in ShapeNet dataset, to train their SDF-Diffusion generative models from scratch. The training involves learning to generate low-resolution SDFs and then super-resolve them to high-resolution SDFs, as detailed in Section 3 and demonstrated through experiments in Section 4."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4, Experiments",
          "reasoning": "The introduced voxel-shaped SDF dataset is used for performance evaluation and benchmarking of the proposed generative model. Metrics like Chamfer Distance and Earth Mover's Distance are computed on point clouds sampled from meshes reconstructed from generated SDFs, as explained in Section 4."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-550-rubric-5",
    "token_usage": {
      "prompt_tokens": 22226,
      "completion_tokens": 340,
      "total_tokens": 22566
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper including Abstract, Introduction, and Methodology sections.",
          "reasoning": "The dataset consists of 3D shape data represented as signed distance fields and meshes; there is no mention of any human language data other than English used to describe or annotate the data. The paper is written in English, and all data and labels (such as category names) are provided in English. Therefore, the dataset effectively contains only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3 Methodology and abstract equation descriptions in Background on Denoising Diffusion Models (3.1).",
          "reasoning": "The dataset entries include voxel-shaped signed distance fields represented mathematically. The paper includes formal mathematical notation and symbolic representations to describe the diffusion models and the SDF representation, indicating that mathematical expressions form part of the dataset's content or are essential to its definition."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-550-rubric-6",
    "token_usage": {
      "prompt_tokens": 19444,
      "completion_tokens": 203,
      "total_tokens": 19647
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "-",
          "reasoning": "The paper does not mention or provide any links to code repositories or publicly available resources containing the code used for data preprocessing, dataset creation, or 3D shape SDF voxel conversion. The website URL given (https://kitsunetic.github.io/sdf-diffusion) is for the project page but the paper itself does not specify availability of code for dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 'Dataset' in Methodology (page 11 approx.)",
          "reasoning": "The paper documents the dataset creation process for their new dataset format by describing in detail how they convert the ShapeNet meshes into voxel-shaped signed distance fields (SDF). They explain normalization, voxel resolutions used, the use of Mesh2SDF, clipping procedures to form truncated SDF, and how they sample points for evaluation. Thus, the dataset creation process is transparently documented for reproducibility purposes."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-552-rubric-0",
    "token_usage": {
      "prompt_tokens": 11350,
      "completion_tokens": 174,
      "total_tokens": 11524
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2, Section 4.4, and Section 5",
          "Reasoning": "The new dataset introduced is a small personal photo album of about 10-20 portrait images per identity used for Stage 2 finetuning to learn personalized facial priors. These images are explicitly described as 'portrait photos of the person' and 'a tiny dataset of roughly 20 images of one person of interest'. This dataset consists of real images captured of the subjects (celebrities and non-celebrities) and is distinct from the large generic FFHQ dataset used in Stage 1. Therefore, the modality is image and the dataset originates from human-generated data through photography, not model generation or unknown source."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-552-rubric-1",
    "token_usage": {
      "prompt_tokens": 12150,
      "completion_tokens": 268,
      "total_tokens": 12418
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.2 and 4.4",
            "reasoning": "The paper describes finetuning the diffusion model on a small dataset (~20 images) of a specific person to learn personalized priors. The annotation (i.e., generation of these personalized images) is performed by the trained model itself during this finetuning stage, not by human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit instructions for annotators described.",
            "reasoning": "The paper does not mention any instructions or guidelines provided to human annotators for dataset annotation since the dataset is created by automatic finetuning of the model on existing images, not labeled by annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No scoring rubrics or evaluation criteria detailed for annotations.",
            "reasoning": "There is no mention of rubric-based scoring or evaluation criteria used during dataset creation or annotation, as the dataset creation involves finetuning on curated personal photos."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation example guidelines provided.",
            "reasoning": "The paper does not include any examples or guidelines given to annotators because the annotated data (personal facial photos) are collected from personal photo albums without manual annotation instructions or examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-552-rubric-2",
    "token_usage": {
      "prompt_tokens": 13340,
      "completion_tokens": 352,
      "total_tokens": 13692
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert annotator on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of multiple human experts conducting quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any quality assurance process by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple non-expert human annotators performed quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe use of an AI model as quality assurance judge for the dataset introduced."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated or algorithmic quality assurance process for the new datasets is described in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces personalized datasets consisting of small collections (10-20 images) of photos of specific individuals but does not document any quality assurance process performed to validate or curate these datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-552-rubric-3",
    "token_usage": {
      "prompt_tokens": 12950,
      "completion_tokens": 592,
      "total_tokens": 13542
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.2, 4.4",
          "reasoning": "The paper introduces new personalized datasets consisting of small portrait photo collections (around 10-20 images) of individual persons curated by the authors for finetuning their model. These images are collected by humans and are original photos, not derived from existing datasets but from personal photo albums or similar sources, created by human contributors. This is explicitly described in Section 4.2 where fine-tuning on a 'tiny dataset of roughly 20 images of one person' is performed, and Section 4.4 which details the use of FFHQ for Stage 1 and the curated sets of 10-20 images per identity for Stage 2."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe generating any new datasets entirely by AI or machine learning models without reference to or transformation of existing data. The datasets used for training include existing datasets and curated human photo sets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated by translating content from another language using machine translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.4",
          "reasoning": "The Stage 1 training data is the FFHQ dataset, a publicly available, pre-existing large-scale face dataset with 70,000 images. The paper uses this existing dataset without significant modification for learning generic facial priors, which qualifies as collated data usage."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.2",
          "reasoning": "The paper derives physical buffers (surface normals, albedo, Lambertian rendering) from images using DECA, an off-the-shelf 3D face estimator. This processing constitutes a transformation of existing image data to derived data to serve as conditioning inputs for the model. Additionally, the personalized datasets are further processed to extract FLAME shape means for finetuning."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper sufficiently documents the origin and generation process of the data used, including large public datasets, curated small personal datasets, and derived physical buffers."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-552-rubric-4",
    "token_usage": {
      "prompt_tokens": 13468,
      "completion_tokens": 345,
      "total_tokens": 13813
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.4 Implementation Details",
          "reasoning": "The paper describes training DiffusionRig from scratch on the large FFHQ dataset (70,000 images) during the first stage to learn generic facial priors, training for 50,000 iterations with substantial compute resources."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.2 Learning Personalized Priors and Section 4.4 Implementation Details",
          "reasoning": "The paper details a second stage where the pre-trained model is fine-tuned on a small personalized dataset (10-20 images) of a specific person using supervised learning, enabling learning of personalized priors to preserve identity and high-frequency details during facial appearance editing."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper performs evaluation on metrics like identity preservation and relighting quality, the evaluation uses pre-existing datasets or the small personalized photo collections, but no new dataset introduced by the authors is used exclusively for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses datasets for training and fine-tuning rather than primarily for analysis of trends or patterns."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are not used as knowledge bases for augmentation or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-552-rubric-5",
    "token_usage": {
      "prompt_tokens": 14191,
      "completion_tokens": 536,
      "total_tokens": 14727
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset containing multiple human languages. The datasets are primarily image datasets of human faces without linguistic content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of datasets containing two human languages is found in the paper. The datasets used or introduced pertain to facial images only."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the new datasets introduced contain any English-only textual content. The datasets consist of images primarily."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced do not contain textual entries in any specific non-English language; they are image datasets."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce a dataset of programming code or structured code content; it focuses on photographic images."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper contains mathematical notations for model explanations, the new datasets introduced are image datasets and do not contain entries of mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced contain human portrait images and do not include biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are included or referenced in the dataset introduced by the paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset is clearly non-applicable as it consists of images without linguistic data, so language specification is not relevant."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced are photographic face image collections (around 10-20 images per person), containing no language data; therefore, they do not contain any entries with language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-552-rubric-6",
    "token_usage": {
      "prompt_tokens": 11409,
      "completion_tokens": 184,
      "total_tokens": 11593
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Acknowledgment",
          "reasoning": "The abstract mentions that the project website (https://diffusionrig.github.io) provides code and data. The acknowledgment section thanks people for their photos used in the paper, implying some personal data release. Thus, the paper indicates that code and data related to the dataset are made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.4 Implementation Details",
          "reasoning": "The paper describes the two-stage dataset creation and training process in detail in Section 4.4, specifying training datasets used (FFHQ for generic priors) and personalized photo collections (10-20 images per person) for the second stage, as well as how DECA outputs are computed and the averages used for shape parameters. This provides sufficient documentation on the dataset creation process for reproducibility."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-576-rubric-0",
    "token_usage": {
      "prompt_tokens": 13742,
      "completion_tokens": 189,
      "total_tokens": 13931
    },
    "response": {
      "sources": [
        {
          "Modality": "time series",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1 Introduction, Section 4.1 Datasets and Experimental Setting - TED Hands Dataset",
          "Reasoning": "The TED Hands dataset is a newly collected large-scale 3D hand gestures dataset extracted from the videos of TED talks. The authors used a human-operated video capture situation (TED talk speeches from in-the-wild scenarios) and applied a state-of-the-art 3D pose estimator (FrankMocap) to extract 3D body-hand joints sequences. The data consists of sequences of 3D joint poses represented by 3D axis-angle vectors. Since the original data are video recordings of human speakers, the underlying raw data originate from human activities captured by cameras and human involvement in the collection process, and the resulting dataset is time series of 3D skeletal joint data."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-576-rubric-1",
    "token_usage": {
      "prompt_tokens": 14542,
      "completion_tokens": 258,
      "total_tokens": 14800
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1, Dataset section describing TED Hands Dataset collection and annotation",
            "reasoning": "The TED Hands Dataset annotations are obtained by leveraging a state-of-the-art 3D pose estimator FrankMocap (Section 4.1). The paper states that the dataset was created by applying this 3D pose estimator to video links from the TED Gesture dataset, implying an automated annotation process rather than manual labeling by humans."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention of annotation instructions in the paper",
            "reasoning": "The paper does not provide any details or references to annotation instructions given to annotators because the annotations are generated automatically via the FrankMocap system, not by human annotators following instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No description of any scoring rubrics for annotations in the paper",
            "reasoning": "Since the annotation is automated, no human scoring or rubric system is described or applicable."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of annotations or annotation guidelines are provided anywhere in the paper",
            "reasoning": "No demonstration or examples of human annotation processes are mentioned, as annotation is automatic."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-576-rubric-2",
    "token_usage": {
      "prompt_tokens": 15732,
      "completion_tokens": 332,
      "total_tokens": 16064
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts conducted quality assurance on the TED Hands dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any quality assurance conducted by a single non-expert annotator for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple non-expert human annotators performing quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used in processing and prediction, the paper does not state that an AI model was used specifically for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets and Experimental Setting, paragraph about TED Hands Dataset",
          "reasoning": "The TED Hands dataset is collected by applying the 3D pose estimator FrankMocap to extract whole body-hand joints from TED Gesture videos. This is an automated process of pose estimation used to create the dataset annotations, effectively serving as a form of automated quality assurance for the dataset content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes an automated method (FrankMocap) for extracting dataset annotations, indicating some form of quality assurance, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-576-rubric-3",
    "token_usage": {
      "prompt_tokens": 15342,
      "completion_tokens": 509,
      "total_tokens": 15851
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets and Experimental Setting, especially paragraphs describing TED Hands Dataset",
          "reasoning": "The TED Hands Dataset is newly collected by the authors from in-the-wild TED talk videos of more than 1,700 different speakers, resulting in approximately 100 hours of sequential 3D hand gesture data. This collection involves human contributors (the speakers) naturally performing gestures, and the data is extracted via human-guided procedures (3D pose estimators), producing original content that is not adapted or translated from existing datasets. Hence, the data is originally created by human contributors through real-world recordings."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that the dataset was generated purely synthetically or fully generated by AI models without relying on real human data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset constructed by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets created by machine translation from other languages."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The TED Hands dataset is not described as simply aggregated or collected from existing datasets without modification; rather, it is newly collected from video data and processed."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets and Experimental Setting, TED Hands Dataset paragraph",
          "reasoning": "The TED Hands dataset is derived by applying the FrankMocap 3D pose estimator to extract 3D body and hand joints from existing TED Gesture video data. This processing adapts existing video content into a new structured 3D hand gesture dataset, constituting derivation through transformation and adaptation of existing source data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies the origin and generation process of the TED Hands dataset; hence, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-576-rubric-4",
    "token_usage": {
      "prompt_tokens": 15860,
      "completion_tokens": 536,
      "total_tokens": 16396
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the new TED Hands dataset being used for pre-training models in an unsupervised or self-supervised way."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 (Datasets and Experimental Setting) and Section 4.2 (Quantitative Results)",
          "reasoning": "The TED Hands dataset is used to train the proposed model from scratch. The authors explicitly split the dataset into training, validation, and testing sets and perform training of their model using this dataset, as indicated in Section 4.1 and shown in quantitative performance results compared to baselines in Section 4.2."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention fine-tuning a pre-trained model on the TED Hands dataset with supervised learning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of using the TED Hands dataset for reinforcement learning post-training techniques such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 (Quantitative Results) and Section 4.3 (Qualitative Evaluation)",
          "reasoning": "The TED Hands dataset is used as a benchmark for evaluating and comparing the performance of the proposed method and several baselines on metrics such as L2 distance, FHD, MPJRE, and diversity. Qualitative user studies and visualizations are also performed on this dataset."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as being primarily used for analysis of trends or characteristics, but rather for training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The TED Hands dataset is not used as a knowledge base for retrieval-augmented generation or model augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides clear documented usage of the TED Hands dataset for training from scratch and evaluation; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-576-rubric-5",
    "token_usage": {
      "prompt_tokens": 16583,
      "completion_tokens": 624,
      "total_tokens": 17207
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 4.1, TED Hands Dataset description",
          "reasoning": "The TED Hands dataset includes body and hand gesture data but there is no mention of multiple human languages being represented in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 4.1, TED Hands Dataset description",
          "reasoning": "The dataset contains 3D hand pose data from TED talk videos, but the paper does not specify or indicate presence of exactly two human languages in the dataset."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1, TED Hands Dataset description",
          "reasoning": "The TED Hands dataset is collected from TED talk speeches which are primarily delivered in English. The paper references TED Gesture dataset and TED talks videos, which predominantly contain English language content. Hence, the dataset entries correspond to English speech contexts."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 4.1, TED Hands Dataset description",
          "reasoning": "The dataset is based on TED talks that are mostly in English, and the paper does not mention inclusion of monolingual datasets in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Overall paper",
          "reasoning": "The dataset consists of 3D hand pose data extracted from videos; no programming or structured code snippets are included in the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Overall paper",
          "reasoning": "Though the paper uses mathematical notation to describe models and losses, the datasets themselves do not contain entries with mathematical or logical symbolic data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Overall paper",
          "reasoning": "The dataset consists of human hand gestures and body dynamics, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Overall paper",
          "reasoning": "There is no mention or indication that any constructed or fictional language data is contained in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Overall paper",
          "reasoning": "The language (English) associated with the dataset is clearly stated or can be reasonably inferred from TED talks origin."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is derived from spoken and gestural content associated with human communication in English; therefore, it contains language-related data."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-576-rubric-6",
    "token_usage": {
      "prompt_tokens": 13801,
      "completion_tokens": 224,
      "total_tokens": 14025
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 4.1 (Datasets and Experimental Setting)",
          "reasoning": "The paper explicitly provides a URL at the end of the abstract (https://github.com/XingqunQi-lab/Diverse-3D-Hand-Gesture-Prediction) indicating that the dataset and code are publicly available. This strongly suggests that the code required for dataset construction (including 3D pose extraction with FrankMocap) is accessible in this repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 (Datasets and Experimental Setting)",
          "reasoning": "The paper documents the dataset creation process in Section 4.1, describing how the TED Hands dataset was collected by applying FrankMocap 3D pose estimation on TED Gesture videos, including details on dataset size (134,456 sequences from 1,755 speakers), sequence length, normalization to 3D axis-angle representations, and the split into train/val/test. These details qualify as sufficient documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-580-rubric-0",
    "token_usage": {
      "prompt_tokens": 11328,
      "completion_tokens": 110,
      "total_tokens": 11438
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset",
          "Reasoning": "The paper uses three standard benchmark datasets\u2014DomainNet, Office-Home, and CIFAR-10\u2014for evaluation, but it does not introduce any new dataset. Instead, these are well-known public datasets, thus no new dataset is introduced by the authors in this work. Therefore, there is no new dataset introduced in the paper to extract data source entries from."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-580-rubric-1",
    "token_usage": {
      "prompt_tokens": 12128,
      "completion_tokens": 190,
      "total_tokens": 12318
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3 and 4.2",
            "reasoning": "The annotation process involves human experts labeling selected target samples during active learning loops, as described in the problem formulation and experimental setup. The labeling is done by human experts, not automatic or AI model annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not explicitly provide or describe detailed annotation instructions given to the human annotators for labeling the selected samples."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not mention scoring rubrics or formal guidelines for annotation quality or decision rules used by annotators."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "No examples or sample annotations are provided or described for annotators in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-580-rubric-2",
    "token_usage": {
      "prompt_tokens": 13318,
      "completion_tokens": 318,
      "total_tokens": 13636
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human annotator who is an expert or belongs to the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or indication in the paper that multiple human experts annotated or validated any dataset or data content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper lacks any mention of quality assurance involving multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the method uses a Gaussian mixture model for sample categorization as part of the model framework, no indication is given that an AI model performs quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention of automated verification of dataset annotation quality, or dataset validity through algorithmic or rule-based processes as quality assurance."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; it uses existing benchmark datasets like DomainNet, Office-Home, and CIFAR-10 without reporting any quality assurance or validation process related to annotations or dataset content. No QA process is described or performed for dataset annotation or content validation."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-580-rubric-3",
    "token_usage": {
      "prompt_tokens": 12928,
      "completion_tokens": 390,
      "total_tokens": 13318
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creating any new dataset created entirely from scratch by human contributors. Instead, it uses existing public benchmark datasets such as DomainNet, Office-Home, and CIFAR-10."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new data generated entirely by AI or machine learning models without reference to existing data is described in the paper."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate using or creating datasets from machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper utilizes existing datasets such as DomainNet, Office-Home, and CIFAR-10 for evaluation. These datasets are public benchmark datasets collected from previous works, implying the data is collated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe creating a derived dataset based on existing data with modifications or transformations. Instead, it focuses on model training and sample selection strategies."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources are clearly specified as existing benchmark datasets, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-580-rubric-4",
    "token_usage": {
      "prompt_tokens": 13446,
      "completion_tokens": 310,
      "total_tokens": 13756
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.3 Candidate Selection and Training Objectives",
          "reasoning": "The authors use the target datasets to fine-tune pre-trained models by selectively annotating informative samples (uncertain-inconsistent) for supervised learning and applying tailored training techniques for other data subsets with stratified transferable properties, thus employing supervised fine-tuning on these datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Experiments, particularly 4.1 Dataset and 4.3 Main Results",
          "reasoning": "The datasets DomainNet, Office-Home, and CIFAR-10 are used as benchmarks to evaluate the active domain adaptation performance and compare against baselines."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3.2 and 4.4 Ablation Study",
          "reasoning": "The proposed datasets are partitioned and analyzed into four categories based on domainness and uncertainty to understand data stratification's impact on model adaptation, demonstrating analysis-driven use of the datasets."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-580-rubric-5",
    "token_usage": {
      "prompt_tokens": 14169,
      "completion_tokens": 454,
      "total_tokens": 14623
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes datasets used in computer vision domain adaptation tasks but does not mention the use of more than two human languages in any new dataset introduced by the authors."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that any newly introduced dataset contains exactly two human languages in its entries."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The new dataset settings used for evaluation involve commonly used computer vision datasets (DomainNet, Office-Home, CIFAR-10) that consist of images with English labeling or annotation. The paper does not mention the use of other human languages in annotations or data entries, hence the proposed datasets or subsets are effectively monolingual with English labels."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any non-English language exclusive dataset being proposed or used in the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper presents algorithmic descriptions and formulae, it does not propose or introduce datasets containing programming or code content as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes mathematical formulas and notations to define the method, but does not introduce any dataset containing mathematical or logical expressions as dataset content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data are presented or introduced as new datasets in the paper."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or constructed languages are involved in the datasets introduced in the paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of datasets used (images with labels) is documented and understood (English labeling), so unknown does not apply."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets referenced involve labeled image data, which inherently contain human language in the form of annotations or labels."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-580-rubric-6",
    "token_usage": {
      "prompt_tokens": 11387,
      "completion_tokens": 165,
      "total_tokens": 11552
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not mention or provide any links or references to publicly available code repositories for the dataset construction or dataset-related data processing. The discussed datasets (DomainNet, Office-Home, CIFAR-10) are standard, publicly available benchmark datasets, not newly introduced by the authors. No code for dataset generation or construction is included or referenced."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "",
          "reasoning": "The paper does not document any new dataset creation process because it only uses existing benchmark datasets (DomainNet, Office-Home, CIFAR-10). There is no description of new dataset construction, collection, or preprocessing, only mentions of applying their method on standard datasets. Therefore, dataset creation documentation is not present."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-600-rubric-0",
    "token_usage": {
      "prompt_tokens": 9649,
      "completion_tokens": 167,
      "total_tokens": 9816
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 paragraph 'Motion from Video'",
          "Reasoning": "The paper states that DyNCA is trained to match target motion from videos, using 59 target dynamic texture videos provided in [24]. These videos serve as new target data for training DyNCA. Since these are videos of dynamic textures, they are of video modality. The videos are sourced from [24], which is a pre-existing dataset, but this paper uses them directly as target motion data during training; however, the paper does not introduce a new dataset of videos, rather uses existing video data for training (dynamic texture videos from [24]). Therefore, no new dataset of videos is introduced by the authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-600-rubric-1",
    "token_usage": {
      "prompt_tokens": 10449,
      "completion_tokens": 262,
      "total_tokens": 10711
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 5.1, 5.4, and throughout the paper",
            "reasoning": "The paper describes training DyNCA models on various target appearances and motions; these are generated by the model using training procedures involving loss functions and optimization, not human annotation. The 'data' is synthesized via the DyNCA framework, which is a neural cellular automata model trained with losses defined against exemplar videos or vector fields (which are manually designed or from existing datasets). Thus, annotation is performed via an automatic process (training procedure with losses), not by humans."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit annotation instructions mentioned",
            "reasoning": "The paper does not provide any instructions or guidelines intended for human annotators, as annotation is not performed by humans but by automated loss-based training."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No scoring rubrics or guidelines described",
            "reasoning": "No mention of rubrics or scoring guidelines for annotations, since no human annotation process is involved."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples provided",
            "reasoning": "As there is no human annotation, no examples or guidelines for annotation examples are present."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-600-rubric-2",
    "token_usage": {
      "prompt_tokens": 11639,
      "completion_tokens": 286,
      "total_tokens": 11925
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention of multiple human experts performing quality assurance on any dataset in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single non-expert annotator for any dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is present in the paper regarding quality assurance by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses a pre-trained optical flow network for motion loss, but this is part of the loss function during training, not described as a quality assurance procedure on dataset content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic/rule-based quality assurance process applied to any datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets, nor does it document any quality assurance process for dataset validation or annotation. Therefore, no quality assurance process is present or applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-600-rubric-3",
    "token_usage": {
      "prompt_tokens": 11249,
      "completion_tokens": 291,
      "total_tokens": 11540
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.1",
          "reasoning": "The paper states that the method trains on target vector fields manually designed and on target dynamic texture videos provided in [24]. The dataset of 59 target dynamic texture videos used for training was not created by the authors but sourced from an existing dataset. The 12 target vector fields are hand-designed but not new real-world data. Therefore, their training uses existing video data and hand-designed vector fields, hence 'collated' datasets."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-600-rubric-4",
    "token_usage": {
      "prompt_tokens": 11767,
      "completion_tokens": 483,
      "total_tokens": 12250
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the introduced dataset being used for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 (DyNCA Training), Section 5.1 (Dynamic Texture Synthesis)",
          "reasoning": "The introduced dataset of target appearance images and motion targets (either vector fields or videos) is used directly to train the DyNCA model from scratch, i.e., from randomly initialized parameters, to synthesize dynamic textures."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention fine-tuning a pre-trained model on the introduced dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of using reinforcement learning or RL-based post-training methods with the introduced dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.3 (User Study)",
          "reasoning": "Some target dynamic texture videos from prior datasets are used for evaluation, but there is no new dataset introduced solely for evaluation purposes. However, since the authors trained DyNCA on their collected set of 59 dynamic texture videos [24], these serve as training and evaluation data. The paper uses these datasets to benchmark and compare performance."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The introduced dataset is not used primarily for analysis of trends or patterns, but rather for training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not serve as a knowledge base to augment models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The introduced dataset has practical usage in training and evaluation as described."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-600-rubric-5",
    "token_usage": {
      "prompt_tokens": 12490,
      "completion_tokens": 550,
      "total_tokens": 13040
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset containing entries with more than two human languages. It focuses on dynamic texture videos and vector fields, which are not multilingual datasets."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets with exactly two human languages in the paper; the datasets relate to visual dynamic textures and motion vector fields."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets introduced in the paper consist of text data in English or any language. The paper focuses on dynamic texture videos and associated motions, not linguistic content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset characterized by a single non-English human language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper describes a model involving algorithms, neural networks, and mathematical notations, it does not introduce any new datasets containing code or programming language data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Throughout the paper (Sections 3 and 4)",
          "reasoning": "The paper contains mathematical expressions describing the model and loss functions but does not introduce a new dataset containing mathematical or logical notations as dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets involving biological sequences or non-human communication are introduced in the paper."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not involve fictional or constructed languages in any dataset introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets introduced contain language data of unspecified or undocumented origin."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets used for training and evaluation consist of video data (dynamic textures), motion vector fields, and images, none of which contain any human or programming language content."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-600-rubric-6",
    "token_usage": {
      "prompt_tokens": 9708,
      "completion_tokens": 136,
      "total_tokens": 9844
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention",
          "reasoning": "The paper does not include any explicit mention or link to code repositories related to data collection, preprocessing, or dataset generation. Although they refer to a demo website for interactive visualization, the paper does not specify public availability of code for dataset creation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No explicit section or description",
          "reasoning": "The paper does not introduce any new dataset created by the authors, nor does it document any dataset creation process. It uses existing datasets or predefined vector fields for training and evaluation, but no detailed documentation about dataset construction is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-601-rubric-0",
    "token_usage": {
      "prompt_tokens": 13153,
      "completion_tokens": 135,
      "total_tokens": 13288
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4: Implementation details, Data",
          "Reasoning": "The paper introduces a new approach evaluated on monocular video sequences depicting dynamic scenes. The data used are monocular videos of dynamic scenes, derived from existing synchronized multi-view camera datasets, following prior work to simulate monocular video input. These videos are captured by human-operated cameras, thus human-generated. The paper does not introduce a new raw dataset but uses existing datasets converted to monocular videos; thus, no new dataset is introduced by the authors in terms of new data collection."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-601-rubric-1",
    "token_usage": {
      "prompt_tokens": 13953,
      "completion_tokens": 207,
      "total_tokens": 14160
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4, Data subsection",
            "reasoning": "The paper mentions using estimated masks derived from a motion segmentation module trained within a Bayesian learning framework for motion segmentation, and supervision with segmentation masks initialized from Omnimatte. These masks are generated by the system rather than by human annotators. No mention is made of manual annotation or use of human annotators for these masks."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not provide any explicit annotation instructions for human annotators regarding the motion segmentation masks or any related annotation task."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not mention any scoring rubrics or evaluation criteria for annotation quality or process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not provide annotation examples or guidelines in any appendix or section."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-601-rubric-2",
    "token_usage": {
      "prompt_tokens": 15143,
      "completion_tokens": 264,
      "total_tokens": 15407
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the introduced datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance on the new datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance by multiple non-expert humans is described or mentioned."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using AI models for quality assurance or judging dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification or algorithmic quality assurance processes applied to dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces new datasets derived from existing benchmarks and in-the-wild videos but does not describe any quality assurance process for the datasets. The evaluation relies on standard image similarity metrics but no dataset annotation QA is reported."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-601-rubric-3",
    "token_usage": {
      "prompt_tokens": 14753,
      "completion_tokens": 396,
      "total_tokens": 15149
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly state that the authors created new datasets entirely from scratch by human contributors. The datasets used are existing benchmark datasets and in-the-wild videos."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any new dataset was generated entirely by AI or machine learning models independently, without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of data translation from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of data being created via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets mentioned (Nvidia Dynamic Scene Dataset and UCSD Dynamic Scenes Dataset) are existing datasets; there is no indication that the authors collected or aggregated data without modification to create a new dataset."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4 Implementation details - Data",
          "reasoning": "The paper uses existing multi-view dynamic scene datasets from Nvidia and UCSD. They derive monocular videos from these multi-view sequences by selecting a single view sequence (and removing frames without large moving objects), effectively producing new monocular dynamic videos adapted from existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is specified clearly as being derived from existing datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-601-rubric-4",
    "token_usage": {
      "prompt_tokens": 15271,
      "completion_tokens": 246,
      "total_tokens": 15517
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.2, Table 1, and Table 2",
          "reasoning": "The new datasets introduced in the paper are two dynamic scene benchmarks used to evaluate the proposed novel view synthesis method. Specifically, the Nvidia Dynamic Scene Dataset and the UCSD Dynamic Scenes Dataset are used for numerical evaluation, with error metrics such as PSNR, SSIM, and LPIPS reported for comparison with prior state-of-the-art. The datasets are utilized exclusively for benchmarking and performance measurement rather than training or fine-tuning."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-601-rubric-5",
    "token_usage": {
      "prompt_tokens": 15994,
      "completion_tokens": 510,
      "total_tokens": 16504
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the introduction or use of any dataset containing multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any newly introduced dataset contains exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets referenced are dynamic scene video datasets and there is no textual or linguistic content specified; therefore, no implication of monolingual English text."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets with non-English language content are introduced or described."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains code descriptions and formulas, it does not introduce any dataset comprised of programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes mathematical expressions and notation as part of the methodology description, but the new datasets themselves do not contain mathematical or logical notation as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of datasets with biological sequences or non-human communication systems is found in the paper."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss datasets consisting of constructed or artificial languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset entries is explicitly not applicable or relevant since the datasets are videos of dynamic scenes without specified textual language content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced and used in the paper are dynamic scene video datasets consisting of frames of visual content without any linguistic content or language entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-601-rubric-6",
    "token_usage": {
      "prompt_tokens": 13212,
      "completion_tokens": 146,
      "total_tokens": 13358
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not contain any link to code or mention availability of code related to data collection, preprocessing, or dataset generation. No URL or repository reference is provided in the text."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 (Implementation details)",
          "reasoning": "The paper documents the datasets used for evaluation and testing, specifying that they use Nvidia Dynamic Scene Dataset and UCSD Dynamic Scenes Dataset, both existing datasets, with explanations on preprocessing (monocular video derivation, frame removal) and test protocols used (held-out images per time instance). This indicates some documentation about dataset preparation and usage for this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-609-rubric-0",
    "token_usage": {
      "prompt_tokens": 11499,
      "completion_tokens": 294,
      "total_tokens": 11793
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets, Evaluation Metrics and Settings",
          "Reasoning": "The paper evaluates the introduced method on IU-Xray and MIMIC-CXR datasets, which are described as large collections of chest X-ray images and associated radiology reports. These images are obtained from real radiological examinations, thus they are human generated via clinical medical imaging equipment."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets, Evaluation Metrics and Settings",
          "Reasoning": "The datasets include radiology reports associated with chest X-ray images. These reports are authored by radiologists and thus are human generated natural language text."
        },
        {
          "Modality": "graph",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Sections 3.2 Dynamic Graph and 4.1 Datasets, Evaluation Metrics and Settings",
          "Reasoning": "The paper introduces a dynamic knowledge graph constructed by the authors combining pre-constructed general knowledge graph nodes and edges (derived from existing knowledge graphs) with specific knowledge extracted via automatic processing from retrieved reports using RadGraph triplets. The graph structure and nodes are dynamically updated during training. As this graph is constructed and updated algorithmically via graph encoder and automated extraction from text data, it is considered model generated."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-609-rubric-1",
    "token_usage": {
      "prompt_tokens": 12299,
      "completion_tokens": 239,
      "total_tokens": 12538
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2 (Dynamic Graph Construction)",
            "reasoning": "The dynamic graph nodes and edges are constructed automatically by retrieving similar reports from the training set, extracting specific knowledge triplets from those reports via RadGraph, and integrating them with a pre-constructed general knowledge graph. This process is an automatic procedure without mention of manual human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper.",
            "reasoning": "The paper does not provide any annotation instructions or guidelines for the dynamic graph node and edge construction since the process is automatic."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper.",
            "reasoning": "There is no description of any scoring rubrics or evaluation criteria provided for annotation in the dynamic graph construction, as it is data-driven and automatic."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper.",
            "reasoning": "The paper does not provide examples of annotated data or dynamic graph construction examples as annotation guidance; only a conceptual figure (Fig. 3) illustrates the framework."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-609-rubric-2",
    "token_usage": {
      "prompt_tokens": 13489,
      "completion_tokens": 307,
      "total_tokens": 13796
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process conducted by a single human expert for any newly introduced dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance involving multiple human experts for dataset annotations or content for any newly introduced dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that a single human non-expert performed quality assurance on any new dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that multiple human non-experts were involved in any quality assurance process for new datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using an AI model as a judge or for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of an automated verification process or algorithmic rule-based techniques used for quality assurance of dataset annotations or content in any new datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets by the authors; it uses existing datasets (IU-Xray and MIMIC-CXR) without describing any quality assurance processes for dataset annotations or content. Therefore, no quality assurance process is applied or documented for new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-609-rubric-3",
    "token_usage": {
      "prompt_tokens": 13099,
      "completion_tokens": 490,
      "total_tokens": 13589
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the authors created any original new datasets by human contributors. The datasets used, IU-Xray and MIMIC-CXR, are publicly available and pre-existing."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any new data was generated entirely by AI or machine learning models. The work focuses on using existing datasets and enhancing representation learning techniques."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe translating datasets from another language to create new data."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using machine translation to generate or convert datasets."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets, Evaluation Metrics and Settings",
          "reasoning": "The datasets used for evaluation (IU-Xray and MIMIC-CXR) are established public datasets collected and aggregated from existing medical sources without significant modification by the authors. The paper explicitly refers to these publicly available datasets and follows commonly used splits and preprocessing. However, these are not new datasets introduced by the authors."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 Dynamic Graph, Section 4.1 Datasets",
          "reasoning": "The paper constructs a dynamic knowledge graph by combining a pre-constructed general knowledge graph [46] with specific knowledge extracted via RadGraph [17] from retrieved reports within existing datasets. This dynamic graph reflects data derived by transformation and augmentation (adding nodes and edges) based on existing sources rather than creating new datasets. The approach constitutes derived data as it modifies existing knowledge graphs using further extracted clinical triplets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources and method of generation are explicitly documented, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-609-rubric-4",
    "token_usage": {
      "prompt_tokens": 13617,
      "completion_tokens": 258,
      "total_tokens": 13875
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": true,
          "reference": "Section 3.2 Dynamic Graph, Section 4 Experiments",
          "reasoning": "The authors introduce a dynamic knowledge graph dataset constructed by integrating general knowledge (from a pre-constructed graph) and specific knowledge extracted from retrieved radiology reports via RadGraph. This dynamic graph dataset serves as a knowledge base that augments the model's input\u2014visual representations are enhanced through the graph for improved report generation. This usage is explicitly described in Section 3.2, and experiments in Section 4 demonstrate its contribution to performance, indicating the dataset's role as a knowledge base."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-609-rubric-5",
    "token_usage": {
      "prompt_tokens": 14340,
      "completion_tokens": 337,
      "total_tokens": 14677
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 (Datasets) and throughout the paper",
          "reasoning": "The datasets used (IU-Xray and MIMIC-CXR) consist of radiology reports written in English only. This is evident from the description of the datasets and the use of English medical terminology in the examples and evaluations, implying the dataset entries are monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3.1 (Background) and Section 3.2 (Dynamic Graph)",
          "reasoning": "The dataset entries include structured representations such as knowledge graphs with triplets (<subjective entity, relation, objective entity>), adjacency matrices representing graph edges, and formal mathematical notations describing the encoder and decoder layers (e.g., equations for transformer layers and attention mechanisms). These representations involve formal symbolic notation consistent with mathematical and logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-609-rubric-6",
    "token_usage": {
      "prompt_tokens": 11558,
      "completion_tokens": 138,
      "total_tokens": 11696
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention",
          "reasoning": "The paper does not provide any link, URL, or mention of public code repositories for the dataset construction or processing code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Datasets, Evaluation Metrics and Settings",
          "reasoning": "The paper documents the dataset creation/use by describing the datasets IU-Xray and MIMIC-CXR, their sizes, splits, preprocessing steps (e.g., report splitting, entity extraction by Stanza), and use of RadGraph annotations for specific knowledge extraction, thus providing detailed documentation on the dataset characteristics and preparation."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-630-rubric-0",
    "token_usage": {
      "prompt_tokens": 13722,
      "completion_tokens": 93,
      "total_tokens": 13815
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Implementation; Training paragraph",
          "Reasoning": "The authors train their model on 1.5 million full-resolution images paired with corresponding EXIF metadata from a random subset of the YFCC100M dataset. These images originate from human-captured photos available in this dataset, not generated by models or simulations."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-630-rubric-1",
    "token_usage": {
      "prompt_tokens": 14522,
      "completion_tokens": 215,
      "total_tokens": 14737
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Implementation and Section 3.1 Learning Cross-Modal Embeddings",
            "reasoning": "The dataset consists of 1.5M image-EXIF pairs sampled from YFCC100M, used to train the model by an automatic process (contrastive learning between image patches and metadata). There is no mention of human annotation for these data; rather the EXIF metadata is automatically extracted from photos."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There is no indication in the paper that annotation instructions were given since the data is automatically collected metadata paired with images."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics are discussed or relevant for this automatically paired dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples are provided, as the dataset is raw image-metadata pairs obtained automatically without human annotation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-630-rubric-2",
    "token_usage": {
      "prompt_tokens": 15712,
      "completion_tokens": 272,
      "total_tokens": 15984
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe QA by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence of quality assurance by multiple non-expert human annotators is described."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report that an AI model was used as a judge for performing quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated verification process for validating dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses the YFCC100M dataset as the source of images and EXIF metadata, but does not mention any quality assurance procedures applied or performed by the authors on this data; thus, no quality assurance process is documented for the dataset used in training."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-630-rubric-3",
    "token_usage": {
      "prompt_tokens": 15322,
      "completion_tokens": 347,
      "total_tokens": 15669
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any new dataset was created entirely from scratch by human contributors. The data used are images and their associated EXIF metadata from an existing dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any dataset was generated entirely by AI or machine learning models as original content. The data involves real photos and their metadata."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any machine translation applied to produce the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation (Training)",
          "reasoning": "The training data consists of 1.5 million images and associated EXIF metadata pairs sampled from a random subset of the YFCC100M dataset, which is an existing large-scale dataset. The authors collect and use these existing images and their metadata without significant modification for their training tasks."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper derives text representations from the EXIF metadata, it does not describe the dataset itself as a derived dataset with substantial transformation or derivation. The dataset consists of existing image and metadata pairs as provided by YFCC100M."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data (YFCC100M subset) is clearly specified, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-630-rubric-4",
    "token_usage": {
      "prompt_tokens": 15840,
      "completion_tokens": 401,
      "total_tokens": 16241
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.1, Section 4.1",
          "reasoning": "The paper describes training a joint embedding between image patches and their corresponding camera metadata by contrastive learning, using 1.5 million images from the YFCC100M dataset. This training is done in a self-supervised manner to learn a visual representation capturing camera properties."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The learned visual network features are used for downstream classification tasks such as predicting camera-related properties via linear probing, which amounts to supervised fine-tuning on labeled datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning or RL-based post-training methods using the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The model is evaluated for zero-shot splice detection and localization on multiple forensics benchmark datasets (e.g., CASIA I and II, Columbia), using the learned features without additional training."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.4",
          "reasoning": "The authors conduct extensive ablation studies to analyze the impact of different metadata representations, tag formats, and architectures on the learned representations and downstream performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not described as being used as a knowledge base to augment models via retrieval or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced (image patches paired with EXIF metadata from YFCC100M) are actively used for pre-training, supervised fine-tuning, evaluation, and analysis as detailed in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-630-rubric-5",
    "token_usage": {
      "prompt_tokens": 16563,
      "completion_tokens": 622,
      "total_tokens": 17185
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced is based on image patches and corresponding camera metadata in EXIF format, which is text representing camera parameters. There is no mention of multiple human languages being used or present in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains exactly two human languages; only English-like text representing metadata fields is used."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.2 (Representing the Camera Metadata), Section 4.1 (Implementation)",
          "reasoning": "The paper explains that EXIF metadata is typically stored in a human-readable text format resembling English (e.g., tag names and values like 'Camera Make: Canon'), and they use a pretrained English language model DistilBERT to encode this text. The metadata is converted to English textual strings describing camera properties, indicating the dataset entries contain English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The EXIF metadata text is represented in English; no mention or evidence of non-English language entries is given."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain programming or structured code-related content. The EXIF metadata is converted to plain text describing camera tags and values, not programming languages."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While some numeric data such as aperture values or exposure times appear in the metadata, these are not formal mathematical expressions or symbolic mathematical notation. No explicit formal mathematical or logical expressions are stated as part of the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of constructed or fictional languages being part of the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the textual metadata is clearly described and processed using English language models, therefore the language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes textual data (EXIF tags and values converted to text), which is in English, so it contains language and is not language-free."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-630-rubric-6",
    "token_usage": {
      "prompt_tokens": 13781,
      "completion_tokens": 192,
      "total_tokens": 13973
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention",
          "reasoning": "The paper does not include any explicit mention or link to code repositories for the dataset construction or data preprocessing related to the training or evaluation datasets. There is no indication that the code for data collection or processing is publicly released."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 and Section 4.1",
          "reasoning": "The paper documents the dataset creation process, detailing that they use 1.5 million full-resolution images and EXIF metadata pairs from a random subset of the public YFCC100M dataset. It also explains criteria such as discarding images with insufficient EXIF tags and which EXIF tags are used. Furthermore, Section 4.1 describes the preprocessing steps including image cropping, tag selection, and batch sizes for training. Thus, the dataset construction and usage details are sufficiently documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-646-rubric-0",
    "token_usage": {
      "prompt_tokens": 10312,
      "completion_tokens": 198,
      "total_tokens": 10510
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets and throughout Section 4 Experiments",
          "Reasoning": "The new dataset entries referenced in the paper include FFHQ, MetFaces, LSUN-Church, and Flickr-Scenery datasets used for training and evaluating the proposed CREPS model. These datasets are collections of images: FFHQ and MetFaces contain human faces, LSUN-Church contains outdoor church photographs, and Flickr-Scenery includes landscape images. The paper explicitly mentions using these to train and test their generative model, but these datasets themselves are human-generated images (collected/photographed by humans and curated) rather than model-generated or of unknown origin. The paper does not introduce any new dataset created by the authors; instead, they utilize these existing datasets for experiments. Hence, there is no new dataset introduced by the authors in this paper."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-646-rubric-1",
    "token_usage": {
      "prompt_tokens": 11112,
      "completion_tokens": 219,
      "total_tokens": 11331
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1",
            "reasoning": "The paper discusses using public datasets (FFHQ, MetFaces, LSUN-Church, Flickr-Scenery) for training and evaluation. These datasets are established and not newly introduced in the paper. The work involves training models on these datasets but does not mention collecting or annotating a new dataset; rather, the contribution is a model architecture and method. Therefore, there is no new dataset introduced in the paper that requires annotation by humans or others."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "Since no new dataset is introduced, there are no annotation instructions applicable or described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation rubrics are described because no new dataset is introduced."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples are provided due to the lack of any new dataset and corresponding annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-646-rubric-2",
    "token_usage": {
      "prompt_tokens": 12302,
      "completion_tokens": 313,
      "total_tokens": 12615
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for datasets. The datasets used are standard public datasets without documented expert annotation or QA."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence in the paper that multiple human experts performed any quality assurance for the datasets involved."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any QA performed by a single non-expert annotator on the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about multiple non-expert annotators performing any quality assurance on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance is described to be performed by AI models on any datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic/rule-based quality assurance process used to validate dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses several public datasets (FFHQ, MetFaces, LSUN-Church, Flickr-Scenery) for training and evaluation but does not introduce any new datasets. Furthermore, it does not mention applying or documenting any quality assurance process for dataset annotations or content. Hence, no QA process is applied or described."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-646-rubric-3",
    "token_usage": {
      "prompt_tokens": 11912,
      "completion_tokens": 501,
      "total_tokens": 12413
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 (Datasets)",
          "reasoning": "The paper uses existing datasets such as FFHQ, MetFaces, LSUN-Church, and Flickr-Scenery, which are publicly available. However, the paper states that their models are trained from scratch on these datasets indicating no new human-created image datasets introduced by the authors. There is no indication that the authors created new datasets from scratch specifically for this work."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 4.2 (Image generation); Section 4.3 (Generate arbitrary-scale images)",
          "reasoning": "The authors generate images using their proposed CREPS generative model at various scales and transformations. These synthesized images are novel outputs produced by the model and thus represent new data generated entirely by the AI model. The paper shows qualitative samples (Fig. 4, Fig. 6) and quantitative evaluations on generated images."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset translated by human translators or any translation process."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any dataset or data translated by machine translation in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 (Datasets)",
          "reasoning": "The paper uses several existing datasets (FFHQ, MetFaces, LSUN-Church, Flickr-Scenery) collected from other sources. These datasets are collated collections of images without significant modification indicated. The authors do not claim to have modified or adapted these datasets substantially."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creating new datasets derived or adapted from existing ones with transformations or modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly mentions the datasets used and how generated images are obtained. The data origins are well specified."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-646-rubric-4",
    "token_usage": {
      "prompt_tokens": 12430,
      "completion_tokens": 508,
      "total_tokens": 12938
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental setup, Section 4.2 Image generation",
          "reasoning": "The datasets FFHQ, MetFaces, LSUN-Church, and Flickr-Scenery are used by the authors to train their CREPS models from scratch until convergence, as explicitly stated in Section 4.1 and demonstrated through quantitative and qualitative results in Section 4.2."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of these datasets for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication in the paper of the datasets being used for reinforcement learning or RL-based post-training techniques."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the datasets serve as benchmarks for quality evaluation via FID scores and scale-consistency metrics, these datasets themselves are not introduced by the authors as new datasets for evaluation only. They are used primarily for training and generation; thus, evaluation is not the exclusive use."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used primarily for analyzing trends or patterns distinct from training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as a knowledge base to augment models or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new datasets are introduced by the authors in this paper. All datasets mentioned (FFHQ, MetFaces, LSUN-Church, Flickr-Scenery) are existing datasets, and the paper uses them for training and evaluation of the proposed model. Therefore, there is no practical utility of a new dataset introduced by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-646-rubric-5",
    "token_usage": {
      "prompt_tokens": 13153,
      "completion_tokens": 546,
      "total_tokens": 13699
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset containing entries with more than two human languages. The datasets used (FFHQ, MetFaces, LSUN-Church, Flickr-Scenery) are image datasets without linguistic content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset introduced in the paper contains entries with exactly two human languages. The datasets are visual image datasets without language components."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's new datasets are image datasets and do not contain English-only textual content. Thus, no monolingual (English) dataset is introduced."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset introduced contains entries with a single non-English language; the datasets are image datasets without linguistic data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are image datasets; although the paper describes code implementations and algorithms, the datasets themselves do not contain code or programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced consist of images (human faces, landscapes, etc.), not data entries with mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets related to biological sequences or non-human communication systems are introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets involving constructed (fictional or artificial) languages are introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language origins are either not applicable or not relevant because the datasets are purely visual and do not contain linguistic entries."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced and used for training in the paper are purely image datasets containing no linguistic content. Therefore, the dataset entries do not contain any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-646-rubric-6",
    "token_usage": {
      "prompt_tokens": 10371,
      "completion_tokens": 177,
      "total_tokens": 10548
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit code link provided for dataset construction",
          "reasoning": "The paper provides a code link for the CREPS model at https://github.com/VinAIResearch/CREPS but does not provide any code or mention for dataset construction or collection. The datasets used (FFHQ, MetFaces, LSUN-Church, Flickr-Scenery) are standard and pre-existing datasets, and no new dataset creation code or details are given."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No dataset creation process documented",
          "reasoning": "The paper uses existing datasets such as FFHQ, MetFaces, LSUN-Church, and Flickr-Scenery and does not introduce any new datasets or describe any dataset collection or preprocessing process in detail. Therefore, there is no dataset creation documentation provided in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-726-rubric-0",
    "token_usage": {
      "prompt_tokens": 11082,
      "completion_tokens": 275,
      "total_tokens": 11357
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.3 (Point Cloud Completion) and Figure 7",
          "Reasoning": "The authors apply their method to real-world lidar point cloud completion for autonomous driving scenarios. The 3D lidar point cloud data is captured by human-operated sensor equipment (lidar scanners) and presented as partial point clouds from scenes (e.g., vehicles in nuScenes dataset). This dataset constitutes human-generated sensor data used in the new application evaluation of the proposed method."
        },
        {
          "Modality": "other",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 (Unconditional point cloud generation)",
          "Reasoning": "The new dataset used for training the unconditional 3D point cloud generation is composed of 3D shape point clouds from the training split following PointFlow and PVD settings, which are human-contributed datasets of 3D objects in the form of point clouds. These point clouds are collected or manually prepared as representations of 3D objects (such as airplanes, chairs, cars), thus are human generated data in 3D point cloud format. The paper uses their own training split consistent with prior works, so the authors introduce or utilize this dataset for their proposed method."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-726-rubric-1",
    "token_usage": {
      "prompt_tokens": 11882,
      "completion_tokens": 332,
      "total_tokens": 12214
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3 and 4",
            "reasoning": "The paper describes a novel point cloud generation method 'Point Straight Flow' (PSF) that is trained and evaluated on standard 3D point cloud datasets following established splits (e.g., PointFlow and PVD datasets) without mention of human annotators providing labels or categories. The annotations correspond to the use of existing datasets and performance metrics, computed automatically via Chamfer distance, Earth Mover's distance, and 1-NNA metrics. The process includes training a neural velocity flow network and distillation techniques, which are automatic computational methods rather than human annotation efforts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No description of annotation instructions applicable to a human annotator on the new dataset; only training procedures for neural models.",
            "reasoning": "The paper does not describe any human annotation process that requires instructions; all data handling is automated through neural network training and evaluation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No human annotation rubric is present; quantitative metrics like Chamfer and EMD are used for evaluation in Section 4 and 4.1.",
            "reasoning": "The evaluations and metrics used are computational and do not constitute human-annotated rubric scoring; thus, rubrics for annotation are not applicable."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples for human annotators are provided in the main text or appendix.",
            "reasoning": "As annotation is performed via automatic processes and neural network training, there are no examples provided as guidelines for annotators."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-726-rubric-2",
    "token_usage": {
      "prompt_tokens": 13072,
      "completion_tokens": 311,
      "total_tokens": 13383
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert specifically for dataset annotations or content validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts have performed quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors do not describe any QA process performed by a single non-expert annotator related to the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any QA conducted by multiple human non-experts on dataset content or annotations."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of quality assurance performed by an AI model serving as a judge for the datasets introduced."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic QA specifically applied to validate the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces new generative model methods and performs experiments on existing standard datasets (e.g., Chair, Airplane, Car, 55-class dataset) commonly used in 3D point cloud generation literature. It does not introduce any new datasets. Therefore, no quality assurance process related to new datasets is described or applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-726-rubric-3",
    "token_usage": {
      "prompt_tokens": 12682,
      "completion_tokens": 320,
      "total_tokens": 13002
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1, General training settings, and Section 4.1 Unconditional point cloud generation",
          "reasoning": "The paper utilizes the existing point cloud datasets following prior splits and settings (e.g., from PointFlow and PVD) for training and evaluation, indicating that data are collected from existing sources without indication of new creation."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1, General training settings, and Algorithm 1",
          "reasoning": "The training procedure involves generating intermediate point cloud states via neural velocity flow models starting from Gaussian noise towards existing real data point clouds, applying transformations and distillation, thus creating derived data from existing point cloud datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-726-rubric-4",
    "token_usage": {
      "prompt_tokens": 13200,
      "completion_tokens": 193,
      "total_tokens": 13393
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. It uses existing 3D point cloud datasets following prior works for training and evaluation of the proposed model. Hence, there is no new dataset introduced or its utility described in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-726-rubric-5",
    "token_usage": {
      "prompt_tokens": 13923,
      "completion_tokens": 507,
      "total_tokens": 14430
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of 3D point cloud shapes from single object classes and does not contain any human language data, let alone multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains exactly two human languages or bilingual content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is about 3D point cloud data and does not contain English text content entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English language data is present or discussed for the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper describes methods and equations, the dataset itself does not contain programming code or structured programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notations appear in the paper's method description but are not part of the datasets themselves, which are collections of 3D point cloud data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no presence of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language or communication type of the dataset entries is clearly specified as 3D point clouds; no unknown language usage."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced consist of 3D point clouds representing shapes and objects, which are non-linguistic data without any human language or symbolic language content."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-726-rubric-6",
    "token_usage": {
      "prompt_tokens": 11141,
      "completion_tokens": 188,
      "total_tokens": 11329
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper",
          "reasoning": "The paper does not provide any link or mention of publicly available code for data collection, preprocessing, generation, or for the dataset used. There is no indication that code is shared for reproducing the dataset or experiments."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 (Experiment) and related descriptions",
          "reasoning": "The paper documents the dataset usage and setup in multiple places, including Section 4 where experiments are described. It discusses the train/test splits following prior works like PointFlow and PVD, the batch size, learning rate settings, evaluation metrics, and synthetic and real-world datasets (e.g., nuScenes). Thus, while no new dataset is specifically created by the authors, the process and usage of datasets for experiments is described in reasonable detail for understanding and reproducing experimental results."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-738-rubric-0",
    "token_usage": {
      "prompt_tokens": 18875,
      "completion_tokens": 139,
      "total_tokens": 19014
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 (Implementation Details) and Introduction",
          "Reasoning": "The paper applies their proposed Federated Incremental Semantic Segmentation (FISS) method on benchmark datasets Pascal-VOC 2012 and ADE20k, which consist of images with pixel-level annotations. These images and their annotations are human-generated as they originate from manually collected and labeled image datasets. The authors do not introduce any new dataset created by themselves; instead, they use these existing benchmark datasets. The paper's focus is methodological and experimental evaluation rather than proposing a new dataset."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-738-rubric-1",
    "token_usage": {
      "prompt_tokens": 19675,
      "completion_tokens": 210,
      "total_tokens": 19885
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 5.1 Implementation Details",
            "reasoning": "The paper mentions using benchmark datasets Pascal-VOC 2012 and ADE20k, which are publicly available datasets with pre-existing annotations. There is no mention of new dataset creation or annotation performed by the authors. The focus is on the federated incremental semantic segmentation method, not on new annotated datasets."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "No new dataset annotation instructions are described for any new dataset because there is no new dataset introduced or annotated by humans in the paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "No scoring rubrics or evaluation criteria for annotation are discussed since no new annotated dataset is introduced."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "No annotation examples are shown or provided for a new dataset as no new dataset annotation is performed."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-738-rubric-2",
    "token_usage": {
      "prompt_tokens": 20865,
      "completion_tokens": 377,
      "total_tokens": 21242
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset annotation or quality assurance being conducted or verified by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of multiple human experts performing quality assurance on any newly introduced datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss quality assurance conducted by a single human non-expert for datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the method uses model-generated pseudo labels for training, these are part of the learning process rather than an explicit quality assurance process to validate dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated verification or algorithmic rule-based quality assurance on the dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses existing benchmark datasets (Pascal-VOC 2012 and ADE20k) but does not introduce any new datasets, nor does it describe any quality assurance processes for datasets. Therefore, no quality assurance is applied or documented for new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-738-rubric-3",
    "token_usage": {
      "prompt_tokens": 20475,
      "completion_tokens": 273,
      "total_tokens": 20748
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.1 Implementation Details",
          "reasoning": "The paper utilizes publicly available benchmark datasets Pascal-VOC 2012 and ADE20k for evaluations, which were collected from existing sources without indication of modification or transformation. Therefore, the data used is collated from pre-existing datasets."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the datasets used and describes their source. Hence, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-738-rubric-4",
    "token_usage": {
      "prompt_tokens": 20993,
      "completion_tokens": 382,
      "total_tokens": 21375
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new dataset being used exclusively for pre-training large models. Instead, it focuses on federated incremental semantic segmentation framework and existing segmentation datasets."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is introduced or used from scratch training in the paper. The experiments use benchmark datasets (Pascal-VOC 2012 and ADE20k) that are pre-existing."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new dataset that is used exclusively for supervised fine-tuning a pre-trained model. It focuses on new federated incremental learning methods, not dataset-specific fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning post-training or RL-based methods using any new dataset in the paper."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing datasets (Pascal-VOC 2012 and ADE20k) for evaluation and benchmarking, but does not introduce any new dataset for evaluation purposes."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are for training and evaluation of methods rather than for analysis of trends or characteristics. No new dataset is introduced for analysis purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is introduced or used as a knowledge base to augment models in this paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. It uses existing benchmark datasets for evaluation and experimentation but does not describe or demonstrate practical usage of any newly proposed dataset."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-738-rubric-5",
    "token_usage": {
      "prompt_tokens": 21716,
      "completion_tokens": 609,
      "total_tokens": 22325
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the proposed datasets contain entries with more than two human languages. The dataset is based on image segmentation datasets Pascal-VOC 2012 and ADE20k, which are English-labeled datasets."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets contain exactly two human languages. Labels and annotations appear in English only."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5.1 Implementation Details; Introduction refers to medical lesions segmentation example; datasets used are Pascal-VOC 2012 and ADE20k",
          "reasoning": "The new datasets introduced are based on Pascal-VOC 2012 and ADE20k segmentation datasets, which are annotated in English. There is no mention of other languages in the dataset labels or annotations. The paper refers to these datasets and dataset partitions but does not introduce new datasets with multilingual or other languages. Thus, the datasets are monolingual (English)."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets and annotations used are in English, and no non-English-only languages are mentioned."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the paper includes equations and losses formulations, the datasets themselves do not contain entries in programming or structured code languages. The dataset is images with semantic segmentation labels."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains mathematical notations but the datasets do not contain mathematical or formal logical expressions or symbolic representations as content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though medical lesion segmentation is mentioned as application domain, the datasets used are standard image datasets; no biological sequences or non-human communication systems are part of the dataset entries."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets contain fictional or artificially created language content."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The linguistic content of dataset entries is known and specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets do contain language in the form of English label annotations."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-738-rubric-6",
    "token_usage": {
      "prompt_tokens": 18934,
      "completion_tokens": 232,
      "total_tokens": 19166
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and main text",
          "reasoning": "The paper provides a GitHub link (https://github.com/JiahuaDong/FISS) for their proposed model code, but this repository appears to include code related to the federated incremental semantic segmentation model rather than code specifically for constructing any new datasets. Moreover, the paper does not mention releasing any dataset construction code or scripts for creating new datasets, and the benchmark datasets used (Pascal-VOC 2012 and ADE20k) are pre-existing datasets. No indication is present that any new datasets were introduced or dataset construction code released."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Throughout the paper",
          "reasoning": "The paper uses existing benchmark datasets (Pascal-VOC 2012 and ADE20k) under various experimental settings, detailing these settings and task splits for evaluation of their model, but does not introduce or document any new datasets created by the authors. There is no dedicated section or discussion describing dataset creation processes, data collection protocols, or dataset statistics for new datasets, implying no new dataset introduction or documentation."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-744-rubric-0",
    "token_usage": {
      "prompt_tokens": 11945,
      "completion_tokens": 126,
      "total_tokens": 12071
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets",
          "Reasoning": "The paper introduces a new problem setup that involves training and testing on subsets of two existing video datasets: ImageNet-VidVRD and VidOR. The authors split these datasets into train and test sets with disjoint predicates and videos to create their few-shot referring relationship experimental setup. There is no mention of collecting or introducing any new dataset beyond re-splitting existing datasets. Therefore, no truly new dataset is introduced in this paper."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-744-rubric-1",
    "token_usage": {
      "prompt_tokens": 12745,
      "completion_tokens": 272,
      "total_tokens": 13017
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1, 3.2, 3.3",
            "reasoning": "The annotation involves the use of Faster-RCNN pre-trained detector to generate object bounding boxes automatically, and the relation network is meta-trained via support set videos to learn visual relationship potentials. The annotations used for training relation similarity are derived from existing datasets (VidOR and ImageNet-VidVRD), but the paper does not mention any human annotators or manual annotation process for the new task; instead, the support videos' ground-truth object pairs are used programmatically for training the relation network."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "The paper does not provide any instructions related to manual annotation guidelines. The annotation is performed automatically using pre-trained object detectors and meta-learning on support set videos."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "No scoring rubrics or detailed evaluation criteria for annotators are described since the annotation process is automatic and based on ground-truth object bounding boxes from existing datasets."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "The paper provides no example annotations or manual annotation examples since annotations are derived from existing datasets and automated processes."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-744-rubric-2",
    "token_usage": {
      "prompt_tokens": 13935,
      "completion_tokens": 283,
      "total_tokens": 14218
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human annotator who is an expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance by multiple human experts in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about quality assurance involving multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the model itself is AI-based, the paper does not describe the use of an AI model to perform quality assurance on dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention automated verification processes or rule-based techniques used explicitly for quality assurance of dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses two pre-existing benchmark datasets, ImageNet-VidVRD and VidOR, and does not introduce any new dataset annotations. Therefore, no quality assurance process for dataset annotations is described or applied for new datasets in this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-744-rubric-3",
    "token_usage": {
      "prompt_tokens": 13545,
      "completion_tokens": 449,
      "total_tokens": 13994
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the authors created any new dataset entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any dataset entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of data being produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was generated via machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The experiments use two publicly available video benchmark datasets, ImageNet-VidVRD and VidOR, which are existing sources. The paper states that these datasets are used for evaluation and that they split the datasets into train and test sets based on predicate disjointness. The data is collected from other sources such as ILVSRC2016-VID and YFCC100M, indicating aggregation from existing datasets without mention of significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The authors create new splits of existing datasets (ImageNet-VidVRD and VidOR) by randomly assigning predicates to train and test sets for few-shot referring relationships. This splitting and episodic setting effectively derive new task-specific data partitions for few-shot training and evaluation, constituting a derived form of data from existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data is clearly documented; they use existing public datasets and create derived splits."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-744-rubric-4",
    "token_usage": {
      "prompt_tokens": 14063,
      "completion_tokens": 294,
      "total_tokens": 14357
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.3 and Section 4",
          "reasoning": "The proposed few-shot referring relationship in videos dataset is used to meta-train the relation network from support set videos in an episodic manner, effectively training the model from scratch on this new task and dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 and Table 1",
          "reasoning": "The same dataset is used to evaluate and benchmark the performance of the proposed method against baselines with several metrics such as spatiotemporal accuracy and mean IoU across test videos and predicates."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.5 and Table 3, Table 4, Table 5",
          "reasoning": "The dataset is used for detailed ablations and analysis such as studying predicate frequency effects, unseen subject/object cases, and comparing embedding techniques to understand characteristics and trends."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-744-rubric-5",
    "token_usage": {
      "prompt_tokens": 14786,
      "completion_tokens": 547,
      "total_tokens": 15333
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper and the datasets used (ImageNet-VidVRD and VidOR) contain only English language content, specifically English language subject, predicate, and object labels. No mention of multiple human languages is made."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the dataset entries involve exactly two human languages. All predicate and object labels are in English."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The datasets used (ImageNet-VidVRD and VidOR) have annotations such as predicates and object categories given as English words (e.g., 'plane', 'fly above', 'person'). No other languages are indicated in the annotations. Hence the dataset entries contain only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains entries with exactly one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the method involves programming and uses machine learning models, the dataset itself does not contain entries with programming code or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The method description contains mathematical notation to describe the model and optimization, but this pertains to the method and not the dataset entries themselves."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not include biological sequences or non-human communication content."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are used in the dataset entries."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset linguistic content is specified (English)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Dataset entries include language annotations in English, so the dataset does contain language."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-744-rubric-6",
    "token_usage": {
      "prompt_tokens": 12004,
      "completion_tokens": 154,
      "total_tokens": 12158
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention in the paper about dataset construction code availability.",
          "reasoning": "The paper uses two public benchmarks, ImageNet-VidVRD and VidOR, and references these datasets but does not provide or mention any publicly available code for dataset construction. The paper focuses on the proposed method rather than constructing new datasets. There is no link or indication that code related to data collection or processing is provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section detailing dataset creation or construction.",
          "reasoning": "The paper uses existing public datasets for experiments and does not introduce any new datasets. Hence, there is no documentation related to dataset creation or construction processes described in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-76-rubric-0",
    "token_usage": {
      "prompt_tokens": 10205,
      "completion_tokens": 110,
      "total_tokens": 10315
    },
    "response": {
      "sources": [
        {
          "Modality": "other",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1, Dataset",
          "Reasoning": "The new dataset introduced is the 'Big CAD dataset' called ABC, used for training and testing. It consists of 3D CAD models which are created by humans, as evidenced by reference [20] describing ABC as a CAD model dataset. The shapes have no clear category labeling and are challenging, indicating human design rather than synthetic or simulation-based generation."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-76-rubric-1",
    "token_usage": {
      "prompt_tokens": 11005,
      "completion_tokens": 233,
      "total_tokens": 11238
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 3.2 and 4.1 Implementation details",
            "reasoning": "The paper describes that the occupancy predictions and related annotations are performed by the ARO-Net neural network model trained on input point clouds. The method uses implicit neural representations and attention modules to predict occupancies, indicating annotation is an output of a trained AI model rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not provide any explicit instructions for human annotators for data annotation; instead, the annotations (occupancy values for query points) are generated by the AI model."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "No mention of scoring rubrics or evaluation rubrics for annotation guidelines is provided in the paper for the newly introduced dataset or model outputs."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "There are visual examples of results and network activations, but no explicit annotation examples provided as guidelines for annotators."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-76-rubric-2",
    "token_usage": {
      "prompt_tokens": 12195,
      "completion_tokens": 287,
      "total_tokens": 12482
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human annotators or experts performing quality assurance on the introduced dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no indication of multiple expert annotators involved in quality assurance for the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is given about any single non-expert annotator conducting quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any quality assurance related to multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of an AI model being used as a judge or for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automatic verification or rule-based quality assurance processes applied specifically to the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces no new dataset annotations or content that require quality assurance. It uses existing public datasets (ABC, ShapeNet V1, PSB) and does not document any quality assurance process for these datasets or new annotations. Therefore, no quality assurance is applied or described for dataset quality."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-76-rubric-3",
    "token_usage": {
      "prompt_tokens": 11805,
      "completion_tokens": 425,
      "total_tokens": 12230
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Experiment setting and Dataset",
          "reasoning": "The authors conducted experiments on the ABC dataset and ShapeNet V1 dataset, which are publicly available existing datasets. The paper does not mention the creation of any new dataset by human contributors from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe generating new datasets entirely by AI or machine learning models. The data used are from existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of human translation of data is present in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of machine translation of data is present in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The authors make use of existing publicly available datasets such as ABC and ShapeNet for training and evaluation. These datasets are used without modification as sources of input point clouds."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 ARO-Net for occupancy prediction and elsewhere",
          "reasoning": "The authors generate new data representations called anchored radial observations (ARO) based on existing input 3D point clouds from the datasets. These ARO encodings and associated occupancy labels constitute derived data from the original datasets as they transform and encode the input point clouds into a new representation for training."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies the data origin and does not leave the data source undocumented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-76-rubric-4",
    "token_usage": {
      "prompt_tokens": 12323,
      "completion_tokens": 377,
      "total_tokens": 12700
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation details; Section 4.2 Comparisons to state-of-the-art methods",
          "reasoning": "The ABC and ShapeNet V1 datasets are used to train ARO-Net from scratch, as indicated by the description of the training setup and reported performance metrics in Section 4.1 and 4.2. The paper states 'ARO-Net receives 2,048 points as input when training' and compares results with several methods trained on the same data, implying training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset and Evaluation metrics; Section 4.2 Quantitative comparisons",
          "reasoning": "The ABC and ShapeNet datasets are used to evaluate the reconstruction performance of ARO-Net against state-of-the-art methods using multiple quantitative metrics such as Chamfer distance, Hausdorff distance, and IOU. The evaluation results are presented in Tables 1 and 2 and discussed in Section 4.2."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3 Ablation study",
          "reasoning": "The paper uses the introduced datasets for ablation studies including anchor placement strategy, number of anchors, decoder architecture, and qualitative visualization of anchor activations to analyze the characteristics and effects of the proposed method's components."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-76-rubric-5",
    "token_usage": {
      "prompt_tokens": 13046,
      "completion_tokens": 438,
      "total_tokens": 13484
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the dataset contains content in more than two human languages. The data is 3D shape models without linguistic content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any dataset having entries in exactly two human languages. The datasets used are 3D shape collections with no language annotations."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises 3D CAD and shape models (ABC, ShapeNet), which do not contain English linguistic content as part of the data entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the dataset contains exactly one non-English language. The data is shape geometry, not linguistic."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset composed of programming or code-related content entries. It uses code to implement methods but does not release such code datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains mathematical formulas and symbolic representations in the method description, but the proposed datasets (ABC, ShapeNet) themselves do not contain mathematical or logical notation as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The introduced datasets relate to 3D shapes from CAD models and objects; there is no inclusion of biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or use of constructed or fictional languages within the dataset or data entries introduced by the authors."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language content is clearly non-existent (not text), so unknown language status does not apply."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The datasets introduced and used consist of 3D geometric shapes and point clouds with no natural or constructed language data present."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-76-rubric-6",
    "token_usage": {
      "prompt_tokens": 10264,
      "completion_tokens": 145,
      "total_tokens": 10409
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not include any references, links, or indications that the code used for constructing or processing the datasets is publicly available. There is no mention of code release for the datasets or the preprocessing pipelines."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper uses existing datasets such as ABC and ShapeNet V1, which are publicly available. However, the paper does not introduce any new dataset created by the authors, nor does it document any dataset creation process. Therefore, no dataset creation documentation is provided in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-767-rubric-0",
    "token_usage": {
      "prompt_tokens": 9938,
      "completion_tokens": 104,
      "total_tokens": 10042
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 (Datasets)",
          "Reasoning": "The paper introduces their method on four datasets: Something-Something V1 & V2, Kinetics400, and HMDB51. These are existing public datasets, so no new dataset is introduced by the authors. The paper does not present any novel dataset; only experimental protocols and a new method are introduced."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-767-rubric-1",
    "token_usage": {
      "prompt_tokens": 10738,
      "completion_tokens": 278,
      "total_tokens": 11016
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 5.1 Datasets; Section 4 Frame Flexible Network; Section 5 Experiments",
            "reasoning": "The new data annotations pertain to the construction and use of video datasets such as Something-Something V1 & V2, Kinetics400, and HMDB51, which are pre-existing benchmarks. The paper does not introduce a new dataset; rather, it introduces a new model and framework (Frame Flexible Network) trained and evaluated on existing datasets. Therefore, annotation and labeling of video data was done in prior work, not by the authors. The 'annotations' referenced in the paper are model predictions or settings for training and evaluation rather than human annotations or newly labeled data."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any new annotation instructions, guidelines, or protocols introduced for data labeling, as no new dataset is introduced."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No mention of scoring rubrics or criteria for annotation quality or labels evaluation since no new annotated dataset is presented."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper provides no examples of annotation guidelines or annotated data because no new dataset is introduced or labeled by the authors."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-767-rubric-2",
    "token_usage": {
      "prompt_tokens": 11928,
      "completion_tokens": 290,
      "total_tokens": 12218
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper about quality assurance performed by multiple human experts for dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance carried out by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert human annotators performing quality assurance on the datasets introduced or used."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that an AI model was used as a judge for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses algorithmic methods for managing frame flexibility and training, it does not describe an automated verification process for quality assurance of dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets and does not describe any quality assurance process for datasets. The datasets used are existing ones, and no new dataset annotation or quality assurance is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-767-rubric-3",
    "token_usage": {
      "prompt_tokens": 11538,
      "completion_tokens": 275,
      "total_tokens": 11813
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the introduction of any novel dataset created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report generating any new dataset through AI or machine learning model-generated data exclusively."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by human translation from other languages in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any dataset was created through machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing benchmark datasets such as Something-Something V1 & V2, Kinetics400, and HMDB51, but does not indicate that it collected or aggregated new datasets from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe creating new datasets derived or adapted from existing datasets. Instead, it uses standard existing datasets for validation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. It only utilizes existing popular benchmarks for empirical validation without stating any new data creation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-767-rubric-4",
    "token_usage": {
      "prompt_tokens": 12056,
      "completion_tokens": 304,
      "total_tokens": 12360
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new dataset is introduced by the authors in the paper. The authors use several existing datasets (Something-Something V1 & V2, Kinetics400, HMDB51) for empirical validation but do not propose or collect a new dataset. Therefore, no practical usage of a new dataset in the machine learning pipeline is described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-767-rubric-5",
    "token_usage": {
      "prompt_tokens": 12779,
      "completion_tokens": 567,
      "total_tokens": 13346
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce or mention any dataset containing more than two human languages. All datasets referenced are video datasets focused on action recognition, without specifying linguistic content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any proposed dataset contains entries involving exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset featuring English textual content. The datasets mentioned (Something-Something V1 & V2, Kinetics400, HMDB51) are pre-existing and primarily video datasets for action recognition, without focus on English language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not propose any dataset that is monolingual with a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets mentioned and proposed are video datasets, containing video samples for action recognition, not programming or code-related data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes mathematical formulas and notation to describe their model and losses, these are not entries in the dataset itself, but rather part of the methodological description."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or introduction of datasets containing biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce datasets with fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset languages or types are explicitly known (video data), even if not linguistic, so this category does not apply."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced relate to video recognition datasets containing video entries; no linguistic content or language entries are present in the datasets themselves. The datasets consist of videos depicting human actions and do not contain text or language data as dataset entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-767-rubric-6",
    "token_usage": {
      "prompt_tokens": 9997,
      "completion_tokens": 193,
      "total_tokens": 10190
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and sections throughout the paper",
          "reasoning": "The paper mentions that the code for the proposed Frame Flexible Network (FFN) is available at a GitHub repository (https://github.com/BeSpontaneous/FFN), but this code is related to the model implementation and training, not to data collection or dataset construction. The paper does not introduce any new datasets; rather, it uses several existing datasets (Something-Something V1 & V2, Kinetics400, HMDB51) for evaluation. Therefore, no code related to constructing new datasets is provided or applicable."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "N/A",
          "reasoning": "The paper does not introduce any new datasets, thus there is no dataset creation process documented. The evaluation datasets used are all publicly existing and no new dataset creation or data collection procedure is described in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-78-rubric-0",
    "token_usage": {
      "prompt_tokens": 11058,
      "completion_tokens": 133,
      "total_tokens": 11191
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Setup and Implementation Details",
          "Reasoning": "The new dataset introduced implicitly in the paper consists of 3D point clouds of indoor scenes and optionally corresponding RGB images. These are human-generated data captured by sensors. The authors train and evaluate their method on existing datasets (ScanNet, SUN RGB-D) and do not mention creation of any new datasets. Therefore, the paper does not introduce a new dataset; it only uses existing datasets. Hence, there are no new datasets introduced by the authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-78-rubric-1",
    "token_usage": {
      "prompt_tokens": 11858,
      "completion_tokens": 238,
      "total_tokens": 12096
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1, Section 3.4",
            "reasoning": "The paper uses existing datasets (ScanNet V2 and SUN RGB-D) for evaluation, but does not introduce or release any new dataset. Instead, it generates foreground/background point labels automatically by spatial criteria using existing bounding box annotations during training (Section 4.1). This labeling is an automatic process, not manual annotation. There is no indication of new dataset collection or annotation by humans or experts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No section describing new data annotation instructions",
            "reasoning": "The paper does not introduce any new dataset or annotation task; therefore, no instructions for data annotation are provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No section describing rubrics for annotations",
            "reasoning": "No new dataset annotation or scoring process is described; hence, no rubric definitions exist."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No appendix or section providing annotation examples",
            "reasoning": "Since no new dataset or annotation task is introduced, no annotation examples are provided."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-78-rubric-2",
    "token_usage": {
      "prompt_tokens": 13048,
      "completion_tokens": 268,
      "total_tokens": 13316
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert on the datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts conducting quality assurance on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about single non-expert human annotators performing quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that multiple non-expert human annotators performed quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of AI models being used specifically for quality assurance of the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated or algorithmic quality assurance process applied to the datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses two existing datasets, namely SUN RGB-D and ScanNet V2, for experimental evaluation. No new dataset is introduced by the authors, and thus no quality assurance process for new dataset annotation or validation is described or applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-78-rubric-3",
    "token_usage": {
      "prompt_tokens": 12658,
      "completion_tokens": 261,
      "total_tokens": 12919
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1: Setup and Implementation Details",
          "reasoning": "The datasets used for evaluation (SUN RGB-D and ScanNet V2) are pre-existing publicly available datasets. The paper uses these datasets for training and evaluation without indicating that the authors collected or created new data. Thus, the data originates from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-78-rubric-4",
    "token_usage": {
      "prompt_tokens": 13176,
      "completion_tokens": 304,
      "total_tokens": 13480
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. Instead, it utilizes existing datasets (SUN RGB-D and ScanNet V2) to evaluate and demonstrate the effectiveness of their proposed AShapeFormer module. No new dataset is created, described, or used for training, fine-tuning, evaluation, analysis, or as a knowledge base within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-78-rubric-5",
    "token_usage": {
      "prompt_tokens": 13899,
      "completion_tokens": 543,
      "total_tokens": 14442
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, including Abstract, Introduction, Sections 3 and 4",
          "reasoning": "The paper's content, descriptions, and all textual information are presented exclusively in English. The experimental datasets mentioned (SUN RGB-D and ScanNet V2) are standard 3D object detection datasets and their documentation is in English. There is no mention or inclusion of any other human language dataset entries. Thus, the new dataset scope relevant to the paper is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses neural network modules, multi-head attention and loss functions, it does not introduce any new dataset containing programming code or structured code entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical formulas to describe model components and loss functions, these do not relate to the dataset entries themselves. The dataset content is 3D point cloud data without symbolic or formal language entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data or sequences are introduced or involved in any new dataset in the paper."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any artificial or fictional languages within dataset entries."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of dataset entries is clearly documented as they relate to 3D point clouds and associated annotations in English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries involve 3D scene point clouds and annotations with English labels; therefore, language is present in terms of annotation and documentation."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-78-rubric-6",
    "token_usage": {
      "prompt_tokens": 11117,
      "completion_tokens": 176,
      "total_tokens": 11293
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 4.1",
          "reasoning": "The paper mentions that code for the method will be available on GitHub (https://github.com/ZechuanLi/AShapeFormer), but there is no indication that any code related to dataset construction, preprocessing, or generation is provided. The code availability seems to pertain only to the proposed method implementation and not dataset construction."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Throughout the paper, e.g., Section 4.1 and dataset descriptions",
          "reasoning": "The paper uses existing public datasets (SUN RGB-D and ScanNetV2) for evaluation without introducing any new datasets. There is no mention or documentation about the creation or construction process of new datasets by the authors. Therefore, documentation related to dataset creation is absent."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-780-rubric-0",
    "token_usage": {
      "prompt_tokens": 10753,
      "completion_tokens": 105,
      "total_tokens": 10858
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.2 Datasets: Synthetic Dataset",
          "Reasoning": "The paper introduces a new synthetic DefocusNet Dataset generated by the authors using Blender rendering with various focused distances. The dataset consists of 1000 focal stacks rendered synthetically with precise camera parameters. Thus, images are generated by a model/simulation process but the dataset is introduced by the authors as a new dataset."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-780-rubric-1",
    "token_usage": {
      "prompt_tokens": 11553,
      "completion_tokens": 346,
      "total_tokens": 11899
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.2 Datasets, Synthetic Dataset",
            "reasoning": "The synthetic dataset referred to as DefocusNet Dataset is rendered using Blender, a virtual optical camera simulation, and automatically generated. No human annotators are involved."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "There is no mention of annotation instructions provided for automated dataset generation processes."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "There is no indication of scoring rubrics or guidelines for annotations because the dataset is synthetically generated automatically."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No annotation examples exist for automated synthetic data rendering, as it is a data generation process not manual annotation."
          }
        },
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.2 Datasets, Real Image with Synthetic Defocus Blur",
            "reasoning": "The real dataset with synthetic defocus blur is created by rendering focal stacks from RGB-D datasets (NYUv2) using thin-lens equation and PSF convolution layers, a physical simulation process done automatically without human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No instructions given since data generation is by a computational physical model, not manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No scoring rubrics or annotation guidelines are applicable for automatic synthetic data rendering."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "Examples in annotation guidelines are irrelevant to this automatic data rendering procedure."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-780-rubric-2",
    "token_usage": {
      "prompt_tokens": 12743,
      "completion_tokens": 320,
      "total_tokens": 13063
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for any of the datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts performed quality assurance on the introduced datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert performed quality assurance on any dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe multiple non-experts performing quality assurance on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe that an AI model was used for quality assurance of dataset annotations or quality."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4.2",
          "reasoning": "The synthetic focal stacks were generated automatically by rendering using an optical model (thin-lens equation and PSF convolution). This automatic rendering process serves as both the data generation and, implicitly, the quality assurance step since the synthetic data is verified through this physical optical process. Furthermore, the training and evaluation use these algorithmically generated datasets, implying an automated verification of their consistency and suitability."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes an automatic process for synthetic dataset generation and uses existing datasets, so QA is implicitly present via this automated rendering technique."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-780-rubric-3",
    "token_usage": {
      "prompt_tokens": 12353,
      "completion_tokens": 401,
      "total_tokens": 12754
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any original dataset was created entirely by humans from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data is reported to be generated solely by AI or machine learning models without references; rather, synthetic focal stacks are rendered based on existing RGB-D datasets using known optical models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any dataset was produced by human translation of content from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation applied to dataset content in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.2 (Datasets)",
          "reasoning": "The authors use several existing datasets (e.g., NYUv2 RGB-D dataset and Mobile Depth dataset) and leverage them as bases for further processing, thus aggregating data from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 4.2",
          "reasoning": "The paper discusses generating new focal stack datasets by rendering synthetic defocus blur images from existing RGB-D datasets using optical models (thin-lens equation and PSF convolution). This constitutes derived data based on existing sources with modifications and transformations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data generation and origins are clearly specified in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-780-rubric-4",
    "token_usage": {
      "prompt_tokens": 12871,
      "completion_tokens": 453,
      "total_tokens": 13324
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.3 and 4.2 Synthetic Dataset",
          "reasoning": "The newly generated DefocusNet Dataset with more distributed focus distances is used to train models from scratch, as described in Section 4.2 where the dataset consists of 1000 focal stacks split into training and testing sets. The authors explicitly state training their method on this new dataset (Section 4.3), indicating its usage for training models from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any fine-tuning of pre-trained models using any newly introduced dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning or RL-based post-training methods are discussed in the paper."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset introduced (re-generated DefocusNet Dataset) is used for training and testing in a standard split, but it is not described as used exclusively for evaluation or benchmarking purposes."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses the new dataset for training and testing models, not primarily for analyzing trends or characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset does not serve as a knowledge base or retrieval-augmented resource to augment models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper uses the newly generated dataset explicitly for training and testing, so the N/A label does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-780-rubric-5",
    "token_usage": {
      "prompt_tokens": 13594,
      "completion_tokens": 633,
      "total_tokens": 14227
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing entries in more than two human languages. The datasets discussed are image datasets related to depth estimation and defocus images, which are typically language-neutral."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the proposed datasets contain exactly two human languages. The paper focuses on image data related to focal stacks and depth maps without any linguistic content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset introduced (the regenerated DefocusNet dataset with new camera parameters) and other datasets mentioned (real focal stack datasets) are image datasets and do not involve textual entries in English or any other language."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The proposed datasets do not contain any textual data in any non-English language. The paper does not mention any non-English language contents in the datasets."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper involves implementation details in PyTorch (Section 4.1) and mentions CUDA implementations (Section 3.1.2), the datasets themselves do not contain code or programming language entries as data. The paper only discusses datasets of images for depth estimation."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets themselves do not contain mathematical or formal logical notation as entries; mathematical formulas are used in the paper to describe models and optical relationships, but not as part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets focus on human-captured image focal stacks for depth estimation; no biological sequences or non-human communication systems are included in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any fictional or constructed languages being part of the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of datasets is either non-applicable or clearly none; no presence of unspecified or undocumented language is indicated."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new dataset introduced is an image dataset composed of focal stacks and depth maps, which do not contain any human language entries or linguistic content. The datasets are image-based with no textual or language data."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-780-rubric-6",
    "token_usage": {
      "prompt_tokens": 10812,
      "completion_tokens": 247,
      "total_tokens": 11059
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 4.2 Datasets",
          "reasoning": "The paper states that source code is publicly available at https://github.com/Ehzoahis/DEReD. The authors mention regenerating the DefocusNet dataset with adjusted focus distances using code provided by the original dataset author, indicating usage and availability of code related to data generation. This repository likely contains the code relevant to dataset creation, including regeneration and synthetic focal stack rendering."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.2 Datasets",
          "reasoning": "The paper documents the dataset creation and regeneration process in Section 4.2 Datasets, describing how they regenerated the DefocusNet dataset with different focus distances using author-provided code, detailing parameters such as f-number, focal length, and focus distances. They also describe rendering synthetic focal stacks from the NYUv2 RGB-D dataset using the thin-lens equation and PSF convolution as explained in Section 3.1, with parameters given, and provide details about the Mobile Depth dataset as a real dataset used qualitatively. Thus, documentation on dataset generation and creation is provided and reasonably detailed."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-786-rubric-0",
    "token_usage": {
      "prompt_tokens": 10613,
      "completion_tokens": 76,
      "total_tokens": 10689
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset",
          "Reasoning": "The new dataset introduced involves scenes captured as photos with camera parameters estimated via structure from motion. These images are taken by human-operated cameras, making them human generated image data for training and evaluation."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-786-rubric-1",
    "token_usage": {
      "prompt_tokens": 11413,
      "completion_tokens": 242,
      "total_tokens": 11655
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 3.3, Section 4.2",
            "reasoning": "The camouflage textures are generated by a model trained via adversarial learning using a GAN. The texturing function G is learned through self-supervised multi-view camouflage that camouflages randomly augmented shapes at random positions during training. The paper describes training one network per scene, indicating an AI model performs the annotation equivalent (texture labeling)."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any instructions provided for annotation or labeling by humans or models. The training procedure is fully automated and self-supervised without human instructions for annotation tasks."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No scoring rubrics or formal evaluation criteria for annotation are described as part of the annotation process. While evaluation metrics are used, these are for model evaluation post-training, not for annotation guideline rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide annotation examples as part of annotation guidelines. Examples shown are qualitative results, not annotation guidance."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-786-rubric-2",
    "token_usage": {
      "prompt_tokens": 12603,
      "completion_tokens": 295,
      "total_tokens": 12898
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the introduced datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by a single human non-expert for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report multiple non-expert human annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the method includes adversarial learning with a discriminator, this is part of the texture generation process and not described as quality assurance of datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic rule-based quality assurance applied to the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper presents new datasets of animal and cube meshes placed in scenes, but does not describe any quality assurance process applied to these datasets or their annotations. The evaluations rely on automated metrics and human perceptual studies, but these are measurements of model performance rather than dataset quality assurance."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-786-rubric-3",
    "token_usage": {
      "prompt_tokens": 12213,
      "completion_tokens": 470,
      "total_tokens": 12683
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creation of any new dataset from scratch by human contributors. Instead, it relies on existing scenes captured as background images."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces a model that generates camouflage textures for 3D objects, but it does not introduce a dataset composed entirely of model-generated data. The generated textures are outputs rather than datasets intended for use as a new dataset."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of any data being produced by translating from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of machine translation being used to produce data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The dataset used is based on the scene dataset from Owens et al. [33], consisting of existing background images captured without objects present. These images, along with the pre-existing animal meshes [60] and furniture shapes [11], are collated from prior works without significant modification to form the evaluation datasets."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset and Section 3.1 Texture Representation",
          "reasoning": "The paper generates new camouflaged textures for 3D objects derived by adapting existing scenes and object meshes. The camouflaged textured objects are produced by deriving textures conditioned on input images and geometry, representing a transformation of existing data (scene images and object shapes) to create new textured models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origins of the data are clearly documented and explained in the paper, eliminating the need to classify as N/A."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-786-rubric-4",
    "token_usage": {
      "prompt_tokens": 12731,
      "completion_tokens": 316,
      "total_tokens": 13047
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.3 Learning to Camouflage; Section 4.2 Implementation Details",
          "reasoning": "The paper describes training a texturing function (i.e., the camouflage model) from scratch using a self-supervised multi-view camouflage approach. The model parameters are optimized from randomly initialized weights to generate camouflage textures for 3D objects based on input scenes and viewpoints."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset; Section 4.3 Experimental Settings; Section 4.4 Perceptual Study",
          "reasoning": "The paper uses the introduced animal shape dataset and scene images mainly to evaluate the camouflage model's performance, including human perceptual studies and automated metrics, benchmarking against prior methods."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.6 Automated evaluation metrics; Section 5 Discussion",
          "reasoning": "The authors use the datasets to analyze model components and performance impact, perform ablation studies, and discuss strengths and limitations of the method."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-786-rubric-5",
    "token_usage": {
      "prompt_tokens": 13454,
      "completion_tokens": 609,
      "total_tokens": 14063
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset entries containing multiple human languages. The datasets involve images and 3D shapes for camouflaging objects, with no indication of multilingual textual data."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any dataset entries containing exactly two human languages. The dataset primarily consists of images and 3D shapes without any linguistic components."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper is written in English, it does not introduce a new dataset containing English text entries. The datasets focus on visual and 3D data, not textual datasets."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets with non-English language content are introduced. The focus is on visual and 3D object data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains algorithmic descriptions and network architectures but does not present a dataset comprising programming or structured code data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The paper includes datasets of 3D objects and associated mathematical representations such as meshes and texture fields, which are formally represented using mathematical notation (e.g., functions mapping points in 3D space to colors, positional encodings, losses). The dataset entries effectively contain mathematical/formal representations of 3D geometry."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper references biological camouflage for motivation, the introduced datasets are synthetic 3D object models and scene images, not biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce dataset entries comprising fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are well documented and their data types are described; there is no indication that their linguistic content is unspecified or unknown."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced consist mainly of 3D object meshes, images, and camera parameters without any linguistic or language content."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-786-rubric-6",
    "token_usage": {
      "prompt_tokens": 10672,
      "completion_tokens": 160,
      "total_tokens": 10832
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not mention or provide any link or indication that the code for dataset construction or the dataset itself is publicly available. There is only a reference to a project webpage that provides videos but no mention of code release."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper describes the dataset used for evaluation, including the sourcing of scenes (from Owens et al. [33]), the number of images per scene, camera parameter estimation via structure from motion, and the animal mesh dataset from [60]. It specifies the procedures for selecting images, object placement, and test/train splits, providing sufficient information about the dataset creation and usage for evaluation purposes."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-810-rubric-0",
    "token_usage": {
      "prompt_tokens": 13347,
      "completion_tokens": 441,
      "total_tokens": 13788
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4, Table 1",
          "Reasoning": "The GeneCIS benchmark is introduced as a new benchmark in Section 4, with four evaluation tasks constructed by re-purposing existing public datasets like VAW and COCO. However, these datasets themselves are pre-existing, so the benchmark is an assembly rather than a newly collected dataset. The benchmark includes galleries of images and reference images, making the data modality 'image' which originates from human photography and annotations."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4, Table 1 and Section 5.2",
          "Reasoning": "The GeneCIS benchmark includes text conditions (prompts) for conditioning similarity, which represent text modality data. These text conditions appear hand-curated or derived from annotations like object and attribute labels in VAW and COCO, thus human generated. The textual conditions used in training data mining come from image-caption datasets which are human-generated captions paired with images."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": true,
          "Reference": "Section 5.2",
          "Reasoning": "The training dataset mined by the authors from large-scale image-caption datasets (e.g., Conceptual Captions 3M) is used to train the conditional similarity model. This dataset consists of images sourced from the internet, paired with human-generated captions, but the exact provenance of all images is not fully specified in the paper, implying unknown origin for some images. The authors note open-source caption datasets with billions of images exist, but exact origins and full data provenance are not fully known."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": true,
          "Reference": "Section 5.2",
          "Reasoning": "The captions associated with the mined image-caption datasets, such as Conceptual Captions, are human generated, but the paper does not specify complete details on caption origins and consistency, so the origin is not fully specified, implying unknown origin."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-810-rubric-1",
    "token_usage": {
      "prompt_tokens": 14147,
      "completion_tokens": 424,
      "total_tokens": 14571
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4, Section 3.1, and Section 5.2",
            "reasoning": "The GeneCIS benchmark tasks are constructed by re-purposing existing public datasets: VAW and COCO Panoptic segmentation. The paper states that the construction process involves automatic mining of relationships and creation of retrieval templates (Section 4), and that currently manual human verification is ongoing but the presented version is 'GeneCIS v0' with noise from underlying datasets (Section 4). Hence, annotations are primarily derived via automatic processes rather than human annotation. Furthermore, for training data, triplets are automatically mined from existing image-caption datasets using a text-to-scene-graph parser and filtering methods (Section 5.2). This supports the categorization as an Automatic Process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4 and Section 5.2",
            "reasoning": "The paper does not mention providing detailed annotation instructions to human annotators for the GeneCIS tasks or the mined training data. Instead, annotation is via automated parsing and repurposing existing annotations or datasets. It is stated human verification is ongoing but no explicit instructions or guidelines are provided or described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4",
            "reasoning": "There is no explicit mention of scoring rubrics or detailed evaluation criteria provided as annotation guidelines for creating or verifying the GeneCIS benchmark data. The evaluation metrics are Recall@K, which are standard retrieval metrics, but these are not annotation rubrics for annotators."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figure 2 and Appendix G.1 (mentioned in Section 4)",
            "reasoning": "The paper presents example retrieval tasks in Figure 2 illustrating the annotation data, such as images with a reference, conditions, and gallery images with designated positive examples, serving as explicit annotation examples. Appendix G.1 (referred to in Section 4) presumably contains further example templates for the benchmark datasets."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-810-rubric-2",
    "token_usage": {
      "prompt_tokens": 15337,
      "completion_tokens": 344,
      "total_tokens": 15681
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance performed by a single human expert on the new GeneCIS benchmark."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts participated in quality assurance of the GeneCIS dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single non-expert human annotator for the GeneCIS benchmark."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple non-expert human annotators performing quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any AI model was used to perform quality assurance on the new GeneCIS datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper details an automatic process to mine training triplets from image-caption data, there is no explicit mention of an automated quality assurance or verification step applied to validate the GeneCIS benchmark annotations or data content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper reports that the GeneCIS benchmark is constructed by repurposing existing public datasets (VAW and COCO) and that noise is present due to inconsistencies in these underlying datasets. The authors note that manual annotations and human verification are planned but currently absent. Therefore, there is no documented quality assurance process currently applied to the new GeneCIS benchmark."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-810-rubric-3",
    "token_usage": {
      "prompt_tokens": 14947,
      "completion_tokens": 418,
      "total_tokens": 15365
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the authors created any dataset entirely from scratch by human contributors. Instead, the benchmark is constructed by repurposing existing public datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated entirely by AI or machine learning models without reference to existing data. The method involves mining and parsing existing image-caption datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any dataset being produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation used to generate any dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4, The GeneCIS Benchmark (paragraph 'Benchmark Details'), Table 1",
          "reasoning": "The GeneCIS benchmark is constructed by re-purposing and aggregating publicly available datasets (V AW, Visual Genome, COCO) without significant modification, assembling retrieval templates and galleries from these existing sources to create the evaluation tasks."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4, The GeneCIS Benchmark (paragraph 'Benchmark Details' and 'Noise and human verification'), Section 5.2 Scalable training for conditional similarity, Figure 4",
          "reasoning": "The benchmark data tasks are derived from existing datasets by applying transformations such as template construction, filtering, and pairing reference and target images with conditions. Furthermore, the training data triplets are derived by automatically mining and parsing large-scale image-caption datasets (e.g., Conceptual Captions 3M), extracting relational triplets, filtering for visual concreteness, and constructing training triplets. Thus, the data is transformed and adapted from existing data sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation of the datasets are clearly specified and documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-810-rubric-4",
    "token_usage": {
      "prompt_tokens": 15465,
      "completion_tokens": 371,
      "total_tokens": 15836
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4, The GeneCIS Benchmark",
          "reasoning": "The paper introduces the GeneCIS benchmark, a set of four evaluation tasks designed exclusively for zero-shot evaluation of models' ability to adapt to various conditional similarity notions. It explicitly states that GeneCIS is designed solely for evaluation and benchmarking purposes, with retrieval tasks constructed to assess model performance on conditional similarity without fine-tuning."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 6.3, Analysis on GeneCIS",
          "reasoning": "The authors use the new GeneCIS benchmark dataset not only for evaluation but also for analysis of model performance trends, correlations with ImageNet accuracy, and ablation studies to understand design choices' effects on performance, indicating analytical use of the dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-810-rubric-5",
    "token_usage": {
      "prompt_tokens": 16188,
      "completion_tokens": 640,
      "total_tokens": 16828
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes the GeneCIS benchmark and associated datasets mined primarily from English image-caption datasets such as Conceptual Captions 3 Million (CC3M) and SBU Captions. There is no indication that the datasets contain entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided that the dataset contains exactly two human languages. The descriptions and examples are in English only."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Sections 4, 5.2, and 6.2",
          "reasoning": "The datasets introduced (GeneCIS tasks) are constructed by re-purposing Visual Genome, VAW, and COCO datasets, and training data is mined from large-scale image-caption datasets such as Conceptual Captions 3 Million (CC3M) and SBU Captions, which are known to be English language datasets. The paper consistently describes text conditions and captions in English. No other languages are mentioned."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are in English and no evidence of a non-English monolingual dataset is presented."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries consist of images, text captions, and text conditions in natural English language. There is no programming or structured code content in the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes mathematical expressions and formulas to describe model functions and losses, these are not part of the dataset itself. The dataset entries do not contain mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries involve natural images and natural language descriptions. There is no inclusion of biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include fictional or artificially created languages. All textual data is natural English."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of the dataset entries are clearly specified as English through textual descriptions and captions. The dataset language is documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains natural language text (English) as conditions and captions, so language data is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-810-rubric-6",
    "token_usage": {
      "prompt_tokens": 13406,
      "completion_tokens": 210,
      "total_tokens": 13616
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or link",
          "reasoning": "The paper mentions a public project page (sgvaze.github.io/genecis) but does not explicitly state that the code for dataset construction is publicly available. The paper does not provide a direct link or mention releasing the data construction code. Thus, there is no clear evidence that all code related to dataset generation and preprocessing is accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4, 'The GeneCIS Benchmark' and Appendix A.1 (mentioned)",
          "reasoning": "The paper provides a detailed description of the dataset construction process in Section 4 and mentions full details of template construction in Appendix A.1. It explains that the tasks are constructed by re-purposing existing public datasets (Visual Genome, VAW, COCO Panoptic) and describes the process of extracting object and attribute annotations and forming retrieval tasks. Thus, the dataset creation process is documented transparently in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-811-rubric-0",
    "token_usage": {
      "prompt_tokens": 11013,
      "completion_tokens": 100,
      "total_tokens": 11113
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4, Experiments",
          "Reasoning": "The paper conducts experiments on benchmark datasets including MNIST, SVHN, CIFAR-10, and CIFAR-100. These datasets are standard image classification datasets broadly used in computer vision research. The images in these datasets are captured or created by humans, not generated by models, thus are human-generated images."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-811-rubric-1",
    "token_usage": {
      "prompt_tokens": 11813,
      "completion_tokens": 303,
      "total_tokens": 12116
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4 Experiments, overall paper context",
            "reasoning": "The annotations (labels) for the new datasets used in the experiments such as CIFAR-10, CIFAR-100, MNIST, SVHN are standard benchmark datasets with existing labels, but the paper does not introduce any new dataset or mention creating new annotations; the datasets are used as benchmarks. The paper does not describe any annotation process performed by humans or models specifically for new datasets. The training process uses existing annotated data. Given the absence of any explicit description of new dataset annotations, nor indication that annotation was conducted by models or automatic processes, the safest categorization is that the standard benchmark datasets were used with existing human expert annotation documented elsewhere."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No instructions described for annotation in the paper",
            "reasoning": "The paper does not describe any instructions, guidelines or processes for data annotation related to any new dataset introduced, because no new dataset is introduced."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No rubrics described for data annotation in the paper",
            "reasoning": "No annotation rubric or scoring system is mentioned for any datasets, since no annotation of new data is described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples described in the paper",
            "reasoning": "Since new annotation guidelines or processes are not described, no examples illustrating annotation are provided."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-811-rubric-2",
    "token_usage": {
      "prompt_tokens": 13003,
      "completion_tokens": 291,
      "total_tokens": 13294
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process conducted by a single human expert for datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any involvement of multiple human experts performing quality assurance on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that a single human non-expert performed quality assurance on the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance by multiple human non-experts on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance using an AI model as a judge on dataset annotations or content is described in the paper."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper involves algorithmic methods and adversarial training but does not describe any automatic verification or rule-based quality assurance process applied to the datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets, but rather uses existing standard benchmarks such as MNIST, SVHN, CIFAR-10, and CIFAR-100. Consequently, no quality assurance process for new dataset annotations or content is described or applied."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-811-rubric-3",
    "token_usage": {
      "prompt_tokens": 12613,
      "completion_tokens": 314,
      "total_tokens": 12927
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments) and throughout the paper",
          "reasoning": "The paper does not introduce any new datasets created by the authors. Instead, it uses existing benchmark datasets such as MNIST, SVHN, CIFAR-10, and CIFAR-100 for evaluation. These datasets are established and publicly available. The novelty is in the proposed training framework (Generalist), not in new data creation. Thus, the data used is derived from existing sources that are adapted for adversarial training by generating adversarial examples (perturbed versions) using known techniques, constituting derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-811-rubric-4",
    "token_usage": {
      "prompt_tokens": 13131,
      "completion_tokens": 223,
      "total_tokens": 13354
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; instead, it uses several well-known benchmark datasets (MNIST, SVHN, CIFAR-10, CIFAR-100) for experiments, which are pre-existing. There is no description or demonstration of newly proposed datasets in the paper, thus no utility in the machine learning pipeline can be attributed to a new dataset within this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-811-rubric-5",
    "token_usage": {
      "prompt_tokens": 13854,
      "completion_tokens": 566,
      "total_tokens": 14420
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new dataset that includes entries in more than two human languages. The focus is on image classification datasets and adversarial training."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of a dataset containing exactly two human languages. The datasets used are standard image classification benchmarks with no language attribute."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets consisting of English linguistic content. The datasets used (MNIST, CIFAR-10, CIFAR-100, SVHN) are image datasets without linguistic data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets with only non-English languages are introduced. The datasets are visual image classification datasets unrelated to human languages."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset consisting of programming or structured code content. The study focuses on image classification datasets with adversarial training."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper presents mathematical formulas and theoretical analysis, these do not constitute new datasets containing mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets involving biological sequences or non-human communication systems are introduced in this paper."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset involving fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of any dataset introduced by the paper is clearly described or is non-linguistic in nature."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset containing language entries. The new datasets described (e.g., training data in MNIST, CIFAR-10, CIFAR-100, SVHN) consist of images with labels, not language data."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-811-rubric-6",
    "token_usage": {
      "prompt_tokens": 11072,
      "completion_tokens": 163,
      "total_tokens": 11235
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, last paragraph",
          "reasoning": "The abstract explicitly states that code is available at https://github.com/PKU-ML/Generalist, implying public availability of code relevant to the work. Given that the paper introduces a new learning framework (Generalist) but does not introduce any new datasets, this code likely includes the implementations related to data processing or training."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets. It uses standard benchmark datasets (MNIST, SVHN, CIFAR-10, CIFAR-100) with established data splits and characteristics. Therefore, there is no dataset creation process documented because no new dataset is constructed or introduced by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-834-rubric-0",
    "token_usage": {
      "prompt_tokens": 13621,
      "completion_tokens": 147,
      "total_tokens": 13768
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 3 (Dataset curation) and Section 3.1 through 3.3",
          "Reasoning": "The new dataset introduced is GeoNet, which contains images collected primarily from Flickr and WebVision dataset. Images inherently are human-generated data as they are photographs taken by humans. The text explicitly mentions collection of geotagged images from Flickr and WebVision, both datasets consisting of real images captured by humans. Thus, the modality is 'image' and origin is 'Human Generated'. The data is not model generated nor unknown origin since collection and curation details are provided."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-834-rubric-1",
    "token_usage": {
      "prompt_tokens": 14421,
      "completion_tokens": 287,
      "total_tokens": 14708
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1 and 3.2",
            "reasoning": "The paper states that the Asia domain test images for GeoPlaces benchmark are manually labeled (Section 3.1), and for GeoImNet, the test images are manually quality-checked and cleaned by humans to remove noisy labels (Section 3.2). This implies multiple human experts performed annotations for the test data to ensure label quality."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not explicitly mention providing detailed annotation instructions to annotators for either the GeoPlaces or GeoImNet datasets, or for the GeoUniDA benchmark. Therefore, we cannot confirm the presence of annotation instructions based on the paper content."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "There is no explicit mention of scoring rubrics or criteria used in the annotation process for the newly introduced GeoNet datasets (GeoPlaces, GeoImNet, GeoUniDA). Thus, we conclude rubrics are not described in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not provide or mention annotation examples or guidelines with example annotations for the newly introduced datasets. Hence, no annotation examples are provided as per paper content."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-834-rubric-2",
    "token_usage": {
      "prompt_tokens": 15611,
      "completion_tokens": 407,
      "total_tokens": 16018
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any description or indication that a single human expert conducted quality assurance on the dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance or labeling in the dataset creation process."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions manual quality-checking of test images and removal of noisy labels but does not specify multiple non-expert annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of AI models used to perform quality assurance or judge the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 Dataset curation and Dataset filtering",
          "reasoning": "The dataset filtering process includes automated criteria such as removing images with multiple matching tags, de-duplication to remove similar images between train and test sets, and selection based on GPS coordinates and metadata tags. These steps indicate use of automated, rule-based processes for quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Although the paper does not detail an extensive manual QA process, it describes automated filtering and some manual checking of test labels, indicating that some form of quality assurance was performed."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-834-rubric-3",
    "token_usage": {
      "prompt_tokens": 15221,
      "completion_tokens": 520,
      "total_tokens": 15741
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any data was created entirely from scratch by human contributors. Data collection is based on existing web-sourced images."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data is described as being generated by AI or machine learning models independently; the dataset is based on images collected from public web sources."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper mentions expansions of category names into translations in 6 Asian languages for query usage, it does not mention the data itself is human translated."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that any translations used are machine translations, nor that data was machine translated."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 (GeoPlaces) and Section 3.2 (GeoImNet)",
          "reasoning": "The datasets GeoPlaces, GeoImNet, and GeoUniDA are created by collecting and aggregating images from existing sources such as Flickr, WebVision, and Places-205. The authors scrape and filter geotagged images from these public datasets and web sources without stating creation of new images, representing an aggregation of existing data with filtering steps."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 (Additional Data for Asia domain), Section 3.2 (GeoImNet Dataset filtering and Concept Selection), Section 3.3 (GeoUniDA)",
          "reasoning": "The data is derived from existing datasets (e.g., Places-205, WebVision, Imagenet-5k). The authors expand category names using translations for querying Asian images, filter out multi-labeled and noisy images, perform de-duplication, and select classes to create benchmarks suitable for geographic adaptation, constituting modifications, filtering, and adaptions on pre-existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and collection methodology is explicitly described in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-834-rubric-4",
    "token_usage": {
      "prompt_tokens": 15739,
      "completion_tokens": 273,
      "total_tokens": 16012
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.3 Large-scale pre-training and architectures",
          "reasoning": "The GeoNet dataset is used to fine-tune pre-trained models (like ResNet, Vision Transformers) on geographically biased datasets to evaluate geographical robustness and adaptation performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 Extensive benchmarking",
          "reasoning": "GeoNet serves as a benchmark to evaluate and benchmark several state-of-the-art unsupervised domain adaptation algorithms, demonstrating their limitations in addressing geographic shifts."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.4 Analysis of Distribution Shifts",
          "reasoning": "The dataset is analyzed to study the nature of domain shifts caused by geographic disparity, including context shift, design shift, and prior shift, to understand challenges in geographic adaptation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-834-rubric-5",
    "token_usage": {
      "prompt_tokens": 16462,
      "completion_tokens": 580,
      "total_tokens": 17042
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 3.1 (GeoPlaces), Section 3.2 (GeoImNet), and Additional Data subsection in Section 3",
          "reasoning": "The dataset includes images collected from photo-sharing websites like Flickr with geotags from multiple geographies such as USA and Asia. To collect more images from Asia, category names were translated from English to six Asian languages: Hindi, Korean, Japanese, Chinese, Russian, and Hebrew, and used in search queries. Thus, the dataset contains entries with tags and metadata in multiple languages, making it multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes tags and metadata in more than two languages (at least English plus six Asian languages), so it is not limited to exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though some data and categories are in English, the dataset extends beyond English by incorporating multiple non-English languages for queries and tags."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not restricted to a single non-English language; multiple languages are involved."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are images and associated metadata (captions, hashtags), no mention of code or programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of mathematical or logical expressions in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Dataset consists of images and metadata related to geographic adaptation in vision tasks; no biological sequences or non-human communication data reported."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of fictional or constructed languages in dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Languages of data entries are described and specified as English and six Asian languages used for tag matching."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Entries in the dataset contain language metadata such as captions and hashtags, so language is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-834-rubric-6",
    "token_usage": {
      "prompt_tokens": 13680,
      "completion_tokens": 232,
      "total_tokens": 13912
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Introduction, and Dataset sections",
          "reasoning": "The paper provides a URL (https://tarun005.github.io/GeoNet) for the dataset, indicating public availability. It also mentions that prior domain adaptation methods were trained using their publicly available code and hyperparameters, implying the authors have made their dataset's code or at least associated resources public. Although the paper does not explicitly detail the codebase for dataset construction, the public URL and mention of data availability strongly suggest that relevant code and resources are shared for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3, 3.1, 3.2, 3.3, 3.4",
          "reasoning": "The paper thoroughly documents the dataset creation process, including data collection methods (e.g., scraping Flickr with seed concepts and multilingual translations), dataset filtering steps, selection of categories, train/test splits, and elaboration on various distribution shifts (context, design, prior) observed in the data. These details provide transparency and completeness necessary for reproducing the dataset creation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-856-rubric-0",
    "token_usage": {
      "prompt_tokens": 10255,
      "completion_tokens": 124,
      "total_tokens": 10379
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.4: Constructing V_CLIP via Concept Prompting",
          "Reasoning": "The new dataset introduced is a concept library constructed from text prompts using concept keywords from predefined concept libraries such as BRODEN, AwA2 attribute names, and CUB attribute names. These concept keywords and associated textual templates are human-generated as they represent semantic concepts and categories manually defined by the authors, used to create text-driven concept directions in the CLIP latent space without collecting new images."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-856-rubric-1",
    "token_usage": {
      "prompt_tokens": 11055,
      "completion_tokens": 268,
      "total_tokens": 11323
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3, 3.4, 4.1",
            "reasoning": "The concept directions are derived only from text prompts encoded by the CLIP text encoder, and perturbation weights are optimized with losses; the annotations correspond to automated computation steps (e.g., projection, inverse projection, text-driven concept direction construction) without human annotators labeling data manually."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit annotation instructions for human annotators are mentioned in the paper.",
            "reasoning": "The paper does not describe any manual data annotation process or provide instructions for human annotators; concepts are defined purely from text prompting and automated embeddings, not manual labeling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No description of annotation scoring rubrics is present in the paper.",
            "reasoning": "Since annotation is performed via automated processes (text encoding and optimization) and no human annotations are collected, no scoring rubrics apply."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples are provided for human annotators in the paper.",
            "reasoning": "The paper lacks any mention or inclusion of example annotations or guidelines for manual annotation, as the new dataset uses text-driven concept directions without annotated image data."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-856-rubric-2",
    "token_usage": {
      "prompt_tokens": 12245,
      "completion_tokens": 352,
      "total_tokens": 12597
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for dataset annotations or content validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance on any dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no evidence of quality assurance done by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description or mention of multiple non-expert human annotators conducting quality assurance is provided."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used explicitly for quality assurance purposes on dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper describes automated methods to construct concept vectors via text prompting and projection functions, these serve model components or explanation mechanisms rather than quality assurance of dataset annotations. The paper does not explicitly document automated quality assurance of datasets or annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces no new datasets requiring manual or automated annotation or content validation; the concept directions are derived directly from text using CLIP without additional human annotation. Moreover, there is no description of any quality assurance process applied or documented for dataset annotations. The datasets used (ImageNet, AwA2, CUB) are pre-existing and not newly created by the authors. Consequently, there is no quality assurance process to evaluate for new datasets in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-856-rubric-3",
    "token_usage": {
      "prompt_tokens": 11855,
      "completion_tokens": 497,
      "total_tokens": 12352
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe creation of new datasets entirely from scratch by human contributors. Instead, it references existing datasets such as ImageNet, AwA2, and CUB."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset generated solely by AI or machine learning models. While it uses pre-trained models (CLIP) and model embeddings for explanations, it does not generate new datasets from models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that any datasets were produced by translating content from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of data generated via machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used for evaluation (ImageNet, AwA2, CUB) are existing well-known datasets without indication that the authors collected or aggregated new data from different sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1, Section 3.5, 4.5",
          "reasoning": "Though the paper does not introduce entirely new datasets, it derives new data representations and concept embeddings from existing datasets. In particular, it defines concept direction embeddings (V_CLIP) using prompts and concept libraries derived from existing datasets and resources, adapting textual concept prompts (such as from CBRODEN and attributes of AwA2 and CUB). Projection and inverse projection are trained using ImageNet images but without annotation, creating adapted representations rather than new raw datasets. Furthermore, the concept direction bank V_CLIP is constructed by transformations of textual concepts using CLIP embeddings, which are derived from pre-trained models and existing text prompts."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes origins and transformations of data used for explanations."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-856-rubric-4",
    "token_usage": {
      "prompt_tokens": 12373,
      "completion_tokens": 256,
      "total_tokens": 12629
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 4.3",
          "reasoning": "The datasets Animals with Attributes2 (AwA2) and CUB-200-2011 (CUB) are used to quantitatively evaluate the proposed method by comparing generated concept importance scores against ground-truth class-wise attributes for benchmarking explanation accuracy."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 and Section 4.3",
          "reasoning": "The datasets are used to analyze and demonstrate the quality and characteristics of conceptual counterfactual explanations, including qualitative evaluation on correctly and misclassified images and quantitative evaluation of explanation faithfulness."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-856-rubric-5",
    "token_usage": {
      "prompt_tokens": 13096,
      "completion_tokens": 534,
      "total_tokens": 13630
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence or mention of dataset entries composed of exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.4 Concept Prompting; Section 4.1 Concept Library",
          "reasoning": "The concept libraries consist of English words and phrases only (e.g., 'stripe', 'green', 'water'), and all textual prompts described are in English. The paper exclusively references English prompts and English concept keywords for defining concept directions, implying the dataset contains only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets or concept libraries use any non-English language content as per the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not include programming or code-related content; the textual concepts and definitions are natural language only."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While mathematical notation appears in the paper to describe methods, the datasets themselves do not contain mathematical or logical expressions as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses standard image classification datasets (ImageNet, AwA2, CUB) focusing on human language concepts; no biological or animal communication data is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include fictional or artificial languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of the dataset entries are explicitly English and documented, so unknown language labeling is not applicable."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains textual entries in English; thus, language is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-856-rubric-6",
    "token_usage": {
      "prompt_tokens": 10314,
      "completion_tokens": 224,
      "total_tokens": 10538
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section mentions code release for dataset construction or preprocessing.",
          "reasoning": "The paper does not mention or provide any link to source code related to constructing any new datasets, nor does it mention releasing code for dataset creation or processing. The datasets used are standard benchmark datasets (ImageNet, AwA2, CUB) and pre-defined concept libraries. The concept directions are derived textually via CLIP embeddings without new dataset collection or annotation code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.4, Section 4.1",
          "reasoning": "The paper documents how the concept libraries are constructed from existing attribute libraries and explains in Section 3.4 how concept directions are created by prompting with text in CLIP latent space, including the templates used. Section 4.1 describes the use of pre-defined concept libraries from prior work and datasets. Although no new conventional image datasets were collected, the process to build the concept direction bank is clearly described, thus the dataset construction (concept prompting) process is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-865-rubric-0",
    "token_usage": {
      "prompt_tokens": 13638,
      "completion_tokens": 108,
      "total_tokens": 13746
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset and experimental settings",
          "Reasoning": "The paper uses seven public semantic segmentation datasets for assembling evaluation settings, including Cityscapes, ACDC, BDD, Mapillary, GTAV, SYNTHIA, and Cityscapes-C. All of these datasets are pre-existing and publicly available; none of them is introduced as a new dataset by the authors in this work."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-865-rubric-1",
    "token_usage": {
      "prompt_tokens": 14438,
      "completion_tokens": 332,
      "total_tokens": 14770
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3 Methods and 4 Experiments",
            "reasoning": "The paper proposes and utilizes a hierarchical grouping transformer model (HGFormer) that performs segmentation grouping and classification automatically without mention of human annotators for the new dataset's mask annotations. The part-level masks partition is generated by a similarity-based local clustering algorithm embedded in the model itself, and whole-level masks are obtained by transformer decoders. Given the domain generalization setting and no explicit mention that humans annotated these grouping masks, they are automatically generated by the model during training on publicly available standard datasets used for training segmentation models. The segmentation ground truths used for training are likely from the public datasets but the actual hierarchical part-level masks are produced by the model automatically without explicit manual annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not mention any detailed annotation instructions given to any annotators for the new data. The groupings at part-level are automatically computed by model processes without human labeling requiring annotation guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "No scoring rubrics or evaluation rubrics are described in the context of human annotation for the new data. The evaluation metrics reported are standard semantic segmentation performance metrics, not annotation rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "No examples for data annotation guidelines or annotation processes are included. Visual examples shown relate to model output masks, not annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-865-rubric-2",
    "token_usage": {
      "prompt_tokens": 15628,
      "completion_tokens": 273,
      "total_tokens": 15901
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance of dataset annotations performed specifically by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper of quality assurance conducted by multiple human experts for any dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance done by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information about quality assurance carried out by multiple non-expert annotators is provided in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of AI models as a judge in quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic rule-based quality assurance process for the dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses seven public semantic segmentation datasets without introducing new ones. There is no mention of any quality assurance process applied or documented for dataset annotations. Therefore, no QA process is applied or documented for datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-865-rubric-3",
    "token_usage": {
      "prompt_tokens": 15238,
      "completion_tokens": 426,
      "total_tokens": 15664
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the creation of any new dataset collected or created from scratch by human contributors. Instead, it utilizes existing public semantic segmentation datasets for experiments."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe generating any dataset entirely through AI or machine learning model synthesis without reference to existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper of any data being produced by translating content from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information in the paper suggests machine translation has been used to generate any data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 (Experimental Setup) and throughout the experimental sections",
          "reasoning": "The authors assembled multiple cross-domain semantic segmentation evaluation settings by using seven public datasets (such as Cityscapes, ACDC, Mapillary, BDD, GTAV, Synthia, Cityscapes-C). This assembly of datasets is a collection of existing sources without significant modification presented as novel evaluation settings rather than new datasets."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any derived datasets obtained by modifying or adapting existing datasets. The evaluation settings are assembled from existing datasets rather than transformed or modified datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and methods of data generation are specified clearly as public existing datasets; therefore, this category is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-865-rubric-4",
    "token_usage": {
      "prompt_tokens": 15756,
      "completion_tokens": 222,
      "total_tokens": 15978
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. It only assembles experimental settings from multiple existing public semantic segmentation datasets (e.g., Cityscapes, ACDC, Mapillary, GTA V, SYNTHIA) to evaluate domain generalization. Therefore, there is no new dataset introduced by the authors and no practical usage of new datasets is described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-865-rubric-5",
    "token_usage": {
      "prompt_tokens": 16479,
      "completion_tokens": 510,
      "total_tokens": 16989
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses semantic segmentation datasets for images. These datasets are related to visual data rather than human language data. There is no mention of multiple human languages in the dataset entries themselves."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the dataset entries include exactly two human languages. The datasets are image collections for segmentation tasks, not linguistic datasets."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any datasets composed of English text-only entries. The datasets mainly consist of images with pixel-level segmentation annotations, not textual or linguistic content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets containing exactly one non-English language-only content as entries. The dataset entries consist of images, not language texts."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets discussed contain image data for semantic segmentation and related annotations. There are no presented programming code entries as part of the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3: Methods (Formally defining image partitions and assignment matrices); Equations (1)\u2013(6)",
          "reasoning": "The paper formalizes the semantic segmentation problem using mathematical notation, including sets, partition definitions, assignment matrices, cosine similarity functions, and loss functions. These symbolic representations are part of explaining and defining the datasets and tasks, indicating the dataset entries include mathematical and logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described are image datasets for computer vision tasks, with no biological or non-human communication content such as DNA or animal signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any fictional or artificially created languages as part of the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages or modalities of the datasets are clearly described. The dataset primarily consists of images with semantic segmentation annotations, so the language content is not unspecified or undocumented."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The entries of the datasets are images with pixel-level semantic segmentation annotations; such entries do not contain any human language content. Hence, the datasets do not contain entries with any languages."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-865-rubric-6",
    "token_usage": {
      "prompt_tokens": 13697,
      "completion_tokens": 191,
      "total_tokens": 13888
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and sections describing datasets (e.g., Section 4.1)",
          "reasoning": "The paper references use of multiple existing public datasets for evaluation (e.g., Cityscapes, ACDC, Mapillary, GTAV, SYNTHIA) and mentions that code for the proposed method will be made available. However, there is no indication that new datasets are introduced or that any code related to dataset construction or generation is provided. The datasets used are publicly available pre-existing datasets rather than new datasets created by the authors."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 4, Dataset descriptions",
          "reasoning": "The paper describes experimental settings using existing public datasets, listing details such as number of images, resolution, and conditions, but does not describe any new dataset creation process. Since no new datasets are introduced, no documentation on dataset construction is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-878-rubric-0",
    "token_usage": {
      "prompt_tokens": 12032,
      "completion_tokens": 321,
      "total_tokens": 12353
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4: Data and 4.1 Data Pre-Processing",
          "Reasoning": "The paper states that the new combined dataset consists of 5,754 videos of Sign language signs with their HamNoSys transcriptions. The data is collected from three existing datasets (DGS Corpus, Dicta-Sign, and the corpus-based dictionary of Polish Sign Language). These original videos were recorded with front-facing persons signing single signs; thus, the video modality data is human generated, as it involves human signing captured by recording devices."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4: Data",
          "Reasoning": "The dataset includes HamNoSys transcriptions of the signs performed in the videos. HamNoSys is a human-designed universal Sign language notation, so the text data (HamNoSys sequences) originates from human-generated annotations provided as parallel transcriptions to the videos."
        },
        {
          "Modality": "time series",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Data Pre-Processing",
          "Reasoning": "The pose keypoints sequences are extracted from the human-generated videos using the OpenPose pose estimation model, an automated system. The pose sequences comprise 2D keypoints with confidence scores over time, thus forming time series modality data. Since this data results from a model (pose estimator) processing the video, it is model generated."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-878-rubric-1",
    "token_usage": {
      "prompt_tokens": 12832,
      "completion_tokens": 316,
      "total_tokens": 13148
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4, Data and Data Pre-Processing",
            "reasoning": "The dataset used in this paper is a combination of existing HamNoSys-to-video datasets [25, 26, 32]. The annotations (HamNoSys notations) were provided by those original datasets. The paper describes their data pre-processing step which includes automatic pose keypoint extraction using the OpenPose model [7], filtering by confidence scores, flipping left-handed sign videos for consistency, and normalization. Therefore, the annotation related to pose keypoints is obtained by an automatic pose estimation process, not human annotation in this work. The paper does not mention any new manual annotations done by human experts or annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No relevant description for annotation instructions in data collection or pre-processing (Section 4)",
            "reasoning": "The paper does not describe or provide any detailed annotation instructions given to annotators for any new annotations, since the dataset consists of combining and processing existing annotated datasets and automatically extracting poses."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention of annotation rubrics in the paper",
            "reasoning": "No scoring rubrics or evaluation rubrics for annotation quality are reported or described for the new dataset or its processing."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No mention of annotation examples provided in the paper",
            "reasoning": "The paper does not include or reference annotation examples as part of its data description or processing."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-878-rubric-2",
    "token_usage": {
      "prompt_tokens": 14022,
      "completion_tokens": 317,
      "total_tokens": 14339
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that a single human expert performed quality assurance on the datasets used."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by multiple human experts on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of a single non-expert performing quality assurance on the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that multiple non-expert annotators performed quality assurance on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the model itself is used for pose generation, there is no description of an AI model used specifically for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.1 Data Pre-Processing",
          "reasoning": "The paper describes an automated process of extracting pose keypoints from videos using the OpenPose model, applying filtering based on confidence thresholds, masking legs, flipping left-handed videos for consistency, and pose normalization. These steps constitute an automatic quality assurance and data processing pipeline applied to the ground truth annotations prior to training."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents an automated data pre-processing and pose keypoints extraction process, so quality assurance is not absent."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-878-rubric-3",
    "token_usage": {
      "prompt_tokens": 13632,
      "completion_tokens": 418,
      "total_tokens": 14050
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any entirely new data was created from scratch by human contributors. Instead, it uses existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data used in the paper is not generated by models but collected from existing datasets and then processed."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data was generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4: Data",
          "reasoning": "The paper states that the dataset consists of 5,754 videos collected from multiple existing HamNoSys-to-video datasets - DGS Corpus, Dicta-Sign, and the corpus-based dictionary of Polish Sign Language. This constitutes collecting or aggregating data from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1: Data Pre-Processing",
          "reasoning": "The authors extract pose keypoints from the collected videos using OpenPose and apply several processing steps such as filtering low confidence keypoints, trimming frames, flipping left-handed videos, and pose normalization. This is a modification and transformation of existing data, thus the data used for training is derived from the original video datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are clearly specified and documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-878-rubric-4",
    "token_usage": {
      "prompt_tokens": 14150,
      "completion_tokens": 321,
      "total_tokens": 14471
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5, 6",
          "reasoning": "The combined dataset of 5,754 videos with HamNoSys transcriptions and extracted pose keypoints from multiple corpora (Section 4) is used to train the proposed model from scratch. This training involves learning a model to generate pose sequences from HamNoSys input, as detailed in Section 5 (Method) and Section 6 (Implementation Details). The paper specifies training from scratch on this combined dataset without mentioning prior pre-training."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 7.1, 7.2, 7.3",
          "reasoning": "The paper introduces a new distance measurement (nDTW-MJE) for evaluating pose sequence similarity and validates it on a large-scale external dataset (AUTSL). This measurement is then used to evaluate the quality of their generated pose sequences quantitatively (Section 7). This use of data pertains to evaluation and benchmarking purposes."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-878-rubric-5",
    "token_usage": {
      "prompt_tokens": 14873,
      "completion_tokens": 390,
      "total_tokens": 15263
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 4 (Data)",
          "reasoning": "The new dataset introduced is a combination of data from multiple corpora containing four different Sign languages: Polish Sign Language (PJM), German Sign Language (DGS), Greek Sign Language (GSL), and French Sign Language (LSF). This entails that the dataset contains entries from more than two human languages, making it multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human sign languages represented via HamNoSys transcriptions and video pose data, and therefore includes language entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-878-rubric-6",
    "token_usage": {
      "prompt_tokens": 12091,
      "completion_tokens": 202,
      "total_tokens": 12293
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 4 Data",
          "reasoning": "The paper states in the Abstract: 'Code for the data pre-processing, the model, and the distance measurement is publicly released for future research.' This indicates that the code used for data pre-processing (which includes dataset construction steps) is publicly released. No explicit link is given in the main body, but the public release statement confirms availability."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 Data and Section 4.1 Data Pre-Processing",
          "reasoning": "Section 4 describes the dataset collection from three existing HamNoSys-to-video datasets and specifies the sign languages, number of videos, signers, and number of signs. Section 4.1 provides detailed documentation of the data pre-processing steps applied, including pose extraction with OpenPose, filtering, masking, flipping, and normalization steps. This documentation is reasonably detailed, enabling reproduction of dataset processing."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-896-rubric-0",
    "token_usage": {
      "prompt_tokens": 11252,
      "completion_tokens": 110,
      "total_tokens": 11362
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset",
          "Reasoning": "The paper explicitly states that the authors used the Vimeo90K septuplet dataset for training, which contains 91,701 7-frame video sequences. However, the Vimeo90K dataset is cited from a prior work [33], thus is not introduced by the authors as a new dataset. Therefore, it is not the new dataset introduced by the authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-896-rubric-1",
    "token_usage": {
      "prompt_tokens": 12052,
      "completion_tokens": 198,
      "total_tokens": 12250
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1 Dataset",
            "reasoning": "The dataset used for training the proposed method is the Vimeo90K septuplet dataset, which is a pre-existing dataset. The paper does not mention introducing any new datasets; therefore, no new annotation process is described associated with new datasets."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There is no mention of annotation instructions related to any new dataset introduced in the paper, as no new dataset is introduced."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There is no description of annotation scoring rubrics for any new dataset in the paper since no new datasets are presented."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No examples of annotations are provided because the paper does not introduce any new dataset or annotation procedure."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-896-rubric-2",
    "token_usage": {
      "prompt_tokens": 13242,
      "completion_tokens": 322,
      "total_tokens": 13564
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance involving multiple human experts for the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that a single non-expert performed quality assurance for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple non-expert annotators engaged in quality assurance processes for the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used as a judge to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper describes automated training and optimization procedures for the compression model, it does not explicitly describe any automated verification or algorithmic/rule-based quality assurance processes for the dataset itself."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses the Vimeo90K septuplet dataset, a publicly available dataset, for training and new learned video compression models are evaluated on established datasets (UVG and HEVC Class B). There is no documentation or description of any quality assurance process applied to these datasets by the authors. No new dataset was introduced by the paper; any quality assurance for datasets is not reported."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-896-rubric-3",
    "token_usage": {
      "prompt_tokens": 12852,
      "completion_tokens": 459,
      "total_tokens": 13311
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The authors used the Vimeo90K septuplet dataset, which contains video sequences created by human contributors, but did not create a new dataset themselves. However, the paper does not explicitly introduce a new dataset created by humans from scratch. The training is done on an existing dataset and augmented but no new dataset creation is described. Therefore, this category is not applicable to new dataset introduction."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe generating any dataset purely from AI or machine learning models without reference to existing data. The datasets used for training and evaluation are existing video datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of the dataset being translated from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of the dataset being generated via machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper uses existing datasets, such as the Vimeo90K septuplet dataset for training and UVG and HEVC Class B datasets for evaluation. These datasets were collected and aggregated by previous researchers. The authors do not mention significant modification, so their usage can be considered collated data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the authors perform data augmentations like cropping and flipping, these are standard preprocessing steps and not described as creating a new derived dataset. They do not explicitly mention altering or creating new datasets derived from existing ones."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources for training and evaluation are explicitly documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-896-rubric-4",
    "token_usage": {
      "prompt_tokens": 13370,
      "completion_tokens": 410,
      "total_tokens": 13780
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new dataset used exclusively for pre-training large models on general patterns."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit description that a new dataset introduced by the authors is used to train models from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using any new dataset for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that reinforcement learning based post-training methods are applied using any new dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The evaluation datasets mentioned (UVG, HEVC Class B, Vimeo90K) are pre-existing datasets; the paper does not introduce a new dataset for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is introduced or used primarily for analyzing trends or characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset serving as a knowledge base to augment models."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; all datasets referenced are publicly available or pre-existing. Accordingly, no practical usage of any new dataset is described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-896-rubric-5",
    "token_usage": {
      "prompt_tokens": 14093,
      "completion_tokens": 516,
      "total_tokens": 14609
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce or describe any dataset containing multiple human languages; the main dataset used is Vimeo90K, which is a collection of videos without linguistic content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of any dataset containing exactly two human languages in the paper."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's introduced dataset is a video dataset containing visual content, not text or speech in English; hence this does not apply."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no dataset introduced containing a non-English human language exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses code architectures and models but does not introduce any dataset composed of programming language content or code snippets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical expressions, no dataset consisting of mathematical or logical notation entries is introduced."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not contain or introduce datasets involving biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset involving constructed or fictional languages is described or introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced is clearly described as video sequences of natural scenes; the language content is visual and not linguistic in nature; hence language is not applicable."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The dataset introduced is a video dataset (Vimeo90K) consisting of sequences of images with no linguistic content. Therefore, no language is involved in the dataset entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-896-rubric-6",
    "token_usage": {
      "prompt_tokens": 11311,
      "completion_tokens": 244,
      "total_tokens": 11555
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "The paper's abstract and main text mention that the code for the compression method is available at https://nycu-clab.github.io, but no explicit link or indication is provided regarding the availability of code related to dataset construction or processing.",
          "reasoning": "The authors provide code for the video compression system at a given URL, but there is no indication that code related to the creation, preprocessing, or generation of any new datasets is provided. The datasets used are standard publicly available datasets (e.g., Vimeo90K, UVG, HEVC), and no new datasets are introduced by the authors. Therefore, code related to dataset construction is not discussed or linked."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 4.1 Dataset and other relevant sections describing experimental setup or training do not mention the creation or construction process of any new datasets.",
          "reasoning": "The paper only uses public datasets (Vimeo90K septuplet, UVG, HEVC Class B) for training and evaluation. There is no description of any new dataset creation or construction process documented in the paper. Hence, documentation for dataset creation is absent."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-925-rubric-0",
    "token_usage": {
      "prompt_tokens": 15738,
      "completion_tokens": 109,
      "total_tokens": 15847
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 6, Dataset",
          "Reasoning": "The paper evaluates the proposed method on several existing benchmark datasets such as CIFAR-10, CIFAR-100, Clothing1M, and WebVision; each consists of images collected from human-generated sources. The paper does not introduce any new datasets; it only uses these well-known datasets for evaluation. No new data collection or generation is described in the paper."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-925-rubric-1",
    "token_usage": {
      "prompt_tokens": 16538,
      "completion_tokens": 233,
      "total_tokens": 16771
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 6 Experiments, Dataset",
            "reasoning": "The paper introduces new noisy label datasets by adding synthetic noise (symmetric and asymmetric) on CIFAR-10 and CIFAR-100, which is an automatic process of generating noisy labels rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper sections about detailed instructions for annotators for the newly introduced noisy datasets",
            "reasoning": "Since the noisy labels are generated synthetically via automatic noise introduction, no human annotators are involved, so no annotation instructions are necessary or described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No discussion of scoring rubrics in the dataset creation or noise generation process sections",
            "reasoning": "The synthetic noise addition process does not entail scoring or rubric-based human annotation; hence, no rubrics exist."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of annotation guidelines or noisy label examples are described",
            "reasoning": "As noise generation is automatic and synthetic, the paper does not include examples for annotation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-925-rubric-2",
    "token_usage": {
      "prompt_tokens": 17728,
      "completion_tokens": 327,
      "total_tokens": 18055
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert for any datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance involving multiple human experts for any datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance being done by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify quality assurance performed by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance conducted by AI models as judges of dataset quality."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper introduces theoretical and algorithmic approaches for training models robust to label noise, it does not describe any automated verification or algorithmic quality assurance process applied to validate or clean the datasets' annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The datasets used in the experiments are existing standard benchmarks (CIFAR-10, CIFAR-100, Clothing1M, WebVision) with their own known label noise characteristics. The paper focuses on modeling and training methods to handle noisy labels, but does not describe any new dataset creation or specific quality assurance or annotation verification process for any new datasets. Therefore, no quality assurance process regarding dataset annotation is documented in this paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-925-rubric-3",
    "token_usage": {
      "prompt_tokens": 17338,
      "completion_tokens": 411,
      "total_tokens": 17749
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not involve datasets generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any dataset is produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention machine-translated datasets."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses pre-existing datasets such as CIFAR-10, CIFAR-100, Clothing1M, and WebVision; these are widely recognized benchmark datasets, but the authors do not compile or aggregate new data from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 6, Dataset",
          "reasoning": "The paper employs existing clean datasets CIFAR-10 and CIFAR-100 and introduces synthetic noise by randomly flipping labels (symmetric and asymmetric noise) to generate noisy labels for experiments. This synthetic noisy data is derived from existing datasets with modifications applied (label perturbations). The automatic noise generation is done by the authors for evaluation purposes, hence these datasets with simulated noise are derived from existing clean datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and generation of the synthetic noisy datasets are specified in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-925-rubric-4",
    "token_usage": {
      "prompt_tokens": 17856,
      "completion_tokens": 350,
      "total_tokens": 18206
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 6, Implementation Details and Experiments",
          "reasoning": "The paper uses standard noisy and real-world noisy datasets (CIFAR-10, CIFAR-100, Clothing1M, WebVision) with created or inherent noisy labels to train models from scratch with their proposed Gradient Switching Strategy (GSS) method. The datasets are used as training data starting from pre-trained weights or training from scratch as a baseline to evaluate their method's effectiveness."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 6.1 Quantitative Evaluation",
          "reasoning": "The datasets CIFAR-10, CIFAR-100 (with synthetic noise), Clothing1M, and WebVision are used for evaluation of classification accuracy metrics to compare the performance of the proposed GSS method versus existing methods."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.3 Effectiveness analysis of Gradient Switching and Section 6.2 Ablation Study",
          "reasoning": "The datasets are also used for analyzing the gradient biases, the effect of gradient switching, and ablation studies on the weights of different labels in the gradient direction pool to understand the behavior and effectiveness of GSS."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-925-rubric-5",
    "token_usage": {
      "prompt_tokens": 18579,
      "completion_tokens": 637,
      "total_tokens": 19216
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces new experimental evaluations done on datasets like CIFAR-10, CIFAR-100, Clothing1M, and WebVision, which contain labels in English or non-specified languages. However, none of these datasets are introduced by the authors as new datasets containing multiple human languages. The paper focuses on noisy-label learning methods rather than presenting a dataset with multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper of the authors introducing new datasets containing exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not present any newly introduced datasets consisting solely of English language data. The datasets used (CIFAR-10, CIFAR-100, Clothing1M, WebVision) are standard datasets not introduced by the authors, and no new monolingual English dataset is introduced."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset of a single non-English language is introduced in the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper presents algorithms, pseudo-code descriptions (e.g., Gradient Switching Strategy), and mathematical formulae, there is no new dataset introduced containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses mathematical and logical notation extensively in the methodological and theoretical analysis sections; however, these are used to describe the approach and are not datasets. The authors do not introduce a dataset composed of mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset related to biological sequences or non-human communication is introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are part of any newly introduced datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no newly introduced dataset with unspecified or undocumented languages."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. The work evaluates the proposed Gradient Switching Strategy on existing public datasets (e.g., CIFAR-10, CIFAR-100, Clothing1M, WebVision). Therefore, there are no new dataset entries containing any language introduced by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-925-rubric-6",
    "token_usage": {
      "prompt_tokens": 15797,
      "completion_tokens": 153,
      "total_tokens": 15950
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or link in the paper",
          "reasoning": "The paper does not mention or provide any links to code repositories for data collection, preprocessing, or dataset generation. It mainly evaluates on standard benchmark datasets with synthetic and real-world noise, but no new dataset or dataset generation code is introduced by the authors."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No dataset creation discussed in the paper",
          "reasoning": "The paper does not introduce any new datasets or describe data collection or dataset construction processes. It uses existing public datasets (CIFAR-10, CIFAR-100, Clothing1M, WebVision) with simulated noise, and does not document any dataset creation steps."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-947-rubric-0",
    "token_usage": {
      "prompt_tokens": 10195,
      "completion_tokens": 224,
      "total_tokens": 10419
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1, Table 1, Section 4.3",
          "Reasoning": "The new dataset introduced by the authors is the MVP dataset, used extensively in experiments (e.g., Table 1 and in Section 4.3). It contains 3D point clouds, which are typically collected from 3D sensors capturing geometric data (depth or point cloud scans). The paper refers to it as a dataset for point cloud completion, which involves processing 3D sensor data. This implies the data modality is 'signal/sensor' representing 3D sensor-captured point clouds. The data is human-generated in the sense that it originates from real-world objects scanned by humans operating or deploying 3D sensors, i.e., human involvement in creating the dataset via manual recording. It is not model generated or unknown origin, as the dataset is a newly introduced benchmark derived from actual 3D sensor captures collected deliberately and explicitly for training and evaluation purposes."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-947-rubric-1",
    "token_usage": {
      "prompt_tokens": 10995,
      "completion_tokens": 269,
      "total_tokens": 11264
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1 and Section 4",
            "reasoning": "The paper introduces a hyperspherical module that transforms embeddings output from the encoder using an MLP followed by L2 normalization as a part of the model architecture for point cloud completion. This process, including embedding transformation and normalization, is implemented within the neural network, indicating the annotation (labeling) of data or embedding generation is conducted automatically by the model rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not discuss any manual annotation process or guidelines involving human annotators; therefore, no instructions for manual annotation are provided or applicable."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "No description of scoring rubrics or evaluation rubrics for human annotators is provided, as the embeddings are produced automatically by the model during training."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper or appendix",
            "reasoning": "There are no examples related to human annotation in the paper; all data processing related to embeddings is automatically generated by the model components. No annotation examples are provided relevant to the introduced module."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-947-rubric-2",
    "token_usage": {
      "prompt_tokens": 12185,
      "completion_tokens": 300,
      "total_tokens": 12485
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance being conducted by a single human expert for dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving multiple human experts for dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that a single non-expert performed quality assurance of dataset annotations or content."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance done by multiple non-expert human annotators for dataset annotations or content."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by AI models as judges on dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report using automated verification of dataset annotations or content via algorithmic or rule-based techniques as a quality assurance procedure."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a hyperspherical module for point cloud completion but does not introduce any new datasets. All datasets used are pre-existing public datasets. Since no new datasets are introduced, there is no documented quality assurance process described or applied for new datasets in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-947-rubric-3",
    "token_usage": {
      "prompt_tokens": 11795,
      "completion_tokens": 360,
      "total_tokens": 12155
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any novel datasets created entirely by humans from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe generating new datasets by models; datasets mentioned are used for evaluation but not newly generated by AI or models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data has been produced by human translators from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or evidence of datasets generated by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses several existing public datasets (e.g., MVP, GraspNet, ModelNet40) for evaluation but does not introduce new datasets by collecting or aggregating existing data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify deriving new datasets by modification or transformation of existing datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; all evaluation is conducted on existing public datasets and the source or generation method of data is not newly created, collated, or derived by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-947-rubric-4",
    "token_usage": {
      "prompt_tokens": 12313,
      "completion_tokens": 222,
      "total_tokens": 12535
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper proposes a hyperspherical module for point cloud completion and evaluates this method on several public existing datasets such as MVP, ModelNet40, and GraspNet. However, it does not introduce or create any new datasets. All datasets referenced are pre-existing standard datasets used for experimental evaluation. Therefore, no new dataset is introduced or described for any utility in the ML pipeline."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-947-rubric-5",
    "token_usage": {
      "prompt_tokens": 13036,
      "completion_tokens": 363,
      "total_tokens": 13399
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes code-related references to multi-layer perceptrons (MLP) and optimizers like Adam and RMSprop, but it does not introduce any new dataset entries containing code or programming language content. The datasets are about 3D point clouds, not code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3.2, Section 3.1",
          "reasoning": "The paper includes mathematical equations and formal notations describing the hyperspherical embedding, gradient computations, and singular value decomposition in Section 3. Specifically, Proposition 1 to Proposition 4 and related equations (Equations 1, 2, 3) capture mathematical descriptions relevant to the embedding and training process."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset that contains human language entries\u2014textual or linguistic data in any language\u2014nor biological or constructed languages. The datasets discussed are 3D point cloud datasets used for shape completion and related tasks, which do not contain language content."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-947-rubric-6",
    "token_usage": {
      "prompt_tokens": 10254,
      "completion_tokens": 172,
      "total_tokens": 10426
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention in the paper",
          "reasoning": "The paper does not provide any link, URL, or explicit mention of a public repository containing code related to dataset collection, preprocessing, or generation. It primarily focuses on a methodological contribution (hyperspherical embedding) and experiments on existing public datasets, and does not claim to introduce any new datasets or code for dataset creation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No explicit section or mention in the paper",
          "reasoning": "The paper does not describe any dataset creation process because it does not introduce any new datasets. All datasets used in experiments are publicly available, pre-existing datasets (e.g., MVP, ModelNet40, GraspNet). Therefore, there is no documentation provided for dataset construction or generation."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-981-rubric-0",
    "token_usage": {
      "prompt_tokens": 11750,
      "completion_tokens": 136,
      "total_tokens": 11886
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Preliminary, Section 5.1 Dataset and Implementation Details",
          "Reasoning": "The new dataset introduced consists of augmented source domains generated by a Fourier-based augmentation technique applied to original image datasets. The original images are human-generated as they are collected from real-world sources (PACS, Office-Home, mini-DomainNet). The augmented images are model generated via Fourier amplitude mixing, creating new images algorithmically. Thus, the modality is image with both human generated (original) and model generated (augmented) data."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-981-rubric-1",
    "token_usage": {
      "prompt_tokens": 12550,
      "completion_tokens": 231,
      "total_tokens": 12781
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1, 3.2, 3.3",
            "reasoning": "The paper describes a sample filter constructed based on the regularization term that automatically scores and filters low-quality samples during training. The annotation (filtering of samples) is performed by a deterministic scoring function (input-gradient product) and selection of top-k samples, thus an automatic process rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not provide any detailed annotation instructions applicable to the automatic sample filtering process or human annotators on the new dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No explicit scoring rubrics or grading guidelines for sample filtering annotations are presented. The scoring is algorithmically defined rather than via rubric."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No concrete annotation examples are provided for instructions or sample filtering; the examples shown are visualizations of filtered samples, not annotation guideline examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-981-rubric-2",
    "token_usage": {
      "prompt_tokens": 13740,
      "completion_tokens": 310,
      "total_tokens": 14050
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human expert involvement in verifying or assuring the quality of the datasets used or introduced."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts were involved in quality assurance of the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence provided of a single non-expert performing quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence provided of multiple non-experts performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used as a judge for quality assurance of datasets or annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper describes automated augmentation techniques (e.g., Fourier-based augmentation) and automated training procedures, there is no explicit mention of automated quality assurance processes for dataset annotations or content validation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new way to generate augmented domains by Fourier-based domain augmentation (Section 3.1), but does not describe any quality assurance or validation process for these newly generated datasets. There is no documentation or description of any quality assurance process applied to the datasets or augmented data. Hence, we conclude that no quality assurance process is applied or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-981-rubric-3",
    "token_usage": {
      "prompt_tokens": 13350,
      "completion_tokens": 388,
      "total_tokens": 13738
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are popular existing benchmarks (PACS, Office-Home, mini-DomainNet) and are not newly created by humans in this work."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The authors use a Fourier-based augmentation technique to generate novel augmented source domains by synthesizing new samples through mixing amplitude components of different source domain images, thus generating new data samples artificially via a model-based method."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translations."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by machine translations."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are existing benchmarks; however, the paper does not indicate that the data are merely collected or aggregated from existing sources without modification. Instead, augmented data are generated."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The augmented data are generated by modifying existing source domain samples via Fourier spectrum manipulation (mixing amplitude with other samples), thus deriving new data based on existing ones with transformations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origins and data generation methods are explicitly described."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-981-rubric-4",
    "token_usage": {
      "prompt_tokens": 13868,
      "completion_tokens": 411,
      "total_tokens": 14279
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training models from randomly initialized parameters using the proposed datasets. Instead, it uses pre-trained ResNet backbones."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.1 and 5.2",
          "reasoning": "The diversified and augmented source domains introduced by the authors are used to fine-tune pre-trained ResNet models using supervised learning, as detailed in the training settings for domain generalization tasks across PACS, Office-Home, and Mini-DomainNet datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of reinforcement learning or RL-based post-training methods with any dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The proposed augmented domains are used for training rather than evaluation; evaluation is done on standard held-out target domains from benchmark datasets."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper provides analyses of trends and patterns, this is performed on pre-existing benchmark datasets, not on any new dataset introduced by the authors."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are not employed as knowledge bases for model augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2023-981-rubric-5",
    "token_usage": {
      "prompt_tokens": 14591,
      "completion_tokens": 520,
      "total_tokens": 15111
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset containing entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the new dataset contains exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5.1 Dataset and Implementation Details",
          "reasoning": "The paper introduces new datasets based on domain augmentations of existing English-labeled image datasets (PACS, Office-Home, mini-DomainNet). These datasets contain image samples with English category labels; there is no indication of other human languages being included. Thus, the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not indicated to contain any non-English languages exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of images and their labels; they do not contain entries in programming or structured code languages."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes mathematical formulae relating to the method and regularization, the datasets themselves do not contain mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not include datasets with biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificially created languages are part of the new datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of dataset entries is specified (English labels); thus, it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain human language (English) labels, so they are not language-free."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-981-rubric-6",
    "token_usage": {
      "prompt_tokens": 11809,
      "completion_tokens": 204,
      "total_tokens": 12013
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention about code release for dataset construction",
          "reasoning": "The paper does not provide any link, URL, or explicit mention that the code for constructing the dataset (including augmentation code) is publicly released or available. While the paper details the augmentation technique used (Fourier-based), it does not mention releasing associated code repositories or scripts."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Preliminary and throughout Sections 3 and 5",
          "reasoning": "The paper thoroughly documents the dataset construction and augmentation process, especially in Section 3.1 where they explain the Fourier-based augmentation for creating novel domains from original source data. The methodology for domain augmentation is described with sufficient detail, including references to prior works for implementation details. The paper also discusses the sample filtering method and provides experimental details about data splits and augmentation usage, which together form a comprehensive documentation of the dataset creation process within their experimental framework."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-992-rubric-0",
    "token_usage": {
      "prompt_tokens": 11629,
      "completion_tokens": 188,
      "total_tokens": 11817
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4 (Implementation Details) and Section 5 (Experiments)",
          "Reasoning": "The new dataset used for pre-training is the ImageNet-1K (IN1K) training set, specifically without labels, as presented throughout the paper. The paper explicitly mentions the use of ImageNet-1K training set for self-supervised pre-training as a new dataset setting without additional data or labels (Section 4 Implementation Details and Section 5 Experiments). ImageNet-1K consists of photographic images collected through human effort and thus is considered human generated data. There is no indication in the paper that the images are model generated or of unknown origin. The paper's novel contributions relate to methods applied on ImageNet-1K but do not introduce any synthetic or novel dataset beyond this."
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-992-rubric-1",
    "token_usage": {
      "prompt_tokens": 12429,
      "completion_tokens": 300,
      "total_tokens": 12729
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3 (Methodology), Section 4 (Implementation Details), Section 5 (Experiments)",
            "reasoning": "The paper describes a self-supervised pre-training method using masked autoencoders with perceptual and adversarial losses. The 'annotations' correspond to the supervision signals generated automatically from the input images without human labeling or manual annotation. All experiments use the ImageNet-1K training set without explicit human annotations. There is no mention of human annotators or manual labeling for the training data; the processes are fully automated as part of the self-supervised learning framework."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "Since the data annotations are automatically generated as part of the self-supervised learning task (masked reconstruction and adversarial discriminator training), there are no human annotators receiving instructions. The paper does not describe providing specific instruction guidelines for annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "There is no description of manual scoring rubrics or evaluation guidelines used during dataset annotation. The learning process uses automated loss functions rather than rubric-based scoring by annotators."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not provide annotation examples because there is no human annotation task; rather, the model learns from data and defined losses automatically."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2023-992-rubric-2",
    "token_usage": {
      "prompt_tokens": 13619,
      "completion_tokens": 304,
      "total_tokens": 13923
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset annotation or quality assurance being conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process involving multiple human experts for dataset validation or annotation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of a single human non-expert performing quality assurance on dataset content."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description of quality assurance involving multiple non-expert human annotators is provided."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses AI models (discriminators) for training, this is within the model learning process and not described as a quality assurance process for dataset annotation or dataset content validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated verification of dataset annotations or content as a quality assurance step."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces no new dataset and does not document any quality assurance process related to dataset annotation or content validation. The datasets used (e.g., ImageNet-1K, MS COCO, ADE20K) are existing datasets; therefore, no quality assurance process for new datasets is described."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-992-rubric-3",
    "token_usage": {
      "prompt_tokens": 13229,
      "completion_tokens": 370,
      "total_tokens": 13599
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the creation of any new dataset by human contributors; it uses existing datasets for training and evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the method generates reconstructed images during training, these are model-generated outputs and not considered a new dataset introduced by the authors."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data generated by machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 Implementation Details",
          "reasoning": "The paper uses existing publicly available datasets such as ImageNet-1K, MS COCO, ADE20K for pre-training, fine-tuning, and evaluation. These datasets are collated from existing sources and are not generated anew by the authors."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report creating a new dataset derived from modifying existing data; it primarily focuses on training methods and evaluations using existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources used in the paper are well documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-992-rubric-4",
    "token_usage": {
      "prompt_tokens": 13747,
      "completion_tokens": 272,
      "total_tokens": 14019
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 (Implementation Details) - Fine-tuning; Section 5.2 (Downstream Learning Results)",
          "reasoning": "The paper describes using ImageNet-1K and other datasets such as MS COCO and ADE20K to fine-tune the pre-trained models on specific downstream tasks including image classification, object detection, and semantic segmentation, using supervised learning methods."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; it uses existing standard datasets like ImageNet-1K, MS COCO, and ADE20K for pre-training, fine-tuning and evaluation. Hence, no new dataset is introduced or utilized in any described task."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-992-rubric-5",
    "token_usage": {
      "prompt_tokens": 14470,
      "completion_tokens": 566,
      "total_tokens": 15036
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset containing entries with more than two human languages. The datasets used are ImageNet-1K and others which are image datasets without multilingual text content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset containing exactly two human languages. The datasets are image datasets without language annotations."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset introduced in the paper contains only English text entries. The used datasets are primarily image datasets focused on image content without language-specific textual data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no dataset introduced with exactly one non-English language. The paper uses standard image datasets which do not include non-English language data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset consisting of code or programming language content. The paper focuses on masked autoencoders for visual data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper includes mathematical loss functions and formulae in the methodology, these do not constitute a dataset with mathematical or logical notations. The paper does not introduce any dataset containing mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce datasets involving biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or artificial languages are part of any introduced dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used and referenced are well documented standard image datasets; no dataset language origin is unspecified or unknown."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper only introduces methodology improvements on image datasets such as ImageNet-1K with no new textual datasets introduced. These datasets do not contain human language entries, but are purely visual (image) datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2023-992-rubric-6",
    "token_usage": {
      "prompt_tokens": 11688,
      "completion_tokens": 172,
      "total_tokens": 11860
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention for code release for dataset construction.",
          "reasoning": "The paper does not mention providing any code or repository link related to dataset construction or data preprocessing. The datasets used are existing standard benchmarks (ImageNet-1K, MS COCO, ADE20K) which were not created by the authors, and no new dataset is introduced by the authors that would require code for dataset creation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No dataset creation process described in the paper.",
          "reasoning": "The paper exclusively uses existing publicly available datasets (ImageNet-1K, MS COCO, ADE20K) for pre-training and evaluation. It does not introduce any new dataset nor describe any process for dataset creation, thus there is no dataset creation documentation provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1-rubric-0",
    "token_usage": {
      "prompt_tokens": 12348,
      "completion_tokens": 157,
      "total_tokens": 12505
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4, Datasets",
          "Reasoning": "The paper states they render images from 3D point models of DeepFashion3D to create 72 images at resolution 1024x1024 per model, indicating these images are generated via rendering by the authors. The DTU and BlendedMVS datasets consist of images captured by cameras in studio settings\u2014human-generated data through human-operated acquisition devices. Therefore, the data modality is images, with origins either captured by humans (DTU, BlendedMVS) or generated by the authors through rendering (DeepFashion3D images rendered by authors). No other new dataset is introduced."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1-rubric-1",
    "token_usage": {
      "prompt_tokens": 13148,
      "completion_tokens": 224,
      "total_tokens": 13372
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4, Datasets",
            "reasoning": "The paper uses public datasets (DeepFashion3D, DTU, BlendedMVS) and renders images from them for training; the 'annotation' in this context is dataset preparation which is automatically generated or rendered from existing 3D point sets without manual human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "No mention of annotation guidelines or human annotator instructions for the new dataset. The dataset images are rendered automatically from 3D models without human annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "No scoring rubrics or evaluation rubrics are provided or referenced related to dataset annotation since the data is generated automatically for training and evaluation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "No annotation examples are provided as there is no manual annotation or labeling process described for the new datasets; data preparation is automatic rendering from 3D models."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1-rubric-2",
    "token_usage": {
      "prompt_tokens": 14338,
      "completion_tokens": 329,
      "total_tokens": 14667
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple human experts performing quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that quality assurance was performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information suggesting quality assurance was conducted by non-expert annotators in any form."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that an AI model was used as a judge for quality assurance of the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any automated verification or algorithmic rule-based quality assurance process applied to the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces new datasets by rendering images from the DeepFashion3D models and uses existing datasets (DeepFashion3D, DTU, BlendedMVS). There is no documented process or discussion of quality assurance applied explicitly to these datasets. The paper focuses on the method and evaluations on these datasets rather than on dataset curation or QA. Therefore, no quality assurance process is applied or documented for the datasets introduced or utilized in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1-rubric-3",
    "token_usage": {
      "prompt_tokens": 13948,
      "completion_tokens": 498,
      "total_tokens": 14446
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4 Experiments (Datasets)",
          "reasoning": "The authors create new image data by rendering 72 multi-view images for each model from the DeepFashion3D dataset, which consists of 3D clothing models with open boundaries. This data is newly generated by humans through rendering, specifically mentioned in Section 4 describing how they prepared data for their experiments."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate generating any dataset purely by models or AI without input data; rather, the data is prepared by rendering existing 3D models or collected from existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of any data produced by translating content from another language by human translators in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of any data produced by automatic machine translation in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 Experiments (Datasets)",
          "reasoning": "The authors utilize existing datasets including DeepFashion3D, DTU, and BlendedMVS. The DeepFashion3D dataset exists as 3D point data and other datasets are established benchmarks. They collect these existing sources for evaluation without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4 Experiments (Datasets)",
          "reasoning": "The authors render multi-view images from the 3D point cloud data of DeepFashion3D using white backgrounds to generate training images, thus creating new data derived from existing 3D models via rendering process. This processing transforms original 3D points into a novel dataset of multi-view images used for UDF learning."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents the provenance of all datasets used, including descriptions of new rendered image data and usage of existing datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1-rubric-4",
    "token_usage": {
      "prompt_tokens": 14466,
      "completion_tokens": 401,
      "total_tokens": 14867
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Experiments, especially Section 4.1, 4.2 and Tables 1, 2",
          "reasoning": "The authors used the datasets DeepFashion3D, DTU, and BlendedMVS mainly for evaluation of their proposed method. These datasets were employed to benchmark and measure the performance of their two-stage UDF learning approach via quantitative metrics such as Chamfer distances and qualitative visual comparisons. There is no indication that these datasets were introduced by the authors as new datasets; rather, they are existing datasets used for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new datasets are introduced by the authors in the paper. All datasets (DeepFashion3D, DTU, BlendedMVS) are pre-existing and used solely for evaluation of the proposed method. Hence, no practical usage of newly proposed datasets is described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1-rubric-5",
    "token_usage": {
      "prompt_tokens": 15189,
      "completion_tokens": 560,
      "total_tokens": 15749
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout paper, particularly Sections: Abstract, Introduction, Method (Section 3), Experiments (Section 4)",
          "reasoning": "The new dataset introduced by the authors is the rendered DeepFashion3D dataset (called DF3D#Ours), from 3D clothing models, rendered into 72 images each with resolution 1024x1024, with white background. There is no indication that any human language text is present in the dataset or annotations. The paper and dataset content is presented in English. The datasets referenced or used are described or named in English with no mention of any other human languages present in the data. Therefore the dataset textual content can be considered monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset entries with programming or structured code content are introduced as datasets; only code for algorithm implementation is mentioned."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical expressions appear in the text of the paper, but the dataset entries themselves do not contain mathematical or logical notations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequence or non-human communication content is part of the datasets introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed languages are present in dataset entries."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language in the datasets is specified (English labels, naming, and generally images with no textual language content)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains multi-view rendered images (with possible associated camera parameters etc.), which can be considered data but not devoid of language since dataset annotations and documentation are in English."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1-rubric-6",
    "token_usage": {
      "prompt_tokens": 12407,
      "completion_tokens": 250,
      "total_tokens": 12657
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "The paper does not mention publishing code related to dataset construction or generation.",
          "reasoning": "Although the paper mentions that their code for the method is available at https://bitbucket.org/jkdeng/2sudf/, there is no mention of releasing code for constructing the datasets themselves, such as rendering images from the available 3D models or preparing dataset splits. The datasets used are mostly pre-existing (DeepFashion3D, DTU, BlendedMVS), and while they rendered images from DeepFashion3D models themselves, no code link or further information on dataset construction code is provided."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4: Experiments, Dataset subsection.",
          "reasoning": "The paper documents clearly how they prepare datasets for their experiments, for example, rendering 72 images per model in DeepFashion3D with white background for each garment model, mentions which datasets are used (DeepFashion3D, DTU, BlendedMVS), and explains dataset characteristics such as model types (open/watertight) and presence or absence of texture. This shows the dataset creation and preparation process is documented to a reasonable degree."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1017-rubric-0",
    "token_usage": {
      "prompt_tokens": 10456,
      "completion_tokens": 92,
      "total_tokens": 10548
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1, Datasets.",
          "Reasoning": "The paper states that the model is trained on the 88 training scenes of the DTU dataset, which consists of images captured from multiple views by humans or human-operated camera setups. These images serve as the input data for their new GeFu method's training."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1017-rubric-1",
    "token_usage": {
      "prompt_tokens": 11256,
      "completion_tokens": 230,
      "total_tokens": 11486
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 4 (Method) and Section 5.1 (Experiments)",
            "reasoning": "The paper describes the dataset as the DTU dataset divided into 88 training scenes and 16 test scenes. The model is trained end-to-end on RGB images from these training scenes without mentioning human annotators. The annotations for geometry and novel views are generated via learned models and automated training, not human labeling."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not describe any instructions provided to annotators, nor mention human annotation, so no annotation instructions are provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "No scoring rubrics or evaluation rubrics for human annotations are mentioned or described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "There is no indication of annotation examples or guidelines for human annotators; the data is generated via learning-based processes."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1017-rubric-2",
    "token_usage": {
      "prompt_tokens": 12446,
      "completion_tokens": 357,
      "total_tokens": 12803
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human expert performing quality assurance on dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts conducted quality assurance for the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no details about a single non-expert annotator performing quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence of multiple non-expert human annotators conducting quality assurance reported in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of any AI model to perform quality assurance on dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of an automated verification process for dataset quality assurance described in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets nor does it describe any quality assurance process applied to datasets. The datasets used are pre-existing (DTU, Real Forward-facing, NeRF Synthetic) and referenced accordingly without QA discussion."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1017-rubric-3",
    "token_usage": {
      "prompt_tokens": 12056,
      "completion_tokens": 415,
      "total_tokens": 12471
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the authors collected or created entirely new datasets from human contributors. Instead, they utilize existing datasets such as DTU, Real Forward-facing, and NeRF Synthetic for training and evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report the creation of any dataset generated entirely by AI or machine learning models. The data used for training and evaluation are from standard real-world datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any dataset was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any dataset was produced by machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used (DTU, Real Forward-facing, NeRF Synthetic) are publicly available and not newly collated or aggregated by the authors. The paper does not state any new data collection from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating new datasets derived or adapted from existing datasets. The work focuses on a model/approach rather than introducing new datasets created through modification or transformation of existing data."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. It only uses existing publicly available datasets for training and evaluation without documentation of new data creation or collection."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1017-rubric-4",
    "token_usage": {
      "prompt_tokens": 12574,
      "completion_tokens": 216,
      "total_tokens": 12790
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper utilizes existing public datasets such as DTU, Real Forward-facing, and NeRF Synthetic for training, testing, and evaluation. There is no indication that the authors introduce any new datasets. Therefore, no new dataset is introduced by the authors in this work, and no practical usage of a novel dataset is described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1017-rubric-5",
    "token_usage": {
      "prompt_tokens": 13297,
      "completion_tokens": 533,
      "total_tokens": 13830
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any datasets containing entries with more than two human languages. The datasets used are image datasets related to novel view synthesis with no mention of multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that the datasets include exactly two human languages. The datasets consist of image data and associated features for view synthesis tasks."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Entire paper, especially Section 5.1 Datasets",
          "reasoning": "The paper and all dataset descriptions are exclusively in English. The annotations, instructions, and textual content for model training and evaluation are presented only in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or evidence that the datasets contain entries in any non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper provides code online and discusses implementation details, the datasets themselves do not contain entries of programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notation is extensively used in the paper to describe methods, but the datasets do not contain mathematical expressions as data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are image datasets for multi-view synthesis and do not contain biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are involved or mentioned in the dataset entries."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset content is clearly documented as English, not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets do contain human language as annotations and descriptions in English, so this label does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1017-rubric-6",
    "token_usage": {
      "prompt_tokens": 10515,
      "completion_tokens": 139,
      "total_tokens": 10654
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract",
          "reasoning": "The paper explicitly mentions that code is available at a GitHub repository, but this code corresponds to the method implementation, not to any code specifically for constructing a new dataset. Furthermore, no new datasets are introduced by the authors, so no code related to dataset construction is provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Sections 5.1 and elsewhere",
          "reasoning": "The paper uses existing datasets (DTU, Real Forward-facing, NeRF Synthetic) for evaluation and does not introduce or create new datasets. Therefore, there is no dataset creation process documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1059-rubric-0",
    "token_usage": {
      "prompt_tokens": 11207,
      "completion_tokens": 257,
      "total_tokens": 11464
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 3.2, Section 3.3, Section 4.1",
          "Reasoning": "The authors introduce HOIDiffusion, a model generating realistic 3D hand-object interaction images conditioned on 3D geometric structures and text descriptions. The dataset consists of synthesized images with corresponding 3D ground-truth annotations, generated by the authors' two-stage pipeline combining GrabNet for grasp generation and a fine-tuned conditional diffusion model for image synthesis. The images are not human-captured but are generated by AI models as part of this new dataset introduced in the paper."
        },
        {
          "Modality": "time series",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2, Figure 2, Section 4.4",
          "Reasoning": "The paper describes generating hand-grasping trajectories by interpolating between initial and final hand poses predicted by GrabNet. These trajectories represent temporal sequences (time series) of hand poses during grasping, generated algorithmically via interpolation and model prediction, constituting part of the new dataset introduced for generating realistic 3D hand-object interaction data."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1059-rubric-1",
    "token_usage": {
      "prompt_tokens": 12007,
      "completion_tokens": 461,
      "total_tokens": 12468
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2: Hand Grasping Trajectory Generation",
            "reasoning": "The hand grasp poses are generated using the pretrained GrabNet model, which outputs hand grasp parameters conditioned on object models. This is a deterministic machine-based process, not human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper of instructions for this process.",
            "reasoning": "The paper describes the use of a pretrained model for grasp generation and interpolation, no manual annotation or instructions are provided for humans."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No rubric description related to this process in the paper.",
            "reasoning": "This is an automatic model-based generation process without manual scoring rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples provided in the paper for this automated grasp generation.",
            "reasoning": "The paper shows qualitative examples of generated grasps but does not provide examples in an annotation guideline context."
          }
        },
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.1: Implementation Details; Section 3.3: Hand-Object-Interaction image Synthesis",
            "reasoning": "The HOIDiffusion model is fine-tuned using a few human demonstrations from the DexYCB dataset, which contains high-quality images from human demonstrations with diverse backgrounds. Human experts collected and annotated this dataset."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.1: Implementation Details",
            "reasoning": "DexYCB is a benchmark with detailed annotations, implying the presence of annotation instructions to ensure high-quality labels and human demonstrations used for fine-tuning."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention in the paper of scoring rubrics or detailed annotation evaluation criteria for annotations in DexYCB or new dataset creation.",
            "reasoning": "The paper utilizes existing DexYCB human annotations and does not describe any rubric for annotations in their introduced generation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention of annotation examples in annotation guidelines.",
            "reasoning": "The paper does not provide or reference specific annotation examples as part of guidelines; rather it uses pre-existing datasets and automatic generation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1059-rubric-2",
    "token_usage": {
      "prompt_tokens": 13197,
      "completion_tokens": 419,
      "total_tokens": 13616
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert to validate the generated dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper about multiple human experts performing quality assurance on the generated dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple non-expert humans performed quality assurance of the data."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the generation pipeline uses pre-trained models and estimation models, there is no explicit mention that an AI model was used as a quality assurance judge for the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 and 3.3",
          "reasoning": "The quality of the generated data depends on automated processes including the use of the pretrained GrabNet model to generate hand grasp poses and rendering processes to extract annotations such as segmentation, masks, and depth maps. The framework relies on algorithmic methods and automated rendering to generate and annotate samples, constituting an automatic quality assurance process rather than manual validation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is some level of automated verification and use of pretrained models to generate and annotate data, indicating that some form of QA process exists."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1059-rubric-3",
    "token_usage": {
      "prompt_tokens": 12807,
      "completion_tokens": 514,
      "total_tokens": 13321
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention collecting any new datasets created entirely from scratch by human contributors. Instead, it builds upon existing datasets and models."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Sections 3.2 and 3.3",
          "reasoning": "The authors generate new 3D hand-object interaction images and corresponding 3D geometry using a two-stage pipeline: first, they generate hand grasping poses using a pretrained model (GrabNet); then, they synthesize images conditionally with a fine-tuned diffusion model based on these generated 3D conditions and text prompts. This process results in new data created by AI or machine learning models without direct transformation of existing image data, constituting new data generated by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was produced by human translating content from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation being used to generate or transform the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The generated dataset is not simply collected or aggregated from existing sources without modification; it is synthetically generated using models conditioned on existing datasets and models."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Sections 3.2, 3.3, and 4.1",
          "reasoning": "The generated data is based on existing sources, such as object models from existing datasets, the DexYCB dataset for training, and pretrained models like GrabNet and Stable Diffusion. The authors apply transformations and modifications\u2014including generating new grasp poses, interpolating trajectories, and fine-tuning diffusion models on few demonstrations\u2014to produce the new dataset. Therefore, the data is derived from existing sources with modifications and adaptations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The method of data generation is clearly described in the paper; thus, this does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1059-rubric-4",
    "token_usage": {
      "prompt_tokens": 13325,
      "completion_tokens": 506,
      "total_tokens": 13831
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the generated hand-object interaction dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence that the newly generated dataset is used to train a model from scratch; rather, the approach involves fine-tuning pretrained diffusion models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.3, Section 4.1, Section 4.4",
          "reasoning": "The HOIDiffusion model is fine-tuned on the DexYCB dataset and augmented data generated by their pipeline in a supervised manner (Section 3.3). Additionally, the generated dataset is used for supervised fine-tuning to improve object 6D pose estimation models as a downstream application (Section 4.4). This shows practical supervised fine-tuning usage of the dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any usage of the dataset in reinforcement learning-based post-training methods such as RLHF."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper evaluates the model's performance using several metrics, it does not use the generated dataset exclusively for evaluation or benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the dataset is primarily used for analyzing trends or characteristics; rather, it is used for generation and downstream training."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset serves as a knowledge base to augment models through retrieval or similar."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly demonstrates practical usage of the generated dataset for supervised fine-tuning and downstream task improvement."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1059-rubric-5",
    "token_usage": {
      "prompt_tokens": 14048,
      "completion_tokens": 595,
      "total_tokens": 14643
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset entries that contain more than two human languages. The presented data and annotations are visual and 3D geometric information, rather than multilingual textual data."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence from the paper indicates the dataset includes exactly two human languages in its entries."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, e.g., Abstract and Sections 3.3, 4.1, and 4.2",
          "reasoning": "The dataset introduced involves images and corresponding 3D geometries along with associated text descriptions used for conditioning and training. The text descriptions and prompts are given in English, generated by ChatGPT and detailed with the LLaVA model, indicating that the data entries' linguistic content is exclusively in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of any non-English language in the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or code-like data entries are part of the dataset itself. The dataset is composed of images, 3D geometric data, segmentation maps, normal maps, and text descriptions, but no code data entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper describes mathematical formulas to explain their diffusion model and training, the dataset entries themselves do not contain mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises human hand and object images and 3D pose data; there is no biological sequence or non-human communication data included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly documents the language of the text data entries as English, so the language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes text descriptions and prompts in English as part of the data, so it does contain language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1059-rubric-6",
    "token_usage": {
      "prompt_tokens": 11266,
      "completion_tokens": 198,
      "total_tokens": 11464
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and throughout the paper",
          "reasoning": "The paper mentions a project page but does not provide any explicit link or statement indicating that the code used to generate the new 3D hand-object interaction dataset is publicly available. There is no mention of a GitHub repository or other code release. Therefore, we cannot confirm that the code for constructing the dataset is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and 4",
          "reasoning": "The paper provides a detailed description of the dataset creation process in Section 3 (Method) and Section 4 (Experiments), including the two-stage pipeline involving GrabNet for hand grasp generation and the diffusion-based image synthesis conditioned on 3D geometric structures and text descriptions. The authors explain how the data is generated, including specifics about conditions, fine-tuning, and regularization. Thus, the process is well documented within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1076-rubric-0",
    "token_usage": {
      "prompt_tokens": 11829,
      "completion_tokens": 132,
      "total_tokens": 11961
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 3; Table 1",
          "Reasoning": "The HardMo dataset introduced in the paper consists of 7 million images collected from 1,500 sequences of dance and martial arts categories. These images are real-world frames extracted from videos collected online, hence human-generated through human performances captured by cameras, as described in the automatic annotation pipeline (Section 4). The dataset provides precise annotations including 2D keypoints and SMPL parameters, but the raw data modality is image data originating from human recordings."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1076-rubric-1",
    "token_usage": {
      "prompt_tokens": 12629,
      "completion_tokens": 305,
      "total_tokens": 12934
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4 (Automatic Annotation Pipeline), Section 4.2 (Hardcase Annotation)",
            "reasoning": "The paper describes the annotation process as an automatic pipeline involving video crawling, person detection (YOLOv8), 2D keypoint estimation (RTM), initial 3D SMPL parameter prediction (4DHumans), followed by angle-based hardcase mining and learning-based optimization to refine annotations. No indication of human manual annotation for the new HardMo dataset is given, indicating an automatic deterministic or model-driven process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4 (Automatic Annotation Pipeline)",
            "reasoning": "The paper does not mention any detailed manual annotation instructions provided for annotators since the process is automatic using pretrained models and algorithms."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.2 (Hardcase Annotation)",
            "reasoning": "There is no mention of scoring rubrics or scoring criteria applied by human annotators. The selection criteria for hardcases are algorithmic (angle-based thresholds), not rubric-based human evaluation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4 and Fig. 2 (Overview of HardMo)",
            "reasoning": "While the paper shows examples of dataset images, it does not provide annotation guideline examples for annotators. Since annotation is automatic, no annotation examples or instructions are given for human annotators."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1076-rubric-2",
    "token_usage": {
      "prompt_tokens": 13819,
      "completion_tokens": 387,
      "total_tokens": 14206
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert or specialized annotator on the dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description in the paper of multiple human experts manually performing quality assurance or validation of the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any single non-expert human performing quality assurance on the dataset annotation."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no evidence that multiple non-expert human annotators were involved in QA of the dataset."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 4.2 Hardcase Annotation and Hardcase Optimization",
          "reasoning": "The annotation pipeline uses AI models (e.g., 4DHumans [10], RTM-pose) to estimate 2D keypoints and SMPL parameters initially. Furthermore, a learning-based method is employed to refine and optimize SMPL annotations for hardcase samples. Thus, AI models actively perform annotation and verification, functioning as a form of quality assurance by judging and optimizing labels."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.2 Hardcase Annotation and Hardcase Mining",
          "reasoning": "An automatic angle-based hardcase mining algorithm is used to identify hardcase samples by computing joint angles from keypoints and confidence scores. This rule-based algorithm automatically filters samples requiring further optimization, representing an automated verification process contributing to QA."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes a detailed automatic annotation and optimization pipeline involving AI models and algorithmic hardcase mining; thus, some form of QA is documented and applied."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1076-rubric-3",
    "token_usage": {
      "prompt_tokens": 13429,
      "completion_tokens": 441,
      "total_tokens": 13870
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset images are collected from internet videos and not created entirely from scratch by human contributors; annotations are automatic or model-assisted rather than manual creation of novel data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The image data itself is collected from real-world videos, not generated purely by AI or models; only annotations are predicted or refined via models but the data content is not model-generated."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that any data was generated by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of machine translation used to create or transform data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3 and 4 (especially Abstract and Section 4)",
          "reasoning": "HardMo is formed by collecting (crawling) large-scale videos from the Internet covering dance and martial arts sequences, thus aggregating existing sources (online videos) without creating new original images."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Sections 3 and 4 (Abstract, Section 4)",
          "reasoning": "Annotations (2D keypoints and 3D SMPL parameters) are derived/predicted using models (RTM-pose, 4DHumans) and refined further by proposed optimization methods. The original data from videos are transformed and annotated to generate supervised labels, constituting a derived data process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data collection and annotation pipeline, so the origin is specified."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1076-rubric-4",
    "token_usage": {
      "prompt_tokens": 13947,
      "completion_tokens": 301,
      "total_tokens": 14248
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5.1",
          "reasoning": "The dataset HardMo is used to train models, including training HMR from scratch on HardMo, as shown in Section 5.1 where HardMo-HMR trained solely on HardMo outperforms models trained on other datasets, demonstrating its use for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.1 and 5.2",
          "reasoning": "HardMo and its subsets HardMo-Foot and HardMo-Hand are used to fine-tune pre-trained models, such as 4DHumans, improving performance on hardcase problems, as detailed in experiments and qualitative comparisons."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1 and 5.2",
          "reasoning": "The HardMo dataset and its subsets are used as benchmarks to evaluate and measure performance of various models on challenging poses and hardcase problems."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1076-rubric-5",
    "token_usage": {
      "prompt_tokens": 14670,
      "completion_tokens": 460,
      "total_tokens": 15130
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The HardMo datasets consist of images with motion capture annotations; there is no indication that the data include any human language content, let alone multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper suggests that the dataset contains entries with exactly two human languages. The dataset is image-based motion capture data."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of images and their SMPL annotations, with no textual entries or English language content described."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English language content is indicated in the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the paper displays mathematical notation and pseudo-code style equations to describe the annotation and mining algorithms, the dataset itself does not include programming or code content entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 4.2 (Hardcase Annotation), with Equations 1 to 7 describing vector calculations and optimization loss functions.",
          "reasoning": "The paper details mathematical expressions and formulas related to angle calculations for foot and hand rotations and the optimization loss used for refining annotations. While these relate to the methodology, the presence of such formal notation is evident in describing the dataset annotations and processing."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human motion capture data but no biological sequences like DNA or animal communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset is not relevant or specified, but the dataset is image and annotation-based with no implied language ambiguity."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The dataset primarily consists of images and SMPL motion capture annotations, which do not contain any human language text or symbolic language entries. Thus, the dataset does not include any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1076-rubric-6",
    "token_usage": {
      "prompt_tokens": 11888,
      "completion_tokens": 180,
      "total_tokens": 12068
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper.",
          "reasoning": "The paper does not provide any URLs or explicit statements about publicly available code for the dataset construction pipeline or data processing. Although the dataset is stated to be publicly available at a project webpage, there is no mention of the code used for data collection, annotation, or hardcase mining being shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and 4",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, describing the automatic annotation pipeline, hardcase mining algorithm, and optimization methods for SMPL annotations. Sections 3 (HardMo Dataset) and 4 (Automatic Annotation Pipeline) thoroughly explain the data collection, filtering criteria, annotation steps, and processing methods, indicating good transparency and completeness in describing how the dataset was constructed."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1096-rubric-0",
    "token_usage": {
      "prompt_tokens": 9396,
      "completion_tokens": 100,
      "total_tokens": 9496
    },
    "response": {
      "sources": [
        {
          "Modality": "time series",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Datasets",
          "Reasoning": "The paper uses ETH/UCY and Stanford Drone Dataset (SDD), which are established pedestrian trajectory datasets with observed historical coordinates and corresponding future trajectories. However, these datasets are pre-existing benchmarks, not introduced by the authors in this paper, so they are not considered new datasets introduced in the paper."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1096-rubric-1",
    "token_usage": {
      "prompt_tokens": 10196,
      "completion_tokens": 273,
      "total_tokens": 10469
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1, 4.2, and 5.1",
            "reasoning": "The paper introduces a novel social graph construction method using a collision-aware kernel function and graph convolutional networks for higher-order relational reasoning applied to pedestrian trajectory prediction. The dataset used for experiments (ETH/UCY and Stanford Drone Dataset) are existing standard datasets. There is no mention of any new dataset being collected or annotated manually; the annotations and data processing are performed via automated graph-based model computations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "Since the annotations (social interaction graphs) are generated automatically by the proposed kernel function and GCN methodology rather than humans annotating data, there are no instructions provided for manual annotation in the paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "Because the annotation process is an automatic computational method, the paper does not provide any scoring rubrics or criteria that would be relevant for human annotation tasks."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There are no annotation examples given because the social graph construction is automated by a collision-aware kernel function and graph powers, not by human annotators following example annotations."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1096-rubric-2",
    "token_usage": {
      "prompt_tokens": 11386,
      "completion_tokens": 309,
      "total_tokens": 11695
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance performed by a single human expert on any dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human expert annotators performing quality assurance or validation on any dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The manuscript does not describe any QA performed by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains no information about multiple non-expert humans performing quality assurance or validation of dataset content or annotations."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the paper states that an AI model was used to perform quality assurance or judge dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper proposes algorithmic methods (like collision-aware kernel functions), there is no explicit discussion on automatic verification or algorithmic QA of the datasets used."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses two pre-existing benchmark datasets (ETH/UCY and Stanford Drone Dataset) and does not introduce any new dataset. Furthermore, it does not describe any quality assurance processes for datasets, new or existing, nor any validation or annotation procedures. Therefore, no QA processes are documented or performed in this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1096-rubric-3",
    "token_usage": {
      "prompt_tokens": 10996,
      "completion_tokens": 280,
      "total_tokens": 11276
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.1 Datasets",
          "reasoning": "The authors utilize widely-used existing benchmark datasets (ETH/UCY and Stanford Drone Dataset) for evaluation. These datasets consist of pedestrian trajectory data collected from prior works and are used as is without mention of modification or adaptation. Therefore, the data used can be considered collated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data used in the paper is specified and documented as existing benchmark datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1096-rubric-4",
    "token_usage": {
      "prompt_tokens": 11514,
      "completion_tokens": 203,
      "total_tokens": 11717
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses publicly available standard benchmark datasets (ETH/UCY and Stanford Drone Dataset) for evaluation and analysis, but does not introduce any new datasets. Therefore, there is no new dataset introduced by the authors, and no practical usage of a proposed dataset is described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1096-rubric-5",
    "token_usage": {
      "prompt_tokens": 12237,
      "completion_tokens": 565,
      "total_tokens": 12802
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper only discusses datasets with pedestrian trajectory data without mention of multiple human languages. No indication that entries include more than two languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not indicate the dataset includes exactly two human languages; it focuses on trajectory coordinates and social dynamics."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains trajectory data and coordinates, not linguistic content in English or any natural language text."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No evidence of dataset entries containing natural language text in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of pedestrian trajectory coordinates, not programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3.1 (Trajectory Prediction), Section 3.2 (Graph Convolutional Networks), and Section 4 (HighGraph)",
          "reasoning": "The dataset entries contain trajectory coordinates represented as numerical vectors and include formal definitions with equations modeling adjacency matrices, kernels, and graph convolution operations\u2014indicating presence of mathematical and logical symbolic representations in the dataset processing."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The data represents human pedestrian movement and trajectories; no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not include any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language or symbolic system used is clearly described as trajectory coordinates and adjacency matrices; the dataset is well documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains numerical and symbolic data related to trajectories and graphs, representing mathematical notations; hence not devoid of any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1096-rubric-6",
    "token_usage": {
      "prompt_tokens": 9455,
      "completion_tokens": 143,
      "total_tokens": 9598
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section mentions code availability related to dataset construction.",
          "reasoning": "The paper does not mention releasing code or scripts related to dataset construction, collection, or preprocessing. The datasets used, ETH/UCY and Stanford Drone Dataset (SDD), are established benchmarks, and no new dataset construction code is provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 5.1 Datasets and throughout the paper.",
          "reasoning": "The paper uses existing public datasets (ETH/UCY and SDD) and does not introduce any new dataset. There is no documentation or description about dataset construction or new dataset creation in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-111-rubric-0",
    "token_usage": {
      "prompt_tokens": 15618,
      "completion_tokens": 127,
      "total_tokens": 15745
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets and throughout Section 4 Experiments",
          "Reasoning": "The authors use the VTAB benchmark, which consists of images captured by humans or specialized equipment, and the FGVC benchmark composed of various fine-grained image classification datasets such as CUB-200-2011, NABirds, Oxford Flowers, Stanford Dogs, and Stanford Cars. These datasets are pre-existing and not introduced by the authors; no new dataset is introduced by the authors themselves in the paper."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-111-rubric-1",
    "token_usage": {
      "prompt_tokens": 16418,
      "completion_tokens": 305,
      "total_tokens": 16723
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Datasets and Section 4.2 Experimental settings",
            "reasoning": "The paper introduces experiments on the Visual Task Adaptation Benchmark (VTAB) and Fine-grained Visual Classification (FGVC) datasets as new datasets in this work. However, VTAB is cited as [62], indicating it is pre-existing. FGVC is compiled following Jia et al. [30], also pre-existing. The paper does not introduce any new dataset explicitly; instead, it focuses on an improved adapter method (Adapter+). Since no new dataset is introduced in the paper, annotations per se were not performed by human annotators; rather, the datasets are standard benchmarks and splits used, implying evaluation is through automatic process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No section specifies annotation instructions for datasets introduced by this paper, as no new dataset is introduced.",
            "reasoning": "The paper does not introduce any new dataset requiring manual annotation or instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No section specifies scoring rubrics for annotations, as no new dataset is introduced.",
            "reasoning": "No annotation scoring rubrics are described since no new dataset is introduced with manual annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No section describes annotation examples for new datasets.",
            "reasoning": "There are no examples of annotations because no new dataset or annotation task is introduced by the paper."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-111-rubric-2",
    "token_usage": {
      "prompt_tokens": 17608,
      "completion_tokens": 354,
      "total_tokens": 17962
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets so there is no mention of quality assurance conducted by a single human expert for dataset annotation or validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets are introduced by the authors; hence, there is no indication of quality assurance by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention dataset annotation or quality assurance by a single non-expert human."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about multiple human non-experts performing quality assurance for any newly introduced dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any quality assurance performed by AI models on datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or algorithmic QA processes for datasets are described in the paper, as no new dataset is introduced."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets and thus does not describe or apply any quality assurance processes for dataset annotations or content."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-111-rubric-3",
    "token_usage": {
      "prompt_tokens": 17218,
      "completion_tokens": 356,
      "total_tokens": 17574
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the creation of any new dataset generated entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets generated purely by AI or machine learning models as new data are introduced in the paper."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any datasets created by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any datasets generated by machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper references existing datasets such as VTAB and FGVC; however, it does not introduce any new datasets collated or aggregated from multiple existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing public benchmark datasets for evaluation without creating derived datasets; there is no mention of modifying or adapting existing datasets to create new ones."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets or document the source or method of generation of new data; evaluations are performed solely on pre-existing datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-111-rubric-4",
    "token_usage": {
      "prompt_tokens": 17736,
      "completion_tokens": 308,
      "total_tokens": 18044
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.4",
          "reasoning": "The authors use the VTAB and FGVC datasets to fine-tune a pre-trained Vision Transformer (ViT-B/16) model by tuning adapter modules and the classifier, demonstrating parameter-efficient transfer learning. This involves supervised fine-tuning of the frozen backbone with adapter modules on target tasks."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.4, and 4.5",
          "reasoning": "The VTAB and FGVC datasets are used to evaluate and benchmark the performance of the Adapter+ and other adaptation methods on various downstream tasks, including average accuracy and robustness assessments."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3 and 4.5",
          "reasoning": "The authors carry out systematic studies analyzing adapter positions, inner structures, initialization, and normalization effects using these datasets to understand performance impacts and design effective adapter configurations."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-111-rubric-5",
    "token_usage": {
      "prompt_tokens": 18459,
      "completion_tokens": 547,
      "total_tokens": 19006
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper evaluates Adapter+ on multiple image datasets from different domains but all datasets involve images and task adaptation in vision, not multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper that the datasets introduced involve exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used (VTAB and FGVC) are visual datasets without explicit linguistic content labeled as English-only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset introduced consists of entries in exactly one non-English human language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "The paper discusses adapter modules implemented in deep learning models (Section 3) but does not introduce any dataset containing code or programming languages.",
          "reasoning": "The datasets introduced, namely VTAB and FGVC benchmarks, are image classification datasets without code entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "The paper uses mathematical notation to describe models in Section 3, but does not introduce a dataset containing mathematical or logical expressions.",
          "reasoning": "Mathematical notation is present in the paper to describe methods but is not part of any new dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets with biological or non-human communication content are introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional language datasets are introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages or symbols involved in the datasets are known or clearly visual image domains; the language attribute is not unspecified."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The introduced datasets (VTAB and FGVC) are image classification datasets containing visual data without linguistic entries; therefore, they do not contain entries with any human or programming language content."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-111-rubric-6",
    "token_usage": {
      "prompt_tokens": 15677,
      "completion_tokens": 175,
      "total_tokens": 15852
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or statement in the paper provides a link or mentions the availability of code related to dataset construction or processing.",
          "reasoning": "The paper extensively discusses experiments on existing datasets such as VTAB and FGVC but does not introduce any new datasets nor does it mention providing code for data collection, preprocessing, or generation. There is also no mention of a code repository or supplementary material related to dataset creation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No sections or paragraphs in the paper describe the dataset creation process for any new dataset.",
          "reasoning": "The paper uses existing datasets (VTAB and FGVC) for evaluation without introducing new datasets. The descriptions provided focus on the usage and experimental protocols rather than any dataset creation or construction process, thus no documentation on new dataset creation is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1141-rubric-0",
    "token_usage": {
      "prompt_tokens": 13462,
      "completion_tokens": 168,
      "total_tokens": 13630
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 (Dataset)",
          "Reasoning": "The paper evaluates the proposed method on the nuScenes dataset, which consists of LiDAR point cloud data captured by LiDAR sensors (signal/sensor modality) in real autonomous driving scenarios collected by humans. Thus, the LiDAR data modality is human generated from sensor recordings."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 (Dataset)",
          "Reasoning": "The nuScenes dataset also provides synchronized multi-view RGB images captured by six calibrated cameras, recorded in real driving scenes. These images are human generated via camera sensors operated in the real environment."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1141-rubric-1",
    "token_usage": {
      "prompt_tokens": 14262,
      "completion_tokens": 231,
      "total_tokens": 14493
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Dataset",
            "reasoning": "The paper uses the nuScenes dataset to evaluate their method, which is a publicly available, pre-existing dataset. There is no introduction or creation of a new dataset by the authors; hence, the annotations (3D bounding boxes) are part of the standard nuScenes benchmark annotations, collected and verified by the original nuScenes creators through a rigorous process likely involving multiple human experts. The paper does not mention any new dataset or custom annotation effort."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No mention in paper",
            "reasoning": "Since the paper does not introduce a new dataset, it does not provide any annotation instructions specific to dataset creation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention in paper",
            "reasoning": "No scoring rubrics or annotation criteria are detailed for any dataset annotations in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No mention in paper",
            "reasoning": "No annotation examples of ground truth labels or guidance are provided in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1141-rubric-2",
    "token_usage": {
      "prompt_tokens": 15452,
      "completion_tokens": 292,
      "total_tokens": 15744
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human annotation or quality assurance performed by a single human expert for any new dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance or annotation process conducted by multiple human experts for any new dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of dataset annotation or quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description or indication of quality assurance by multiple human non-experts is given in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using any AI model to perform quality assurance or validation of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any automatic verification or quality assurance process applied to any dataset annotations in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses the existing nuScenes dataset for evaluation and does not introduce any new datasets. Furthermore, it does not document any quality assurance process related to data annotation or validation. Therefore, no quality assurance process is applicable to a new dataset, as none is introduced."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1141-rubric-3",
    "token_usage": {
      "prompt_tokens": 15062,
      "completion_tokens": 400,
      "total_tokens": 15462
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental Setup - Dataset",
          "reasoning": "The paper states that the evaluation is conducted on the nuScenes benchmark dataset, which is a large-scale autonomous driving dataset. The data used are existing nuScenes scenes consisting of LiDAR point cloud and multi-view image data, captured and annotated previously. The authors do not indicate that they created any new dataset; rather, they conduct experiments using this existing dataset."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3 Methodology and 4 Experiments",
          "reasoning": "The authors generate derived data representations such as BEV features and instance-level features from the existing nuScenes data by applying their proposed encoding, transformer fusion modules (HSF and IGF), and other processing steps. These features are transformations and adaptations of existing raw data rather than new raw data. Hence, the processed features used as inputs for further learning can be understood as derived data based on existing sources with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly mentions the usage of the nuScenes dataset and describes the processing steps; thus, the data origin is specified and documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1141-rubric-4",
    "token_usage": {
      "prompt_tokens": 15580,
      "completion_tokens": 200,
      "total_tokens": 15780
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. It exclusively uses the existing nuScenes dataset for training and evaluation purposes. There is no description or demonstration of any new dataset created by the authors; therefore, no practical usage of a new dataset is described in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1141-rubric-5",
    "token_usage": {
      "prompt_tokens": 16303,
      "completion_tokens": 508,
      "total_tokens": 16811
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only mentions the nuScenes dataset, which contains data captured in a single linguistic context (English) and does not include multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that the dataset includes exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 (Experimental Setup), Dataset",
          "reasoning": "The dataset used is nuScenes, a dataset from autonomous driving research with annotations and metadata provided in English only. The paper text and references are all in English, and there is no mention of any other language data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English monolingual dataset is introduced or used."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper refers to code availability and implementation frameworks, the dataset itself contains no programming or structured programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes 3D point cloud and image data; mathematical notation is used in the methodology description but is not part of the dataset entries themselves."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain any biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional language data is part of the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is clearly English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains annotated language content (English) and therefore this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1141-rubric-6",
    "token_usage": {
      "prompt_tokens": 13521,
      "completion_tokens": 180,
      "total_tokens": 13701
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "General paper content, Abstract and Experiment section",
          "reasoning": "The paper provides a link to code for the IS-FUSION model (https://github.com/yinjunbo/IS-Fusion) related to the method implementation. However, the paper does not mention or provide any code or tools specific to the construction or creation of datasets. The only dataset used is nuScenes, which is a pre-existing public dataset. Therefore, there is no code related to dataset construction provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Entire paper",
          "reasoning": "The paper does not describe or document the process of dataset creation. It uses the standard nuScenes dataset for experiments, which is a public pre-existing dataset. There is no new dataset introduced or any information about dataset creation or collection process documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-115-rubric-0",
    "token_usage": {
      "prompt_tokens": 17556,
      "completion_tokens": 146,
      "total_tokens": 17702
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 and Tables 1 and 2",
          "Reasoning": "The paper evaluates the proposed method on established medical imaging datasets (ACDC and PROMISE12) for medical image segmentation tasks. These datasets contain medical images (MRI and CT scans) which are collected and annotated by humans (medical experts). The paper does not introduce any new dataset but uses these existing datasets for evaluation. There is no indication that the authors created a new dataset. Since the instruction asks to list new datasets introduced by the authors only, and none is introduced, there are no new datasets to report."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-115-rubric-1",
    "token_usage": {
      "prompt_tokens": 18356,
      "completion_tokens": 207,
      "total_tokens": 18563
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3 and Figure 2",
            "reasoning": "The paper describes an automatic process applying Adaptive Bidirectional Displacement (ABD) using model prediction confidences and patch displacement operations to generate augmented training samples automatically during training without human annotators involved."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit annotation instructions described in the paper",
            "reasoning": "The paper does not mention or describe any human annotation instructions provided, as the approach generates training samples algorithmically."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No rubric description in the paper",
            "reasoning": "There is no mention of scoring rubrics or guidelines for annotations, since annotation is not manual but automatic."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples provided for annotation guidelines",
            "reasoning": "The paper does not include examples related to annotation instructions or rubrics, as the data augmentation is done automatically by the proposed method."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-115-rubric-2",
    "token_usage": {
      "prompt_tokens": 19546,
      "completion_tokens": 285,
      "total_tokens": 19831
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance of dataset annotations was performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts conducting quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper lacks any indication that a single non-expert performed quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report that multiple non-expert annotators were involved in quality assurance of the data."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of any AI model acting as a judge for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses algorithmic operations such as adaptive bidirectional displacement, it does not describe any automated verification process for validating dataset annotations or content as quality assurance."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses existing datasets (e.g., ACDC, PROMISE12) for evaluation and does not introduce any new datasets. Therefore, no quality assurance process on new dataset annotations is described."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-115-rubric-3",
    "token_usage": {
      "prompt_tokens": 19156,
      "completion_tokens": 422,
      "total_tokens": 19578
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly mention the creation or collection of any new dataset that is originally generated by human contributors from scratch. Instead, it uses existing public medical imaging datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper proposes a method to generate new samples during training by adaptive bidirectional displacement of image patches, these are synthetic augmentations created on existing datasets for training rather than new datasets released. Therefore, no new dataset is introduced that is generated purely from models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention of data translated by humans from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any machine translation of datasets from other languages."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses well-known existing public datasets such as ACDC and PROMISE12 for evaluation but does not describe compilation or aggregation of existing data to form a new dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the method generates transformed images (via adaptive bidirectional displacement) as training samples, these transformed samples are not presented or released as novel datasets. The paper focuses on methods applied on existing datasets rather than releasing modified datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. The data used are existing public datasets, and there is no specification or documentation of any new dataset generation."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-115-rubric-4",
    "token_usage": {
      "prompt_tokens": 19674,
      "completion_tokens": 243,
      "total_tokens": 19917
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3, Table 1 and 2",
          "reasoning": "The datasets mentioned (ACDC and PROMISE12) are used exclusively for evaluation and benchmarking semi-supervised medical image segmentation methods, as evidenced by extensive experiment results and performance comparisons in Section 4.3."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new datasets are introduced by the authors in the paper. The datasets used for experiments are pre-existing (e.g., ACDC, PROMISE12), so no practical usage of a new dataset is demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-115-rubric-5",
    "token_usage": {
      "prompt_tokens": 20397,
      "completion_tokens": 583,
      "total_tokens": 20980
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset containing entries with more than two human languages. The datasets mentioned (e.g., ACDC, PROMISE12) are medical image datasets without language content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of datasets containing exactly two languages in the entries. The datasets focus on medical images and annotations, which are not language-based."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper (e.g., Abstract, Sections 1 and 3), all textual content and dataset descriptions are in English.",
          "reasoning": "The datasets used and discussed in the paper (ACDC dataset, PROMISE12) are medical image datasets, and all descriptive and annotation content referred to in the paper is in English, indicating that the dataset entries contain English content only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset entries in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses methods and algorithms, it does not introduce any new dataset containing programming or code-related entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes mathematical expressions and formulas as part of method descriptions, but these are not part of any newly introduced dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the datasets are medical imaging datasets, they do not include biological sequences or non-human communication system data such as DNA or chemical signaling."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any dataset entries with fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language content is clearly English, so the language of dataset entries is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets discussed contain linguistic content (English labels and annotations), so it is not the case that the dataset does not contain any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-115-rubric-6",
    "token_usage": {
      "prompt_tokens": 17615,
      "completion_tokens": 157,
      "total_tokens": 17772
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 1",
          "reasoning": "The paper provides a link to the source code for their method at https://github.com/chy-upc/ABD, but this code appears to be for the method implementation rather than for dataset construction or dataset preprocessing. There is no indication that code for constructing any dataset is provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Throughout the paper (Section 4.1 and dataset descriptions)",
          "reasoning": "The paper uses existing public medical imaging datasets such as ACDC and PROMISE12 for evaluation. It does not introduce any new datasets. There is no documentation related to dataset creation or construction process since no new dataset is introduced by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1159-rubric-0",
    "token_usage": {
      "prompt_tokens": 10052,
      "completion_tokens": 202,
      "total_tokens": 10254
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3: Method (Stage 1 and Stage 2), Section 4.1: Experimental Setup (Datasets)",
          "Reasoning": "The new dataset introduced by the authors for their BEV autoencoder training and RGB-BEV alignment pipeline consists of bird's eye view (BEV) segmentation maps and monocular RGB images derived from large-scale autonomous driving datasets (nuScenes and Argoverse). These datasets involve human-collected real-world images and their corresponding semantic BEV segmentation maps. The paper explicitly states using nuScenes and Argoverse datasets for experiments and describes processing steps involving BEV segmentation maps and RGB images. Although these datasets are previously existing ones, the paper does not introduce a new dataset but uses these existing datasets as the source of training data for their novel method. Therefore, no explicitly new dataset is introduced by the authors in the paper."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1159-rubric-1",
    "token_usage": {
      "prompt_tokens": 10852,
      "completion_tokens": 243,
      "total_tokens": 11095
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Experimental Setup, Datasets subsection",
            "reasoning": "The paper uses the nuScenes and Argoverse datasets, which are well-known public datasets; the annotation data for BEV segmentation maps are generated through a ground truth generation process referenced from PON [31], indicating an automatic or pre-existing method for creating labels, not manual human annotation performed specifically for this paper."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "The paper does not provide any detailed annotation instructions or guidelines for annotators concerning the new dataset. The datasets used are existing ones with generated ground truth, not manually annotated anew in this work."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "No scoring rubrics or explicit evaluation criteria for annotation are described in the paper as part of data annotation guidelines."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "The paper does not provide examples of annotations or annotation guidelines since no new manual annotation was introduced."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1159-rubric-2",
    "token_usage": {
      "prompt_tokens": 12042,
      "completion_tokens": 277,
      "total_tokens": 12319
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process involving a single human expert annotator for the datasets used."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information or mention about quality assurance being performed by multiple human experts for the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state any QA conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple non-expert annotators were involved in dataset quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance was not indicated to be performed by any AI model."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe automated verification or algorithmic rule-based quality assurance applied to the datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses pre-existing datasets (nuScenes and Argoverse) and does not introduce any new datasets. It also does not document any quality assurance process for dataset annotations or content. Hence, no QA process is applied or documented for any new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1159-rubric-3",
    "token_usage": {
      "prompt_tokens": 11652,
      "completion_tokens": 440,
      "total_tokens": 12092
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any new datasets were created from scratch by human contributors. Instead, it uses existing datasets such as nuScenes and Argoverse for training and evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that new datasets were generated entirely by AI or machine learning models as original data for training or evaluation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention translation from other languages, human or otherwise, as part of the data generation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided about the use of machine translation in the data preparation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 (Experimental Setup)",
          "reasoning": "The datasets used, nuScenes and Argoverse, are existing large-scale datasets collected by others. The paper employs these datasets for training and evaluation without generating new raw data, i.e., it utilizes collated data from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 (Experimental Setup), Section 3.1 (Coordinate Transformation)",
          "reasoning": "The authors derive targets by transforming BEV segmentation maps from Cartesian to polar coordinates as stated in Section 3.1, and process ground truth maps following prior works such as PON [31]. This indicates modifications and transformations of existing data to prepare training targets, thus constituting derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies the data sources and the data preparation steps sufficiently."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1159-rubric-4",
    "token_usage": {
      "prompt_tokens": 12170,
      "completion_tokens": 214,
      "total_tokens": 12384
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper mentions use of existing datasets nuScenes and Argoverse for training, validation, and evaluation, but does not introduce any new datasets. The new contributions relate to method and model architecture, not to a novel dataset. Therefore, no new dataset introduced by the authors is described or used in any part of the machine learning pipeline."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1159-rubric-5",
    "token_usage": {
      "prompt_tokens": 12893,
      "completion_tokens": 580,
      "total_tokens": 13473
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset entries containing more than two human languages. The datasets used (nuScenes and Argoverse) relate to autonomous driving and semantic maps, which are not language-based data."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced or used do not consist of exactly two human languages; no mention of bilingual language content is made."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental Setup, Datasets",
          "reasoning": "The paper uses the nuScenes and Argoverse datasets, which are collected and annotated in English as standard for autonomous driving datasets. All accompanying text, labels, and documentation are in English, and there is no indication of other human languages in the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication or mention of any dataset entries that are purely in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes code and programming content in describing methods, there is no new dataset introduced containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notations such as equations and losses appear in the paper, but these are part of the method description; the datasets themselves do not contain entries of mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets involve autonomous driving and do not include biological or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or use of constructed or fictional languages in the dataset or paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language used in the dataset is clearly specified as English, so this is not applicable."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language entries (labels and annotations) in English, so it is not the case that the dataset lacks any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1159-rubric-6",
    "token_usage": {
      "prompt_tokens": 10111,
      "completion_tokens": 192,
      "total_tokens": 10303
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and throughout paper",
          "reasoning": "The paper provides a URL for the code repository related to their method (https://github.com/happytianhao/TaDe) but does not mention any code release specifically related to dataset construction, preprocessing, or generation. The datasets used (nuScenes and Argoverse) are well-known public datasets, and the paper does not introduce any new dataset of their own, nor code for dataset creation is discussed or linked."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 4.1 Experimental Setup",
          "reasoning": "The paper describes standard and publicly available datasets (nuScenes and Argoverse) used for evaluation, with references to prior works for ground truth generation and data splits. There is no presentation or documentation of new datasets introduced by the authors themselves. Hence, no dataset creation process documentation is present in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1226-rubric-0",
    "token_usage": {
      "prompt_tokens": 12399,
      "completion_tokens": 115,
      "total_tokens": 12514
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1 NDDA Dataset Design and Abstract",
          "Reasoning": "The NDDA dataset is a large-scale dataset newly created by the authors, consisting of images generated by three state-of-the-art diffusion models with text prompts designed to remove or alter key robust features essential for human visual recognition. The images are synthetic and generated via AI models, specifically diffusion models, as explicitly stated in Section 3.1 and throughout the paper."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1226-rubric-1",
    "token_usage": {
      "prompt_tokens": 13199,
      "completion_tokens": 396,
      "total_tokens": 13595
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.1, Section 4",
            "reasoning": "The NDDA dataset is described as newly created by the authors to evaluate the natural attack capability. The annotation involves generating images via diffusion models using controlled text prompts designed to remove or alter robust features. The paper references conducting a user study with human subjects (Section 4) to evaluate stealthiness, but this study involved crowdsourcing and is not framed as formal annotation of the dataset images. The dataset creation and labeling appear to be performed by the authors (experts), as it involves designing prompts and defining robust feature removal in synthesized images, without indication of multiple non-expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "Section 3.1 details the dataset design including the consideration of robust features essential to human vision, the definition of the 4 robust features (shape, color, text, pattern), and the specific text prompts template used to generate images removing these features. This constitutes a clear instruction guideline for annotation and dataset creation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "The paper presents specific templates and combinations (16 different combinations with and without robust features, 50 images per combination), reflecting a rubric or systematic criteria for image generation and labeling regarding which robust features are removed or altered. The detection rates against models and human detection rates further confirm consistent scoring or measurement criteria."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.1, Table 1, Figure 2",
            "reasoning": "The paper provides examples of text prompt templates (Table 1) and example images generated with different feature removals (Figure 2), clearly showing examples used in the annotation/generation process. This serves as annotation examples illustrating the application of the instructions and rubric."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1226-rubric-2",
    "token_usage": {
      "prompt_tokens": 14389,
      "completion_tokens": 286,
      "total_tokens": 14675
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any QA performed by a single human expert for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA conducted by multiple human experts for the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of QA done by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 4 RQ2: Human User Study",
          "reasoning": "The paper describes a user study with 82 human subjects using a crowdsourcing platform who answered whether the object of interest was in the images. This implies QA was performed by multiple human non-experts as these subjects are typical crowdworkers without domain expertise."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report using AI models for QA of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that any automated verification or rule-based QA process was applied to the dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is documentation and evidence of QA through the user study detailed in the paper, so QA is not absent."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1226-rubric-3",
    "token_usage": {
      "prompt_tokens": 13999,
      "completion_tokens": 452,
      "total_tokens": 14451
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any dataset created entirely by humans from scratch. The NDDA dataset images are generated by diffusion models guided by designed text prompts, not direct human creation of images."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.1 NDDA Dataset Design; Abstract; Section 3 Attack Capability Analysis",
          "reasoning": "The NDDA dataset consists of images generated by multiple state-of-the-art diffusion models (e.g., DALL-E 2, Stable Diffusion 2, DeepFloyd IF) via text prompts that alter or remove robust features. The data are original AI-generated images created by these models without direct reference to existing images, hence newly generated by AI models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data in the NDDA dataset were created by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of machine translation as a data generation method."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The NDDA dataset is not an aggregation of existing images but specifically generated by AI models; hence it is not collated data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the diffusion models are trained on existing data, the dataset introduced (NDDA) is generated anew by models under different text prompt conditions, not transformed or adapted from existing images by humans."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin of the dataset as AI-generated images created by diffusion models, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1226-rubric-4",
    "token_usage": {
      "prompt_tokens": 14517,
      "completion_tokens": 462,
      "total_tokens": 14979
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The NDDA dataset is not used for pre-training large models; the paper does not describe using it for unsupervised or self-supervised pre-training."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the NDDA dataset to train models from scratch with randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper suggests that the NDDA dataset is used to fine-tune pre-trained models with supervised learning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate utilizing the NDDA dataset for any reinforcement learning post-training techniques."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 3.1 - 3.3, 4",
          "reasoning": "The NDDA dataset is primarily used to evaluate the natural attack capability of diffusion models against object detectors and classifiers, performing large-scale empirical studies and user studies to benchmark attack success rates."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 4, 5",
          "reasoning": "The dataset is leveraged to analyze the characteristics, trends, and root causes of the natural attack capability, including evaluating stealthiness to humans and the role of non-robust features."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The NDDA dataset is not utilized as a knowledge base to augment models in retrieval-augmented generation or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset has practical and demonstrated utility for evaluation and analysis as described in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1226-rubric-5",
    "token_usage": {
      "prompt_tokens": 15240,
      "completion_tokens": 435,
      "total_tokens": 15675
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 (NDDA Dataset Design), Table 1, and throughout the paper",
          "reasoning": "The newly created NDDA dataset is composed of images generated by text-to-image diffusion models with English text prompts. The dataset includes manipulations of English text phrases such as 'stop sign', 'square stop sign', 'blue stop sign', and modifications of the text on the object (e.g., altering the 'STOP' text). There is no indication that any other natural language besides English is used in the dataset generation or annotation process. Hence, the dataset entries contain only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains images generated from text prompts in English, so it includes language in the form of natural language text prompts."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1226-rubric-6",
    "token_usage": {
      "prompt_tokens": 12458,
      "completion_tokens": 188,
      "total_tokens": 12646
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Dataset release section, Introduction paragraph",
          "reasoning": "The paper mentions that the NDDA dataset is released on their website (https://sites.google.com/view/cav-sec/ndd-attack) but does not explicitly indicate that the code used to generate or construct the dataset is publicly available. There is no direct mention or link to a code repository associated with dataset construction or generation processes in the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Dataset Design",
          "reasoning": "The paper provides a detailed description of the dataset construction process in Section 3.1, including the design goals, the definition of robust features, text prompts templates, the number of images generated per condition, object classes chosen, and the diffusion models used. This documentation is sufficiently detailed to understand the dataset creation methodology and supports reproducibility to some degree."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1233-rubric-0",
    "token_usage": {
      "prompt_tokens": 12620,
      "completion_tokens": 271,
      "total_tokens": 12891
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 and Section 4.1 Implementation Details",
          "Reasoning": "The authors introduce a new baseline called BEV-Planner for open-loop end-to-end autonomous driving. This baseline processes bird's-eye-view (BEV) features generated from multi-camera images (visual images captured by cameras). These images are human-recorded sensor data from real-world driving scenes, as indicated by the use of the R50 backbone on images of resolution 256x704 and description of multi-camera inputs. Therefore, the data modality is image, and the data is human-generated through camera sensors capturing real-world driving scenes."
        },
        {
          "Modality": "time series",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 and Section 4.1 Implementation Details",
          "Reasoning": "The dataset used for the new baseline involves historical and current ego vehicle states over time, including velocity, acceleration, and yaw angle, which are time series sensor data recorded from the ego vehicle. The paper mentions concatenating BEV features from past 4 timesteps and using ego vehicle status information as input. These temporal sequences are human-generated sensor data from actual vehicle telemetry, thus time series modality and human-generated origin."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1233-rubric-1",
    "token_usage": {
      "prompt_tokens": 13420,
      "completion_tokens": 277,
      "total_tokens": 13697
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3, 4",
            "reasoning": "The paper introduces a new baseline method BEV-Planner and a new metric called Curb Collision Rate (CCR) evaluated on existing nuScenes dataset. The annotations, such as trajectory supervision used for training BEV-Planner, rely on existing ground truth trajectory data rather than new human annotation. No explicit mention of human annotators or manual labeling for a new dataset is provided; thus, the annotation is performed through automatic processes leveraging existing data."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No specific section discusses annotation instructions for new dataset annotations",
            "reasoning": "The paper does not describe any detailed annotation instructions for annotators, as the dataset utilized is nuScenes and existing ground truth data; no new human annotations are introduced or instructed."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No specific section discusses annotation rubrics for new dataset annotations",
            "reasoning": "There is no mention of scoring rubrics for annotations related to any new dataset because no new human annotation tasks are performed."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No specific section or appendix provides annotation examples for new dataset",
            "reasoning": "Since the paper does not present any novel dataset with manual annotations, there are no examples of annotations provided."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1233-rubric-2",
    "token_usage": {
      "prompt_tokens": 14610,
      "completion_tokens": 307,
      "total_tokens": 14917
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on any new dataset introduced by the authors."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by multiple human experts on any new dataset introduced by the authors."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about quality assurance performed by a single human non-expert on any new dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple human non-experts performing quality assurance on any new dataset in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that quality assurance was performed by an AI model for any dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that automated verification or algorithmic/rule-based QA was applied to new datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets itself, but rather analyzes and experiments on existing benchmarks such as nuScenes. No new dataset creation or annotation process, nor quality assurance procedure for new data, is described in the paper. Therefore, no quality assurance process is applied or documented for new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1233-rubric-3",
    "token_usage": {
      "prompt_tokens": 14220,
      "completion_tokens": 391,
      "total_tokens": 14611
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the creation of any new dataset by human contributors; it rather analyzes and uses existing datasets such as nuScenes."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset generated purely by AI or machine learning models is introduced in the paper. The paper discusses new metrics and baseline methods but does not generate new model-only datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced via human translation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing datasets, primarily nuScenes, but does not collate or aggregate new data from multiple sources to create a new dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces a new evaluation metric (Curb Collision Rate) and a new baseline method, but does not describe creating any derived dataset that modifies or adapts existing datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. It only analyzes and critiques existing datasets (nuScenes) and proposes new evaluation metrics and baseline methods without creating or releasing new data. Therefore, the data origin is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1233-rubric-4",
    "token_usage": {
      "prompt_tokens": 14738,
      "completion_tokens": 272,
      "total_tokens": 15010
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2, 4.3",
          "reasoning": "The paper introduces a novel metric called Curb Collision Rate (CCR) used to evaluate predicted trajectories in terms of adherence to road boundaries, which enhances existing evaluation metrics. This metric and the nuScenes dataset are used explicitly for benchmarking and performance measurement of autonomous driving models' planning behaviors."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 1, 4.3",
          "reasoning": "The paper performs comprehensive analyses on the limitations of the nuScenes dataset and current metrics in end-to-end autonomous driving research, highlighting dataset imbalances and inadequacies in existing evaluation metrics. These analyses help explain model behaviors and suggest the need for better benchmarks."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1233-rubric-5",
    "token_usage": {
      "prompt_tokens": 15461,
      "completion_tokens": 552,
      "total_tokens": 16013
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the introduction of any new dataset containing entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset with exactly two human languages is introduced in the paper."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper references and uses existing datasets (e.g., nuScenes) but does not introduce any new dataset consisting solely of English language entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset introduced containing non-English language data only."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces a new baseline method (BEV-Planner) and new evaluation metric (Curb Collision Rate) but does not present a dataset composed of programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper includes mathematical expressions to describe model formulations and metrics, these are not datasets but rather part of the method description. No new dataset with math or logic notation entries is introduced."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper is focused on autonomous driving datasets and evaluation metrics; no biological or non-human communication datasets are introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or introduction of a dataset containing constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the datasets is clear (e.g., nuScenes, which involves no linguistic content), and no new dataset has unspecified or undocumented language entries."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. Instead, it analyzes the existing nuScenes dataset and introduces a new evaluation metric and baseline method for autonomous driving. Therefore, the dataset entries do not contain any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1233-rubric-6",
    "token_usage": {
      "prompt_tokens": 12679,
      "completion_tokens": 218,
      "total_tokens": 12897
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Not specified in the paper",
          "reasoning": "The paper provides a URL (https://github.com/NVlabs/BEV-Planner) presumably for code and models related to their work; however, this code repository contains code and models for their method and evaluations, but there is no indication that the authors introduce or release new datasets or code related to dataset construction, collection, or preprocessing. The paper mainly uses the existing nuScenes dataset for evaluation and does not present a newly constructed dataset. Hence, there is no code associated with new dataset creation available."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Not applicable",
          "reasoning": "The paper does not introduce any new dataset; therefore, it does not provide documentation or description of any dataset creation process. The dataset used is nuScenes, which is a pre-existing dataset. The paper focuses on analysis and evaluation of models on this dataset and on proposing a new metric, not on dataset construction. Consequently, there is no dataset creation documentation in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-127-rubric-0",
    "token_usage": {
      "prompt_tokens": 10781,
      "completion_tokens": 491,
      "total_tokens": 11272
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 6 Experiments, Implementation Details, and Figures 2, 6, 7, 8, 9",
          "Reasoning": "The new dataset introduced implicitly via the evaluation of ASD consists of images generated or optimized by the generator during 2D score distillation and image editing tasks. The source images for editing are generated by Stable Diffusion v2-1 (a model), while the generator optimizes images using NeRF or pixels. The images are model generated since the generator produces them during distillation and editing, but they represent photorealistic or modified images, implying human involvement in source image creation is partial or indirect. However, the source images for editing are model generated from a pretrained model, and the optimized 3D representations (NeRF) render images. Therefore, images are both model generated and use human-derived source data (e.g., training data for the diffusion models used), but no explicit human-created dataset is introduced by the authors. Hence, these images stem from model generation and human involvement indirectly. The paper states usage of Stable Diffusion for pretrained diffusion models, but no explicit new human-created or natural images dataset is introduced by the authors. Thus, the origins are marked human generated (due to underlying natural images used in pretrained models indirectly) and model generated (for outputs). This is explicitly discussed in Section 6 and evidenced in Figures 2, 6, 7, 8, 9."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 6 Experiments, Prompts used for distillation and editing tasks (Figures 2, 5, 6, 7, 9)",
          "Reasoning": "The new data introduced implicitly are text prompts used to condition the diffusion model during distillation and editing tasks. These prompts are human-generated natural language sentences describing images or 3D scenes, such as 'a photograph of an astronaut riding a horse' or 'a dog is posing next to a laptop computer.' These are created by humans to guide generation and are clearly stated in multiple sections (Figures and Experiments). There is no indication that these prompts are generated by models. Thus, text modality data are human-generated prompts introduced by the authors for evaluation and experimentation."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-127-rubric-1",
    "token_usage": {
      "prompt_tokens": 11581,
      "completion_tokens": 217,
      "total_tokens": 11798
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 6 (Experiments) - Implementation Details",
            "reasoning": "The paper implements their method ASD using existing codebases and pretrained models, and the process involves algorithmic optimization of generators and discriminators with computational steps; no mention of human annotators or manual annotation is given for data labeling."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not provide any annotation guidelines, instructions, or procedural manuals for human annotators; the new data generation is a result of algorithmic optimization rather than human annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No rubric or formal scoring criteria for human annotation is discussed in the paper, as no manual annotations are mentioned."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No examples of annotation or labeled data are provided; the outputs shown are model-generated samples and results, not human-annotated data examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-127-rubric-2",
    "token_usage": {
      "prompt_tokens": 12771,
      "completion_tokens": 274,
      "total_tokens": 13045
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for any dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by multiple human experts for any dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that a single non-expert performed quality assurance on any dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by multiple non-expert annotators on any dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of AI models as judges for quality assurance of datasets or annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the paper involves algorithmic optimization and model training, it does not explicitly describe an automated quality assurance process applied to any newly introduced dataset or annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets, nor does it describe any quality assurance process applied to data. Therefore, no quality assurance process is applied or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-127-rubric-3",
    "token_usage": {
      "prompt_tokens": 12381,
      "completion_tokens": 375,
      "total_tokens": 12756
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 6 (Experiments); Figure 2; Implementation Details in Section 6",
          "reasoning": "The paper introduces new data generated via their proposed Adversarial Score Distillation (ASD) method, which generates novel 2D images and 3D NeRF models autonomously without relying on existing datasets. The outputs such as 2D distillation results, text-to-3D models, and image editing results are generated by the AI models using ASD and pretrained diffusion models as priors, thus representing original content synthesized by the model rather than collected or translated data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not present datasets collected or aggregated from existing sources; instead, it focuses on generating new content through model distillation and generative processes."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the method builds on pretrained diffusion models and employs latent representations, the new data generated is original output from the models rather than merely adapted or modified existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-127-rubric-4",
    "token_usage": {
      "prompt_tokens": 12899,
      "completion_tokens": 243,
      "total_tokens": 13142
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. It discusses and uses pre-existing diffusion models and code bases (e.g., Stable Diffusion, ProlificDreamer-2D, Three-studio) and applies proposed methods on tasks like 2D distillation, text-to-3D, and image editing. There is no mention of a new dataset created or introduced by the authors. Therefore, no practical usage of a new dataset is described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-127-rubric-5",
    "token_usage": {
      "prompt_tokens": 13622,
      "completion_tokens": 569,
      "total_tokens": 14191
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset formed by entries in more than two human languages. No multilingual dataset is introduced by the authors."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the proposed dataset contains exactly two human languages. The paper focuses on English language prompts and does not describe bilingual datasets."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 6: Experiments, Implementation Details and throughout the paper",
          "reasoning": "The new datasets introduced implicitly through the distillation and text-to-3D tasks utilize English prompts, as shown by examples such as 'a photograph of an astronaut riding a horse' and other English captions used in text-to-3D and image editing. The paper specifically references English text prompts and uses pretrained diffusion models trained on English. No other language is indicated, so the dataset entries are monolingual in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset with exactly one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses code bases and implementation details, it does not introduce a dataset comprising programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper presents mathematical expressions and derivations to explain the methodology, it does not present a new dataset consisting of mathematical or logical notations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication datasets are introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or constructed language datasets are introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The linguistic composition of the dataset(s) is clearly stated, i.e., English, so language is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset involves textual prompts and generated images, which contain language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-127-rubric-6",
    "token_usage": {
      "prompt_tokens": 10840,
      "completion_tokens": 205,
      "total_tokens": 11045
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 6, Implementation Details",
          "reasoning": "The paper explicitly mentions the use of existing code bases for implementation: ProlificDreamer-2D [61] for 2D score distillation, Three-studio [15] for the text-to-3D task, and DDS [17] for image editing. It also refers to using Stable Diffusion models from HuggingFace repositories [47,48]. These references indicate that the authors base their experiments on publicly available code resources, suggesting that the code related to dataset generation and processing is accessible."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset created by the authors. Instead, it leverages pretrained diffusion models and existing frameworks for various downstream tasks such as 2D distillation, text-to-3D generation, and image editing. Hence, there is no dataset creation process documented or described in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1277-rubric-0",
    "token_usage": {
      "prompt_tokens": 11610,
      "completion_tokens": 263,
      "total_tokens": 11873
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.2 and 4.3 (Training details and Experiments)",
          "Reasoning": "The paper states that their LEAP model is trained on the TAP-Vid-Kubric training set, which is a dataset for tracking any point in video. TAP-Vid-Kubric is a synthetic dataset generated with procedural simulation, implying the video data is model generated. TAP-Vid-Kubric is not introduced by this paper but only used for training. Therefore, it is not a new dataset introduced by the authors, so it is not included as a new dataset source."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets and Metrics",
          "Reasoning": "The paper evaluates LEAP and LEAP-VO on Replica, MPI Sintel, and TartanAir-Shibuya datasets. However, all these datasets are pre-existing, publicly available datasets and are not introduced by the authors. Moreover, the paper does not introduce any new dataset explicitly. The evaluation datasets are human generated (captured or rendered videos) but are not new datasets introduced by the authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1277-rubric-1",
    "token_usage": {
      "prompt_tokens": 12410,
      "completion_tokens": 215,
      "total_tokens": 12625
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 3.2 Anchor-based Dynamic Track Estimation, and Section 3.3 Temporal Probability Modeling",
            "reasoning": "The dynamic track labels and visibility are predicted by neural network modules (e.g., an MLP and attention mechanisms) within the LEAP pipeline, indicating the annotations are generated by the AI model during training and inference, not by human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit description",
            "reasoning": "The paper does not provide explicit annotation instructions or guidelines for manual annotators regarding dynamic track labels or visibility."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit description",
            "reasoning": "There is no mention of scoring rubrics or criteria for human annotation of dynamic track labels in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit description",
            "reasoning": "The paper does not include annotation examples or samples for dynamic track labels; the annotations are generated by the model itself."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1277-rubric-2",
    "token_usage": {
      "prompt_tokens": 13600,
      "completion_tokens": 308,
      "total_tokens": 13908
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on any new datasets introduced."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance by multiple human experts for any dataset introduced by the authors."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of quality assurance conducted by a single human non-expert in the content of the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance carried out by multiple human non-experts on datasets introduced."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used within the LEAP system, the paper does not state that AI models were used specifically as a quality assurance process for any new dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification processes or algorithmic QA applied specifically to datasets introduced."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new datasets are introduced by the authors in this paper; all datasets used (Replica, MPI Sintel, TartanAir Shibuya) are pre-existing and sourced externally. Therefore, no quality assurance process for new dataset annotations or content is applicable or documented in this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1277-rubric-3",
    "token_usage": {
      "prompt_tokens": 13210,
      "completion_tokens": 373,
      "total_tokens": 13583
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets and Metrics",
          "reasoning": "The paper uses existing public datasets such as Replica [29], MPI Sintel [4], and TartanAir Shibuya [26] for evaluation, which are pre-existing datasets collected by other authors and reused here. The paper does not mention creating any new datasets; thus, any data used are collated from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.2 Implementation Details",
          "reasoning": "The authors train their LEAP model on the TAP-Vid-Kubric [9] training set, which is an existing dataset. They utilize a pre-trained CoTracker model and fine-tune or adapt it with their own anchors, track filtering, and probabilistic modeling modules. This training and adaptation process constitutes deriving new datasets or annotations based on existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper sufficiently documents its data and datasets used; thus, the origin is not unknown."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1277-rubric-4",
    "token_usage": {
      "prompt_tokens": 13728,
      "completion_tokens": 490,
      "total_tokens": 14218
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2 Implementation details",
          "reasoning": "The paper states that LEAP is trained on the TAP-Vid-Kubric training set with 100,000 steps using multiple GPUs. This indicates the introduction and utilization of a new dataset or training data for training the LEAP model from scratch rather than only employing pre-trained weights."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly mention using the dataset for supervised fine-tuning of a pre-trained model."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or discussion of reinforcement learning based methods or RLHF using the introduced dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3 Visual Odometry Results",
          "reasoning": "The datasets Replica, MPI Sintel, and TartanAir Shibuya are employed for benchmarking and comparing the visual odometry performance of the proposed LEAP-VO system and baselines, indicating their use as evaluation datasets."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.4 Dynamic Track Estimation Results and Section 4.5 Ablation Study",
          "reasoning": "The authors utilize datasets to analyze dynamic track estimation quality and ablation studies on filtering criteria and uncertainty estimation, thus the datasets aid in analysis of performance characteristics beyond pure training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the dataset serving as a knowledge base for retrieval-augmented generation or similar augmentations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents multiple practical usages of the datasets including training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1277-rubric-5",
    "token_usage": {
      "prompt_tokens": 14451,
      "completion_tokens": 571,
      "total_tokens": 15022
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the dataset entries contain more than two human languages. The datasets used (Replica, MPI Sintel, TartanAir) involve visual imagery without mention of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the datasets contain exactly two human languages. The datasets focus on visual sequences rather than linguistic content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are primarily visual and related to camera tracking and visual odometry, not language data. The paper itself is written in English, but this does not reflect the nature of the datasets introduced."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any datasets containing only one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset entries are described as containing programming language or structured code-related content. The datasets are composed of image sequences and trajectories."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper contains mathematical formulations and equations describing the tracking models and optimization procedures, these represent the methodology, not the dataset entries themselves. The datasets comprise images and trajectories, not symbolic or mathematical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not contain biological sequences or non-human communication. They consist of RGB image sequences for visual odometry."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of datasets with fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the datasets is not unspecified; rather, the datasets contain visual data without language."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The proposed datasets (Replica, MPI Sintel, TartanAir) in the paper consist solely of visual data (RGB images and trajectories) and do not contain any language entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1277-rubric-6",
    "token_usage": {
      "prompt_tokens": 11669,
      "completion_tokens": 160,
      "total_tokens": 11829
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention",
          "reasoning": "The paper does not provide any explicit reference, link, or statement about the code for dataset construction or dataset generation being publicly available or released. The paper primarily discusses datasets used (Replica, MPI Sintel, TartanAir Shibuya), which are existing datasets, and does not introduce any new datasets or specify code release."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section or explicit description",
          "reasoning": "The paper does not introduce any new datasets. Therefore, there is no documentation on new dataset creation process, data collection, or preprocessing specific to datasets. The datasets used are existing public datasets, and their details are briefly summarized but no new dataset creation is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1307-rubric-0",
    "token_usage": {
      "prompt_tokens": 11901,
      "completion_tokens": 120,
      "total_tokens": 12021
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1, Datasets",
          "Reasoning": "The new dataset introduced involves a collection of calibrated images with associated SAM segmentations and CLIP features for training the LangSplat method. These images are human-captured 3D scenes as indicated by the original dataset capture protocol using devices like the iPhone App Polycam, and manual annotations to extend the LERF dataset with ground truth masks for textual queries, showing human involvement in data creation."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1307-rubric-1",
    "token_usage": {
      "prompt_tokens": 12701,
      "completion_tokens": 247,
      "total_tokens": 12948
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1, 'Datasets' paragraph",
            "reasoning": "The paper states that they extend the existing LERF dataset by annotating ground truth masks for textual queries and further manually annotated additional challenging localization samples. This annotation was done manually, implying human experts performed the annotation for their introduced extended version."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1, 'Datasets' paragraph",
            "reasoning": "The paper does not provide any mention of annotation instructions, protocols, or guidelines used for the manual annotations or extensions of the LERF dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1, 'Datasets' paragraph",
            "reasoning": "No scoring rubrics or formal evaluation criteria for annotation quality are described in the paper regarding the annotation process of the new extended annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.1, 'Datasets' paragraph",
            "reasoning": "The paper does not include or reference any annotation examples or samples provided as guidance for the annotators during the annotation of the new dataset entries."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1307-rubric-2",
    "token_usage": {
      "prompt_tokens": 13891,
      "completion_tokens": 293,
      "total_tokens": 14184
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any quality assurance conducted by a single human expert on dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence in the paper indicating that multiple human experts performed quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance by a single non-expert annotator for the dataset annotations."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about multiple non-expert human annotators performing quality assurance on dataset annotations."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using an AI model as a judge for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that automated verification or rule-based techniques were used for quality assurance of dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces new annotations extending the LERF dataset (e.g., additional ground truth masks and challenging localization samples) but does not describe any quality assurance process for these newly introduced annotations or datasets. Therefore, no quality assurance is applied or documented for the newly introduced dataset extensions."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1307-rubric-3",
    "token_usage": {
      "prompt_tokens": 13501,
      "completion_tokens": 440,
      "total_tokens": 13941
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The authors extend the LERF dataset by manually annotating ground truth masks for textual queries, creating new ground truth annotations by human contributors to enable evaluation of open-vocabulary 3D semantic segmentation, which constitutes new data created from scratch by humans."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any new dataset is generated entirely by AI or machine learning models without reference to existing data. The data generated by models are features or embeddings, not new datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of data produced by translating content from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of machine translation to generate dataset content."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The LERF dataset and the 3D-OVS dataset are existing datasets collected or aggregated from existing sources. The authors do not claim to create these datasets but use them for evaluation."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The authors extend the original LERF dataset by manually annotating additional challenging localization samples and ground truth masks for textual queries, representing modification and addition to existing datasets, hence the derived data category applies."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources and methods for dataset generation are documented and specified in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1307-rubric-4",
    "token_usage": {
      "prompt_tokens": 14019,
      "completion_tokens": 245,
      "total_tokens": 14264
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.2, 4.3 (Datasets and Experiments)",
          "reasoning": "The newly annotated masks in the extended LERF dataset are used to enable the evaluation of open-vocabulary 3D semantic segmentation, and the datasets are used for benchmarking various methods including LangSplat."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper introduces the extended LERF dataset with additional ground truth masks for evaluation of semantic segmentation. This extended dataset is practically used for evaluation and benchmarking, not merely described."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1307-rubric-5",
    "token_usage": {
      "prompt_tokens": 14742,
      "completion_tokens": 548,
      "total_tokens": 15290
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper and dataset description do not mention multiple human languages being used; all language queries and features are based on English."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that exactly two human languages are present; all data and queries appear to be monolingual English."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.4 (Open-vocabulary Querying), Section 4.1 (Settings - Datasets), Abstract",
          "reasoning": "The dataset annotations and open-vocabulary queries are all described using English text. The CLIP model used is English-based, and all textual queries and annotations (such as 'bear', 'object', etc.) are English phrases. No other human languages are mentioned or used in the dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of non-English languages is made for the dataset or its annotations."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of captured 3D scenes with language annotations; no entries containing programming or structured code are indicated."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical expressions appear only in method descriptions and loss functions, not as entries in the dataset itself."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human-scene images and text queries, with no biological sequences or non-human communication content."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or constructed languages is made for the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language used in the dataset is clearly specified and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains textual annotations and queries in English, so it does contain language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1307-rubric-6",
    "token_usage": {
      "prompt_tokens": 11960,
      "completion_tokens": 164,
      "total_tokens": 12124
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention",
          "reasoning": "The paper does not mention or provide any link or reference to publicly available code related to data collection, preprocessing, generation, or dataset creation. The provided URL (https://langsplat.github.io/) is for video results, not code or dataset repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1, Datasets subsection",
          "reasoning": "The paper documents the datasets used and the extension efforts done, such as annotating ground truth masks for textual queries on the LERF dataset and manually annotating additional challenging localization samples. It also describes the datasets used in detail and the metrics employed. This constitutes documentation on dataset creation and annotation procedures relevant to their introduced dataset extensions."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-136-rubric-0",
    "token_usage": {
      "prompt_tokens": 13640,
      "completion_tokens": 204,
      "total_tokens": 13844
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2, Stage 1: 3D Synthesis",
          "Reasoning": "The paper describes synthesizing a static 3D Gaussian-based object using score distillation with multiview image priors and text-to-image models to generate high-quality 3D shapes from images created by the diffusion models; hence, the images are model generated, not from human capture or manual creation."
        },
        {
          "Modality": "video",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2, Stage 2: Adding Dynamics for 4D Synthesis",
          "Reasoning": "The paper explicitly states training a dedicated text-to-video diffusion model on large video-text datasets to provide temporal feedback during 4D optimization. Videos rendered from the dynamic 4D Gaussian scenes are processed by this model, making them model generated synthetic video content."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-136-rubric-1",
    "token_usage": {
      "prompt_tokens": 14440,
      "completion_tokens": 324,
      "total_tokens": 14764
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 3, especially 3.2 and 3.3",
            "reasoning": "The paper describes that the 4D scenes are synthesized by optimizing dynamic 3D Gaussian parameters and deformation fields using score distillation gradients from multiple text-conditioned diffusion models (text-to-image, text-to-video, and 3D-aware text-to-multiview-image). These diffusion models are trained AI models providing feedback during optimization, thus they serve as the annotators guiding the creation of the 4D data rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention of annotation instructions in the paper",
            "reasoning": "The paper does not provide or describe any instructions given to humans or models for annotating or labeling data. Instead, the method relies on pretrained diffusion models used as scorers or guides via gradients, without user instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No description of annotation scoring rubrics",
            "reasoning": "There is no mention of rubrics or formal scoring criteria used for annotation or evaluation guidance. The evaluation is done quantitatively (e.g., user studies, R-Precision) and qualitatively, but no rubrics for annotation guidelines are described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples are given for annotation process",
            "reasoning": "The paper does not provide example annotations or labeled data examples as part of annotation guidelines. The process is fully automated via score distillation from pretrained diffusion models without explicit annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-136-rubric-2",
    "token_usage": {
      "prompt_tokens": 15630,
      "completion_tokens": 370,
      "total_tokens": 16000
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on any new dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance on any new dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses AI models (diffusion models) for optimization of 4D scenes but does not describe their use for quality assurance or dataset annotation validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of an automatic process or code-based verification used for quality assurance of dataset annotations or content introduced by this paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce or discuss any new datasets nor describes any quality assurance process related to dataset annotations or content validation. It focuses on synthesis and generation methods without mentioning dataset creation or QA procedures."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-136-rubric-3",
    "token_usage": {
      "prompt_tokens": 15240,
      "completion_tokens": 367,
      "total_tokens": 15607
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.2 and Abstract",
          "reasoning": "The paper details the creation of new dynamic 4D scenes generated by their method Align Your Gaussians (AYG) via score distillation sampling using composed diffusion models (text-to-image, text-to-video, and 3D-aware diffusion models), resulting in entirely novel 4D sequences. The data consists of synthesized dynamic 3D objects represented as collections of moving 3D Gaussians, generated algorithmically from AI models conditioned on text prompts."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While existing pretrained diffusion models (like Stable Diffusion and MVDream) are used as priors, the 4D dynamic scenes are newly synthesized from these models and not directly adapted or transformed existing datasets. The paper does not describe creating datasets by modifying or transforming existing data; instead, data is generated de novo via model synthesis."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-136-rubric-4",
    "token_usage": {
      "prompt_tokens": 15758,
      "completion_tokens": 211,
      "total_tokens": 15969
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. It describes new methods and models for text-to-4D generation using existing pretrained diffusion models and synthetic optimization techniques. While it mentions training a text-to-video diffusion model, no new dataset itself is introduced by the authors. Therefore, no dataset usage is documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-136-rubric-5",
    "token_usage": {
      "prompt_tokens": 16481,
      "completion_tokens": 651,
      "total_tokens": 17132
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention datasets containing entries with more than two human languages. The primary language content referenced in the context of the dataset is English."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any datasets contain exactly two human languages. Only English is explicitly used for text prompts and conditioning."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract, Section 3.2 (Text-to-4D as Compositional Generation)",
          "reasoning": "The new dataset introduced involves text guidance with prompts and conditioning in English. The text-to-image, text-to-video, and 3D-aware multiview diffusion models are all conditioned on English text prompts, as evidenced by references to text prompts and examples such as 'front view' and 'low motion, static statue' used for negative prompting. There is no mention of any other human languages in the dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English single language dataset is introduced or mentioned in the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses programming constructs such as neural networks and MLP parameters, it does not mention any datasets containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 2 (Background), Section 3.1 (AYG's 4D Representation), Section 3.3 (AYG's Score Distillation in Practice), Section 3.4 (Scaling Align Your Gaussians)",
          "reasoning": "The paper introduces mathematical expressions and formal notation related to the 3D Gaussian representation, diffusion models, Jensen-Shannon divergence regularization, and score distillation sampling. These are integral parts of the dataset's representation and optimization but are formal mathematical expressions rather than human languages."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset involves biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not involve any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages used in the dataset are clearly documented and specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset involves textual prompts in English; therefore, it contains language entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-136-rubric-6",
    "token_usage": {
      "prompt_tokens": 13699,
      "completion_tokens": 164,
      "total_tokens": 13863
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper or abstract",
          "reasoning": "The paper and abstract do not include any explicit links, URLs, or statements indicating that the code for constructing datasets or related to dataset generation is publicly available. The project page link provided is for the method but does not explicitly confirm dataset construction code availability."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section or part of the paper explicitly documents the creation process of any new dataset",
          "reasoning": "The paper does not introduce or describe any new dataset creation process; instead, it uses existing diffusion models and datasets (e.g., HDVG-130M, Webvid-10M) for training or guidance. Therefore, no documentation specific to new dataset creation is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1413-rubric-0",
    "token_usage": {
      "prompt_tokens": 11039,
      "completion_tokens": 113,
      "total_tokens": 11152
    },
    "response": {
      "sources": [
        {
          "Modality": "other",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets",
          "Reasoning": "The paper introduces a new dataset called UHM12k created by registering 6k heads from the original UHM model data into a new quads-based template with 12k vertices. This dataset is human-generated as it originates from 3D scans of human heads captured or collected by humans. It is explicitly introduced by the authors as new data in this work."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1413-rubric-1",
    "token_usage": {
      "prompt_tokens": 11839,
      "completion_tokens": 221,
      "total_tokens": 12060
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Datasets",
            "reasoning": "The new dataset UHM12k introduced is created by registering 6k heads from original UHM model data into a new template; no mention of human annotation or manual labels, the data is sampled and processed automatically."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1 Datasets",
            "reasoning": "There is no description or mention of providing annotation instructions for this dataset because annotation is not performed by humans but data is generated or sampled automatically."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1 Datasets",
            "reasoning": "No scoring rubrics or annotation evaluation criteria are mentioned for this dataset since no manual annotations or subjective labels are involved."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.1 Datasets",
            "reasoning": "No example annotations or labeling samples are provided in the paper as the data represents registered 3D meshes created automatically, without manual annotation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1413-rubric-2",
    "token_usage": {
      "prompt_tokens": 13029,
      "completion_tokens": 379,
      "total_tokens": 13408
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the new datasets introduced."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts performed quality assurance on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any QA conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of multiple non-expert humans performing quality assurance on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that an AI model was used to perform quality assurance on the new datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes dataset creation primarily via registration and sampling from existing models, but does not specifically mention any automated verification or quality assurance process applied to validate the datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces new datasets (UHM12k, synthetic expressions on UHM12k, and sampled hands from Handy latent space) but does not detail any quality assurance process applied to these datasets. There is no documented or described procedure for verifying or validating dataset annotations or content."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1413-rubric-3",
    "token_usage": {
      "prompt_tokens": 12639,
      "completion_tokens": 382,
      "total_tokens": 13021
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper states that some datasets used are registered versions of existing data sources, such as 6k heads from the original UHM model data (Section 4.1). Furthermore, the authors sampled data from existing latent spaces (e.g., Handy latent space and UHM models) to create datasets (Section 4.1). This indicates that datasets were collected and aggregated or sampled from existing sources without full creation from scratch."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper describes processing and adapting existing data by registering the UHM model data into a new quads-based template (Section 4.1), creating synthetic expression data by applying 28 blendshapes to this registered data, and sampling from latent spaces of existing models (Handy, UHM). This demonstrates that the datasets are modifications or transformations applied on existing data sources, thus considered 'derived'."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1413-rubric-4",
    "token_usage": {
      "prompt_tokens": 13157,
      "completion_tokens": 546,
      "total_tokens": 13703
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses datasets solely in a self-supervised training scheme within the proposed framework, not explicitly for general pre-training purposes for large models."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 3.2",
          "reasoning": "The authors utilize the introduced UHM12k dataset (registered 6k heads) and synthetic expression data to train their model from scratch for mesh reconstruction and manipulation tasks using self-supervised objectives (Section 3.2). The training details specify training of the model for 1,500 epochs from randomly initialized parameters, clearly showing training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or indication in the paper of using the datasets to fine-tune pre-trained models with supervised learning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss reinforcement learning methods or use the datasets for RL-based post-training."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 and Section 4.3",
          "reasoning": "The datasets UHM12k and synthetic expression data are used for quantitative and qualitative evaluation of 3D shape reconstruction and local control performance of models, including error metrics and disentanglement analysis."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3 and Section 5",
          "reasoning": "The datasets are also used for detailed analysis of disentanglement, local control properties, region swapping and sampling capabilities. The paper presents studies on how manipulation of control vertices affects geometry reconstruction and disentanglement, a form of characteristic analysis of the data in context of the model."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as a knowledge base for retrieval-augmented generation or similar augmentation techniques."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced are actively used for both training and evaluation in the paper; thus, this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1413-rubric-5",
    "token_usage": {
      "prompt_tokens": 13880,
      "completion_tokens": 533,
      "total_tokens": 14413
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of dataset entries containing multiple human languages. All dataset descriptions concern 3D meshes and their geometric data."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Dataset entries do not contain human language content in two languages; the paper focuses solely on 3D mesh data without mention of bilingual linguistic content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper text is in English, the dataset itself consists of 3D mesh geometry data, not language content in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of dataset entries containing any single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains 3D mesh data, not programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of 3D mesh vertices and related geometry; although mathematical notation is used to describe methods, the datasets are not collections of formal mathematical symbols or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets include biological or non-human communication data; datasets are human 3D mesh shapes and hand models."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed languages appear in the dataset; the data consist of 3D geometric shapes."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of the dataset are not applicable as dataset does not contain language entries."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The proposed datasets, such as UHM12k and the hand model (Handy), contain 3D mesh geometry data with no language-based content; thus, the datasets do not contain any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1413-rubric-6",
    "token_usage": {
      "prompt_tokens": 11098,
      "completion_tokens": 188,
      "total_tokens": 11286
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 1 (Introduction)",
          "reasoning": "The paper explicitly states in the abstract and introduction that code and pretrained models are released at https://github.com/michaeltrs/LAMM, indicating that code for training and evaluation is publicly available. This suggests that code related to dataset processing such as mesh registration and generation is also likely included or accessible to reproduce datasets."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 (Implementation details)",
          "reasoning": "The paper provides detailed description of the dataset creation, including how they registered 6k heads into a new template (referred to as UHM12k), creation of synthetic expression data via blendshapes, the use of existing datasets such as UHM and Handy, and sampling details for train/test splits. This documentation is sufficient to understand dataset construction and reproduction."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1461-rubric-0",
    "token_usage": {
      "prompt_tokens": 11355,
      "completion_tokens": 161,
      "total_tokens": 11516
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5, Datasets",
          "Reasoning": "The paper uses textual descriptions (text modality) paired with motion sequences provided by the HumanML3D and KIT-ML datasets. These textual descriptions are human-written natural language annotations describing human motion."
        },
        {
          "Modality": "time series",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5, Datasets",
          "Reasoning": "The motion data are 3D human motion sequences\u2014time series of human poses over time\u2014originating from motion capture datasets such as AMASS, HumanAct12, KIT, and CMU, which are human-captured motions converted into sequences."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1461-rubric-1",
    "token_usage": {
      "prompt_tokens": 12155,
      "completion_tokens": 215,
      "total_tokens": 12370
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 5 Experiments - Datasets",
            "reasoning": "The paper uses the HumanML3D and KIT Motion-Language (KIT-ML) datasets as benchmarks for evaluation, which are pre-existing datasets not newly introduced by this work. No indication is given that the authors introduced or manually annotated new datasets."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 5 Experiments - Datasets",
            "reasoning": "There is no description or mention of annotation guidelines, instructions, or annotation process performed by humans for new datasets, as the datasets used are pre-existing."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 5 Experiments - Datasets",
            "reasoning": "No scoring rubrics are described or provided since no new annotation was conducted."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 5 Experiments - Datasets",
            "reasoning": "No annotation examples are presented in the paper related to data annotation for new datasets."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1461-rubric-2",
    "token_usage": {
      "prompt_tokens": 13345,
      "completion_tokens": 390,
      "total_tokens": 13735
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving a single human expert for dataset annotation or validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description in the paper that multiple human experts conducted quality assurance on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset quality assurance performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no documentation in the paper about quality assurance by multiple non-expert human annotators for the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report the use of any AI model as a judge or for quality assurance of the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification processes for dataset quality assurance, such as rule-based or algorithmic checks, are described in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The datasets used in experiments are pre-existing ones (HumanML3D and KIT-ML) with no new datasets introduced by the authors. The paper does not describe any quality assurance processes for these datasets. Furthermore, no new dataset is introduced by the paper that would require annotation or quality assurance."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1461-rubric-3",
    "token_usage": {
      "prompt_tokens": 12955,
      "completion_tokens": 422,
      "total_tokens": 13377
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the creation of any new dataset collected or recorded by humans; instead, it uses existing datasets (HumanML3D and KIT-ML) for experiments."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the model generates new motion sequences during inference, the paper does not introduce any new dataset composed of data generated entirely by AI models to be used as a new dataset."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss machine translation of datasets."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe collecting or aggregating data from existing sources without modification as part of new dataset creation."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 5, Experiments - Datasets",
          "reasoning": "The experiments use data from existing datasets HumanML3D and KIT-ML, which are derived from other datasets (e.g., AMASS, HumanAct12, CMU) and contain processed motion sequences and accompanying text annotations. Thus, the data the authors use is based on existing sources with some transformations (downsampling, trimming, pairing with textual descriptions), but not newly created from scratch for this work."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented in the paper as derived from existing datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1461-rubric-4",
    "token_usage": {
      "prompt_tokens": 13473,
      "completion_tokens": 461,
      "total_tokens": 13934
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any proposed new dataset being used exclusively for pre-training large models."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3. Method; Section 5. Experiments",
          "reasoning": "The datasets HumanML3D and KIT-ML are used to train the proposed masked motion model from scratch, including training the motion tokenizer and the conditional masked motion transformer. The training loss and procedure described in Section 3 indicate training the model with these datasets from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that the new dataset(s) are used for supervised fine-tuning of any pre-trained model. The paper focuses on training from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any reinforcement learning post-training (e.g., RLHF) with the dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets HumanML3D and KIT-ML are existing standard datasets used for evaluation here, not newly introduced datasets."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce a new dataset used primarily for analysis of trends or characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No usage of any new dataset as a knowledge base is described in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper utilizes existing datasets (HumanML3D, KIT-ML) but does not introduce any new datasets. Hence, no new dataset utility is described."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1461-rubric-5",
    "token_usage": {
      "prompt_tokens": 14196,
      "completion_tokens": 612,
      "total_tokens": 14808
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes datasets used for text-to-motion generation that include human language descriptions, but does not mention multiple languages beyond English."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper suggests the datasets contain exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5 (Experiments) - Datasets",
          "reasoning": "The paper uses HumanML3D and KIT-ML datasets which contain textual descriptions paired with motion sequences. Although the paper does not explicitly state the language of these descriptions, the descriptions and example prompts given (e.g., 'a person walks forward then turns completely around and does a cartwheel') are exclusively in English. No other languages are mentioned for these text annotations, indicating that the dataset entries contain only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of datasets with non-English languages as monolingual content is found in the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses programming concepts and architectures such as transformers, vector quantized variational autoencoders, and model training but does not present any dataset entries containing programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes mathematical formulations and symbolic representations in its methodology (e.g., formulas for loss functions, vector quantization), these are not part of the dataset entries but rather part of the paper\u2019s description of the model and training."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper and introduced datasets involve human motion and natural language but do not contain biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains constructed, fictional, or artificial languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the text descriptions in the datasets is known and clearly English based on the textual examples and descriptions."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets described contain text descriptions paired with motion data, so language is present in the dataset entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1461-rubric-6",
    "token_usage": {
      "prompt_tokens": 11414,
      "completion_tokens": 165,
      "total_tokens": 11579
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper about releasing code for dataset construction",
          "reasoning": "The paper describes experiments conducted on existing public datasets (HumanML3D and KIT-ML) and does not mention introducing any new datasets developed by the authors or releasing any code related to dataset collection, processing, or generation. The code linked is for their model and project page, not for dataset creation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section in the paper documenting dataset creation",
          "reasoning": "The paper uses standard public datasets (HumanML3D and KIT-ML) for training and evaluation. It does not introduce any new datasets nor documents any dataset collection or creation process in the text. Therefore, no dataset documentation is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1498-rubric-0",
    "token_usage": {
      "prompt_tokens": 14069,
      "completion_tokens": 165,
      "total_tokens": 14234
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Implementation Details - Training Data Generation",
          "Reasoning": "The new dataset used for training the map-relative pose regressor includes images from 450 scenes taken from the Map-Free Dataset [2], consisting of roughly 500K frames of images captured at various outdoor locations. These images are captured using cameras and come with ground truth poses computed via SfM. The data represent real-world scenes and images collected by humans using cameras, hence are human generated. The paper explicitly states using these images and associated camera intrinsics for training their model. The scene coordinate maps used for training are predicted by scene-specific networks trained rapidly, but the initial images themselves are human generated camera images."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1498-rubric-1",
    "token_usage": {
      "prompt_tokens": 14869,
      "completion_tokens": 352,
      "total_tokens": 15221
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1 Training Data Generation; Section 3 Method; Section 4 Experiments",
            "reasoning": "The new dataset used for training the scene-agnostic map-relative pose regressor M is composed of 450 scenes from the Map-Free Dataset [2] (a new dataset cited in this paper), with associated ground truth poses computed via Structure-from-Motion. The paper describes training 900 scene-specific coordinate regressors G_S on these scenes. The annotations (coordinate maps, camera intrinsics, poses) are generated through an automatic process involving networks and augmentation, but this entire setup and data curation is done by expert researchers, as inferred from detailed architecture and training design described by the authors."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No specific annotation instructions described in the paper for the annotation process of the new dataset.",
            "reasoning": "The paper describes the dataset preparation involving network outputs and augmentation but does not provide explicit annotation instructions to any annotators or human labelers."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention of scoring rubrics or annotation quality standards in the dataset preparation sections (Section 4.1).",
            "reasoning": "Since the data annotations are automatically generated and based on image-pose correspondences and network predictions, the concept of rubrics for manual annotation does not apply here."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of annotated data or guidelines provided in the paper (no appendix or supplement describing annotation examples).",
            "reasoning": "The dataset construction is based on automated coordinate regression predictions and augmentation rather than manual annotations, so no annotation examples are presented."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1498-rubric-2",
    "token_usage": {
      "prompt_tokens": 16059,
      "completion_tokens": 303,
      "total_tokens": 16362
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the new datasets or generated data."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or indication that multiple human experts performed quality assurance on any dataset in this work."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any quality assurance conducted by single human non-experts."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper indicates quality assurance by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using AI models to perform quality assurance on dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not detail any automated verification or algorithmic/rule-based quality assurance process applied to the datasets or annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces the use of the Map-Free Dataset [2] and training scene-specific networks to generate 2D-3D correspondences used in training. However, it does not describe any quality assurance process regarding the dataset annotations or the generated scene coordinate maps. There is no mention of manual or automated verification or validation steps for quality assurance."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1498-rubric-3",
    "token_usage": {
      "prompt_tokens": 15669,
      "completion_tokens": 537,
      "total_tokens": 16206
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Training Data Generation",
          "reasoning": "The authors create new training data by training scene-specific geometry prediction networks G_S on mapping images of the training dataset scenes, which are original captures with ground truth camera poses. They generate numerous 2D-3D correspondences by applying these scene-specific networks to frames, including data augmentation steps like random rotations, rescaling, and crops. This process involves collecting and preparing original mapping images with ground truth annotations specifically for this work, thus constituting new human-created data."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 4.1 Training Data Generation",
          "reasoning": "The scene coordinate maps, which are the 2D-3D correspondences used to train the map-relative pose regressor M, are generated by applying the trained scene-specific coordinate regression networks G_S to the input images. These networks produce predicted scene coordinates (maps) for each pixel, which are outputs generated by machine learning models, constituting data generated entirely by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of any data generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used (such as the Map-Free Dataset) are pre-existing and employed by the authors, but the authors generate their own processed training data (scene coordinate maps) rather than simply aggregating existing data without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Training Data Generation",
          "reasoning": "The training data for the pose regression network M is derived from existing images and poses by applying the scene-specific networks G_S and data augmentation techniques like random jittering and rotations. This constitutes a modification and transformation applied to existing data to produce new training inputs."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origins and generation methods are clearly described in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1498-rubric-4",
    "token_usage": {
      "prompt_tokens": 16187,
      "completion_tokens": 722,
      "total_tokens": 16909
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 (Training Data Generation), Section 3.4 (Optionally Fine-Tuning the Pose Regressor), and Section 4.1 (Training and Hardware Details)",
          "reasoning": "The authors train the scene-agnostic map-relative pose regressor M from scratch over a large corpus of data composed of 450 scenes from an existing Map-Free Dataset (indexing 0 to 459 excluding some). Specifically, 900 scene-specific coordinate regressors G_S are trained for each scene to generate scene coordinate maps used to train M. This training is performed from randomly initialized weights for M (as indicated by training from scratch over the entire dataset) and takes approximately 10 days (Section 4.1). This large training set is not a new dataset introduced by the authors but the data processed for training. However, the authors do introduce a novel construction of training inputs using scene coordinate maps generated by a fast-training network for scene-specific map representations. Thus, as a novel data generation and training pipeline for the pose regressor, this constitutes training from scratch on the dataset of scene-specific coordinate maps across many scenes, which is central to the proposed method."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.4 (Optionally Fine-Tuning the Pose Regressor) and Section 4.2 (Quantitative Evaluation)",
          "reasoning": "The authors describe a fine-tuning step where the pose regressor M is fine-tuned on the mapping images for each new scene, requiring only 1.5 to 10 minutes per scene. This is done after the initial training of M and scene-specific coordinate regression network G_S, improving pose accuracy on the target scenes (marepo S variant). This supervised fine-tuning uses the newly generated scene-specific data for each scene, showing explicit usage of the dataset for supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of reinforcement learning based post-training methods such as RLHF with any datasets."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper evaluates on existing public datasets (7-Scenes, Wayspots) which are pre-existing datasets. No new dataset solely introduced for evaluation purposes is described."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any new dataset is used primarily for analysis or pattern discovery without training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as a knowledge base to augment models or used for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced during the method (scene-specific coordinate maps and generated augmented data) are clearly used for training and fine-tuning the pose regressor network."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1498-rubric-5",
    "token_usage": {
      "prompt_tokens": 16910,
      "completion_tokens": 546,
      "total_tokens": 17456
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only mentions datasets and data with respect to images and poses but does not mention any use of human languages, let alone multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that the dataset contains exactly two human languages; the data involved are image frames with poses without any linguistic content."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, including Abstract and Experiments sections.",
          "reasoning": "The paper's new dataset entries and all annotations, documentation, and descriptions are in English. All textual content including camera poses, metric information, and other descriptions are presented in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of any dataset content or annotations in any non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the paper describes algorithmic methods and network architectures, it does not present or introduce new datasets containing programming code or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains mathematical formulas and notation for method explanation, but these do not constitute dataset entries. Therefore, the dataset itself does not contain mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data is included or described in the new datasets."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or use of fictional or constructed languages in the datasets introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content is known and entirely English as the paper presents."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain images and pose annotations described with English language content; therefore, the dataset does include language and is not language free."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1498-rubric-6",
    "token_usage": {
      "prompt_tokens": 14128,
      "completion_tokens": 126,
      "total_tokens": 14254
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and URL provided",
          "reasoning": "The paper explicitly mentions that code is available at https://nianticlabs.github.io/marepo, indicating that the code used for dataset generation and associated processing is publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Implementation Details",
          "reasoning": "The paper provides detailed documentation on training data generation in Section 4.1, including descriptions of the datasets used, preprocessing steps, augmentation strategies, and parameters for training scene-specific networks, thus clearly documenting the dataset creation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1505-rubric-0",
    "token_usage": {
      "prompt_tokens": 11582,
      "completion_tokens": 143,
      "total_tokens": 11725
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.3 Method - Layout representation and Section 4 Results",
          "Reasoning": "The new dataset consists of floorplan images representing site conditions and layout attributes. These images are preprocessed and encoded for training the MaskPLAN model. The paper describes the layout attributes including room types, locations as images (with channels), areas, adjacency matrices, and explicit room shape regions, all derived from real-world residential layouts. Since these layouts represent actual architectural plans, they are considered human generated; the data is not generated by any model but used as training data for the new method."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1505-rubric-1",
    "token_usage": {
      "prompt_tokens": 12382,
      "completion_tokens": 271,
      "total_tokens": 12653
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3 Method and Data Representation; Section 4 Results",
            "reasoning": "The paper's new dataset consists of annotated floorplan layouts derived from real-world residential layouts provided by the RPLAN dataset [50]. The annotation of layout attributes such as room types, locations, adjacency, areas, and shapes is performed via an automatic process during pre-processing and training, including the use of pretrained models (e.g., ADLM with VQ-VAE) to encode image attributes as visual tokens. The paper describes the use of a computational pipeline and pretrained models for attribute extraction and encoding, rather than manual human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not mention any detailed instructions provided to annotators since the annotation is performed automatically via computational methods."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "No scoring rubrics or grading criteria for annotations are described, consistent with the use of automatic annotation without human evaluation guidelines."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "No example annotations or annotation guidelines with example cases are provided, as the annotation is automatic and based on algorithmic processing."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1505-rubric-2",
    "token_usage": {
      "prompt_tokens": 13572,
      "completion_tokens": 372,
      "total_tokens": 13944
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any indication that quality assurance of the dataset annotations or content was performed by a single human annotator with domain expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not detail any quality assurance involving multiple human experts for the dataset used or introduced."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper of quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any QA process involving multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of an AI model being used as a judge or validator in the quality assurance process for dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or rule-based QA procedures applied to the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce a new dataset, but rather uses the existing RPLAN dataset for training and evaluation. There is no documentation or description of any quality assurance process applied specifically to dataset annotations or content within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1505-rubric-3",
    "token_usage": {
      "prompt_tokens": 13182,
      "completion_tokens": 432,
      "total_tokens": 13614
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 (Results), specifically Section 4 (Overall), and Section 4.1 (Metrics)",
          "reasoning": "The authors conduct experiments on the RPLAN dataset [50], which is a pre-existing dataset consisting of over 80,000 real-world residential floor plan images sampled from Asia. The paper explicitly states that the RPLAN dataset is used for training and evaluation, indicating that the data is collected from existing sources without modification for the purpose of their experiments."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.3 (Layout Representation) and Section 3.4 (Graph-structured Dynamic Masked Autoencoder), Section 4.5 (Ablation Studies)",
          "reasoning": "The paper describes preprocessing and encoding steps to represent layout attributes (e.g., room types, positions, adjacency, room shapes) as matrices and images, which are then encoded using a pretrained Attribute Discrete Latent Model (ADLM). This indicates that the original RPLAN data was transformed and adapted via encoding into a latent space for modeling. Hence, the data used for training the MaskPLAN model is derived from existing sources through modifications and transformations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies the data source and the transformations applied, so the origin is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1505-rubric-4",
    "token_usage": {
      "prompt_tokens": 13700,
      "completion_tokens": 295,
      "total_tokens": 13995
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 4 (Method and Results)",
          "reasoning": "The paper describes training the MaskPLAN model on the RPLAN dataset from scratch, learning to predict layout attributes from masked inputs using supervised objectives (classification and reconstruction losses). The generative model is trained to reconstruct masked layout attributes, which indicates training from scratch on this dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The RPLAN dataset is pre-existing, widely used for benchmarking, but this paper does not introduce a new dataset exclusively for evaluation purposes; it uses RPLAN mainly for training and evaluation of their model."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. The only dataset mentioned and used is the pre-existing RPLAN dataset. Therefore, there is no new dataset introduced or used with practical utility described in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1505-rubric-5",
    "token_usage": {
      "prompt_tokens": 14423,
      "completion_tokens": 393,
      "total_tokens": 14816
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing multiple human languages, only architectural floorplan layouts."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that the dataset contains exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4, Dataset description and throughout paper",
          "reasoning": "The dataset used, RPLAN, consists of floor plan layouts and associated metadata typically described in English; the paper and references are all in English without mention of other languages, indicating a monolingual (English) dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of dataset entries being in any single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the method involves generative models and transformers, the dataset itself consists of floor plan layouts and attributes, not code or programming language."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are not described as containing mathematical or logical expressions; mathematical notation appears only in formulae describing model training and methodology, not in dataset content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains architectural layout data, no biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No presence of fictional or constructed languages in the dataset entries."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly describes the dataset and its nature; language is not unspecified or undocumented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains descriptions in English and associated layout attributes, so data entries do contain language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1505-rubric-6",
    "token_usage": {
      "prompt_tokens": 11641,
      "completion_tokens": 120,
      "total_tokens": 11761
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not mention releasing or providing any code related to the dataset creation, data preprocessing, or data collection. It only uses the pre-existing RPLAN dataset for training and evaluation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not introduce any new dataset; it exclusively uses the existing RPLAN dataset, and there is no documentation regarding dataset creation process or code provided within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-152-rubric-0",
    "token_usage": {
      "prompt_tokens": 12714,
      "completion_tokens": 259,
      "total_tokens": 12973
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Dataset - Training Dataset",
          "Reasoning": "The REBU-Syn dataset includes approximately 6 million real data samples collected from multiple widely-accessible public real scene text datasets, such as COCO-Text, RCTW17, UberText, ArT, LSVT, MLT19, ReCTS, TextOCR, OpenVINO, Total Text, and CTW1500. These real samples originate from images captured or curated by humans, thus human generated."
        },
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Dataset - Generate Synthetic Dataset",
          "Reasoning": "The REBU-Syn dataset also includes 18 million synthetic data samples from public synthetic datasets like MJSynth, SynthText, Curved SyntheText, and SyntheAdd. Additionally, the authors generated 60 million synthetic samples (called MJST+) using data generation tools such as TextRecognitionDataGenerator and SynthText, which algorithmically synthesize scene text images. Therefore, these synthetic image samples are model (algorithm) generated, not human generated."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-152-rubric-1",
    "token_usage": {
      "prompt_tokens": 13514,
      "completion_tokens": 219,
      "total_tokens": 13733
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.2 Training Dataset and Real Dataset description",
            "reasoning": "The paper states that real images were collected and quality-controlled from multiple public datasets labeled for scene text recognition tasks, implying that multiple human experts labeled and curated the data given the complexity and variety of scene text data."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper does not provide specific mention of annotation instructions or guidelines for the newly constructed REBU-Syn dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "There is no discussion or evidence in the paper about scoring rubrics or standardized scoring criteria being provided as part of annotation guidelines for the REBU-Syn dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper does not include any concrete annotation examples for how text lines were annotated or labeled in the REBU-Syn dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-152-rubric-2",
    "token_usage": {
      "prompt_tokens": 14704,
      "completion_tokens": 268,
      "total_tokens": 14972
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process performed by a single human expert on the dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple human experts performing quality assurance for the introduced dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided regarding quality assurance by multiple non-expert humans on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any AI model was used to judge or quality assure the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit description of automatic quality assurance or verification processes applied to validate dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper describes the REBU-Syn dataset construction from public real and synthetic datasets and synthetic data generation technologies but does not document any quality assurance process (human or automated) to validate the dataset annotations or content."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-152-rubric-3",
    "token_usage": {
      "prompt_tokens": 14314,
      "completion_tokens": 465,
      "total_tokens": 14779
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creation of any new data entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.2 Dataset, Generate Synthetic Dataset",
          "reasoning": "The paper states that they synthesized 60M data samples using two data generation tools: TextRecognitionDataGenerator and SynthText, which generate synthetic images with various realism enhancements, constituting data generated entirely by models rather than collected or translated."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that any data was generated via human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced via machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.2 Dataset, Training Dataset, Real Dataset",
          "reasoning": "The REBU portion of REBU-Syn is formed by collecting and combining real images from several publicly available datasets (COCO-Text, RCTW17, UberText, ArT, LSVT, MLT19, ReCTS, TextOCR, OpenVINO, Union14M-L, benchmark data, and detection datasets Total Text and CTW1500) without significant modification, which indicates collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 Dataset, Training Dataset, Polygonal Text",
          "reasoning": "The authors improve upon previous methods to convert polygonal text annotations into minimum bounding boxes using perspective transformation rather than bounding box clipping, which transforms existing detection data into recognition data format, thus being derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides clear documentation on the data origins and composition."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-152-rubric-4",
    "token_usage": {
      "prompt_tokens": 14832,
      "completion_tokens": 411,
      "total_tokens": 15243
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2 Dataset and Section 4.1 Scaling Laws",
          "reasoning": "The REBU-Syn dataset, introduced by the authors, is constructed from a large collection of real and synthetic samples and is explicitly used to train models from scratch. The paper describes training TrOCR and PARSeq models of various sizes on REBU-Syn, demonstrating scaling laws between data volume and performance."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.2 Other Observations and Section 4.3 Comparison with SOTA Methods",
          "reasoning": "The authors use the REBU-Syn dataset for fine-tuning pre-trained STR models such as CLIP4STR, resulting in improved accuracy and state-of-the-art performance. This indicates its use in supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning post-training procedures or the use of REBU-Syn in RL-based methods."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "REBU-Syn is not used exclusively for evaluation; rather, it is a large training dataset. Evaluations are performed on separate public benchmark datasets."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The paper analyzes the impact of varying sizes and composition of the REBU-Syn dataset on scaling laws and model performance, including detailed experiments on data proportions and efficiency, reflecting its use for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "REBU-Syn is not utilized as a knowledge base for retrieval or augmentation; its purpose is for training and analysis."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-152-rubric-5",
    "token_usage": {
      "prompt_tokens": 15555,
      "completion_tokens": 545,
      "total_tokens": 16100
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 3.2 Dataset, paragraph on Real Dataset",
          "reasoning": "The REBU-Syn dataset includes real images collected from multiple publicly accessible datasets that contain text in multiple languages, including datasets such as MLT19, which comprises multilingual data. This indicates that the dataset contains entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any bilingual-only datasets; the dataset contains more than two languages rather than exactly two."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although synthetic data and corpus used for synthesis mention an English corpus, the real datasets included are multilingual, so the dataset is not exclusively English monolingual."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains multiple languages, including English and non-English; it is not restricted to exactly one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on natural scene text recognition and does not contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises natural scene text images and synthetic text images; it does not include biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages included in the dataset are documented and specified; hence, unknown language coverage does not apply."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain natural language text in various human languages, so 'N/A' is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-152-rubric-6",
    "token_usage": {
      "prompt_tokens": 12773,
      "completion_tokens": 170,
      "total_tokens": 12943
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 3.2 Dataset, paragraph Generate Synthetic Dataset",
          "reasoning": "The paper states that 'Code for data synthesis is available' with a footnote link (footnote 4) indicating publicly accessible code for generating synthetic data. This implies that the code related to dataset construction, at least for synthetic data generation, is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.2 Dataset",
          "reasoning": "The paper provides detailed documentation about the dataset construction process in Section 3.2, including comprehensive description of data sources, composition of real and synthetic datasets, data proportions, synthesis methods, and preprocessing steps such as polygonal text handling and removal of invalid characters. This level of detail shows the dataset creation process is well documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1547-rubric-0",
    "token_usage": {
      "prompt_tokens": 11806,
      "completion_tokens": 184,
      "total_tokens": 11990
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1, \"Datasets\" and Section 3.1, \"Object exchange\"",
          "Reasoning": "The paper introduces a new dataset called ScanNet-C (Section 4.3) created by systematically replacing a proportion of objects in ScanNet scenes with randomly selected objects from the entire dataset. ScanNet itself is a real 3D reconstruction dataset captured using sensors scanning indoor scenes (Section 4.1). Consequently, the new ScanNet-C dataset is generated by modifying human-captured real sensor data point clouds, making it sensor-captured data. The creation of ScanNet-C involves procedural modification (object replacement) of existing data point clouds, but the underlying data is from human-captured sensor scans, classifying the origin as human generated."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1547-rubric-1",
    "token_usage": {
      "prompt_tokens": 12606,
      "completion_tokens": 202,
      "total_tokens": 12808
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1 Object exchange, and Section 4.1 Experimental Settings - Object exchange details",
            "reasoning": "The paper describes an unsupervised clustering method using GraphCut algorithm and affinity matrices derived from point features and normals to automatically segment scenes into clusters/objects and perform object exchange between scenes without human annotators. This process is deterministic and based on algorithmic techniques, thus categorized as an Automatic Process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No detailed annotation instructions for human annotators are mentioned since annotation is done via automatic clustering algorithms."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not mention any scoring rubrics or evaluation guidelines for manual annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No annotation examples are given since there is no manual annotation process described."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1547-rubric-2",
    "token_usage": {
      "prompt_tokens": 13796,
      "completion_tokens": 351,
      "total_tokens": 14147
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any manual quality assurance performed by a single human expert for the introduced dataset. The datasets used (ScanNet, S3DIS, Synthia4D) are pre-existing and their annotations are not elaborated upon in terms of manual QA by experts by the authors."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description in the paper that multiple human experts were involved in quality assurance of any new dataset annotations by the authors. The datasets referenced are well-known and previously published; no new annotation QA by experts is described."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about QA performed by a single non-expert human annotator on any new dataset or annotations."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No content in the paper describes any QA process performed by multiple non-expert human annotators on newly introduced data or annotations."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance of datasets or annotations is not reported to be done by an AI model serving as a judge."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention an automated verification process as a form of quality assurance for dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets with accompanying annotation or content quality assurance processes. The datasets utilized are existing ones, and no QA procedures for new datasets are described or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1547-rubric-3",
    "token_usage": {
      "prompt_tokens": 13406,
      "completion_tokens": 457,
      "total_tokens": 13863
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the authors created new original datasets entirely from scratch by human contributors. They use existing datasets (ScanNet, S3DIS, Synthia4D) for pre-training and evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating datasets purely from AI or machine learning models without referencing existing data. All data is based on real scans or existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data being produced via translation from other languages by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any machine translation or related processes."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets and Object exchange details",
          "reasoning": "The authors use existing point cloud datasets (ScanNet, S3DIS, Synthia4D), which are publicly available datasets collected and annotated by other groups. They aggregate these existing data sources for their experiments without creating new raw data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Object exchange and Section 4.1 Object exchange details",
          "reasoning": "The authors apply an object exchange strategy that manipulates existing scenes by swapping objects of similar size between scenes to create novel scene point clouds. This is a derived dataset created by modifying and augmenting existing scenes to break inter-object dependencies. Hence, the new data views generated during training are derived data based on existing sources with transformations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The authors specify the origin of the data they use and how they generate the derived augmented data."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1547-rubric-4",
    "token_usage": {
      "prompt_tokens": 13924,
      "completion_tokens": 521,
      "total_tokens": 14445
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental Settings, and Sections 4.2 Scene Understanding",
          "reasoning": "The paper describes pre-training models on existing datasets like ScanNet using their proposed object exchange strategy and context-aware feature learning method, constituting self-supervised pre-training to learn robust object-level features."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing datasets for pre-training and fine-tuning but does not introduce any new dataset for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.2 Scene Understanding",
          "reasoning": "The pre-trained models on datasets like ScanNet are fine-tuned in a supervised manner on the same or different datasets for downstream tasks (semantic and instance segmentation), indicating usage of datasets for supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any reinforcement learning based post-training or RLHF methods involving any dataset use."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 Scene Understanding",
          "reasoning": "Datasets such as ScanNet, S3DIS, and Synthia4D are used to evaluate the performance of pre-trained and fine-tuned models for semantic segmentation, instance segmentation, and transfer learning experiments."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3 Ablation study and Figure 5",
          "reasoning": "The paper uses the datasets to analyze object co-occurrence statistics, robustness to contextual changes (ScanNet-C), and the impact of the proposed object exchange strategy and loss components."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use any dataset as a knowledge base for retrieval or augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Practical usage of the datasets in pre-training, fine-tuning, evaluation, and analysis is clearly documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1547-rubric-5",
    "token_usage": {
      "prompt_tokens": 14647,
      "completion_tokens": 556,
      "total_tokens": 15203
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes datasets of 3D point clouds related to indoor and outdoor scenes, with no mention of multiple human languages contained in the data."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of datasets containing exactly two human languages in the data. The datasets focus on 3D point cloud representations without linguistic content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described (ScanNet, S3DIS, Synthia4D) consist of 3D spatial data. While the paper text is in English, there is no indication that the datasets themselves contain English language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not contain non-English linguistic content; they are point cloud datasets representing spatial data without linguistic annotations in any language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper includes code references and mathematical expressions, the new datasets introduced are point cloud scene data and do not contain structured programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains mathematical equations and notation explaining methods but does not present this as part of any new dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of 3D scenes and point clouds of indoor and outdoor environments, with no biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets contain fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset linguistic content is clearly non-linguistic; there is no ambiguity or lack of specification as to language content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced in the paper are point cloud datasets representing spatial and color information without any linguistic content."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1547-rubric-6",
    "token_usage": {
      "prompt_tokens": 11865,
      "completion_tokens": 182,
      "total_tokens": 12047
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section",
          "reasoning": "The paper provides a GitHub link (https://github.com/YanhaoWu/OESSL) for their code related to their self-supervised learning method, but this code appears to be for the proposed method and model training, not for dataset construction or dataset code. Additionally, the paper uses existing public datasets (ScanNet, S3DIS, Synthia4D) and does not introduce any new dataset created by the authors. Therefore, there is no code for dataset construction provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No dataset creation description found",
          "reasoning": "The paper does not introduce a new dataset constructed by the authors; it solely uses existing public datasets for training and evaluation. Accordingly, there is no documentation about dataset creation or construction processes provided in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1588-rubric-0",
    "token_usage": {
      "prompt_tokens": 13789,
      "completion_tokens": 409,
      "total_tokens": 14198
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1 Abstract, Section 4 Method, Section 5 Experiments",
          "Reasoning": "The new dataset introduced comprises paired data of 3D scene point clouds and associated language descriptions as input conditions for motion generation. The language descriptions are free-form human-written text used to condition human motion synthesis in 3D scenes. This is explicitly mentioned in the problem definition and method sections where language L is tokenized words describing interaction scenarios, and the dataset pairs scenes, language, and motions. The dataset is curated by the authors for evaluating generalization to unseen scenes and language descriptions (novel evaluation set). Since language is human authored, the text modality is human generated."
        },
        {
          "Modality": "time series",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Preliminaries Problem Definition, Section 4 Method, Section 5 Experiments",
          "Reasoning": "The dataset includes sequences of human motion represented as time series of human skeletal joint positions over frames. These motion sequences correspond to physically plausible and semantically consistent human motions aligned with the language descriptions and 3D scenes. The motions are derived from human capture or manual annotation, not generated by models, as the paper discusses training data limitations implying motions are human recorded or annotated. The sequences are time series data representing poses across time frames."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Preliminaries Problem Definition, Section 4 Method",
          "Reasoning": "The 3D scenes are represented as RGB point clouds (scene point clouds), which are captured data from sensors (e.g., RGB-D scans). This is stated in Section 3 and throughout as the input 3D scene representation is an RGB point cloud S. These point clouds originate from sensor captures of real environments, thus human generated with sensor equipment."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1588-rubric-1",
    "token_usage": {
      "prompt_tokens": 14589,
      "completion_tokens": 323,
      "total_tokens": 14912
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3 and Section 4",
            "reasoning": "The paper introduces a novel evaluation set curated for testing generalization with unseen scenes and language descriptions, created by Turkers (crowdworkers). The dataset creation involves human effort likely by experts or knowledgeable annotators, but the paper specifically mentions Turkers for the new evaluation set. However, the main new dataset introduced is this novel evaluation set itself curated from Turker annotations; the annotations (language-motion pairs) are created or collected by human annotators (Turkers). Thus, the annotation is done by human non-experts (Turkers) since Turkers are usually crowdworkers without domain expertise."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "The paper does not explicitly describe any detailed annotation instructions provided to the annotators (Turkers) when creating the novel evaluation set consisting of unseen descriptions and 3D scenes. Thus, we cannot confirm the presence of instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "No scoring rubrics or specific evaluation guidelines for annotation quality or categorization are mentioned in relation to the novel evaluation dataset or its annotators within the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "The paper does not discuss whether examples were given to annotators as part of the data annotation guidelines for the novel evaluation set or any other newly introduced dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1588-rubric-2",
    "token_usage": {
      "prompt_tokens": 15779,
      "completion_tokens": 281,
      "total_tokens": 16060
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was conducted by a single human expert for the new dataset(s)."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper that multiple human experts performed quality assurance on the new dataset(s)."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single non-expert performed quality assurance of the new dataset(s)."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple non-expert annotators performing quality assurance for the new dataset(s)."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report that an AI model was used to perform quality assurance on the dataset(s)."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any automated or algorithmic quality assurance process conducted on the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any explicit quality assurance process for the new dataset(s) introduced or used in the study. No information about annotation validation or verification procedures is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1588-rubric-3",
    "token_usage": {
      "prompt_tokens": 15389,
      "completion_tokens": 528,
      "total_tokens": 15917
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions using existing datasets such as HumanML3D and HUMANISE as benchmarks and evaluation sets. There is no explicit mention of the authors contributing newly created data from human annotators or human contributors for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes generating human motions via a novel two-stage diffusion model conditioned on language and scene affordances, but it does not state the authors generated a new dataset entirely by AI models to be used as training or evaluation data. Data generation by models is part of their approach, but not a new dataset introduced for use by others."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was produced by human translation of content from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was produced by machine translation of content from other languages."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper references existing datasets such as HumanML3D and HUMANISE, the authors do not state that they collated multiple datasets or data sources without modification to create a new dataset."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Abstract, Introduction, Section 4.1 Affordance Map",
          "reasoning": "The paper introduces an intermediate representation called the scene affordance map derived from existing 3D scenes and human skeleton joints data. This affordance map is computed from distance fields between human joint positions and 3D scene points, effectively transforming existing scene and motion data. The authors use or adapt existing datasets (HumanML3D, HUMANISE) and derive new representations (affordance maps) from them to train their models, representing a derived data source rather than completely new human-collected data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources and preparation approaches are specified; the paper clearly explains derivation of affordance maps and usage of existing datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1588-rubric-4",
    "token_usage": {
      "prompt_tokens": 15907,
      "completion_tokens": 352,
      "total_tokens": 16259
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 Method and Section 5 Experiments",
          "reasoning": "The paper describes training the proposed two-stage model (Affordance Diffusion Model and Affordance-to-Motion Diffusion Model) from scratch using paired data comprising 3D scenes, language descriptions, and human motion sequences. This training is supervised and involves learning to generate affordance maps and human motions conditioned on scenes and language."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 Experiments, Subsection 5.3 and 5.5",
          "reasoning": "The authors evaluate their model's performance on established benchmarks HumanML3D and HUMANISE and on a newly curated evaluation set containing unseen language descriptions and scenes. They use these datasets exclusively for benchmarking and measuring the model's generation quality, grounding accuracy, and generalization ability."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 Experiments, including Ablation Studies in 5.6",
          "reasoning": "The paper uses the datasets and evaluation set to analyze trends in grounding accuracy, contact scores, generalization ability, and the impact of alternative architectures, thereby supporting qualitative and quantitative analyses of model performance and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1588-rubric-5",
    "token_usage": {
      "prompt_tokens": 16630,
      "completion_tokens": 533,
      "total_tokens": 17163
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the newly introduced dataset contains entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the new dataset includes two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract, Introduction, Section 5.5 (Results on Novel Evaluation Set)",
          "reasoning": "The paper clearly indicates that the language descriptions in the proposed datasets and evaluation sets are in English. For example, the example language prompts given are English sentences such as 'A man dances on the bed happily.' and 'A person wanders in the room around the table.' Only English language is mentioned in the descriptions used for motion generation and benchmarks."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The proposed datasets and evaluation do not demonstrate usage of a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains natural language descriptions and 3D scene data rather than programming or code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although mathematical notation is used in the method descriptions, this is not part of the dataset entries but rather part of the paper's methodology description."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data are contained in the proposed dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or constructed languages in the new dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language used in the dataset is clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes natural language descriptions (English), so it does contain language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1588-rubric-6",
    "token_usage": {
      "prompt_tokens": 13848,
      "completion_tokens": 137,
      "total_tokens": 13985
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention",
          "reasoning": "The paper does not mention or provide any link to publicly available code for data collection or dataset construction. Although there is a project website, it does not explicitly state that the dataset construction code is shared."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section or mention",
          "reasoning": "The paper does not describe any new dataset creation process or details on how the dataset was constructed. Instead, the paper uses existing datasets and a novel evaluation set curated from Turkers, but provides no detailed documentation on the curation or construction process of this new evaluation set."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-159-rubric-0",
    "token_usage": {
      "prompt_tokens": 11594,
      "completion_tokens": 307,
      "total_tokens": 11901
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2.1 Text-Compensated Anchor Generation",
          "Reasoning": "The text-compensated anchors are formed by using images from the finetune dataset, which consist of real images captured by humans or devices, thus human-generated.'"
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.2.1 Text-Compensated Anchor Generation",
          "Reasoning": "Text descriptions (captions) for each image in the finetune dataset are generated by a pretrained image captioner model (e.g., BLIP2), thus these texts are model-generated rather than human authored."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2.2 Image-Text Anchor Retrieval",
          "Reasoning": "The retrieved image-text pairs are selected from the CC3M dataset, a web-scale dataset of real images originally collected with human involvement (photographs from the web)."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2.2 Image-Text Anchor Retrieval",
          "Reasoning": "The text in the retrieved image-text pairs from the candidate set (CC3M) is originally human-generated (captions or alt-text descriptions collected from the web)."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-159-rubric-1",
    "token_usage": {
      "prompt_tokens": 12394,
      "completion_tokens": 247,
      "total_tokens": 12641
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.2.1, 4.2.2",
            "reasoning": "The new dataset corresponds to auxiliary data used in the Image-Text Anchor Retrieval (ITAR) module, specifically the CC3M dataset as a candidate set for retrieval, which is constructed and processed automatically using pretrained models and KNN search. There is no mention of manual human annotation for this data in the paper; captions are generated by pretrained captioner models (BLIP2), and image-text pairs are retrieved automatically."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not mention providing instructions to annotators for the new dataset; captions are generated automatically by pretrained captioners without human annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "There is no mention of any scoring rubrics or criteria used by annotators since annotation is automated."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No annotation examples are provided as the text-compensated anchors and image-text pairs are generated or retrieved via automatic processes without human annotation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-159-rubric-2",
    "token_usage": {
      "prompt_tokens": 13584,
      "completion_tokens": 400,
      "total_tokens": 13984
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about dataset annotation being verified by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving multiple human experts for the datasets used or introduced."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert human annotator performed quality assurance on any dataset in the study."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of quality assurance by multiple non-expert human annotators is provided in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models (pretrained captioners and CLIP retrieval) are used to generate captions and retrieve anchors, these are components of the method rather than quality assurance processes for dataset annotation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic quality assurance process for validating dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets or specify any quality assurance process for annotations or dataset content. The datasets used are existing ones (e.g., ImageNet, CC3M), and the paper uses external pretrained models or retrieval methods as part of the methodology rather than for dataset quality assurance."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-159-rubric-3",
    "token_usage": {
      "prompt_tokens": 13194,
      "completion_tokens": 513,
      "total_tokens": 13707
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report the creation of any entirely new datasets produced from scratch by human contributors. It mainly utilizes existing datasets such as ImageNet, DomainNet, CC3M, and existing captioning models."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 4.2.1 Text-Compensated Anchor Generation",
          "reasoning": "The paper uses a pretrained image captioner (e.g., BLIP2) to generate new text captions for images in the finetuning dataset. These captions are generated by a model, representing original text data created by an AI system that did not exist before."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention that any data was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of any machine translation to generate datasets."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.2.2 Image-Text Anchor Retrieval",
          "reasoning": "The Image-Text Anchor Retrieval module obtains image-text pairs by retrieving data from an existing dataset (CC3M), which was collected previously. The authors aggregate relevant examples from this existing dataset as anchors without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.2.1 Text-Compensated Anchor Generation and Section 4.2.2 Image-Text Anchor Retrieval",
          "reasoning": "The generated captions are derived by applying a pretrained captioner model on existing images, thus modifying the existing data by adding new textual descriptions. Similarly, the retrieved image-text pairs are selected and repurposed to serve as anchors to regularize finetuning, which is a form of transformation and adaptation of existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the source and methods of generation for the auxiliary data used (caption generation by model and dataset retrieval)."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-159-rubric-4",
    "token_usage": {
      "prompt_tokens": 13712,
      "completion_tokens": 237,
      "total_tokens": 13949
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": true,
          "reference": "Section 4.2.2 (Image-Text Anchor Retrieval)",
          "reasoning": "The paper describes constructing a candidate set (CC3M) resembling CLIP's pretraining data, which is used as a source to retrieve image-text pairs serving as auxiliary anchors with rich semantics to regularize finetuning. This candidate set is used to augment the finetuning process by preserving the original feature space, effectively acting as a knowledge base to augment the model."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-159-rubric-5",
    "token_usage": {
      "prompt_tokens": 14435,
      "completion_tokens": 597,
      "total_tokens": 15032
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset with multiple human languages. The datasets discussed are image datasets with accompanying English text descriptions or captions."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is described with exactly two human languages; the text modalities are primarily English captions or class prompts."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.2.1 (Text-Compensated Anchor Generation); Section 5.4 (Qualitative Examples of Anchors)",
          "reasoning": "The datasets introduced or used to generate text anchors consist of image-caption pairs where captions are generated by pretrained captioners (e.g., BLIP2) in English. The class prompt texts used are also English phrases such as \"a photo of a [CLASS]\". The candidate set CC3M used for retrieval is an English image-text dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication or description of datasets with non-English-only language content is present in the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper focuses on image and natural language datasets. There are no datasets composed of or containing programming or code-related content introduced."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper includes formulas and notations in the method description, these are part of the methodological explanation and not datasets themselves. No dataset entries contain mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets discussed or introduced do not contain biological sequences or other non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or constructed languages in the datasets introduced or used."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the datasets introduced is specified and documented as English, thus unknown language categorization does not apply."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain textual language components (captions and class prompts), so it is not that language is absent."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-159-rubric-6",
    "token_usage": {
      "prompt_tokens": 11653,
      "completion_tokens": 193,
      "total_tokens": 11846
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or reference in the paper",
          "reasoning": "The paper does not mention any public release or repository link for the code used to generate, collect, or preprocess any new datasets or image-text anchors. It only discusses the use of existing datasets (e.g., CC3M) and pretrained captioners (BLIP2), but does not indicate that code for dataset construction or anchor generation is made available."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No explicit section or detailed description dedicated to dataset construction",
          "reasoning": "The paper describes the process of generating text-compensated anchors via a pretrained captioner and retrieving image-text pairs from an existing dataset (CC3M), but this pertains more to the method rather than introducing or constructing new datasets. There is no documentation on constructing a new dataset with annotation or collection details, only usage and retrieval methods are described."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1606-rubric-0",
    "token_usage": {
      "prompt_tokens": 11423,
      "completion_tokens": 160,
      "total_tokens": 11583
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 4, Experimental Results, and multiple mentions throughout the paper",
          "Reasoning": "The paper introduces a new token fusion method, Multi-criteria Token Fusion (MCTF), evaluated primarily on image classification tasks using the ImageNet-1K dataset. The dataset consists of images manually created and labeled, thus human-generated. Though ImageNet-1K is a pre-existing dataset, the paper introduces a new usage pipeline with token fusion methods and finetuning strategies on this image data. No new datasets are explicitly introduced by the authors; the entire evaluation and method development are demonstrated on existing image datasets indicative of natural images captured and labeled by humans."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1606-rubric-1",
    "token_usage": {
      "prompt_tokens": 12223,
      "completion_tokens": 301,
      "total_tokens": 12524
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2, 3.3, 3.4",
            "reasoning": "The paper describes Multi-criteria Token Fusion (MCTF), a method implemented as an algorithmic process for token fusion in vision transformers. The fusion combines tokens based on calculated criteria such as similarity, informativeness, and size using mathematical functions and matching algorithms. The token reduction consistency for fine-tuning is a training scheme applied to models but does not involve human annotators labeling data. There is no indication of human annotators labeling or annotating a dataset as part of this process; hence, the annotation is performed by an automatic, deterministic process integrated in the model's operation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No section explicitly describes annotation instructions for human annotators in a dataset context.",
            "reasoning": "The paper does not describe any human annotation process for data or tokens, nor instructions guiding human annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No section explicitly describes annotation rubrics for human annotators in a dataset context.",
            "reasoning": "As there is no human annotation, scoring rubrics for annotations are not provided."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No section or appendix contains examples for human annotation.",
            "reasoning": "No examples for annotation guidance or demonstration for human annotators are included, since the annotation process is automatic token fusion computation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1606-rubric-2",
    "token_usage": {
      "prompt_tokens": 13413,
      "completion_tokens": 297,
      "total_tokens": 13710
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving a single human expert for the introduced datasets, nor any human annotation effort."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance on any introduced datasets or annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model is used to perform quality assurance or judge dataset quality in the paper."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses automated methods for token fusion, token reduction, and model training, it does not describe an automated verification or quality assurance process for dataset annotations or data quality."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset nor describes any quality assurance process related to dataset annotation or content. The experiments use existing datasets (such as ImageNet-1K), so there is no separate QA process documented for new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1606-rubric-3",
    "token_usage": {
      "prompt_tokens": 13023,
      "completion_tokens": 362,
      "total_tokens": 13385
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the creation of any new datasets or original data generated entirely by humans from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report the generation of any novel data by AI or machine learning models as a new dataset for training or evaluation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data used or introduced is generated by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing datasets such as ImageNet-1K for evaluation but does not propose new collated datasets."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any dataset created by modifications or transformations of existing data."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper primarily focuses on a method for token fusion and efficient vision transformers; it does not introduce or create new datasets. All evaluations are conducted on existing datasets like ImageNet. No explicit new dataset creation or data generation is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1606-rubric-4",
    "token_usage": {
      "prompt_tokens": 13541,
      "completion_tokens": 313,
      "total_tokens": 13854
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces the Multi-criteria Token Fusion (MCTF) method, which is a technique applied to Vision Transformer architectures to improve efficiency and accuracy. While it extensively uses well-known datasets such as ImageNet-1K for evaluation and fine-tuning, the paper does not introduce any new datasets. Therefore, there is no proposed dataset whose usage within the ML pipeline can be assessed."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1606-rubric-5",
    "token_usage": {
      "prompt_tokens": 14264,
      "completion_tokens": 553,
      "total_tokens": 14817
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset containing multiple human languages. The focus is on vision transformer models and image datasets with no mention of multilingual language content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the introduced dataset contains exactly two human languages. The dataset pertains to visual images without linguistic annotations."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not present a dataset with only English content. It uses established image classification datasets (ImageNet-1K) which mainly consist of images and class labels but not textual language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset with only one non-English language is introduced or discussed."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses algorithmic approaches and includes mathematical formulae, it does not introduce a dataset composed of programming code or structured programming language."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical formulas and logical expressions to describe methods, these are part of methodological exposition and not part of any newly introduced dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset involving biological sequences or non-human communication systems is introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or introduction of datasets involving constructed/artificial languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no introduced dataset whose language content is unspecified or undocumented."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset containing linguistic content. The datasets used, such as ImageNet-1K, are image datasets without associated language data. The proposed method relates to token fusion in vision transformers processing images, so the dataset entries do not contain any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1606-rubric-6",
    "token_usage": {
      "prompt_tokens": 11482,
      "completion_tokens": 166,
      "total_tokens": 11648
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and main text",
          "reasoning": "The paper provides a link to code (https://github.com/mlvlab/MCTF) related to the proposed method and experiments; however, it does not introduce any new datasets constructed by the authors. The datasets used are standard public datasets (e.g., ImageNet-1K). Therefore, the code availability pertains to the proposed method, not to any new dataset construction."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "N/A",
          "reasoning": "The paper does not introduce any new datasets; hence, there is no documentation or description of dataset creation process. The datasets used for evaluation are existing public datasets such as ImageNet-1K, which are not created or modified by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1675-rubric-0",
    "token_usage": {
      "prompt_tokens": 11127,
      "completion_tokens": 127,
      "total_tokens": 11254
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2",
          "Reasoning": "The authors introduce a new trained Neural Feature Synthesizer (NeFeS) model on the Microsoft 7-Scenes dataset, training it on sub-sampled sequences of real RGB images captured by handheld cameras of indoor scenes. The dataset images are captured by humans or human-operated devices, making the image data human generated. This is a new dataset configuration (sub-sampled for training NeFeS) introduced by the authors explicitly for training their model."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1675-rubric-1",
    "token_usage": {
      "prompt_tokens": 11927,
      "completion_tokens": 315,
      "total_tokens": 12242
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.2, Training the Feature Synthesizer",
            "reasoning": "The paper describes the training of the Neural Feature Synthesizer (NeFeS) model using supervised feature distillation with a pre-trained 2D feature extractor and progressive training strategies, indicating expert involvement in designing the annotation and training procedures for this new dataset (features of images used for training NeFeS). There is no indication that non-experts or automatic processes generated the annotations."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2, Training the Feature Synthesizer",
            "reasoning": "The training procedure includes detailed methods such as loss functions, progressive training scheduling, and semantic filtering to mask out transient objects, implying the presence of clear instructions for annotated features during NeFeS training."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.2, Loss Functions and Training the Feature Synthesizer",
            "reasoning": "The use of explicit loss functions (photometric loss, feature-metric losses) and a progressive training schedule serves as rubrics to measure and guide annotation quality and training effectiveness on the synthesized dense feature maps."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper and Appendix not mentioning annotation examples",
            "reasoning": "The paper does not provide or mention explicit annotation examples or exemplar annotated data samples used for the training of the NeFeS model; all descriptions focus on loss definitions and training strategies without example annotations."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1675-rubric-2",
    "token_usage": {
      "prompt_tokens": 13117,
      "completion_tokens": 383,
      "total_tokens": 13500
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance being conducted by a single human expert for the new datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts performed quality assurance on the new datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single non-expert human annotator on any new datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of quality assurance by multiple non-expert humans on new datasets described in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report the use of AI models as judges for quality assurance of the new datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification processes (e.g., algorithmic or rule-based techniques) used specifically for quality assurance of dataset annotations or content are described."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce new datasets; it uses existing datasets such as Cambridge Landmarks and 7-Scenes for evaluation. There is no mention of any new dataset creation or annotation process, nor any quality assurance process applied or documented for any new dataset."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1675-rubric-3",
    "token_usage": {
      "prompt_tokens": 12727,
      "completion_tokens": 409,
      "total_tokens": 13136
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments) and Section 4.1, 4.2",
          "reasoning": "The paper evaluates on existing datasets such as Cambridge Landmarks and Microsoft 7-Scenes, which are pre-existing datasets. The authors use these datasets for evaluation and do not mention creating new data from scratch. Therefore, the data used for training and testing are collected from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 (Neural Feature Synthesizer) and Section 4.2",
          "reasoning": "The authors train the Neural Feature Synthesizer model to produce novel feature renderings from known scenes based on existing RGB images and poses from existing datasets. The training involves some modifications such as semantic filtering to mask out transient objects and a progressive training strategy to improve robustness. Additionally, they use reprocessed ground-truth camera poses by COLMAP from existing datasets, implying derived data. Hence, the data generated for their new Neural Feature Synthesizer is based on existing sources with transformations and adaptations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources and their derivations are described sufficiently in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1675-rubric-4",
    "token_usage": {
      "prompt_tokens": 13245,
      "completion_tokens": 382,
      "total_tokens": 13627
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 and 4.1",
          "reasoning": "The Neural Feature Synthesizer (NeFeS) model is trained progressively using supervised learning with photometric and feature-metric losses. Training details including loss functions and progressive strategy are described in Section 3.2. The model is trained on training sequences of datasets such as Cambridge Landmarks (Section 4.1). Hence, the dataset introduced (NeFeS training data, which comes from the same training sets of scenes) is used to fine-tune a model using supervised methods."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 4.1 and 4.2",
          "reasoning": "The introduced datasets, specifically the training and test splits of scenes used for NeFeS modeling and APR refinement, are used to evaluate and benchmark the pose refinement method. Evaluation metrics and comparisons with baselines on datasets (e.g., Cambridge Landmarks and 7-Scenes) demonstrate usage in benchmarking performance."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.6 and 4.7",
          "reasoning": "Detailed ablation studies and analysis of NeFeS architecture, progressive training, number of iterations versus accuracy trade-off, and normalization strategies are performed on the introduced datasets to analyze model behavior and trends, indicating use for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1675-rubric-5",
    "token_usage": {
      "prompt_tokens": 13968,
      "completion_tokens": 598,
      "total_tokens": 14566
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset entries containing more than two human languages. The datasets used (Cambridge Landmarks, 7-Scenes) are image-based pose datasets without language content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets introduced or used in the paper contain exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.2, and other dataset description sections",
          "reasoning": "The datasets used and introduced in the paper (Cambridge Landmarks, 7-Scenes) are well-known computer vision datasets which contain image data and associated pose labels. There is no mention of multiple languages or non-English languages. The paper itself is in English. The data and annotations are effectively monolingual English or language-agnostic (numeric, image) with English descriptions."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any datasets with entries in exactly one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper includes code snippets and discusses implementation details, it does not introduce a new dataset containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper presents mathematical formulas and symbolic representations to describe their methods, these are not part of any new dataset. The datasets themselves do not contain mathematical or logical notations as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data is mentioned or introduced as a new dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not include or discuss fictional or artificially created languages in its datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages present in the datasets are either English or language-agnostic, and clearly documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced contain image and pose data which include natural language descriptions or metadata, so they are not language free."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1675-rubric-6",
    "token_usage": {
      "prompt_tokens": 11186,
      "completion_tokens": 177,
      "total_tokens": 11363
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and throughout the paper",
          "reasoning": "The paper mentions that code will be released at a GitHub repository (https://github.com/ActiveVisionLab/NeFeS) related to their method and pose regression models, but there is no mention of the availability of code used to construct or generate the datasets themselves. The datasets used for evaluation such as Cambridge Landmarks and 7-Scenes are pre-existing datasets and no new dataset construction code is provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Entire paper",
          "reasoning": "The paper does not introduce any new datasets created by the authors; instead, it evaluates their method on established public datasets (Cambridge Landmarks, 7-Scenes). There is no description or documentation of any dataset construction or creation process documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1690-rubric-0",
    "token_usage": {
      "prompt_tokens": 12284,
      "completion_tokens": 274,
      "total_tokens": 12558
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 4, Tables 1-5",
          "Reasoning": "The new dataset entries used in experiments involve images from existing datasets such as RefCOCO, MSCOCO, SNLI-VE, and VQA. These datasets contain human-generated images (captured by humans or human-operated cameras) as inputs for the vision-language tasks. Although these datasets are pre-existing, the authors evaluate their new NARVL model on these datasets. However, the paper does not introduce any novel image dataset explicitly; it uses existing datasets for evaluation only, so no new dataset with images is introduced by the authors."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 4, Tables 1-5",
          "Reasoning": "Similarly, the target outputs and some inputs in these vision-language tasks are text sequences that are originally human-generated, e.g., captions, questions, or annotations. The paper does not introduce new textual datasets but uses existing textual datasets from the mentioned benchmarks such as VQA, RefCOCO (referring expressions), SNLI-VE (entailment labels). Therefore, no new dataset with text modality introduced by the authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1690-rubric-1",
    "token_usage": {
      "prompt_tokens": 13084,
      "completion_tokens": 348,
      "total_tokens": 13432
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3 and Section 4",
            "reasoning": "The paper describes NARVL, a novel non-autoregressive sequence-to-sequence vision-language model trained with Query-CTC loss. The outputs (annotations) in datasets such as Visual Grounding (RefCOCO, RefCOCO+, RefCOCOg), Visual Entailment (SNLI-VE), VQA, and Image Captioning are not manually annotated by humans as part of this work but are existing benchmark datasets used for evaluation. The paper does not explicitly mention any new human annotation or annotation collection process for these datasets. Instead, the model produces outputs (sequence predictions) automatically during inference, which implies the annotation generation process is automatic (i.e., model outputs). No explicit mention of human annotation for these datasets or bespoke annotation tasks is given."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "There is no indication in the paper that detailed annotation instructions were provided for dataset annotation, as no new human annotation was performed for datasets introduced in this work. The work focuses on model training and inference on existing datasets."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper does not provide or discuss any scoring rubrics used in annotation guidelines, as no annotation procedure was described or introduced for new data."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "No annotation examples are provided because the paper presents a model and evaluates it on benchmark datasets without introducing new annotated datasets or annotation processes."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1690-rubric-2",
    "token_usage": {
      "prompt_tokens": 14274,
      "completion_tokens": 283,
      "total_tokens": 14557
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert on any new dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of quality assurance by multiple human experts for any new dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any quality assurance involving multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used to perform quality assurance on new datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated verification process of code or dataset annotations as part of quality assurance on new datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper presents a new modeling approach leveraging existing public datasets for evaluation on vision-language tasks but does not introduce any new datasets itself. Therefore, there is no indication of any quality assurance process for dataset annotation or content described or performed."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1690-rubric-3",
    "token_usage": {
      "prompt_tokens": 13884,
      "completion_tokens": 377,
      "total_tokens": 14261
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any new datasets that were created from scratch by human contributors. All datasets referenced for training or evaluation are existing standard vision-language benchmarks."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets generated entirely by AI or machine learning models. The focus is on model architecture and training methods using existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of human translation of data from other languages in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss datasets generated by machine translation methods."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention assembling a new dataset by collecting or aggregating existing sources without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of datasets derived or adapted from existing sources introduced by the authors."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. All experiments use pre-existing standard datasets such as RefCOCO, SNLI-VE, VQA, and MSCOCO. Thus, the origin of any new data is not applicable as no new dataset is introduced."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1690-rubric-4",
    "token_usage": {
      "prompt_tokens": 14402,
      "completion_tokens": 226,
      "total_tokens": 14628
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. It uses existing public vision-language datasets such as RefCOCO, SNLI-VE, MSCOCO, and VQA datasets to evaluate and analyze their model. No new datasets created or proposed by the authors are documented or described in the paper as having been used for any task. Therefore, no practical usage of new datasets is described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1690-rubric-5",
    "token_usage": {
      "prompt_tokens": 15125,
      "completion_tokens": 561,
      "total_tokens": 15686
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the introduction of any dataset containing entries with more than two human languages. It mainly involves English language tasks."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new dataset containing exactly two human languages. It primarily works with English datasets."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4 Experiments; Table 1 and throughout evaluation",
          "reasoning": "The new dataset introduced or used within the paper for evaluation consists of standard vision-language datasets such as RefCOCO, RefCOCO+, RefCOCOg, SNLI-VE, VQA, and MSCOCO Image Captioning. These datasets primarily contain English textual content. There is no indication of use of non-English languages or multiple languages in the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets with exclusively non-English languages are introduced or mentioned in the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses programming models, but does not introduce or describe any dataset consisting of programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper presents mathematical equations and formal losses (e.g., Q-CTC loss), these are part of the method description rather than datasets. No dataset entries contain mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not include any biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce datasets involving constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) of the datasets used and introduced are known and primarily English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain textual language data; hence, they are not without language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1690-rubric-6",
    "token_usage": {
      "prompt_tokens": 12343,
      "completion_tokens": 137,
      "total_tokens": 12480
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention",
          "reasoning": "The paper does not mention or provide any links to code repositories or releases for the creation or construction of any dataset. The datasets used in experiments appear to be existing benchmarks, and there is no indication the authors introduced new datasets or the associated dataset construction code."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No specific section or mention",
          "reasoning": "The paper does not provide any documentation or detailed description of dataset creation or construction processes. It primarily uses existing datasets to evaluate the proposed model and does not claim introduction of any new datasets or their construction details."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1696-rubric-0",
    "token_usage": {
      "prompt_tokens": 12210,
      "completion_tokens": 144,
      "total_tokens": 12354
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 and throughout the paper",
          "Reasoning": "The paper introduces a new method (NVSVDE-Net) trained and evaluated exclusively on existing public video datasets, RealEstate10k and MannequinChallenge, which contain human-recorded video sequences from diverse indoor and outdoor scenes. No new dataset was introduced by the authors; they rely on these publicly available videos for training and testing their method. Hence, there is no new dataset introduced by the authors with any modality, and all data used are human-generated video recordings collected by humans (cameras operated by people)."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1696-rubric-1",
    "token_usage": {
      "prompt_tokens": 13010,
      "completion_tokens": 245,
      "total_tokens": 13255
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3 Proposed Method, Section 4 Experiments and Results",
            "reasoning": "The paper describes the NVSVDE-Net method that is trained in a completely self-supervised manner from monocular image sequences without requiring ground truth depths or camera poses. The novel view synthesis and view-dependent effect estimation are performed by this AI model without explicit human annotation of data samples."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not describe any human annotation instructions or guidelines given to annotators because the data supervision is self-supervised using image sequences and no human annotation was performed for the new dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "No scoring rubrics or grading guidelines are described since the data annotation process did not involve human labeling but rather automatic learning from image sequences."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "No annotation examples are provided because there was no manual annotation for the new dataset; the model learns from existing unlabeled image sequences in a self-supervised manner."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1696-rubric-2",
    "token_usage": {
      "prompt_tokens": 14200,
      "completion_tokens": 285,
      "total_tokens": 14485
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance process conducted by a single human expert for the introduced datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance on the datasets described in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any QA conducted by a single human non-expert concerning the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that multiple human non-experts conducted quality assurance was provided in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance step performed by AI models acting as judges for the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of an automated quality assurance process for the dataset annotations or content in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses two existing datasets (RealEstate10k and MannequinChallenge) with no new datasets introduced by the authors. Since no new datasets are introduced, and no quality assurance process for dataset content or annotations is described or applied, N/A is appropriate."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1696-rubric-3",
    "token_usage": {
      "prompt_tokens": 13810,
      "completion_tokens": 380,
      "total_tokens": 14190
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing datasets such as RealEstate10k and MannequinChallenge for training and evaluation, but these are pre-existing datasets; the paper does not collate new datasets itself."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 (Datasets) and Sections 3 & 4",
          "reasoning": "The paper uses existing video datasets (RealEstate10k and MannequinChallenge) collected from YouTube videos. The authors introduce a novel method (NVSVDE-Net) trained on sequences from these datasets, employing self-supervised learning to estimate novel views and view-dependent effects without depth or pose labels. Thus, the new datasets for training/testing are derived from existing datasets by selecting frames and applying transformations such as random crops, horizontal flips, gamma and brightness shifts (Section 4.1). No mention is made of creating entirely new datasets from scratch; rather, they adapt existing datasets for their task, making the data derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1696-rubric-4",
    "token_usage": {
      "prompt_tokens": 14328,
      "completion_tokens": 528,
      "total_tokens": 14856
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 4 Experiments and Results; Subsection 4.1 Datasets",
          "reasoning": "The new proposed NVSVDE-Net model is trained on the RealEstate10k and MannequinChallenge datasets in a self-supervised manner from image sequences, without requiring depth or camera pose annotations. The training procedure is effectively pre-training on these image sequence datasets to enable single-image novel view synthesis with view-dependent effects."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes training the NVSVDE-Net model, which uses a pre-trained ResNet34 backbone; there is no indication that the datasets are used to train the model from randomly initialized parameters from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description or indication is provided that the datasets are used for supervised fine-tuning of a pre-trained model."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning or RL-based post-training techniques using the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 Results on RealEstate10k (RE10k); Section 4.3 Results on MannequinChallenge (MC)",
          "reasoning": "The datasets are also used for evaluation purposes, benchmarking the proposed method against state-of-the-art approaches with various quality metrics such as PSNR, SSIM, LPIPS, and a proposed VDE metric."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using these datasets primarily for analyzing trends or characteristics beyond training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as knowledge bases for retrieval-augmented generation or model augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced and used in this paper have practical usage documented in both training (pre-training) and evaluation contexts."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1696-rubric-5",
    "token_usage": {
      "prompt_tokens": 15051,
      "completion_tokens": 566,
      "total_tokens": 15617
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset containing entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets introduced do not contain exactly two human languages; no such linguistic information is presented."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The newly introduced data used for training and evaluation is derived from video frames from YouTube videos (RealEstate10k and MannequinChallenge) which consist of visual content. The dataset textual references, annotations, and losses use English language (e.g., VGG loss, evaluation metrics), and since these datasets are sourced from publicly available English-labeled video data, and there is no indication of any other language used. Hence, the dataset entries are considered monolingual English in content and annotation."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence or mention of datasets containing data in any non-English single language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes code and mathematical formulas for explanation but does not introduce a dataset containing code or programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains mathematical expressions describing the method but does not introduce a dataset comprised of mathematical or logical symbolic entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced does not involve any biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no dataset with fictional or artificially constructed language entries introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of dataset entries is clearly specified or inferred as English; there is no ambiguity or unknown language involvement."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries do contain some language, namely English, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1696-rubric-6",
    "token_usage": {
      "prompt_tokens": 12269,
      "completion_tokens": 141,
      "total_tokens": 12410
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section in the paper provides code or link to dataset construction code.",
          "reasoning": "The paper includes a project site link for videos but does not provide any code or repository link related to dataset construction or processing, nor does it mention releasing such code."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 4.1 describes existing datasets but does not document dataset construction by the authors.",
          "reasoning": "The paper uses pre-existing public datasets (RealEstate10k and MannequinChallenge) and provides no description or documentation of any new datasets created by the authors. Therefore, there is no dataset creation process described."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1714-rubric-0",
    "token_usage": {
      "prompt_tokens": 12457,
      "completion_tokens": 141,
      "total_tokens": 12598
    },
    "response": {
      "sources": [
        {
          "Modality": "time series",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4 Experiments and Section 3.1 Problem Formulation",
          "Reasoning": "The new dataset consists of point cloud observations sampled over time from physical simulation datasets (Physion and Kubric) where the authors predict future point cloud states given history. The data represents time series of 3D positions and velocities of points representing objects in physics simulation scenarios. The paper states the data derives from these synthetic simulation datasets used for training and evaluation, indicating model-generated data from physics-based simulators or dataset generators, not captured or recorded from real human-world observations."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1714-rubric-1",
    "token_usage": {
      "prompt_tokens": 13257,
      "completion_tokens": 195,
      "total_tokens": 13452
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4 Experiments",
            "reasoning": "The experiments involve synthetic datasets generated by simulation frameworks (Physion and Kubric datasets) which provide data for training and evaluation. The data annotation, i.e., generation of ground truth physical states and contact labels, is done through automatic physics simulation processes."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No explicit mention or description of annotation instructions for human annotators is provided in the paper since annotation is done automatically via simulators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No scoring rubrics for annotation are described, as annotations are not performed by humans but are generated automatically from synthetic datasets."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No example annotations or annotation guidelines are provided or needed for these automatically generated datasets."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1714-rubric-2",
    "token_usage": {
      "prompt_tokens": 14447,
      "completion_tokens": 311,
      "total_tokens": 14758
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset annotation or quality assurance conducted by a single human expert for the new datasets introduced or used."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper indicating that multiple human experts performed quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication or mention of multiple non-expert human annotators performing quality assurance is given in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of AI models performing quality assurance on datasets or annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automatic or algorithmic/rule-based quality assurance processes applied to the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses existing datasets (Physion and Kubric) and does not introduce new datasets themselves. Moreover, there is no description of any quality assurance process for dataset annotations or dataset content. The datasets are assumed to be pre-existing benchmark datasets. Therefore, no QA process is documented or applied for any new dataset introduced by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1714-rubric-3",
    "token_usage": {
      "prompt_tokens": 14057,
      "completion_tokens": 621,
      "total_tokens": 14678
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4 Experiments",
          "reasoning": "The authors utilize the Physion and Kubric datasets for their experiments. The Physion dataset is described as providing training, validation, and testing data for various physical scenarios, and Kubric offers sequences for training and validation. The paper indicates that the authors used publicly available datasets but also states that some of the data sequences are generated or processed for their task, including dense point clouds extracted from meshes or particle sampling. This implies that some new data representations or augmentations were created by human researchers, involving human effort to prepare task-specific data, but relies on existing datasets. The paper does not explicitly claim creation of entirely new original datasets, but no mention of automated or model-generated data creation is given. Thus, the data used can be considered at least as newly created human-derived data representations for their task."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any data being generated automatically or synthetically by AI or machine learning models without reference to existing data. The model is trained and evaluated on existing datasets or human-prepared sequences."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data created by human translators from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data created through machine translation approaches."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 Experiments",
          "reasoning": "The data used in the experiments mainly comes from existing publicly available datasets Physion and Kubric, which are standard benchmarks in the domain. The authors use these datasets without indication of significant modification. In particular, they use the training, validation, and testing splits as provided. This fits the definition of collated data collected from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.7 Inference and Section 4 Experiments",
          "reasoning": "The paper describes that from the original dataset meshes or point clouds, the authors create dense point clouds, compute velocity histories, and sample interaction points on mesh faces dynamically for modeling collision. These processes involve modifications and transformations of the existing data to suit the network input and hierarchical modeling. The production of dense sampled points from meshes or the selection of interaction points on mesh faces is a transformation of existing datasets, so these data are derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly refers to the datasets used and the preparation processes, so the data source is specified."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1714-rubric-4",
    "token_usage": {
      "prompt_tokens": 14575,
      "completion_tokens": 468,
      "total_tokens": 15043
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.6 Training and Section 4 Experiments",
          "reasoning": "The datasets (Physion and Kubric) are used to train the proposed PointConv U-Net model from scratch. The training setup is described in Section 3.6, where the model predicts future point clouds given past observations, and the training loss is supervised Huber loss on point positions. The experiments in Section 4 show performance of models trained on these datasets, indicating training from scratch usage."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of pre-trained models or supervised fine-tuning on the introduced datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or RL-based post-training using the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Experiments",
          "reasoning": "The Physion and Kubric datasets are used for evaluation purposes, as the paper uses the testing data or validation data from these datasets to benchmark and compare their model performance with baselines on tasks such as contact prediction accuracy and Euclidean error."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report use of the datasets primarily for data analysis or characterization."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as knowledge bases to augment models via retrieval or similar techniques."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced are explicitly used for training from scratch and evaluation in the experiments."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1714-rubric-5",
    "token_usage": {
      "prompt_tokens": 15298,
      "completion_tokens": 522,
      "total_tokens": 15820
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset containing entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets containing exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4 Experiments and throughout the paper",
          "reasoning": "The paper describes datasets used for object dynamics simulation such as Physion and Kubric, which are created and described in English. There is no mention of other human languages in dataset entries. All descriptions and explanations regarding the datasets and experiments are solely in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset entries are described as containing a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper describes neural network architectures and uses mathematical formulas, it does not introduce any new dataset that contains code or programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notation is present in the paper for model and method description, but the introduced datasets themselves do not contain entries in mathematical or symbolic form as data. Thus, the dataset entries do not contain mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains biological sequences or communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are part of the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of the dataset are specified; they are English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains English language descriptions and labels; thus, it is not language-free."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1714-rubric-6",
    "token_usage": {
      "prompt_tokens": 12516,
      "completion_tokens": 115,
      "total_tokens": 12631
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not mention or provide any links or information about the availability of code related to dataset construction, preprocessing, or generation. There is no indication that such code is publicly released."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "",
          "reasoning": "The paper does not document or describe any dataset creation process, as it uses existing datasets (Physion and Kubric) and no new datasets introduced by the authors are detailed. Therefore, no documentation about dataset construction is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1725-rubric-0",
    "token_usage": {
      "prompt_tokens": 14537,
      "completion_tokens": 202,
      "total_tokens": 14739
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1, Implementation Details, and Pre-training subsection",
          "Reasoning": "The pre-training dataset used for OMPARSER includes several existing publicly available scene text datasets such as Curved SynthText, ICDAR 2013, ICDAR 2015, MLT 2017, Total-Text, TextOCR, HierText, COCO Text, and Open Image V5. However, these are not new datasets introduced by the authors. The paper does not explicitly state the introduction of any new dataset by the authors for pre-training or fine-tuning. The evaluations are performed on standard benchmarks, implying no new dataset creation. The hybrid dataset for pre-training is a combination of publicly available datasets rather than a novel dataset. Therefore, the paper does not introduce any new dataset, and accordingly no new data source entries are present in the text for new datasets."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1725-rubric-1",
    "token_usage": {
      "prompt_tokens": 15337,
      "completion_tokens": 282,
      "total_tokens": 15619
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.1.2, 4.1.3",
            "reasoning": "The paper describes fine-tuning and evaluation on new benchmark datasets such as CORD, SROIE for key information extraction, and PubTabNet, FinTabNet for table recognition. These datasets are well-established benchmarks and the annotations for these datasets are conducted by human experts as implied by their standard use and detailed entity-level annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "The paper does not explicitly mention providing detailed annotation instructions for creating these datasets, only using them for fine-tuning and evaluation. No section discusses annotation guidelines for these datasets, so no instructions are confirmed."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "Similarly, there is no explicit mention of scoring rubrics or evaluation guidelines related to annotation procedures for these datasets in the paper. Scores are reported for evaluation, but rubrics for annotation are not described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "The paper provides qualitative examples of model output, but does not include annotation guideline examples for the new datasets themselves. No annotation example guidelines are presented in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1725-rubric-2",
    "token_usage": {
      "prompt_tokens": 16527,
      "completion_tokens": 387,
      "total_tokens": 16914
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted specifically by a single human annotator who is an expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper indicating that multiple human experts validated the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any involvement of a single non-expert human annotator for quality assurance of the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention multiple non-expert human annotators performing quality assurance on datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that an AI model was used specifically for quality assurance purposes on the dataset's annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated algorithmic or rule-based process used for quality assurance of dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document any quality assurance process, verification, or validation steps for the annotations or content of the datasets used. The authors rely on pre-existing datasets for evaluation and a hybrid dataset for pre-training, but do not introduce or describe any new dataset curation or QA process."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1725-rubric-3",
    "token_usage": {
      "prompt_tokens": 16137,
      "completion_tokens": 322,
      "total_tokens": 16459
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation Details - Pre-training",
          "reasoning": "The authors pre-train their model on a hybrid dataset which contains several publicly available datasets (Curved SynthText, ICDAR 2013, ICDAR 2015, MLT 2017, Total-Text, TextOCR, HierText, COCO Text, and Open Image V5). This indicates that the dataset used for pre-training is collated from multiple existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the origin of the data used for pre-training and evaluation, mostly from existing datasets. No new dataset creation or unspecified origin is indicated."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1725-rubric-4",
    "token_usage": {
      "prompt_tokens": 16655,
      "completion_tokens": 616,
      "total_tokens": 17271
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation Details - Pre-training",
          "reasoning": "The paper describes pre-training OmniParser on a hybrid dataset consisting of various publicly available datasets (e.g., Curved SynthText, ICDAR 2013/2015, MLT 2017, Total-Text, TextOCR, HierText, COCO Text, Open Image V5) with unsupervised or self-supervised objectives like spatial-aware prompting and content-aware prompting to learn general visual and structural text representations."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention training the model from randomly initialized parameters without pre-training. Pre-training is an integral part of their approach."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation Details - Fine-Tuning",
          "reasoning": "The paper details supervised fine-tuning of the pre-trained OmniParser model on specific datasets for tasks including text spotting (e.g., Total-Text, ICDAR 2015), key information extraction (CORD, SROIE), and table recognition (PubTabNet, FinTabNet). This fine-tuning uses labeled data with task-specific annotations."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication that the authors employ reinforcement learning or related post-training methods such as RLHF on any dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Experiments",
          "reasoning": "The datasets used for evaluation such as Total-Text, ICDAR 2015, CTW1500 for text spotting; CORD, SROIE for key information extraction; and PubTabNet, FinTabNet for table recognition are employed for benchmarking and performance measurement of OmniParser."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 Analysis",
          "reasoning": "The paper uses experimental datasets to analyze the influence of pre-training strategies, architectural designs, and decoder length on OmniParser's performance, providing insights into the model's capabilities and limitations."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss using datasets as a knowledge base to augment models via retrieval or similar techniques."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes clear and diverse usages of datasets within the machine learning pipeline, including pre-training, fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1725-rubric-5",
    "token_usage": {
      "prompt_tokens": 17378,
      "completion_tokens": 594,
      "total_tokens": 17972
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation Details, Pre-training paragraph",
          "reasoning": "The pre-training dataset is a hybrid dataset that includes multiple datasets such as ICDAR 2013, ICDAR 2015, MLT 2017, Total-Text, TextOCR, HierText and COCO Text, among others. Specifically, MLT 2017 is a multi-lingual dataset containing text in multiple languages (several languages including English, Chinese, Arabic, and others). This indicates that the new dataset used for pre-training covers more than two human languages; thus, it is multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset is limited to exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes multiple languages, not only English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": true,
          "reference": "Section 3.1 Task Unification",
          "reasoning": "The paper describes that structured sequences output by the model may be encoded in markup languages such as JSON or HTML, representing complex text and table structures. Additionally, Table Recognition involves HTML sequences representing tables and content. These structured code-like representations are part of the model's dataset outputs and are thus considered as containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention inclusion of mathematical or logical symbolic representations in the datasets."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of biological or non-human communication signals or sequences in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains no references to fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the datasets are specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain language in human text as well as structured code-like text."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1725-rubric-6",
    "token_usage": {
      "prompt_tokens": 14596,
      "completion_tokens": 196,
      "total_tokens": 14792
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and main text",
          "reasoning": "The paper mentions that the code for the proposed model OmniParser is available (e.g., 'The code is available at AdvancedLiterateMachinery'), but does not mention or provide any link or availability of code related to data collection, preprocessing, or dataset construction. Moreover, all datasets used seem to be standard public benchmarks; no new dataset is introduced by the authors in the paper. Therefore, no code related to dataset construction is provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Throughout the paper",
          "reasoning": "The paper uses multiple existing public datasets like Total-Text, ICDAR 2015, CORD, SROIE, PubTabNet, and FinTabNet for evaluation. There is no introduction or description of any new dataset created by the authors. Consequently, there is no documentation regarding dataset construction process provided in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1742-rubric-0",
    "token_usage": {
      "prompt_tokens": 12357,
      "completion_tokens": 187,
      "total_tokens": 12544
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4 (Experiments), Table 2, Table 3",
          "Reasoning": "The new dataset used is ImageNet-1K, an established large-scale image classification dataset. The authors conducted compression experiments of Vision Transformers on this dataset. ImageNet-1K consists of human-generated images collected from real-world images. The paper does not introduce any new dataset beyond using this existing dataset explicitly mentioned in the Experiments section and tables. However, since the instruction specifies to list only new datasets introduced by the authors, and the paper does not introduce any new dataset, we confirm no new dataset is introduced. Thus, no new data source entries are from new datasets. The modality would be 'image' as the data is image data from ImageNet-1K, which is human-generated."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1742-rubric-1",
    "token_usage": {
      "prompt_tokens": 13157,
      "completion_tokens": 426,
      "total_tokens": 13583
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.1, Section 3.2, Section 3.3, Section 3.4, and Appendix A",
            "reasoning": "The paper explicitly describes a new one-stage search paradigm with detailed methodology sections (3.1 to 3.4) including the bi-mask strategy, adaptive one-hot loss, and PMIM, all implying that expert human researchers designed, performed and analyzed the annotation and pruning processes as part of the proposed Vision Transformer Compression (VTC) framework. There is no indication of multiple annotators or automated annotation process for the new dataset; rather, it is a model-based pruning search method devised by human experts."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1, Section 3.2, Section 3.3, Section 3.4",
            "reasoning": "The paper describes detailed methodological instructions for the search and pruning process, such as the bi-mask weight-sharing scheme, the adaptive one-hot loss, and the progressive masked image modeling, specifying exactly how the importance and sparsity scores are learned and optimized. These represent clear, protocol-like instructions for the annotation process (search and pruning) of the new dataset."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.3 and Appendix A",
            "reasoning": "Section 3.3 introduces an adaptive one-hot loss as a form of scoring rubric that regulates how sparsity scores are converted progressively into binary pruning choices. The loss function includes constraints aligning entropy and variance to one-hot vectors as a structured rubric for evaluating and scoring the pruning selection during search. Appendix A provides detailed explanations supporting this rubric design."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not explicitly provide annotated examples or sample annotations illustrating the annotation guidelines, nor does it show example outputs or detailed annotation cases in appendices or main text. The methodology is described in terms of continuous and discrete pruning scores and losses without example annotations."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1742-rubric-2",
    "token_usage": {
      "prompt_tokens": 14347,
      "completion_tokens": 298,
      "total_tokens": 14645
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any involvement of a single human non-expert in quality assurance processes."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that multiple non-expert annotators were involved in QA of datasets or annotations."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that an AI model was used as a judge for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses algorithmic techniques for model compression and search optimization, it does not describe any automated verification or QA process applied on dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset, nor does it document any quality assurance process related to dataset annotations or data content. All described experiments use pre-existing datasets (e.g., ImageNet-1K), and no QA process on data is mentioned."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1742-rubric-3",
    "token_usage": {
      "prompt_tokens": 13957,
      "completion_tokens": 350,
      "total_tokens": 14307
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), particularly tables and evaluations on ImageNet-1K dataset and downstream benchmarks",
          "reasoning": "The paper evaluates the proposed OFB method on standard, widely-used datasets such as ImageNet-1K, CIFAR-10, CIFAR-100, and Cityscape. These datasets are existing public benchmark datasets, and the paper uses them to assess their model's compression and performance. The datasets themselves are not newly introduced or created by the authors, but are collated existing datasets used without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset created by the authors. All data used for experiments is from existing public vision datasets, which are collated. There is no explicit statement about new dataset creation or data generation methodology in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1742-rubric-4",
    "token_usage": {
      "prompt_tokens": 14475,
      "completion_tokens": 238,
      "total_tokens": 14713
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a novel method for Vision Transformer Compression (OFB) and its components such as the bi-mask scheme, adaptive one-hot loss, and progressive masked image modeling. However, no new datasets are proposed or introduced by the authors. Instead, the authors use existing datasets such as ImageNet-1K and CIFAR-10/100 for experiments and evaluation. Therefore, no practical usage of any new dataset is described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1742-rubric-5",
    "token_usage": {
      "prompt_tokens": 15198,
      "completion_tokens": 552,
      "total_tokens": 15750
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset with entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset with exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper primarily describes a method for Vision Transformer compression and references existing datasets such as ImageNet-1K and CIFAR, but it does not introduce any new datasets containing English text or linguistic content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset with non-English language content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses methods and algorithms involving weights, masks, and search strategies, it does not introduce a dataset consisting of programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains mathematical formulas to describe the proposed method; however, it does not introduce a new dataset containing mathematical or formal logical expressions as data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not include or introduce any dataset related to biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or introduction of datasets involving constructed languages or fictional languages is made in the paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset with unspecified or undocumented language(s)."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The authors propose a novel compression method for Vision Transformers, but they do not introduce any new dataset. The evaluation is performed on existing image classification datasets such as ImageNet-1K and CIFAR, which are not introduced by the authors. Thus, there is no dataset containing language entries involved in the contribution."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1742-rubric-6",
    "token_usage": {
      "prompt_tokens": 12416,
      "completion_tokens": 173,
      "total_tokens": 12589
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper about code availability for dataset creation.",
          "reasoning": "The paper does not mention or provide any link or reference to the code used for dataset construction or any data preprocessing scripts. The main focus is on Vision Transformer Compression methods and evaluations on existing datasets like ImageNet-1K, CIFAR-10, CIFAR-100, and Cityscape, which are pre-existing datasets rather than new datasets created by the authors."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No explicit section or description related to dataset creation.",
          "reasoning": "The paper does not introduce any new dataset created by the authors, nor does it document any dataset construction process. It solely uses established datasets for evaluation and experiments, and hence no documentation on dataset creation is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1767-rubric-0",
    "token_usage": {
      "prompt_tokens": 13274,
      "completion_tokens": 182,
      "total_tokens": 13456
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 (Scene graph construction) and Section 4.1 (Experimental Setup, Datasets)",
          "Reasoning": "The new dataset introduced for training Open3DSG uses 3D point clouds from ScanNet, which is a human-generated dataset comprising indoor scene scans with posed RGB-D images and high-quality point clouds. The data are acquired via sensors capturing real-world indoor environments. Open3DSG utilizes the ScanNet dataset's 3D point clouds (signal/sensor modality) along with associated camera poses and RGB-D images for feature alignment and distillation (Sections 3.1 and 4.1). Since ScanNet is an established dataset capturing real scenes with human involvement (manual scanning), it is considered human-generated data."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1767-rubric-1",
    "token_usage": {
      "prompt_tokens": 14074,
      "completion_tokens": 229,
      "total_tokens": 14303
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1 and Section 3.2",
            "reasoning": "The annotation information relates to constructing and extracting features in the 3D scene graph and 2D frames using off-the-shelf instance segmentation methods (e.g., Mask3D) and automatic feature extractors (OpenSeg, BLIP) without mention of human annotators performing explicit annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not describe providing any detailed annotation instructions or guidelines for human annotators related to new dataset annotation; annotation is obtained via automatic processing."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "There is no mention of scoring rubrics for annotation validation or quality control procedures relating to annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not provide annotation examples or guidelines for annotators; data annotation is implicitly performed via automatic processes."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1767-rubric-2",
    "token_usage": {
      "prompt_tokens": 15264,
      "completion_tokens": 320,
      "total_tokens": 15584
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the newly introduced dataset or annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or mention of multiple human experts performing quality assurance on any newly introduced dataset or annotation in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single non-expert human annotator was involved in quality assurance for any new dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of multiple non-expert human annotators conducting quality assurance on newly introduced datasets or annotations is found in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that an AI model was used as a judge or for quality assurance specifically on any dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper describes automated processes such as algorithmic projections and feature extraction, it does not describe any automated verification or quality assurance process applied to a newly introduced dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a method trained on existing datasets (ScanNet) and evaluated on 3DSSG, and does not introduce new datasets or mention any quality assurance process for new datasets. Therefore, there is no documented quality assurance process applied to new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1767-rubric-3",
    "token_usage": {
      "prompt_tokens": 14874,
      "completion_tokens": 477,
      "total_tokens": 15351
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The authors use existing indoor 3D datasets, namely ScanNet for training due to its availability of 3D representations and posed 2D images, and 3DSSG for evaluation, which provides semantic scene graph labels aligned with 3D scenes. The paper states that the 3DSSG dataset is the only existing dataset with such aligned semantic scene graph labels and that the method can be trained using ScanNet, indicating that data from these existing datasets are collected and used without significant modification as training and evaluation data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3 (Method) and Section 4.1 (Experimental Setup)",
          "reasoning": "The paper describes the creation of a novel feature representation by distilling knowledge from 2D vision-language models (OpenSeg, InstructBLIP) into a 3D graph neural network operating on point clouds from existing datasets. This involves processing existing 3D data (point clouds) combined with 2D images and projecting features, performing graph construction, and training a model to produce novel embeddings aligned with language features. Thus, the new dataset features and the predicted scene graphs are derived from existing 3D point clouds and 2D image data with modifications\u2014embedding distillation and graph-based annotations\u2014resulting in adapted data representations rather than entirely new raw data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources and methods of generation are well documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1767-rubric-4",
    "token_usage": {
      "prompt_tokens": 15392,
      "completion_tokens": 476,
      "total_tokens": 15868
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the new datasets exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly state that any introduced dataset is used to train a model from randomly initialized parameters without pre-training."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention that a new dataset introduced by the authors is used to fine-tune a pre-trained model using supervised learning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of any new dataset for reinforcement learning based post-training techniques such as RLHF."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset introduced for training or evaluating Open3DSG is not explicitly presented; the paper primarily uses existing datasets (ScanNet for training without labels, and 3DSSG for evaluation). There is no description of a new dataset solely for benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce a new dataset primarily for analyzing trends or patterns; the datasets discussed are used for training or evaluation but are existing datasets."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use any new dataset as a knowledge base to augment models."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. It uses existing datasets such as ScanNet and 3DSSG for training and evaluation purposes. Therefore, no practical usage of a dataset introduced in this paper is described."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1767-rubric-5",
    "token_usage": {
      "prompt_tokens": 16115,
      "completion_tokens": 303,
      "total_tokens": 16418
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, including Abstract and Sections 3 and 4",
          "reasoning": "The dataset and annotations involve textual labels, queries, and prompts entirely in English. The open-vocabulary object classes and the language model prompting for relationships use English text. There is no indication of any other human languages being used in the dataset or its annotations."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3.1 and Eq. (2), (3), and (4)",
          "reasoning": "The paper describes their dataset in terms of mathematical formulations including equations for projection, visibility computations, and cosine similarity losses which indicate that mathematical notation is part of the dataset's description and processing."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1767-rubric-6",
    "token_usage": {
      "prompt_tokens": 13333,
      "completion_tokens": 191,
      "total_tokens": 13524
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention in the paper",
          "reasoning": "The paper does not provide any explicit indication or link to code repositories related to dataset construction or processing. While the paper provides a project webpage URL (kochsebastian.com/open3dsg), the main paper text does not mention code availability for dataset construction or preprocessing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Scene graph construction and Section 3.2 2D feature extraction",
          "reasoning": "The paper provides detailed descriptions of the dataset creation and processing pipeline, including how point clouds and instance segmentations are used to construct scene graphs, the method for projecting 3D data onto 2D frames, and how 2D features are extracted and aggregated. These descriptions offer transparency on the dataset construction and feature extraction processes, aiding reproducibility despite the lack of code availability."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1768-rubric-0",
    "token_usage": {
      "prompt_tokens": 11504,
      "completion_tokens": 236,
      "total_tokens": 11740
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and 4.1",
          "Reasoning": "The new dataset consists of real textual captions collected from sources like Flickr and COCO, which are human-generated caption datasets. The authors explicitly use these captions as input for their bias proposal phase, leveraging a Large Language Model (LLM) to extract knowledge bases of possible biases from them. Hence, the text modality is human-generated caption data used as the foundational new dataset."
        },
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2 and 4.1",
          "Reasoning": "Images generated by the target text-to-image generative models (Stable Diffusion 1.5, 2, XL) using the same set of human-generated captions. These images are thus generated by generative AI models as part of the experimental pipeline to assess biases. The paper explicitly states the generative model as a black box producing images given prompts. Therefore, this image modality data originates from AI model generation."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1768-rubric-1",
    "token_usage": {
      "prompt_tokens": 12304,
      "completion_tokens": 308,
      "total_tokens": 12612
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 3.1, Section 4.1",
            "reasoning": "The annotation of biases is performed automatically by querying a Large Language Model (LLM) to propose biases and a Vision Question Answering (VQA) model to recognize and quantify these biases in generated images, as described in detail in Section 3.1 and implementation details in Section 4.1."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1, Section 4.1",
            "reasoning": "Section 3.1 describes the use of in-context learning with task descriptions and demonstrations provided to the LLM prompt, which constitutes instructions for the task. Section 4.1 mentions prompt and example details used to instruct LLama, indicating instructions are given to the AI model performing annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.2.3",
            "reasoning": "Section 3.2.3 defines a quantitative bias severity score based on entropy calculations to measure and rank bias severity, effectively serving as a scoring rubric for the annotations by AI models."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 4.1 (Supplementary Material)",
            "reasoning": "Section 4.1 and the Supplementary Material mention prompt and example details used to instruct LLM to perform bias proposal tasks. This indicates that examples are provided in the annotation guidelines given to the AI model."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1768-rubric-2",
    "token_usage": {
      "prompt_tokens": 13494,
      "completion_tokens": 384,
      "total_tokens": 13878
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human annotator who is an expert or member of the target demographic for dataset validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts conducted quality assurance or validation of the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single human non-expert annotator for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description of quality assurance by multiple human non-experts is provided in the paper."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.2 Bias Assessment and Quantification; Section 4.1 Implementation Details; Section 4.2 Quantitative Results",
          "reasoning": "The quality assurance of the dataset annotations (bias presence and quantification) is conducted through the use of AI models, specifically state-of-the-art Visual Question Answering (VQA) models such as Llava1.5-13B, which automatically assess the presence and extent of biases in generated images based on captions and questions provided by a Large Language Model (LLM). This automated AI-based evaluation acts as a judge to validate the bias annotations without human laborious labeling processes."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe an automated or rule-based code verification process for quality assurance of dataset annotations. The validation is done via AI models rather than algorithmic/rule-based automatic checks."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes the quality assurance process using AI models (VQA), thus QA is performed and documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1768-rubric-3",
    "token_usage": {
      "prompt_tokens": 13104,
      "completion_tokens": 636,
      "total_tokens": 13740
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Bias Proposals and Section 4 Experiments",
          "reasoning": "The paper uses real textual captions collected from existing datasets such as Flickr and COCO as inputs to the pipeline. While these datasets are existing, the authors do not introduce entirely new human-created data for their experiments. Therefore, for captions specifically, the data is existing, not newly created by humans for this work. However, the new dataset of 'bias proposals' is generated by querying a Large Language Model rather than direct human creation. Consequently, 'New Data from Human' does not apply to any new dataset introduced by the authors in the paper."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.2 Bias Assessment and Quantification; Section 4 Experiments",
          "reasoning": "The new dataset introduced by the authors includes images generated by the target text-to-image generative models (e.g., Stable Diffusion 1.5, 2, XL) using multiple captions and noise seeds. This image data is generated entirely by AI generative models and did not previously exist. The paper explicitly states generating images using these models in the bias quantification stage. Thus, the new dataset of generated images is original content generated by AI models, qualifying as 'New Data from Model.'"
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation Details",
          "reasoning": "The textual captions used as inputs for the pipeline are collected from existing datasets (Flickr, COCO). These are collected and filtered but are used as-is without extensive modification. Therefore, the captions used in the process are collated data from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Bias Proposals",
          "reasoning": "The authors derive a knowledge base of biases by processing the existing caption datasets through a Large Language Model which generates bias names, classes, and questions. Also, the filtering of the captions with ConceptNet integration constitutes derived data because it modifies existing captions to exclude certain biases. This indicates that the bias proposals and related constructs are derived data based on existing captions, transformed through LLM and filtering."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources and their generation methods are specified clearly in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1768-rubric-4",
    "token_usage": {
      "prompt_tokens": 13622,
      "completion_tokens": 342,
      "total_tokens": 13964
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 'Quantitative Results' and Section 5 'Findings'",
          "reasoning": "The datasets of captions and generated images along with annotated biases derived by OpenBias are used exclusively to evaluate and benchmark text-to-image generative models like Stable Diffusion variants for bias detection and quantification. The paper demonstrates their use for performance measurement and comparison with human judgment and closed-set methods."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 'Findings' and Section 3 'OpenBias'",
          "reasoning": "The dataset is primarily used to analyze trends and characteristics of biases in generative models, including discovering new biases and studying context-aware versus context-free bias behaviors. This analytical use is central to the paper's contributions."
        },
        "Knowledge Base": {
          "is_applicable": true,
          "reference": "Section 3.1 'Bias Proposals' and Section 3 'OpenBias' Pipeline",
          "reasoning": "The dataset constructed from real captions combined with LLM outputs forms a knowledge base of possible biases used to guide bias identification and quantification. This knowledge base is foundational within the pipeline to augment the bias detection process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1768-rubric-5",
    "token_usage": {
      "prompt_tokens": 14345,
      "completion_tokens": 573,
      "total_tokens": 14918
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the inclusion of multiple human languages in the proposed dataset. The captions and content appear to be exclusively in English."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced does not include exactly two human languages; only English is used based on the text."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3: OpenBias; Section 4: Experiments",
          "reasoning": "The proposed dataset is built from real textual captions sourced from datasets like COCO and Flickr, which are primarily in English, used as input to the OpenBias pipeline. There is no mention of any other language used for these captions, implying monolingual English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset with non-English language content; all captions and prompt data are in English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the pipeline leverages code-based models internally (such as the LLM and VQA), the dataset itself consists of textual captions and generated images, not code or programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not represent mathematical or logical expressions; the paper discusses mathematical formulas only as part of the methodology explanation, not as dataset content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises text captions and generated images but does not contain biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any fictional or constructed languages are included in the dataset or captions."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is explicitly English; thus, the language is specified and known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains textual captions in English; therefore, it does contain language and N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1768-rubric-6",
    "token_usage": {
      "prompt_tokens": 11563,
      "completion_tokens": 173,
      "total_tokens": 11736
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Footnote in the first page",
          "reasoning": "The paper provides a GitHub URL for the project (https://github.com/Picsart-AI-Research/OpenBias) in the abstract and at the beginning, indicating that code related to the pipeline and dataset construction is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (OpenBias) and Section 4 (Experiments)",
          "reasoning": "The paper describes in detail the process for constructing the dataset of biases starting from real-world captions, leveraging a Large Language Model to generate bias proposals, and the use of a Vision Question Answering model for recognition and quantification. This includes formal definitions, filtering procedures, and evaluation steps, providing clear documentation of the dataset creation and usage pipeline."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-180-rubric-0",
    "token_usage": {
      "prompt_tokens": 14624,
      "completion_tokens": 219,
      "total_tokens": 14843
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Implementation Details: Datasets",
          "Reasoning": "The paper introduces the use of a small clean set DC derived from the original training datasets (e.g., MNIST, CIFAR-10, GTSRB, and a chest X-ray dataset) for training the reference model within the proposed Attack To Defend (A2D) framework. This clean set is sampled from these known image datasets rather than being entirely new data created by the authors; however, the training of the reference model on this clean subset is a core step newly proposed. Since the data itself are images collected by humans originally (datasets like MNIST, CIFAR-10, etc. are human-collected images of digits, traffic signs, X-rays), the modality is image and the origin is human generated. The authors do not create new data modalities or unknown provenance samples; they rely on these human-generated image datasets as the clean data source for their method."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-180-rubric-1",
    "token_usage": {
      "prompt_tokens": 15424,
      "completion_tokens": 249,
      "total_tokens": 15673
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2 (Algorithm 1) and Section 4.1 (Implementation Details)",
            "reasoning": "The annotation involves generating poisoned and benign models programmatically using established poisoning attacks and automatically evaluating models using the A2D framework which trains reference models and generates adversarial samples automatically. There is no mention of human annotators performing labeling or annotation tasks for the new dataset; the dataset consists of synthetic model training and evaluation data created through computational experiments."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not provide or describe any annotation instructions since the dataset is generated and annotated automatically via experimental procedures, not by human annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "There are no scoring rubrics provided for annotation since no human annotation or manual scoring is involved in the data generation or labeling process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "No examples of annotation are provided as the dataset generation and labeling are done automatically through training and evaluation scripts rather than manual annotation with examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-180-rubric-2",
    "token_usage": {
      "prompt_tokens": 16614,
      "completion_tokens": 334,
      "total_tokens": 16948
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on the new datasets; no annotation or expert validation by a single expert is described."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts were involved in quality assurance or validation of the dataset content or annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by a single non-expert human annotator for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description or indication of multiple non-expert annotators conducting quality assurance on the dataset is provided."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of AI models being used as judges or quality assurance tools to validate dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper involves automated training and evaluation processes, it does not describe any automated verification or algorithmic quality assurance for dataset annotations or content quality."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses publicly available datasets (MNIST, CIFAR10, GTSRB, chest X-ray) without describing any new annotation or dataset construction efforts requiring quality assurance. The new data used is mainly small clean sets sampled from existing datasets for training reference models and adversarial attack generation; no explicit quality assurance process for new datasets is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-180-rubric-3",
    "token_usage": {
      "prompt_tokens": 16224,
      "completion_tokens": 396,
      "total_tokens": 16620
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any new dataset was created entirely from scratch by human contributors. Instead, it uses existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe generating any dataset entirely by AI models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by human translation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses standard image datasets (MNIST, CIFAR10, GTSRB, chest X-ray) which are existing publicly available datasets, not newly collated collections introduced by the authors."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 (Implementation Details)",
          "reasoning": "The paper introduces poisoned models trained by corrupting a fraction of clean training samples using various poisoning attacks to produce poisoned training datasets. These poisoned datasets are derived from existing clean datasets via transformations depending on poisoning attack methods (e.g., trigger insertion, label modification). Thus, new poisoned datasets are generated by modifying existing datasets rather than collecting new data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents the data sources and method of generation for datasets used in experiments."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-180-rubric-4",
    "token_usage": {
      "prompt_tokens": 16742,
      "completion_tokens": 325,
      "total_tokens": 17067
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2, Step 1 and Section 4.1",
          "reasoning": "The small clean set (D_C) is used to train a reference model from scratch (e.g., ResNet-18 architecture) which is crucial in the Attack to Defend (A2D) framework to generate adversarial samples for probing the target model."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and throughout Section 4",
          "reasoning": "The datasets (MNIST, CIFAR10, GTSRB, Chest X-ray) are used to evaluate and benchmark the performance of the proposed A2D detector method against various poisoning attacks and baseline defenses. The datasets serve as evaluation benchmarks rather than for training the target models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The datasets are used to analyze model behavior such as activations and intersection-over-union of neuron pathways between poisoned and benign models to understand why poisoned models are more sensitive to adversarial attacks."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-180-rubric-5",
    "token_usage": {
      "prompt_tokens": 17465,
      "completion_tokens": 578,
      "total_tokens": 18043
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets introduced are image datasets for model training and evaluation on tasks like classification; there is no mention of multiple human languages in the dataset entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets with entries containing exactly two human languages are introduced or discussed."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation Details - Datasets",
          "reasoning": "The clean set used for training and evaluation consists of image datasets like MNIST, CIFAR10, GTSRB, and a chest X-ray dataset. The description and explanation, including code, experimental metrics, and theoretical context, are all exclusively in English. No dataset contains human languages other than English descriptions."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used for evaluation or introduced are not specified to contain any non-English natural languages."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions using various algorithms, attacks, and models described in programming or mathematical terms, but no new dataset composed of code or programming language entries is introduced."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses mathematical notation to describe models and metrics, no new dataset containing entries with mathematical or logical expressions is introduced."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not present any dataset with biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets with constructed or fictional languages are introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) of the data entries are well specified or implied; no unknown or undocumented language entries are present."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper introduces new datasets consisting mainly of images and corresponding labels (human language content is present in labels and metadata). Therefore, the new datasets do contain some human language, making 'N/A' inapplicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-180-rubric-6",
    "token_usage": {
      "prompt_tokens": 14683,
      "completion_tokens": 172,
      "total_tokens": 14855
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No code repository or URL is mentioned in the paper for dataset construction.",
          "reasoning": "The paper evaluates the proposed A2D method on multiple pre-existing datasets such as MNIST, CIFAR10, GTSRB, and a chest X-ray dataset, but does not introduce any new dataset themselves, nor does it provide or mention any code for dataset construction or processing. The focus is on poisoned model detection, not dataset creation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No explicit section or subsection describes dataset creation or construction process.",
          "reasoning": "The datasets used in the experiments are existing public datasets widely known in literature, and the paper does not describe any new datasets or provide detailed documentation on dataset creation. The paper only mentions usage of these datasets for evaluation purposes."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-182-rubric-0",
    "token_usage": {
      "prompt_tokens": 10597,
      "completion_tokens": 170,
      "total_tokens": 10767
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Experimental Setup; Section 4.2 Visual Examples for Qualitative Analysis; Section 4.3 Quantitative Evaluations",
          "Reasoning": "The paper conducts experiments that generate images using the proposed AT-EDM framework applied to the pretrained Stable Diffusion XL (SD-XL) diffusion model. The images are generated by the model based on text prompts, indicating these images are model generated. However, as the paper does not introduce a new benchmark dataset or new collected images, but only uses generated images to evaluate their method, no new human-generated image dataset is introduced. Therefore, considering the model-generated images constitute the new data outputs introduced by the authors for evaluation of their novel framework."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-182-rubric-1",
    "token_usage": {
      "prompt_tokens": 11397,
      "completion_tokens": 252,
      "total_tokens": 11649
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1 Experimental Setup, Section 4.2 Visual Examples for Qualitative Analysis, and Sections 3.2 and 3.3 Methodology",
            "reasoning": "The annotations refer to generated images evaluated for visual quality and text-image alignment using manually designed challenging prompts. These evaluations, including ablation studies and design choices, suggest expert-level subjective assessment by the authors or domain experts rather than automated or crowdsourced non-expert labeling."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly provided",
            "reasoning": "The paper does not provide explicit annotation instructions or detailed guidelines given to annotators or evaluators regarding the image evaluation or manual prompt design process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly provided",
            "reasoning": "No formal scoring rubrics or criteria for annotation are described for the manual evaluation of generated images or prompts; quantitative metrics like FID and CLIP scores are used instead."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly provided",
            "reasoning": "The paper does not mention providing annotation examples for manual evaluation or prompt design related to the new dataset generation or assessment."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-182-rubric-2",
    "token_usage": {
      "prompt_tokens": 12587,
      "completion_tokens": 331,
      "total_tokens": 12918
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process conducted by a single human expert related to dataset annotation or content validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence of multiple human experts performing quality assurance on any dataset in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any involvement of a single human non-expert performing quality assurance for dataset validation."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about multiple non-expert humans conducting quality assurance on dataset content or annotations."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses AI models (diffusion models) for image generation, it does not describe any AI model being used explicitly as a judge or quality assurance mechanism for dataset validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes algorithmic methods for token pruning, token recovery, and schedule design for diffusion models, but these relate to model efficiency and generation, not dataset quality assurance processes."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset, nor does it describe any quality assurance processes related to datasets. It focuses on model efficiency improvements and evaluations using existing datasets (e.g., MS-COCO 2017). Therefore, no quality assurance process for dataset annotations or content is applied or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-182-rubric-3",
    "token_usage": {
      "prompt_tokens": 12197,
      "completion_tokens": 417,
      "total_tokens": 12614
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creation of any original datasets or new data generated entirely from human contributors. The focus is on methods to improve efficiency of diffusion models using attention-driven token pruning."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets are generated by AI models as original data entities. The authors apply their method on existing pre-trained diffusion models and evaluate performance, but do not generate or introduce new data formed entirely by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data produced by translating content via human translators is mentioned in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data generated by machine translation systems is referenced or introduced."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe collecting or aggregating existing data into a new dataset without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses existing models (SD-XL) and applies pruning techniques, it does not create a new dataset derived or adapted from existing sources. The work mainly involves algorithmic method development rather than dataset creation or modification."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. All experiments are performed on existing datasets (e.g., MS-COCO 2017 validation set) and pre-trained models. No information or description about novel data creation, collection, or transformation is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-182-rubric-4",
    "token_usage": {
      "prompt_tokens": 12715,
      "completion_tokens": 206,
      "total_tokens": 12921
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. The datasets mentioned, such as MS-COCO 2017 validation set, are pre-existing and used only for evaluation or benchmarking. There is no description or demonstration of a newly proposed dataset, thus no practical utility of any new dataset is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-182-rubric-5",
    "token_usage": {
      "prompt_tokens": 13438,
      "completion_tokens": 500,
      "total_tokens": 13938
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing multiple human languages. The text prompts used are in English only."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence of the proposed datasets containing exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental Setup",
          "reasoning": "The dataset used for evaluation is the MS-COCO 2017 validation dataset, which is an English-caption based dataset. The prompts and text inputs referenced and used throughout the paper are in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing exactly one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the paper discusses algorithms and includes pseudocode, no dataset entries consisting of programming or structured code content are described as part of a proposed dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains mathematical formulas to describe algorithms but does not propose any dataset entries with mathematical or logical symbolic content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication system data is mentioned or proposed in the paper."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificial constructed languages are part of the dataset as described in the paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of dataset entries is clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset definitely contains language entries (English text prompts and captions)."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-182-rubric-6",
    "token_usage": {
      "prompt_tokens": 10656,
      "completion_tokens": 151,
      "total_tokens": 10807
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and throughout the paper",
          "reasoning": "The paper mentions a project webpage (https://atedm.github.io) but does not explicitly provide or refer to publicly available code for dataset construction or dataset-related code. The focus is on efficiency enhancement methods for diffusion models rather than dataset construction, and no code repository for datasets is mentioned."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "N/A",
          "reasoning": "The paper does not introduce any new datasets nor describes any dataset construction processes. It uses existing datasets (e.g., MS-COCO 2017 validation) for evaluation without detailing dataset creation or curation. Therefore, there is no documentation about dataset construction."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1821-rubric-0",
    "token_usage": {
      "prompt_tokens": 12531,
      "completion_tokens": 254,
      "total_tokens": 12785
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 3.1, Section 5 Conclusion and Limitations",
          "Reasoning": "Panda-70M dataset is constructed by collecting 3.8 million high-resolution long videos from the publicly available HD-VILA-100M dataset, which consists of human-generated video content (e.g., news, television shows, documentaries). The dataset videos are split into semantically coherent clips. This indicates the video data is originally human captured or created."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 3.2, Section 3.3",
          "Reasoning": "The captions for the 70M video clips in Panda-70M are automatically generated by multiple cross-modality teacher models (including image/video captioning and visual question answering models) that take multimodal inputs (video frames, video description, subtitles) to produce candidate captions. Then a fine-grained video-to-text retrieval model finetuned on a human annotated 100K subset selects the best caption. Thus, the textual caption annotations are model-generated rather than manually written."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1821-rubric-1",
    "token_usage": {
      "prompt_tokens": 13331,
      "completion_tokens": 458,
      "total_tokens": 13789
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.3; Appendix C.1",
            "reasoning": "Section 3.3 states that a subset of 100K videos was collected where human annotators acted as an oracle to select the best caption for each video. There is no mention that these annotators were experts, so we categorize them as multiple human non-experts."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Appendix C.1",
            "reasoning": "The paper mentions that this subset was specifically collected with human annotators selecting captions, implying that annotation instructions must have been provided to guide consistent selection of the best caption. This is detailed in Appendix C.1 where dataset collection is described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in paper",
            "reasoning": "The paper does not explicitly mention scoring rubrics or detailed criteria for annotators when selecting the best captions, only a general description of selection as oracle annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in paper",
            "reasoning": "The paper does not describe providing annotation examples to the human annotators in the caption selection process."
          }
        },
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 3.2",
            "reasoning": "Captions for Panda-70M are generated using multiple cross-modality teacher models. These include image captioning and video VQA models such as BLIP-2, MiniGPT-4, Video-LLaMA, and VideoChat, which automatically produce candidate captions for the video clips."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in paper",
            "reasoning": "The paper does not indicate that explicit annotation instructions are provided for the AI models; rather, the models follow their pretrained procedures."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in paper",
            "reasoning": "There is no mention of scoring rubrics guiding model caption generation; models produce outputs based on learned parameters."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in paper",
            "reasoning": "No examples or demonstrations for the models are described, as the models generate captions automatically."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1821-rubric-2",
    "token_usage": {
      "prompt_tokens": 14521,
      "completion_tokens": 524,
      "total_tokens": 15045
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper indicates that human annotators selected the best caption from candidates for 100K videos for finetuning purposes, but there is no indication that these annotators were subject matter experts or members of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions multiple human annotators acted as oracles to select the best caption for a subset of 100K videos, but there is no information suggesting that these annotators were subject matter experts or belonged to the target demographic."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The human annotation process involved multiple annotators rather than a single annotator, and the paper does not specify any single non-expert involved in QA."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.3 Fine-grained Video-to-Text Retrieval",
          "reasoning": "The paper states that a subset of 100K videos was annotated by human annotators who acted as oracles to select the best caption among candidates. There is no specification that these annotators were experts, thus they can be reasonably categorized as multiple human non-expert annotators involved in quality assurance."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.3 Fine-grained Video-to-Text Retrieval",
          "reasoning": "After finetuning the retrieval model (Unmasked Teacher) on the human-annotated subset, this AI model is used to select the best caption among candidates for the entire 70M dataset. This constitutes QA by an AI model acting as a judge for caption selection."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance process involves human annotation and AI model selection; no mention is made of automated verification techniques such as code or formula checking for QA."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes a quality assurance process involving multiple human annotators and AI model selection for caption validation; thus, QA is documented and performed."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1821-rubric-3",
    "token_usage": {
      "prompt_tokens": 14131,
      "completion_tokens": 526,
      "total_tokens": 14657
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset Panda-70M is not created by manual human annotation from scratch for all 70M samples, as manual labeling is noted to be prohibitively expensive. Humans only manually select the best caption on a small subset (100K). Therefore, the main dataset is not entirely newly created by humans."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.2 Captioning with Cross-Modality Teachers",
          "reasoning": "Captions are automatically generated by multiple pretrained cross-modality teacher models (e.g. Video-LLaMA, VideoChat, BLIP-2, MiniGPT-4) using multimodal inputs including video frames, subtitles, and descriptions. This automatic generation composes the main part of the caption annotations for 70M videos, thus the dataset is primarily created by AI models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that any data in Panda-70M was created by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using machine translation to generate any of the captions or data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3 Methodology, Introduction",
          "reasoning": "The video clips are obtained by splitting and processing 3.8M high-resolution long videos from the existing HD-VILA-100M dataset, effectively collecting and aggregating existing video data to create the video part of Panda-70M."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3 Methodology and Abstract",
          "reasoning": "The dataset is derived from HD-VILA-100M videos by splitting them into semantically coherent clips and then annotating captions through a pipeline involving multiple existing models and manual selection on a subset. Thus, the data is adapted and transformed from existing sources with modification and augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset origin and generation pipeline is clearly documented in the paper, so N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1821-rubric-4",
    "token_usage": {
      "prompt_tokens": 14649,
      "completion_tokens": 586,
      "total_tokens": 15235
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 4.1 (Video Captioning), Section 4.2 (Video and Text Retrieval), Section 4.3 (Text-to-Video Generation)",
          "reasoning": "The Panda-70M dataset is used to pre-train models for several downstream tasks, including video captioning, video and text retrieval, and text-to-video generation. The paper demonstrates that models pretrained on Panda-70M achieve substantially better performance compared to other pretraining data, indicating its use as pretraining data for learning general multimodal representations."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 (Video Captioning)",
          "reasoning": "The paper reports experiments where models, such as Video-LLaMA, are trained from scratch on Panda-2M, a subset of Panda-70M, demonstrating improved zero-shot video captioning performance over official pretrained weights. This indicates that the dataset is used for training models from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of Panda-70M for supervised fine-tuning of pretrained models. Instead, Panda-70M is primarily used for pretraining/ training."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or description of reinforcement learning post-training techniques, such as RLHF, applied using Panda-70M."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Panda-70M itself is not used exclusively for evaluation or benchmarking; instead, other established datasets are used for evaluating downstream tasks."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use Panda-70M primarily for analyzing trends or characteristics but focuses on dataset creation and leveraging it for training."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although a fine-grained retrieval model is trained utilizing a subset of Panda-70M for caption selection, the dataset itself is not explicitly described as a knowledge base for augmenting models via retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly uses Panda-70M for training and pretraining models, thus it is not unused."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1821-rubric-5",
    "token_usage": {
      "prompt_tokens": 15372,
      "completion_tokens": 599,
      "total_tokens": 15971
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Not explicitly mentioned in the paper",
          "reasoning": "The paper does not state that the Panda-70M dataset contains captions in more than two human languages. No evidence is given about multiple language captions."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Not explicitly mentioned in the paper",
          "reasoning": "The dataset description and examples show captions primarily in English with no mention of a second language being systematically included."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 1 Introduction; Section 4.1 Video Captioning; Figure 5; Appendix E (sample visualizations)",
          "reasoning": "The dataset is described as containing video-caption pairs with captions generated by vision-language models trained on English data and human annotators selecting English captions. The captions shown and evaluation datasets (MSR-VTT, MSVD) imply English content. Also, the textual inputs (titles, descriptions, subtitles) used in captioning are treated as English text. There is no indication of other languages in the captions."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "No mention that the dataset includes captions in a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "No indication or examples of code or programming language content in the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "No evidence that the dataset contains entries with formal or symbolic mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "The dataset comprises video clips with human language captions; there is no inclusion of biological or non-human communication sequences."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "No mention or evidence of constructed or fictional languages included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "The language of captions is explicitly described or implied to be English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain language in the form of captions and textual metadata; thus, the dataset does contain language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1821-rubric-6",
    "token_usage": {
      "prompt_tokens": 12590,
      "completion_tokens": 222,
      "total_tokens": 12812
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section",
          "reasoning": "The paper does not explicitly mention or provide a link to any public repository or code release for the dataset construction, data collection, preprocessing, or caption generation pipeline. No URLs or GitHub links are given for the code associated with Panda-70M dataset creation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and its subsections (3.1, 3.2, 3.3, 3.4) and Appendix A, B, C, D",
          "reasoning": "The paper provides detailed descriptions of the dataset creation process, including semantics-aware video splitting (Section 3.1), captioning with cross-modality teacher models (Section 3.2), fine-grained video-to-text retrieval (Section 3.3), and training of the student captioning model (Section 3.4). Additional methodological details and analysis are given in several appendices (A, B, C, D). This constitutes comprehensive documentation of dataset construction procedures."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1833-rubric-0",
    "token_usage": {
      "prompt_tokens": 11240,
      "completion_tokens": 105,
      "total_tokens": 11345
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2.3 and Section 4.1",
          "Reasoning": "The dataset for training the CADNet and learning the coded aperture mask is generated synthetically by simulating coded dual-pixel images from scenes in the FlyingThings3D dataset using the proposed forward model. This is a simulated dataset created via a differentiable rendering pipeline and not collected from real-world captures."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1833-rubric-1",
    "token_usage": {
      "prompt_tokens": 12040,
      "completion_tokens": 264,
      "total_tokens": 12304
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2.3 Implementation details, Section 4 Experiments and results",
            "reasoning": "The annotations (depth maps and all-in-focus images) are generated using a synthetic dataset rendered via a differentiable coded dual-pixel image simulator based on scenes from the FlyingThings3D dataset. This simulator uses a physical forward imaging model of the dual-pixel sensor with coded aperture, rendering coded DP left and right images, producing groundtruth depth and all-in-focus images automatically for training CADNet. No human annotators are involved in generating these annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not describe providing any annotation instructions for human annotators since the data annotations are generated via a synthetic simulator (automatic process)."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "No scoring rubrics or criteria for annotation quality assessment by humans are mentioned, consistent with annotations being generated automatically."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "No annotation examples for human annotators are provided or necessary as data is synthetically generated and ground truth is known precisely from simulation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1833-rubric-2",
    "token_usage": {
      "prompt_tokens": 13230,
      "completion_tokens": 446,
      "total_tokens": 13676
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on the dataset. The dataset used for training is a synthetic dataset (FlyingThings3D) which is publicly available and not newly annotated by the authors."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on the dataset annotations or content. The dataset used is a publicly available synthetic dataset (FlyingThings3D), and no expert annotation or QA is described."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance done by a single human non-expert. The data is synthetic and generated algorithmically, not manually annotated."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description of multiple non-expert human annotators performing quality assurance on the dataset is provided."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of an AI model being used as a judge to perform quality assurance or validation of dataset annotations or contents."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2.1, Section 3.2.3 and Section 4",
          "reasoning": "The dataset used for training and evaluation is synthetic data rendered using a differentiable coded dual-pixel image simulator based on well-defined physical and optical models. The authors describe an end-to-end differentiable simulation pipeline that generates coded dual-pixel images with calibrated PSFs. This simulation and rendering pipeline acts as an automatic data generation and verification process, ensuring that input-output pairs used for training have consistent physically-based measurements. Since the dataset is synthetic and derived through automated image formation models and rendering processes, the quality assurance of dataset content is ensured by automated verification of the simulation code and optical formulas."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly explains and uses an automated simulation process to generate and validate the synthetic dataset used for training and evaluation; thus, QA is documented and applied."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1833-rubric-3",
    "token_usage": {
      "prompt_tokens": 12840,
      "completion_tokens": 557,
      "total_tokens": 13397
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.2.3 Implementation details and Section 4 Experiments and results",
          "reasoning": "The authors generated new synthetic datasets by rendering coded dual-pixel images using a forward model from the FlyingThings3D dataset, and also collected real-world coded dual-pixel images using prototypes they built (e.g., DSLR with coded aperture, endoscopic and dermoscopic systems). These datasets are original data collected and created by human contributors specifically for training and evaluating their CADS system."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While neural networks (CADNet) are trained to predict depth and all-in-focus images, no data is generated entirely by AI without reference or transformation of existing data. All datasets are either simulated from existing datasets or captured with human-designed experimental setups."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of data produced by translating from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of data generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.2.3 Implementation details",
          "reasoning": "The training data uses the FlyingThings3D dataset, which is an existing publicly available dataset. The authors used this dataset as a base for depth and intensity maps, and then rendered coded dual-pixel images via simulation. Thus, the base data is collected from existing sources without significant modifications."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 CADS framework and Section 4 Experiments and results",
          "reasoning": "The authors derived new coded dual-pixel images from the existing FlyingThings3D dataset by simulating the coded aperture dual-pixel image formation model. They also derived new datasets by capturing real-world coded dual-pixel images using their prototype devices in DSLR, endoscopy, and dermoscopy setups, which transforms raw sensor captures into datasets suitable for their task."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin of all data used for training and evaluation, including synthetic simulation based on existing datasets and real prototype capture setups."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1833-rubric-4",
    "token_usage": {
      "prompt_tokens": 13358,
      "completion_tokens": 462,
      "total_tokens": 13820
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2, Implementation Details (Section 4.1)",
          "reasoning": "The paper uses a newly created synthetic dataset generated from the FlyingThings3D dataset and simulated coded dual-pixel images for training the CADNet model from scratch. The model is trained end-to-end with this dataset to jointly learn the coded aperture mask and neural network weights."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1 (Real results)",
          "reasoning": "The CADNet model trained on synthetic data is fine-tuned on real-world captures with the calibrated point spread functions (PSFs) to adapt the model to real sensor characteristics, representing supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning post-training or RL-based approaches."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the newly introduced dataset itself is used exclusively for evaluation or benchmarking purposes. The paper mainly uses standard datasets (FlyingThings3D) for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not primarily used for analysis of trends or characteristics but for training and fine-tuning the CADNet."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or retrieval augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is clearly used for training (from scratch) and fine-tuning, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1833-rubric-5",
    "token_usage": {
      "prompt_tokens": 14081,
      "completion_tokens": 625,
      "total_tokens": 14706
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset entries containing more than two human languages. The datasets used are based on images and depth maps without linguistic content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or evidence that the dataset entries contain exactly two human languages. The datasets are visual and depth data."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper is written in English but does not describe any dataset containing textual or linguistic content purely in English. Datasets consist of image-depth pairs."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that dataset entries contain any non-English language textual data. The datasets are visual imaging data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses neural networks and optimization but does not introduce any dataset containing code or programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 3.2",
          "reasoning": "The simulated datasets involve mathematical formulations for PSFs, coded masks, defocus models, and image formation models explicitly defined with mathematical equations (Eqs. 1, 2, 3). This indicates entries include mathematical and formal logical representations associated with image formation and depth mapping."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Section 5.1",
          "reasoning": "Although the paper discusses biological specimens such as lamb chops and human skin for endoscopy and dermoscopy experiments, the datasets themselves are visual imaging data (RGB-D) and do not contain biological sequences or non-human communication systems like DNA or animal signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or evidence of fictional or artificially created languages in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language or symbolic content of the datasets is fully described as visual RGB and depth data with accompanying mathematical representations. There is no indication of unknown or unspecified languages."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The proposed datasets are visual RGB-D images and simulations with mathematical models, so they inherently contain mathematical notation and visual content, which conceptually involve symbolic languages. Therefore, it cannot be considered as containing no language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1833-rubric-6",
    "token_usage": {
      "prompt_tokens": 11299,
      "completion_tokens": 204,
      "total_tokens": 11503
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper",
          "reasoning": "The paper does not provide any links, URLs, or references to publicly available code repositories related to the dataset creation, data simulation, or training code. Although the authors describe their training procedure on the FlyingThings3D dataset and real-world experiments, there is no explicit information about releasing code for dataset construction or simulation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3.2, 4, and 5",
          "reasoning": "The paper provides detailed documentation on the dataset creation and simulation process, including the use of the FlyingThings3D dataset for synthetic data generation with a differentiable rendering pipeline, simulating dual-pixel coded aperture images, and fine-tuning on real captured PSFs. The paper describes the parameters used, the forward model, the training procedure, and details about real-world prototype captures, which allows understanding the dataset creation process for reproducibility purposes."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1844-rubric-0",
    "token_usage": {
      "prompt_tokens": 14169,
      "completion_tokens": 477,
      "total_tokens": 14646
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1, Section 4.1, Section 4.4",
          "Reasoning": "The new dataset consists of images cropped using person bounding boxes from input images to extract image features for training the object interactive diffuser. These images are used as inputs and ground truths for the proposed object-interactive skeleton generation and HOI image editing tasks. The images originate from existing HOI datasets (e.g., V-COCO training dataset) as described in the experiments; however, the paper introduces a new set of generated object-interactive skeletons as a novel dataset for HOI image editing. The images themselves are human-generated, captured originally by people, as typical HOI datasets consist of human-taken photographs. This is explicitly described in the feature extraction (Section 3.1) and experimental sections (Section 4.1, 4.4)."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1, Section 4.1, Section 4.4",
          "Reasoning": "The dataset includes object bounding box coordinates and person bounding box coordinates, which are tabular numeric data specifying object and person locations in images. These bounding boxes are human-annotated or derived from existing datasets but are used as inputs in the new dataset construction. Since they are human-annotated or curated from existing labeled datasets, they are human-generated data. This is described in the input requirements for the framework and feature extraction (Section 3.1)."
        },
        {
          "Modality": "graph",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Sections 3.2, 4.4",
          "Reasoning": "The new dataset introduced by the authors includes generated object-interactive skeletons representing joint connections and their interactions with objects. These skeletons are represented as graphs of joints and their relationships, where the associative attention network employs a graph neural network to encode joint relationships. The skeleton data is generated by the model's object interactive diffuser framework, thus model-generated. These graph-structured skeletons are novel data contributed by the authors, as per the description in Sections 3.2 and evaluation in Section 4.4."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1844-rubric-1",
    "token_usage": {
      "prompt_tokens": 14969,
      "completion_tokens": 316,
      "total_tokens": 15285
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3 (Proposed Method), specifically Sections 3.1 to 3.4 and Experimental Sections 4 and 4.1",
            "reasoning": "The paper describes an automatic object interactive diffuser model that generates object-interactive skeletons from image and object bounding box inputs using a diffusion process with associative attention and a novel HOI loss. The annotations involved are the generation of standard skeleton joint locations and confidences, which are obtained or optimized automatically (e.g., GT joints referenced from V-COCO dataset for training and evaluation). Therefore, the annotation process corresponds to an automatic process of generating skeleton guidance maps rather than manual human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No specific section in the paper describes manual annotation instructions.",
            "reasoning": "The paper does not provide any instructions for manual annotators for the new dataset or annotations, as the skeletons are generated automatically by the proposed model."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in Sections 3 or 4 regarding manual scoring rubrics for annotation tasks.",
            "reasoning": "Rubrics are not applicable since the annotations are generated by an automatic model, not scored by humans."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No example annotations or guidelines are presented in the paper or appendices.",
            "reasoning": "No example annotations are provided for human annotators because the annotations are automatically generated and no manual annotation guidelines or examples for data labeling exist."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1844-rubric-2",
    "token_usage": {
      "prompt_tokens": 16159,
      "completion_tokens": 319,
      "total_tokens": 16478
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance being performed by a single human expert for the datasets. There is no information about human expert involvement in quality assurance."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by multiple human experts or multiple annotators with subject matter expertise."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence or description of QA performed by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses AI models for generating skeletons and image editing, it does not describe AI models being used explicitly as judges or for quality assurance of datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated verification processes or automatic quality assurance checks applied to dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce or describe any new dataset, nor does it document any quality assurance process for dataset annotations or content. All data referred to appear to be from pre-existing datasets (e.g., V-COCO). Therefore, there is no quality assurance process described or applied for new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1844-rubric-3",
    "token_usage": {
      "prompt_tokens": 15769,
      "completion_tokens": 479,
      "total_tokens": 16248
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the creation of original datasets manually constructed from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.2 (Object Interactive Diffuser), Section 3 (Proposed Method)",
          "reasoning": "The authors propose an object interactive diffuser model that automatically generates object-interactive skeletons through a diffusion process with associative attention and GNN-based propagation. This generated skeleton data is newly created output from their model rather than directly derived from existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any involvement of machine-based language translation to generate the data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), Section 4.2 (Quantitative Results)",
          "reasoning": "The paper utilizes existing datasets such as V-COCO to obtain ground truth joint data and bounding boxes for training and evaluation. This data is aggregated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.4 (Network Training), Section 4.4.1 (Effect of Scale Factor), Section 5 (Applications)",
          "reasoning": "The generated skeletons are optimized and modified using SMPLify to obtain aligned pseudo SMPL ground truths, which is a transformation applied to existing 3D body models. Additionally, the training involves adaptations such as the novel HOI loss with scaling parameter, which adapts existing skeleton data to better represent interactions."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origins and generation methods are clearly stated and documented within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1844-rubric-4",
    "token_usage": {
      "prompt_tokens": 16287,
      "completion_tokens": 526,
      "total_tokens": 16813
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training any model from scratch using a newly proposed dataset. Instead, it focuses on generating skeleton guidance and using existing pretrained models for image editing."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made that the proposed new dataset is used to fine-tune any pre-trained models directly. The training described is for the object interactive diffuser module generating skeletons using an HOI loss, but this training does not use a novel dataset introduced by the authors."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of reinforcement learning or RLHF methods applied on any dataset introduced by the authors."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The authors propose two novel metrics (top-N accuracy and skeleton probabilistic distance) to evaluate their generated object-interactive skeletons, demonstrating measurement methods for natural interaction in generated skeletons. The metrics are applied on ground truth from V-COCO dataset (existing dataset), but the focus is on evaluating the generated skeletons. This indicates use of a new evaluation methodology rather than a new dataset."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although evaluation metrics are proposed, the paper does not describe the introduction or use of a new dataset primarily for analysis or study of trends/patterns."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce a new dataset; it uses existing datasets such as V-COCO as ground truth/reference data. The main contribution is an object interactive diffuser model for skeleton generation and image editing methods, not a new dataset."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1844-rubric-5",
    "token_usage": {
      "prompt_tokens": 17010,
      "completion_tokens": 576,
      "total_tokens": 17586
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper and introduced dataset do not mention containing entries with multiple human languages. The data and experimental details focus on image and skeleton data without reference to any multilingual textual content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that exactly two human languages are used in the dataset entries. The paper does not specify the use of bilingual textual data within the newly introduced dataset."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, e.g. Abstract and Sections 1, 3, 4",
          "reasoning": "The paper is written in English and the references to dataset annotations or labels (like bounding boxes, joints, feature maps) are all described in English. There is no mention of any other human language content in the dataset; textual descriptions and annotations appear exclusively English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper that the dataset contains any non-English language entries exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper involves neural network architectures and mathematical formulations, it does not present a dataset that includes code or programming language content as data entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains mathematical notations and formulas describing model architectures and training objectives; however, these relate to the methodological descriptions and not to the dataset entries themselves. The dataset entries do not contain mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of biological sequences or non-human communication data in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of constructed languages in dataset entries."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of data annotations and description is explicitly English, not unknown or undocumented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains entries with English textual annotations and labels, thus it is not language-free."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1844-rubric-6",
    "token_usage": {
      "prompt_tokens": 14228,
      "completion_tokens": 157,
      "total_tokens": 14385
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, last sentence",
          "reasoning": "The abstract provides a GitHub link to the code repository (https://github.com/YangChangHee/CVPR2024_Person-In-Place_RELEASE) indicating that the code related to the dataset construction and model components is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Feature Extraction; Section 4 Experiments",
          "reasoning": "The paper details the dataset usage and feature extraction from datasets such as V-COCO for ground truth in Section 3.1 and Section 4 describes evaluation metrics and experiments using these datasets and generated skeletons. The dataset creation and preprocessing steps including feature extraction are clearly documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1865-rubric-0",
    "token_usage": {
      "prompt_tokens": 12303,
      "completion_tokens": 152,
      "total_tokens": 12455
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets",
          "Reasoning": "The paper explicitly states that the authors prepared a new simulated dataset of scenes captured with a single-photon lidar to evaluate their method and baselines. This dataset was generated through simulation, which is an algorithmic process, thus model generated. It consists of transient time-of-flight signals captured by a lidar sensor (signal/sensor modality). The simulation leverages domain expertise but no direct human recording from real scenes is indicated, so it is not human generated. The dataset is new and introduced by the authors as stated in Section 4.1 and the Abstract."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1865-rubric-1",
    "token_usage": {
      "prompt_tokens": 13103,
      "completion_tokens": 255,
      "total_tokens": 13358
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Datasets and Section 4.2 Results",
            "reasoning": "The newly introduced dataset consists of simulated scenes of room layouts with objects such as chair, bunny, dragon, and an occluded bunny in chair, created using physically-based rendering techniques. These datasets were generated programmatically and automatically simulated for training and evaluation of PlatoNeRF, as per Section 4.1 describing 'Simulated Datasets'. There is no indication of human annotators manually labeling data, rather simulation and physical rendering provide the dataset."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not specify any instructions given to annotators since the dataset is synthetically generated, thus no annotation instructions are applicable."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "Since there are no human annotators, no scoring or rubrics for annotation are provided or applicable."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not provide annotation examples because the data is simulated rather than manually annotated."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1865-rubric-2",
    "token_usage": {
      "prompt_tokens": 14293,
      "completion_tokens": 312,
      "total_tokens": 14605
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human annotator with subject matter expertise for the introduced simulated datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts conducted quality assurance on the new datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about quality assurance by a single human non-expert on the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe multiple human non-experts performing quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using AI models for quality assurance of the dataset content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 3.3 (Data Pre-Processing)",
          "reasoning": "The authors describe data pre-processing steps involving signal processing methods such as match filtering and cross-correlation to extract key parameters like time-of-flight and shadow masks from simulated transient measurements. This process is automated and relies on algorithmic techniques rather than manual annotation. There is no mention of manual verification or human annotation for dataset quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance is applied through automated signal processing techniques as described; thus, not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1865-rubric-3",
    "token_usage": {
      "prompt_tokens": 13903,
      "completion_tokens": 364,
      "total_tokens": 14267
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper introduces new simulated datasets of four scenes (room with chair, bunny, dragon, and occluded bunny in chair) created by the authors specifically for evaluating their method. This is original content created by humans from scratch through simulation and is not translated, adapted, or derived from pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was created by human translators from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence of machine-translated data in the dataset creation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not described as collected or aggregated from existing sources without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not described as adapted or modified from existing datasets; rather, they are newly simulated."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the dataset is clearly stated and documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1865-rubric-4",
    "token_usage": {
      "prompt_tokens": 14421,
      "completion_tokens": 572,
      "total_tokens": 14993
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of the new dataset for pre-training any models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 (especially 3.2 and 3.3), Section 4.2",
          "reasoning": "The new simulated lidar datasets introduced in Section 4.1 are used to train the NeRF model from scratch. The paper explicitly describes training the NeRF model supervised by two-bounce transient data acquired from these datasets (Section 3.2 and 3.3). Experimental results and comparisons (Section 4.2) confirm training is done using this data without reliance on pretraining."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of using the newly introduced dataset to fine-tune a pre-trained model under supervised learning settings."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any reinforcement learning based post-training (e.g., RLHF) using the new dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (4.1, 4.2)",
          "reasoning": "The authors release the introduced simulated single-photon lidar dataset to evaluate and measure the performance of their PlatoNeRF method and baseline methods, as described in Section 4.1 and 4.2. This dataset is used explicitly for benchmarking depth reconstruction metrics and ablations."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The paper uses the introduced datasets to analyze factors such as robustness to ambient light, resolution, number of illumination points, and background albedo effects. These analyses help characterize the method\u2019s performance and limitations."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the new dataset is used as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The new dataset is clearly used for multiple practical purposes including training, evaluation, and analysis, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1865-rubric-5",
    "token_usage": {
      "prompt_tokens": 15144,
      "completion_tokens": 571,
      "total_tokens": 15715
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces only simulated and real datasets for 3D scene reconstruction using single-photon lidar measurements. There is no mention of any dataset containing multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of datasets involving exactly two human languages is found in the paper. The datasets are related to time-of-flight measurements and 3D reconstructions."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset content involves physical scene measurements and simulated time-of-flight data, not textual or linguistic content in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets with any non-English textual or linguistic content are introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the authors provide code and model checkpoints on their project page, the datasets themselves are time-of-flight lidar measurement data, not code or programming language datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Sections 3.1, 3.2",
          "reasoning": "While the paper contains extensive mathematical notation and equations to describe the models and methods, the datasets themselves are composed of simulated and real transient measurements, not mathematical formulas or logical expressions as dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets pertain purely to physical lidar captures of indoor scenes and do not include any biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are part of the datasets introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language or nature of the dataset entries is clearly documented as lidar transient measurements or simulated physics-based data, so this category does not apply."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The proposed datasets consist of time-of-flight transient measurements obtained from single-photon lidar sensors and simulated physical measurements, which do not contain any human language or linguistic content."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1865-rubric-6",
    "token_usage": {
      "prompt_tokens": 12362,
      "completion_tokens": 157,
      "total_tokens": 12519
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 4.1",
          "reasoning": "The paper states in the abstract and in Section 4.1 (Datasets) that they have released the dataset along with their code and model checkpoints on their project page at https://platonerf.github.io, indicating that code related to data collection, preprocessing, and generation is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper provides detailed descriptions in Section 4.1 about the creation of multiple simulated datasets with scenes and describes the simulation and data capture process for the single-photon lidar transients, indicating the dataset creation process is documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-187-rubric-0",
    "token_usage": {
      "prompt_tokens": 12216,
      "completion_tokens": 239,
      "total_tokens": 12455
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Datasets",
          "Reasoning": "The paper introduces the use of AVSBench dataset which the authors use and extend. It indicates the S4 subset includes videos with labeled frames and unlabeled neighboring and distant frames. The MS3 subset is augmented by extracting semantically relevant videos from the VGGSound dataset to obtain distant frames, implying that the distant frames are collected videos from human-generated content, not synthesized or generated by models. Therefore, the video data captured is human-generated content recorded with cameras."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Motivation and Section 4 Method, Section 5.1 Datasets",
          "Reasoning": "Audio clips corresponding to each video frame are used as inputs for the audio-visual segmentation task. The audio is from real videos in AVSBench and VGGSound datasets, implying the audio was recorded from real-world human-generated events, not synthesized or model-generated audio. Thus, the audio modality is human generated."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-187-rubric-1",
    "token_usage": {
      "prompt_tokens": 13016,
      "completion_tokens": 206,
      "total_tokens": 13222
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 5.1, Datasets",
            "reasoning": "The new AVSBench dataset including its subsets S4 and MS3 is described as having segmentation mask annotations for sounding objects, indicating manual annotations by human experts. Multiple experts are likely involved due to large dataset size and standard practice in such tasks."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper does not explicitly describe detailed annotation instructions provided for mask annotation in the new AVSBench datasets or its subsets."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "No mention is made of scoring rubrics or formal rubric guidance for annotation in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper does not present or discuss providing annotation examples or example annotations for annotators."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-187-rubric-2",
    "token_usage": {
      "prompt_tokens": 14206,
      "completion_tokens": 371,
      "total_tokens": 14577
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 5.1 Datasets",
          "reasoning": "The paper describes the AVSBench dataset which includes segmentation mask annotations for sounding objects. The dataset is recently proposed with annotated frames for training, validation, and test sets. Although no explicit details on QA procedures are given, the dataset is a newly introduced resource with segmentation groundtruths, which typically implies that multiple expert annotators performed or verified the annotations to ensure quality, especially given the complexity of segmentation masks. The absence of explicit annotator details suggests they are experts but unspecified in number; however, since the quality assurance must be conducted by humans for such mask annotations, it is reasonable to infer that multiple human experts were involved based on standard practice in this field."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of a single non-expert providing quality assurance of annotations."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify QA by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of AI models for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification or validation of dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper introduces the AVSBench dataset with annotation masks, implying annotation and quality assurance processes; although not detailed, QA is implied and thus N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-187-rubric-3",
    "token_usage": {
      "prompt_tokens": 13816,
      "completion_tokens": 502,
      "total_tokens": 14318
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 5.1 Datasets",
          "reasoning": "The paper uses the AVSBench dataset which was recently proposed for audio-visual segmentation and contains new original frames with segmentation mask annotations created by human contributors. The authors use the annotated frames from this dataset (e.g., 3452 training videos with annotations) as their labeled data for training and evaluation. This constitutes new human-created data relevant to the task."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any new data entirely by AI or models. They only generate pseudo-labels for unlabeled frames during training, but the underlying image and audio data are not AI-generated."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention of translation of data from another language by humans in this paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation applied to any data in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.1 Datasets",
          "reasoning": "For the MS3 subset, the authors extract semantically relevant videos from the VGGSound dataset as distant frames which are unlabeled. This indicates the data is collected or aggregated from an existing external dataset without significant modification for enhancing training diversity."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4 Method, especially 4.2 Distant Frame Exploitation",
          "reasoning": "The authors derive additional training data from existing video frames (labeled frames plus distant frames from VGGSound) by applying augmentations and using pseudo-labels generated via teacher-student training. This constitutes modification and transformation applied to existing data to create new supervisory signals and augmentations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin and nature of all data used, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-187-rubric-4",
    "token_usage": {
      "prompt_tokens": 14334,
      "completion_tokens": 436,
      "total_tokens": 14770
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5.1 (Datasets) and Section 5.2 (Comparison with Prior Arts)",
          "reasoning": "The AVSBench-S4 and MS3 subsets, as well as semantically relevant videos extracted from VGGSound, are used to train the segmentation models from scratch by the authors. The models are trained starting from backbones such as ResNet or PVT, as indicated in the implementation details and experiments, where they train for 120 epochs."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.2 (Comparison with Prior Arts) and Section 5.3 (Ablation Study)",
          "reasoning": "The paper describes the use of pre-trained backbones (e.g., pre-training on S4 subset) and fine-tuning on the MS3 subset, showing improved performance. This indicates that the datasets are used for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1 (Datasets), Section 5.2 (Comparison with Prior Arts)",
          "reasoning": "The AVSBench dataset subsets S4 and MS3 are used for evaluating model performance on audio-visual segmentation, with metrics such as mean Intersection-over-Union and F-score reported in the experiments."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 (Motivation) and Section 5.3 (Ablation Study)",
          "reasoning": "The authors analyze characteristics of unlabeled neighboring frames and distant frames to motivate their approach. Ablation studies analyze the effects of utilizing these frames on model performance, showing analysis usage of datasets for understanding trends and model behavior."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-187-rubric-5",
    "token_usage": {
      "prompt_tokens": 15057,
      "completion_tokens": 638,
      "total_tokens": 15695
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces the AVSBench dataset and utilizes audio-visual data primarily involving videos and accompanying audio. There is no mention or indication that the dataset contains multiple human languages or transcriptions in various languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that the dataset includes exactly two human languages or bilingual data entries."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5.1 Experimental setup - Datasets",
          "reasoning": "The AVSBench dataset is described with videos and audio data, but the paper and all references are in English, and the task is audio-visual segmentation without mention of languages other than English. Additionally, standard audio features (VGGish pretrained on AudioSet) are used, which are largely English-centric. No other languages are mentioned for audio or annotations, implying the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that dataset content is in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are videos with corresponding audio and segmentation masks; the dataset does not contain code or programming languages."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical formulas to describe the method and losses, the dataset itself is video and audio data with masks; the dataset entries do not include mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves human video and audio data related to sounding objects (e.g., instruments, animals like dogs and birds), but it is not specified or documented as biological sequences or non-human communication systems data such as DNA or animal signaling. The paper does not describe the dataset as biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is sufficiently described through references to known datasets and annotations; it is not unknown or undocumented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains videos and audio streams which correspond to human language speech/audio and visual content, so it contains language modality."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-187-rubric-6",
    "token_usage": {
      "prompt_tokens": 12275,
      "completion_tokens": 160,
      "total_tokens": 12435
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not mention any link or availability of code related to dataset construction, data collection, or processing for reproducibility purposes. It mainly builds upon existing datasets (AVSBench and VGGSound) and modifies training strategies but does not provide or state availability of dataset construction code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 5.1 Datasets",
          "reasoning": "The paper provides information about the datasets used (AVSBench with its subsets S4 and MS3 and VGGSound for certain distant frames), including data split details, annotation characteristics, and how distant frames are selected for MS3. This constitutes documentation of the dataset construction or selection process relevant to their training."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1968-rubric-0",
    "token_usage": {
      "prompt_tokens": 18912,
      "completion_tokens": 147,
      "total_tokens": 19059
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Experimental Setup - Datasets; Figure 4 visualization; Table 9 results on Lucchi dataset",
          "Reasoning": "The paper introduces the use of the Lucchi mitochondria segmentation dataset, which consists of electron microscope images (images) for segmentation tasks. These images are collected via human-operated electron microscopy and thus are human generated. There is no indication that these images are synthetically generated by a model or have unknown provenance. The Lucchi dataset is used as a challenging benchmark to demonstrate scalability, indicating that the authors utilize it but do not create new data from scratch here."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1968-rubric-1",
    "token_usage": {
      "prompt_tokens": 19712,
      "completion_tokens": 318,
      "total_tokens": 20030
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1 Dataset, Figure 4, and Table 9",
            "reasoning": "The paper introduces the Lucchi dataset specifically for mitochondria segmentation as a new evaluation benchmark for the proposed RankMatch method. It is a specialized biomedical dataset requiring expert annotation due to the nature of the data (electron microscope images). The annotations are described in related works cited [29] and used as ground truth for semi-supervised training and evaluation, indicating expert annotators performed the labeling."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention of annotation instructions or guidelines for Lucchi dataset in the paper",
            "reasoning": "The paper does not provide any explicit description or citation of detailed annotation instructions for the new Lucchi dataset. The annotation process is not discussed, thus no instructions can be confirmed as present."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No description of annotation rubrics for Lucchi dataset",
            "reasoning": "The paper contains no mention or discussion of scoring rubrics or criteria used by annotators to define or evaluate annotations on the new Lucchi dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples provided for Lucchi dataset (no Appendix or main text showing annotation examples)",
            "reasoning": "Although visualization of the dataset appears in Figure 4, which shows images and ground truth, this does not qualify as annotation examples or detailed guidelines for annotators. The paper does not provide annotation examples or settled guidelines for annotation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-1968-rubric-2",
    "token_usage": {
      "prompt_tokens": 20902,
      "completion_tokens": 303,
      "total_tokens": 21205
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert on the new datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple expert human annotators conducting quality assurance on the datasets in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance by a single non-expert annotator for the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple non-expert human annotators performing quality assurance on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance of the dataset annotations or content by an AI model is not reported in the paper."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification processes used as quality assurance for dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets nor describe any quality assurance process applied to dataset annotations or content. The datasets used (PASCAL VOC 2012, Cityscapes, Lucchi) are existing benchmarks with no newly collected or annotated data described requiring QA. Therefore, no quality assurance process is applied or documented for datasets in this paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1968-rubric-3",
    "token_usage": {
      "prompt_tokens": 20512,
      "completion_tokens": 412,
      "total_tokens": 20924
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creation of any new datasets or original data contributed by humans. It uses existing public datasets such as PASCAL VOC 2012, Cityscapes, and Lucchi for experiments."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset itself was generated by AI or machine learning models. The paper focuses on semi-supervised learning methods using existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data being generated by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of machine translation applied to data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental Setup, Datasets",
          "reasoning": "The paper uses publicly available datasets (PASCAL VOC 2012, Cityscapes, Lucchi) which are established benchmarks collected by other groups. The authors aggregate or utilize these existing sources without significant modification, thus the data can be considered collated."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the creation of new datasets derived from existing data with modifications or transformations. The paper focuses on modeling and method development rather than on dataset derivation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation is specified, as the authors clearly mention using existing, well-known datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1968-rubric-4",
    "token_usage": {
      "prompt_tokens": 21030,
      "completion_tokens": 242,
      "total_tokens": 21272
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3 Scalability for Other Scenarios",
          "reasoning": "The newly introduced dataset Lucchi is utilized to evaluate the scalability and performance of the proposed RankMatch method on mitochondria segmentation tasks, serving exclusively for benchmarking and performance measurement purposes."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; all datasets used (PASCAL VOC 2012, Cityscapes, Lucchi) are pre-existing. Thus, there is no practical usage of new datasets described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1968-rubric-5",
    "token_usage": {
      "prompt_tokens": 21753,
      "completion_tokens": 610,
      "total_tokens": 22363
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's datasets and examples do not include entries with more than two human languages. The datasets used (PASCAL VOC 2012, Cityscapes, and Lucchi) are all English-labeled or language-agnostic image datasets without multilingual text content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset contains exactly two human languages in its entries. The focus is on image segmentation datasets without bilingual textual entries."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental Setup - Datasets",
          "reasoning": "The datasets used for evaluation such as PASCAL VOC 2012 and Cityscapes are labeled in English, as inferred from the standard usage and descriptions. The paper further uses English for all annotations and evaluation metrics descriptions, indicating the dataset entries are in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets referenced are standard English-language datasets; no dataset with only a non-English language is introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper includes pseudo-code and formulas, the proposed new datasets do not contain programming or code content entries. The code is presented as part of the methodology, not as dataset content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper presents mathematical notation extensively to describe the methods, the proposed datasets do not contain entries with mathematical or logical expressions as part of the data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper evaluates on mitochondria segmentation datasets (e.g., Lucchi), which are images of biological structures but the dataset entries themselves are images, not biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are present in the datasets introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly documented as English where text labels are involved."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain English language annotations; hence they contain language data."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-1968-rubric-6",
    "token_usage": {
      "prompt_tokens": 18971,
      "completion_tokens": 140,
      "total_tokens": 19111
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention indicating public availability of dataset construction code",
          "reasoning": "The paper uses pre-existing public datasets (PASCAL VOC 2012, Cityscapes, Lucchi) for evaluation but does not introduce any new dataset nor mentions releasing any code related to dataset construction. No links or supplementary materials with dataset construction code are provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No dedicated section or description on dataset creation process",
          "reasoning": "The paper does not introduce any new dataset but rather evaluates on existing public datasets. Therefore, it does not provide any documentation or details on dataset construction process."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2046-rubric-0",
    "token_usage": {
      "prompt_tokens": 12101,
      "completion_tokens": 402,
      "total_tokens": 12503
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1",
          "Reasoning": "The experiments in Section 4.1 use image pairs from datasets IMC Phototourism 2021, MegaDepth-1500, and ScanNet-1500, where images are captured by humans or human-operated devices and then used for two-view relative pose estimation. These datasets are existing benchmark datasets used to validate the Sampson error theory and are thus not new datasets introduced by the authors."
        },
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.2",
          "Reasoning": "In Section 4.2, the authors generate synthetic camera triplets and simulate 3D points with added Gaussian noise to evaluate different three-view Sampson error approximations. This synthetic data is generated by the authors programmatically as part of their experiments, thus constituting a new dataset introduced by the authors. The modality is image since the data corresponds to projected image points in simulated camera images."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.3",
          "Reasoning": "Section 4.3 uses line-segment and vanishing point pairs collected from existing datasets YUD+ and NYU VP. These datasets contain images and line segment detections obtained via DeepLSD. As these datasets are pre-existing and used for evaluation, they are not new datasets introduced by the authors."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.4",
          "Reasoning": "In Section 4.4, the experiments use the 7Scenes visual localization benchmark dataset of images with 2D-3D correspondences. This dataset is pre-existing and used for evaluation, not newly introduced by the authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2046-rubric-1",
    "token_usage": {
      "prompt_tokens": 12901,
      "completion_tokens": 214,
      "total_tokens": 13115
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4 Experimental Evaluation, especially 4.1 to 4.4",
            "reasoning": "The paper evaluates the Sampson approximation as a computational method for geometric error in various tasks using existing datasets. There is no indication of human annotators labeling data; the error approximations are computed algorithmically and evaluated quantitatively."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not describe any manual annotation process or instructions provided to annotators because the evaluation is purely computational."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics or annotation quality criteria are mentioned since there was no manual annotation; the evaluation is mathematical and experimental in nature."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples are provided because no manual annotation was performed; the paper focuses on theoretical derivations and experimental evaluation of an algorithmic approximation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2046-rubric-2",
    "token_usage": {
      "prompt_tokens": 14091,
      "completion_tokens": 326,
      "total_tokens": 14417
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance involving a single human expert validating dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of multiple human experts performing quality assurance on dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single non-expert human performed quality assurance on dataset annotations or content."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information suggesting that multiple non-expert human annotators were involved in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report using an AI model to perform quality assurance on dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes algorithmic and mathematical validation of approximations, it does not describe an automated quality assurance process on datasets or annotations. The experimental validations use standard datasets without mention of automated annotation QA."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets requiring annotation or quality assurance. The experiments use existing datasets (e.g., IMC Phototourism, MegaDepth, ScanNet, YUB+, NYU VP, 7Scenes) without describing any QA procedures for dataset annotation or content. Therefore, no dataset quality assurance process is documented or applied."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2046-rubric-3",
    "token_usage": {
      "prompt_tokens": 13701,
      "completion_tokens": 421,
      "total_tokens": 14122
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The paper uses datasets such as IMC Phototourism 2021, MegaDepth-1500, and ScanNet-1500 as sources of image pairs and 2D correspondences. These datasets are pre-existing and are aggregated from existing sources to evaluate the Sampson approximation. Hence, the used data in the experiments is collated."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4 (Experimental Evaluation), especially 4.1, 4.2, 4.3, and 4.4",
          "reasoning": "Although the raw data derives from existing datasets, the paper introduces derived data sets such as synthetic camera triplets with added noise in Section 4.2 and combinations of constraints for evaluating novel error approximations. Similarly, in Section 4.3 and 4.4, line segment to vanishing point pairs and 2D/3D correspondences with propagated covariances are used, which are transformations or adaptations of existing datasets. This constitutes derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly states the datasets used for evaluation and how synthetic data was generated; thus the origin of data is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2046-rubric-4",
    "token_usage": {
      "prompt_tokens": 14219,
      "completion_tokens": 289,
      "total_tokens": 14508
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (4.1, 4.2, 4.3, 4.4)",
          "reasoning": "The datasets introduced by the authors, such as those for two-view, three-view geometric estimation, vanishing point estimation, and 2D/3D reprojection error scenarios, are primarily used for experimental evaluation and benchmarking. The authors use these datasets to validate theoretical bounds, compare error approximations, and evaluate pose refinement methods."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 (Bounding the Approximation Error) and Section 4 (Experiments)",
          "reasoning": "The datasets facilitate analysis by enabling investigation into the quality and tightness of the Sampson approximation bounds and the relationship between the geometric and approximate errors in multiple geometric estimation problems."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2046-rubric-5",
    "token_usage": {
      "prompt_tokens": 14942,
      "completion_tokens": 532,
      "total_tokens": 15474
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset containing multiple human languages. The datasets mentioned are related to image, line segments, camera poses, and geometry, not human language data."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset is described as containing exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset based on English textual content; all datasets concern image correspondences and geometric data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There are no datasets with a single non-English human language introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper presents formulas and algorithms, it does not introduce a dataset containing programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Entire paper, especially Sections 2, 3, and 4",
          "reasoning": "The introduced datasets consist of measurements and geometric estimation problems encoded with extensive use of mathematical and symbolic representations such as polynomial constraints, Jacobians, Hessians, and optimization problems. The datasets are defined over geometric and algebraic varieties represented by mathematical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data is introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificially created languages are part of the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of datasets is clearly mathematical and geometric; it is not unspecified or undocumented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced contain data described with mathematical notation and geometric measurements, which constitute a form of language (mathematical language), so this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2046-rubric-6",
    "token_usage": {
      "prompt_tokens": 12160,
      "completion_tokens": 173,
      "total_tokens": 12333
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "N/A",
          "reasoning": "The paper does not mention or provide any link or reference to code repositories associated with data collection, preprocessing, or dataset generation. The content focuses on theoretical analysis, derivation, and evaluation using existing datasets, with no indication of publicly shared code for dataset creation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "N/A",
          "reasoning": "The paper uses pre-existing datasets for experiments (e.g., IMC Phototourism 2021, MegaDepth-1500, ScanNet-1500, YUB+, NYU VP, 7Scenes) but does not introduce any new dataset or provide documentation about dataset creation processes. As the paper focuses on theoretical bounds and evaluations using known datasets, there is no documentation on new dataset creation."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-210-rubric-0",
    "token_usage": {
      "prompt_tokens": 13202,
      "completion_tokens": 168,
      "total_tokens": 13370
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.4 Implementation Details - Training h-space classifiers and Distribution guidance; Section 4.4 Main Results; Section 4.7 Debiasing Text-to-Image Diffusion Models",
          "Reasoning": "The authors create new datasets by generating images using pretrained diffusion models (e.g., P2 and Stable Diffusion v1.5) and generate labeled datasets for training h-space attribute classifiers. These images are generated by models without manual human creation, which makes the data model generated. The paper elaborates on generating 10K images with specific prompts to create labeled data for classifier training (Section 4.7). Thus this new generated image dataset is model generated data of modality image."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-210-rubric-1",
    "token_usage": {
      "prompt_tokens": 14002,
      "completion_tokens": 259,
      "total_tokens": 14261
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.4, Section 4.2 Implementation Details, and associated descriptions",
            "reasoning": "The Attribute Distribution Predictor (ADP) that predicts attribute distributions from h-space features is trained using pseudo labels generated from existing attribute classifiers. The attribute labels used for training are generated automatically using pretrained classifiers without human intervention. Hence, the annotations are performed by an automatic process involving pretrained classifiers and automated feature extraction."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "The paper does not mention any instructions provided to annotators for generating the attribute labels. The labels are automatically generated by pretrained attribute classifiers, so no human annotators were given instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "The process uses pretrained classifiers to generate pseudo labels. There is no mention of scoring rubrics provided to annotators, as no human annotation or scoring appears involved."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "Since the attribute labels are generated automatically by classifiers, there are no annotation examples provided for human annotators in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-210-rubric-2",
    "token_usage": {
      "prompt_tokens": 15192,
      "completion_tokens": 339,
      "total_tokens": 15531
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any involvement of a single human expert performing quality assurance on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance on dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human non-expert on the dataset annotations or content."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about multiple non-expert humans performing quality assurance on the datasets introduced."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.4 (Training h-space classifiers) and Section 4.2 (Implementation Details)",
          "reasoning": "The dataset annotations for training attribute classifiers are generated using pretrained attribute classifiers (AI models) to obtain pseudo labels. These AI models serve as the source of attribute annotations used for training the Attribute Distribution Predictor (ADP), effectively functioning as quality assurance for the attribute labels."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While training and inference processes are automated, no explicit mention is made of automatic verification or algorithmic quality assurance applied to dataset annotations beyond the use of pretrained attribute classifiers for label generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents that attribute labels for the introduced datasets are generated via pretrained attribute classifiers (AI models), hence a quality assurance process is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-210-rubric-3",
    "token_usage": {
      "prompt_tokens": 14802,
      "completion_tokens": 486,
      "total_tokens": 15288
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any datasets created entirely from scratch by human contributors. All data used or referred to are pre-existing or generated by models."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Sections 3.2, 3.3, 4.4, 4.7",
          "reasoning": "The authors generate new datasets by sampling images from pretrained diffusion models guided by their proposed distribution guidance method to debias the data distribution. For example, they generated datasets with balanced gender, race, and other attributes by controlling the pretrained diffusion models without retraining. These generated datasets are novel and originate entirely from the AI diffusion models guided by their method."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of data produced by human translation of other language content."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of data generated by machine translation of other language content."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe collecting or aggregating data from existing sources without modifications. They utilize existing pretrained models and datasets, but no new collated datasets are introduced."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Sections 3.4, 4.2, 4.7, 4.8",
          "reasoning": "The new datasets generated by the authors are derived from existing pretrained diffusion models and existing attribute classifiers. The Attribute Distribution Predictor is trained using pseudo labels from pretrained attribute classifiers. The generated images are modified versions of the data distribution that reflect guided sampling to adjust attribute distributions. Hence, the new datasets are derived through modification and transformation of existing model distributions."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies clearly the data generation process and origin for the new datasets introduced."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-210-rubric-4",
    "token_usage": {
      "prompt_tokens": 15320,
      "completion_tokens": 272,
      "total_tokens": 15592
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.8 Class-imbalance in attribute classification",
          "reasoning": "The paper describes using the newly generated dataset, produced via the proposed distribution guidance method, to augment training data for a downstream race classifier. This augmentation is used for supervised fine-tuning of the classifier to improve performance on underrepresented minority classes."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.4 Main Results and Section 4.1 Evaluation Metrics",
          "reasoning": "The generated datasets are used for quantitative evaluation of fairness and image quality of different debiasing methods using metrics like Fairness Discrepancy (FD) and Fr\u00e9chet Inception Distance (FID), thus serving for benchmarking and performance measurement."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-210-rubric-5",
    "token_usage": {
      "prompt_tokens": 16043,
      "completion_tokens": 583,
      "total_tokens": 16626
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's datasets, including their generated images and attribute classifiers, are all described in terms of attributes like gender, race, and age but no mention is made of datasets containing multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset described in the paper consists of exactly two human languages; the datasets are image-based with annotations or labels for demographic attributes without linguistic content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper is written in English and includes textual prompts (e.g., text-to-image generation with prompts like 'a photo of a male'), the new datasets introduced consist primarily of images and attribute labels, not English textual content as data entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset containing entries in a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced in the paper do not include programming code or structured code-related content; they are image datasets with attribute annotations or classifier pseudo-labels."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical notation to describe diffusion models and algorithms, there is no new dataset introduced that contains entries with mathematical or logical expressions as data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any datasets containing biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional language data is present in the datasets introduced by this paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language or linguistic content of dataset entries is clearly specified or irrelevant (the datasets are image-based with attribute labels), so this category does not apply."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new datasets introduced are image datasets with attribute annotations or pseudo-labels; no language-based entries are included. Therefore, the datasets do not contain any language as data entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-210-rubric-6",
    "token_usage": {
      "prompt_tokens": 13261,
      "completion_tokens": 148,
      "total_tokens": 13409
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and footnote",
          "reasoning": "The abstract states 'Code is available at - project page,' indicating that the code related to the work, including data generation and processing, is publicly available in a repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.2 Implementation Details; throughout Sections 3 and 4",
          "reasoning": "The paper details the dataset creation and labeling process in Section 4.2, including the training of h-space classifiers using labeled datasets obtained via pretrained attribute classifiers, and the use of inversion for latent features. It also explains the training datasets used and provides sufficient methodology to reproduce the dataset generation and usage."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2121-rubric-0",
    "token_usage": {
      "prompt_tokens": 15998,
      "completion_tokens": 361,
      "total_tokens": 16359
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.3 Pre-training Data, Multi-object dataset",
          "Reasoning": "The multi-object dataset is automatically constructed using Blenderproc, a procedural pipeline for photorealistic rendering, by randomly selecting objects from single-object datasets, simulating physical interactions and generating scenes with rendered RGB-D images and labels. This data is generated via simulation without direct human capture."
        },
        {
          "Modality": "tabular",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.3 Pre-training Data, Grasping pose generation",
          "Reasoning": "Grasping poses are obtained by re-using the existing ACRONYM dataset which contains physically verified grasping poses generated through simulation and physical verification processes. These grasp poses are assigned to points in the multi-object scenes. The grasping data is thus model generated via simulation, not human captured."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.3 Pre-training Data, Single-object dataset",
          "Reasoning": "Textual descriptions for single objects are generated by captioning rendered images via existing image captioning systems and metadata retrieval, thus automatically generated by models rather than manually written by humans."
        },
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.3 Pre-training Data, Single-object dataset and Multi-object dataset",
          "Reasoning": "Images are generated by rendering 3D objects from multiple fixed views or virtual camera positions using simulation tools (e.g., Blenderproc), constituting model-generated image data rather than human-captured photos."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2121-rubric-1",
    "token_usage": {
      "prompt_tokens": 16798,
      "completion_tokens": 225,
      "total_tokens": 17023
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3, Multi-object dataset",
            "reasoning": "The paper states that multi-object scenes are automatically constructed using Blenderproc with randomly selected objects and physical simulation, indicating no humans performed annotations manually but an automatic simulation-based process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.3, Multi-object dataset",
            "reasoning": "There is no mention of human annotation instructions related to scene or label creation in the multi-object dataset construction process which is automatic."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.3, Multi-object dataset",
            "reasoning": "The paper does not describe any scoring rubrics used for annotation or labeling during multi-object dataset creation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.3, Multi-object dataset, Figure 1 (top left)",
            "reasoning": "While examples of generated scenes are illustrated (Figure 1), the paper does not describe providing annotation examples as part of annotation guidelines for multi-object dataset construction."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2121-rubric-2",
    "token_usage": {
      "prompt_tokens": 17988,
      "completion_tokens": 323,
      "total_tokens": 18311
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper lacks any description of quality assurance by multiple human experts for the new datasets introduced."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single human non-expert performed quality assurance for the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by multiple human non-expert annotators for the new datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of AI models being used as judges or for quality assurance of the datasets."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3 Pre-training Data",
          "reasoning": "The multi-object dataset is constructed automatically in simulation using Blenderproc with physical simulation for object placement and camera rendering. Grasping poses are obtained from existing physically verified datasets, and collisions are filtered automatically. Thus, the data generation and labeling process relies on automated simulation and code-driven verification without human annotation, indicating an automated quality assurance process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Although explicit human quality assurance is not described, the automated procedures are documented; therefore, it is not correct to say no quality assurance process is applied or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2121-rubric-3",
    "token_usage": {
      "prompt_tokens": 17598,
      "completion_tokens": 473,
      "total_tokens": 18071
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.3 Pre-training Data: Multi-object dataset",
          "reasoning": "The paper describes an automatic multi-object scene dataset constructed by physically simulating scenes using large-scale 3D datasets and Blenderproc. The authors generate 48.9K and 62.8K multi-object scenes using objects from ShapeNet and Objaverse datasets respectively. This dataset is newly created by the authors (human contributors) via scene construction and simulation, which is original content created beyond merely collating existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of datasets generated entirely by AI or ML models without reference to existing data is provided."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any data produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of datasets generated via machine translation is noted."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.3 Pre-training Data: Single-object dataset",
          "reasoning": "The single-object dataset used for pre-training is assembled by combining existing datasets such as ShapeNet, 3D-FUTURE, ABO, and Objaverse. The authors aggregated these datasets without significant modification, i.e., collated them to create a large single-object dataset."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.3 Pre-training Data: Grasping pose generation",
          "reasoning": "The grasping pose generation data reuses the existing ACRONYM grasping dataset and filters invalid grasp poses in multi-object scenes, thereby applying modifications and adaptations to existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources and generation methods are well documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2121-rubric-4",
    "token_usage": {
      "prompt_tokens": 18116,
      "completion_tokens": 490,
      "total_tokens": 18606
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.3 and Section 3.4",
          "reasoning": "The paper clearly states that the authors automatically construct a multi-object dataset using objects from existing single-object datasets, employing physical simulation and rendering to generate cluttered multi-object scenes. This newly constructed multi-object dataset is used directly for pre-training their 3D representation model over multiple pre-training tasks, including masked point modeling, grasping pose synthesis, instance segmentation, and referring expression grounding. The pre-training strategy using this dataset is described in Section 3.4."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the newly introduced dataset being used for training a model from scratch without pre-training."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new multi-object dataset is not used for supervised fine-tuning of pre-trained models; fine-tuning is performed on other downstream task datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the newly constructed multi-object dataset is used for reinforcement learning post-training or RLHF."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The newly constructed multi-object dataset is not used exclusively for evaluation; evaluation is performed on standard benchmark datasets distinct from the constructed dataset."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used primarily for analyzing trends or patterns but rather for model pre-training."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as serving as a knowledge base for retrieval or augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is actively used in pre-training and thus cannot be labeled as not having practical usage."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2121-rubric-5",
    "token_usage": {
      "prompt_tokens": 18839,
      "completion_tokens": 635,
      "total_tokens": 19474
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's new pre-training datasets involve points clouds, rendered images, and text descriptions aligned to objects, but all textual data appear to be English-based as per the description of metadata and natural language refers to English only. There is no evidence of data entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset descriptions do not indicate the presence of exactly two languages; all textual data appears to be in a single language."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.3 Pre-training Data, Single-object dataset",
          "reasoning": "The single-object dataset includes text descriptions built by captioning rendered images and metadata of object names. The paper explicitly mentions using existing image captioning systems and metadata in English. Additionally, referring expression grounding tasks use natural language sentences, assumed to be English as commonly used in robotics datasets referenced (e.g., OCID-Ref, RoboRefit). No other languages are mentioned."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that any of the datasets use non-English language exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper includes equations and model architectures described formally, there is no indication that the datasets themselves contain entries with programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notation is used in the description of model and training objectives, but these are not part of the dataset entries themselves. The paper does not describe dataset entries containing mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not contain biological sequences or non-human communication. They are 3D object point clouds, images, and textual descriptions."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or artificially constructed languages in dataset entries."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language used in the dataset textual entries is identified clearly as English; the paper provides no ambiguity about language documentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain language in form of text descriptions, captions, and referring expressions, so the dataset is not without language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2121-rubric-6",
    "token_usage": {
      "prompt_tokens": 16057,
      "completion_tokens": 167,
      "total_tokens": 16224
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.3 Pre-training Data",
          "reasoning": "The paper describes the process of automatic multi-object scene construction using Blenderproc and reuses existing single-object datasets and grasping pose datasets. However, it does not provide any link or mention that the code for dataset construction is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.3 Pre-training Data",
          "reasoning": "The paper provides detailed documentation on how the multi-object dataset is automatically constructed: selecting objects, physical simulation to generate realistic scenes, camera sampling, and rendering RGB-D images, as well as aligning point clouds and labels. It also describes the single-object datasets used and the grasping pose data from existing sources. This constitutes sufficient documentation of dataset creation."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2169-rubric-0",
    "token_usage": {
      "prompt_tokens": 10630,
      "completion_tokens": 152,
      "total_tokens": 10782
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4 Experiments, especially the paragraph starting with 'Scene Flow [20] is a synthetic dataset...' and Sections 4.3 Comparisons with State-of-the-art",
          "Reasoning": "The paper does not introduce any new dataset; all datasets such as Scene Flow, KITTI, ETH3D, and Middlebury are pre-existing datasets used for training and evaluation. No novel dataset is introduced by the authors. They propose a new method and test it on existing stereo image datasets which consist of rectified stereo image pairs. These datasets are composed of image modality data captured or synthetically generated prior to this work."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2169-rubric-1",
    "token_usage": {
      "prompt_tokens": 11430,
      "completion_tokens": 251,
      "total_tokens": 11681
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1, Section 4.3",
            "reasoning": "The paper describes training and evaluation of the model on several datasets, including Scene Flow, KITTI, ETH3D, and Middlebury, but does not explicitly state any manual annotation processes performed on new data. The 'annotations' in this context correspond to ground truth disparities provided by existing datasets, or synthetic ground truth in Scene Flow. There is no mention of manual annotation or human annotation efforts for a new dataset created by the authors."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit annotation guidelines for new data annotation provided in the paper.",
            "reasoning": "The paper does not mention any manual annotation process or annotation instructions, hence no instructions exist for annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No annotation guidelines or rubrics are discussed anywhere in the paper.",
            "reasoning": "Due to the absence of manual annotation, no rubrics are described or used."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation guideline examples are provided.",
            "reasoning": "Since no manual annotation effort is described, no examples for annotation tasks exist."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2169-rubric-2",
    "token_usage": {
      "prompt_tokens": 12620,
      "completion_tokens": 330,
      "total_tokens": 12950
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper that multiple human experts were involved in the quality assurance of dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No details about quality assurance by multiple non-expert human annotators are given in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that an AI model was used to perform quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification process specifically conducted to quality assure datasets; it focuses on method development, not data annotation or verification."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses several pre-existing datasets (Scene Flow, KITTI, ETH3D, Middlebury) for training and evaluation, and does not introduce any new datasets or report any quality assurance process related to datasets. There is no mention of dataset annotation, curation, or quality control processes. Therefore, no quality assurance process is applied or documented for any new dataset, as none were introduced by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2169-rubric-3",
    "token_usage": {
      "prompt_tokens": 12230,
      "completion_tokens": 301,
      "total_tokens": 12531
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments)",
          "reasoning": "The paper mentions usage of several pre-existing datasets such as Scene Flow, KITTI 2012, KITTI 2015, ETH3D, and Middlebury for training and evaluation. The authors collate these existing datasets as evaluation benchmarks and for finetuning purposes, without indication of significant modification or new data creation."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors propose a novel method, the data used for training and evaluation is based on existing datasets without mention of modifications or adaptations producing derived datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2169-rubric-4",
    "token_usage": {
      "prompt_tokens": 12748,
      "completion_tokens": 528,
      "total_tokens": 13276
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation Details",
          "reasoning": "The authors pretrained their model on the Scene Flow dataset for 200k steps before fine-tuning on other datasets. This indicates that Scene Flow dataset is used for pre-training the stereo matching models."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training models from randomly initialized parameters exclusively on any new dataset introduced by the authors. Training mostly leverages pretraining on Scene Flow followed by fine-tuning."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.3 Comparisons with State-of-the-art",
          "reasoning": "The model trained with pretraining on Scene Flow is fine-tuned on real-world datasets such as KITTI, ETH3D, and Middlebury using supervised learning methods for evaluation and performance improvement."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention reinforcement learning or RL-based post-training methods for any datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Experiments and 4.3 Comparisons with State-of-the-art",
          "reasoning": "Publicly available stereo datasets such as KITTI 2012, KITTI 2015, ETH3D, Middlebury, and Scene Flow test sets are used for benchmarking and evaluating the performance of the proposed model."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 Ablation Study",
          "reasoning": "The authors conducted ablation studies on the effect of their proposed modules on the Scene Flow dataset to analyze performance improvements, demonstrating analytical use of the dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as knowledge bases for augmenting models through retrieval or related mechanisms."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes practical usage of the datasets for pre-training, fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2169-rubric-5",
    "token_usage": {
      "prompt_tokens": 13471,
      "completion_tokens": 424,
      "total_tokens": 13895
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset containing entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset containing entries with exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset introduced in the paper contains entries with only English content. The paper uses existing datasets and does not describe any dataset textual entries in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset introduced in the paper contains entries with only non-English language content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes some pseudocode and formulas for the proposed methods, but does not introduce a new dataset containing programming or structured code related content as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical formulas describing the methods, it does not introduce any new dataset containing entries with mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper is about stereo vision and disparity estimation and introduces no datasets containing biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional/artificial language datasets are introduced in the paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce new datasets with unspecified or undocumented languages."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces no new datasets. It uses existing stereo vision datasets (e.g., Scene Flow, KITTI, ETH3D, Middlebury) which are image datasets with disparity ground truth. Image data does not contain linguistic content, thus the introduced data does not contain any human or natural language. Therefore, the linguistic coverage is Not Applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2169-rubric-6",
    "token_usage": {
      "prompt_tokens": 10689,
      "completion_tokens": 164,
      "total_tokens": 10853
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "The paper mentions 'Code is available at https://github.com/Windsrain/Selective-Stereo' but only for the method, not for dataset construction.",
          "reasoning": "The paper provides a link to code related to their stereo matching method implementation ('Selective-Stereo') rather than code for dataset creation or construction. No code or repository related to building any datasets is provided or referenced."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "The paper does not include any section describing dataset construction or creation processes.",
          "reasoning": "All datasets mentioned (Scene Flow, KITTI, ETH3D, Middlebury) are existing, public benchmarks. The paper exclusively uses these external datasets and does not introduce or document any new dataset creation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2175-rubric-0",
    "token_usage": {
      "prompt_tokens": 10976,
      "completion_tokens": 195,
      "total_tokens": 11171
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1, 3.3, and Experiments (Section 4)",
          "Reasoning": "The new dataset used for training and evaluation is the nuScenes dataset, which consists of LiDAR point clouds collected from real-world autonomous driving scenarios. The paper utilizes sequences of LiDAR point cloud data as input. These point clouds are captured by sensors on vehicles during driving, implying human involvement in the recording process rather than automatic generation. This dataset is employed following previous works, and no novel dataset is introduced by the authors in this paper. However, the paper explicitly states they only train on the unlabeled LiDAR point cloud sequences in this dataset, generating pseudo labels and regularizations themselves for self-supervised training. No new data collection is introduced, only a new self-supervised training approach applied to existing datasets."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2175-rubric-1",
    "token_usage": {
      "prompt_tokens": 11776,
      "completion_tokens": 201,
      "total_tokens": 11977
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3 Pseudo Label Generation",
            "reasoning": "Pseudo labels for the self-supervised learning are generated by solving an optimal transport problem to match points between BEV maps of consecutive frames, which is an algorithmic process rather than manual annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No section describing instructions for manual annotation.",
            "reasoning": "The pseudo label generation is automatic via optimal transport and ground segmentation methods; no human annotators or instructions for humans are mentioned."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No rubric description for annotations found.",
            "reasoning": "No scoring or rubric system is described for evaluating or guiding annotation due to the labels being generated algorithmically."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of annotations provided.",
            "reasoning": "No annotation examples are provided because the labels are pseudo labels generated automatically, not manually annotated data."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2175-rubric-2",
    "token_usage": {
      "prompt_tokens": 12966,
      "completion_tokens": 382,
      "total_tokens": 13348
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert annotator for any new datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication or mention of multiple human experts performing quality assurance on dataset annotations or content is provided in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information suggesting that a single non-expert performed any quality assurance for the presented datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance was not performed by an AI model as a judge as per the paper. The method employs AI for pseudo label generation, but not explicitly for QA of datasets."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3 Pseudo Label Generation; Section 3.4 Spatial-Temporal Consistency Regularizations",
          "reasoning": "The quality of pseudo labels for self-supervised training is improved via an automatic optimal transport solver to establish correspondences between point clouds, and further enhanced with spatial and temporal consistency regularizations implemented algorithmically. Although this is not QA in the traditional sense for annotated datasets, the paper describes automated procedures that serve to improve and verify the quality of the pseudo labels and the resulting motion predictions, ensuring consistency and reliability in the data used for training."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an automated approach (optimal transport and consistency regularizations) that effectively serves as quality assurance for the pseudo labels generated. Therefore, it is not true that no QA is performed or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2175-rubric-3",
    "token_usage": {
      "prompt_tokens": 12576,
      "completion_tokens": 452,
      "total_tokens": 13028
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new data that is manually created or annotated by human contributors from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not generated solely by AI or machine learning models without reference to existing data; rather, the paper uses existing LiDAR point cloud sequences."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper of any data being produced through human-driven translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation involved in the generation of datasets."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), Dataset paragraph",
          "reasoning": "The paper uses existing large-scale autonomous driving datasets such as nuScenes and Waymo, which are publicly available and well-established datasets collected with LiDAR sensors. The authors did not create new raw datasets but use these existing datasets without significant modification to evaluate their method."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.3 (Pseudo Label Generation) and Section 3.4 (Spatial-Temporal Consistency Regularizations)",
          "reasoning": "The authors generate pseudo labels for motion prediction by applying an optimal transport solver to establish correspondences between existing point cloud frames. This process creates derived data labels from the original point cloud sequences by transformation and processing. The pseudo labels and their refinement via spatial and temporal regularizations constitute derived data based on the original datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the source and generation method for the data used and the pseudo labels generated."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2175-rubric-4",
    "token_usage": {
      "prompt_tokens": 13094,
      "completion_tokens": 291,
      "total_tokens": 13385
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2 Overview; Section 4 Experiments",
          "reasoning": "The paper describes using the unlabeled LiDAR point cloud data (e.g., nuScenes dataset) to train the class-agnostic motion prediction model from scratch in a self-supervised manner using pseudo labels and spatial-temporal consistency regularizations without any pre-training."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Experiments; Table 1 and Table 5",
          "reasoning": "The paper uses the nuScenes and Waymo datasets for evaluation and benchmarking of their approach against state-of-the-art methods, measuring prediction accuracy at various speed levels."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.4 Discussions",
          "reasoning": "The paper analyzes characteristics of their clustering method and the effect of ground segmentation and forward-backward divergence to understand performance factors and model behavior."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2175-rubric-5",
    "token_usage": {
      "prompt_tokens": 13817,
      "completion_tokens": 621,
      "total_tokens": 14438
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset that contains entries with more than two human languages. The datasets used (e.g., nuScenes and Waymo) are autonomous driving datasets primarily containing sensor data, not multilingual text data."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the proposed or used datasets contain exactly two human languages. The data is sensor-based point cloud data for motion prediction tasks, not language-based."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset containing English content or text. The datasets referenced are point cloud data for LiDAR-based autonomous driving tasks."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains no description of datasets consisting of exactly one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses implementation details and optimization algorithms, it does not introduce a new dataset composed of programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3.3 and 3.4",
          "reasoning": "The dataset consists of point clouds with associated motion vectors and labels. The paper includes explicit mathematical formulations and notation describing the matching problem, the optimal transport solver, cost matrices, losses, and other symbolic representations related to motion prediction. These symbolic representations are used in describing the dataset labels and pseudo labels."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The proposed dataset is focused on LiDAR point cloud data in traffic scenarios, with no biological or non-human communication systems involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets contain any constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly describes the nature and origin of the dataset (nuScenes, Waymo), so the language or content is known and documented."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The proposed datasets are LiDAR point cloud data representing spatial and temporal sensor readings in autonomous driving scenes. These data do not contain human language text or linguistic content. Hence, the dataset entries do not contain any natural human language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2175-rubric-6",
    "token_usage": {
      "prompt_tokens": 11035,
      "completion_tokens": 278,
      "total_tokens": 11313
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract; Implementation details in Section 4; Code repository link in Abstract",
          "reasoning": "The paper explicitly states in the Abstract that the code will be available at https://github.com/kwwcv/SelfMotion, indicating the authors provide public code related to their method. Though the link is for the method, no separate dataset construction code is described, but as their approach involves pseudo label generation by matching and some preprocessing (like ground segmentation), and as the overall code is provided, the code seems to cover the data processing and generation parts used in their self-supervised training."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.3 (Pseudo Label Generation), 3.4 (Spatial-Temporal Consistency Regularizations), and Section 4 (Experiments)",
          "reasoning": "The paper documents the data preprocessing (like ground segmentation), the pseudo label generation process via optimal transport matching on BEV maps, and the clustering-based consistency regularizations used during training. They clearly describe how pseudo labels are generated from the raw point cloud sequence and how the data is discretized into voxels and BEV maps. They also mention dataset details (nuScenes and Waymo) and how data is split and processed. This detailed description sufficiently documents the dataset creation and preprocessing steps relevant to their method."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2198-rubric-0",
    "token_usage": {
      "prompt_tokens": 10879,
      "completion_tokens": 106,
      "total_tokens": 10985
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Shadow Image Collection; Figure 2",
          "Reasoning": "The DESOBAv2 dataset is constructed from a large collection of real-world outdoor images captured both by photographers and from online public websites with licenses for reuse, as described explicitly in Section 3.1. These are photographic images, hence 'image' modality, and are human generated via photography and manual collection."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2198-rubric-1",
    "token_usage": {
      "prompt_tokens": 11679,
      "completion_tokens": 326,
      "total_tokens": 12005
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3 (Dataset Construction), especially 3.2 (Shadow Removal) and 3.3 (Composite Image Synthesis)",
            "reasoning": "The paper describes a pipeline where a pretrained object-shadow detection model [41] is used automatically to detect object and shadow masks, and a pretrained inpainting model [32] is used to inpaint shadow regions. This is an automated process involving pretrained models rather than humans manually annotating. Moreover, manual filtering is also mentioned but only for filtering images with undesirable shadows or artifacts after automated processing, not for annotation. Therefore, the annotation is primarily via an automatic process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit section or mention in the paper.",
            "reasoning": "The paper does not describe or provide any detailed instructions or guidelines given to the automatic process or human annotators for labeling or annotation. The annotation is done via off-the-shelf pretrained models and subsequent filtering; no instructions provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit section or mention in the paper.",
            "reasoning": "There are no scoring rubrics or formal evaluation criteria described to guide the annotation process. The filtering process is manual but no rubric for how to score or annotate is given."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit section or mention in the paper.",
            "reasoning": "The paper does not provide annotation examples or detailed annotation guideline exemplars; it only includes example images of dataset instances, which are for illustration, not annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2198-rubric-2",
    "token_usage": {
      "prompt_tokens": 12869,
      "completion_tokens": 382,
      "total_tokens": 13251
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the dataset annotations or content of DESOBAv2."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts performed quality assurance on the dataset or its annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.2 Shadow Removal",
          "reasoning": "The paper describes a manual filtering step after automatic inpainting to remove low-quality object-shadow pairs and those with undesirable shadows or artifacts. This manual filtering was conducted by humans, but there is no indication these humans were experts. Also, the process suggests multiple individuals were involved to handle the large dataset size. Therefore, quality assurance was conducted by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used for object-shadow detection and inpainting, these are used to construct the dataset, not for quality assurance validation or judgment."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3 Dataset Construction (especially 3.2 Shadow Removal)",
          "reasoning": "The dataset was constructed using automated pipelines involving pretrained object-shadow detection model and inpainting model to remove shadows and generate deshadowed images. These automated techniques serve as a form of automatic quality control in dataset construction."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes human-involved manual filtering and automated detection/inpainting steps as parts of dataset quality control, so it is not applicable to say no QA was performed or described."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2198-rubric-3",
    "token_usage": {
      "prompt_tokens": 12479,
      "completion_tokens": 463,
      "total_tokens": 12942
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not created entirely from scratch by humans. The paper mentions data collection via crawling online images and photos captured by hired photographers, but the main dataset construction involves significant processing and adaptation rather than purely new human-created images."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset is not generated entirely by models without reference to existing data. Models are used for object-shadow detection and image inpainting for shadow removal, but the base images are from real-world photographs."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset content was produced by translating content from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that machine translation was used in dataset creation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The paper states that they collect a large number of real-world images from online sources and photography, resulting in 44,044 images with object-shadow pairs. This data collection is aggregating existing images without modification, indicating collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 and 3.3",
          "reasoning": "The DESOBAv2 dataset is constructed from existing real images by applying pretrained object-shadow detection and image inpainting models to remove shadows, followed by synthesizing composite images combining deshadowed and original images to generate paired data. These transformations and modifications on collected images classify the dataset as derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and processing pipeline are clearly described; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2198-rubric-4",
    "token_usage": {
      "prompt_tokens": 12997,
      "completion_tokens": 500,
      "total_tokens": 13497
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The DESOBAv2 dataset is not used for pre-training large models in an unsupervised or self-supervised manner as per the paper."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training a model from randomly initialized parameters solely on DESOBAv2 dataset; their model builds upon pre-trained foundation models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 6.3 and Section 6.5",
          "reasoning": "The paper uses the DESOBAv2 dataset to train and fine-tune their shadow generation diffusion model and other baselines for the supervised shadow generation task, as described in the experiments (Section 6.3) and real composite image evaluation (Section 6.5). The dataset provides paired composite images and ground-truth shadow images enabling supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of using the DESOBAv2 dataset for reinforcement learning post-training methods."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 6.1 and Section 6.3",
          "reasoning": "The DESOBAv2 test set is used to evaluate and benchmark different shadow generation methods quantitatively and qualitatively as described in the experiments sections."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not primarily used for analysis of trends or characteristics; the focus is on training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The DESOBAv2 dataset is not used as a knowledge base for augmenting models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly uses the new dataset for supervised training and evaluation, so this label does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2198-rubric-5",
    "token_usage": {
      "prompt_tokens": 13720,
      "completion_tokens": 592,
      "total_tokens": 14312
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's new dataset DESOBAv2 is an image dataset containing real-world outdoor images with object-shadow pairs. There is no indication or mention of multiple human languages present in the dataset entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset DESOBAv2 does not contain entries involving exactly two human languages; it consists solely of visual images without any textual multilingual content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains images, not language content. Although the paper text and documentation is in English, the images themselves do not contain English-only linguistic content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset entries contain any non-English language content; the data primarily consists of real-world images."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Section 3 Dataset Construction",
          "reasoning": "The dataset entries are images and masks related to object-shadow pairs; no programming or structured code content is contained within the dataset itself, only code used for model development is external."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical notation describing their models and losses, the dataset entries themselves are images and masks with no embedded mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of natural outdoor images of objects and shadows; there is no mention of biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that the dataset includes fictional or artificially created language content."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset entries is clearly identifiable as none because it contains images, so the language coverage is known to be none."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The proposed dataset, DESOBAv2, consists purely of visual data\u2014real-world images with object and shadow masks. There is no linguistic content in the dataset entries, hence language coverage is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2198-rubric-6",
    "token_usage": {
      "prompt_tokens": 10938,
      "completion_tokens": 153,
      "total_tokens": 11091
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 3 Dataset Construction; also link provided in Abstract",
          "reasoning": "The paper explicitly states in the Abstract that the dataset, code, and model are released at https://github.com/bcmi/Object-Shadow-Generation-Dataset-DESOBAv2. This indicates that the code related to dataset construction is publicly available in an accessible repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 Dataset Construction",
          "reasoning": "Section 3 provides a detailed pipeline describing dataset construction including shadow image collection, shadow removal via object-shadow detection and inpainting techniques, manual filtering criteria, and composite image synthesis. This comprehensive description documents the dataset creation process thoroughly."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2207-rubric-0",
    "token_usage": {
      "prompt_tokens": 13454,
      "completion_tokens": 131,
      "total_tokens": 13585
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset",
          "Reasoning": "The new benchmark dataset is based on the high-quality CustomHumans dataset, which contains 600 higher-quality scans of 80 subjects in 120 different garments and poses. The authors selected 60 subjects from this dataset for quantitative experiments. As the custom benchmark is based on this newly created dataset of 3D human scans and rendered images, the data modality is image and the origin is human generated, as these are scans and images captured from humans and their clothing."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2207-rubric-1",
    "token_usage": {
      "prompt_tokens": 14254,
      "completion_tokens": 374,
      "total_tokens": 14628
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.1 Dataset and Experimental Setup, and general methodology sections 3.1 and 3.2",
            "reasoning": "The paper describes training and evaluation using the public THuman2.0 dataset (approx. 500 high-quality 3D scans) which is used for training both the hallucination and mesh reconstruction modules. These scans are collected and curated as 3D human scans, which implies human experts conducted or curated these scans. Additionally, evaluation benchmarks such as CustomHumans are newly created datasets with 60 subjects selected for experiments and user studies including 30 participants ranking results (Section 4.2 and 4.1). The creation and annotation of these datasets involve multiple human experts and evaluators. This is supported by references to user studies and dataset selection process in Sections 4.1, 4.2, and related passages."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not explicitly describe any detailed annotation instructions given to annotators or dataset creators for the new datasets. The datasets used are 3D scan collections and benchmarks, not manually annotated via labels requiring instructions. User studies conducted do not specify instructions for annotation or labeling tasks."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not provide any details regarding scoring rubrics used for annotation or dataset creation. Evaluation metrics for reconstruction quality are described as automated metrics or user preference rankings, but no rubric for annotation is described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not described",
            "reasoning": "The paper does not report annotation examples or guidelines provided to annotators or labellers for the new dataset or for the user study evaluations."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2207-rubric-2",
    "token_usage": {
      "prompt_tokens": 15444,
      "completion_tokens": 287,
      "total_tokens": 15731
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance of the dataset was performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process involving multiple human experts for the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence of quality assurance by a single non-expert annotator for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of quality assurance by multiple human non-expert annotators is found in the paper regarding the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of an AI model for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description indicating that any automated process or algorithmic verification was used for quality assurance of the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document any quality assurance process conducted on the datasets used for training or evaluation. The newly created benchmark is based on an existing high-quality dataset (CustomHumans), but no QA process for dataset annotations or content is described."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2207-rubric-3",
    "token_usage": {
      "prompt_tokens": 15054,
      "completion_tokens": 512,
      "total_tokens": 15566
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1, Dataset",
          "reasoning": "The authors created a new benchmark dataset based on the CustomHumans dataset, selecting 60 subjects with high-quality 3D scans for evaluation and user studies. This dataset contains human-created content scanned from real humans wearing various garments and poses, representing original data captured by humans."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any new dataset was generated entirely by AI or machine learning models without reference to existing data. The new datasets stem from human-scanned 3D data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication the dataset was created via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication the dataset was created via machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1, Dataset",
          "reasoning": "The authors use public datasets like THuman2.0 (about 500 scans) for training and CAPE for evaluation, which are existing collections of 3D human scans collected by others. Additionally, the new benchmark is based on aggregating and curating subset data from the existing CustomHumans dataset. Therefore, these datasets are collated from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Sections 3.1 and 3.2",
          "reasoning": "Back-view images used for training the diffusion model are rendered from 3D scans along with UV maps and silhouettes, indicating data is derived by rendering and processing existing 3D scans to create paired training data. Similarly, the authors use SMPL-X fits and alignments to generate positional embeddings and data for mesh reconstruction, indicating derived data processed from the original 3D scans."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data sources and methods of data generation and creation."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2207-rubric-4",
    "token_usage": {
      "prompt_tokens": 15572,
      "completion_tokens": 587,
      "total_tokens": 16159
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the newly introduced datasets exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1, Section 3.2, Section 4.1",
          "reasoning": "The new benchmark dataset created based on the high-quality CustomHumans dataset is used to train and evaluate the proposed model. The paper states that SiTH requires as few as 500 3D human scans for training and uses the newly created benchmark for evaluation and ablation studies. The mesh reconstruction and hallucination modules are trained using this dataset from scratch as indicated in training descriptions."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1 (Back-view Hallucination), Figure 3",
          "reasoning": "The paper details fine-tuning a pretrained diffusion model with multi-view images rendered from 3D human scans (the training dataset) to adapt it for the image-conditioned back-view hallucination task. This corresponds to supervised fine-tuning of a pretrained model using the dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning based post-training techniques or methods such as RLHF using the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, Section 4.2",
          "reasoning": "The newly created benchmark based on CustomHumans dataset is used exclusively for evaluating the performance of the proposed method and comparing to other baselines, as stated in Section 4.1 and 4.2. Quantitative metrics and user studies use this dataset for benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the new dataset being used primarily for analyzing trends or characteristics without training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or for augmenting models through retrieval-augmented generation or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates practical usage of the newly created dataset in training (including fine-tuning), and evaluation, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2207-rubric-5",
    "token_usage": {
      "prompt_tokens": 16295,
      "completion_tokens": 547,
      "total_tokens": 16842
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the introduced dataset contains exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1, Dataset; Section 3.1 and 3.2",
          "reasoning": "The newly created benchmark dataset is based on the CustomHumans dataset and training uses the public THuman2.0 dataset. All dataset descriptions and the paper's text are exclusively in English. There is no mention of any other human languages in the dataset entries, indicating that all textual content associated with the dataset and its annotations is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication exists that the dataset includes entries with a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of 3D scans and images, but does not include any programming or structured code content entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical notation to describe methods, this is not part of the dataset entries. Therefore, the dataset itself does not contain mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises human 3D scans and images without biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language is specified as English; thus, it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes natural language (English) annotations and textual descriptions; hence it contains language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2207-rubric-6",
    "token_usage": {
      "prompt_tokens": 13513,
      "completion_tokens": 184,
      "total_tokens": 13697
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention in the paper",
          "reasoning": "The paper does not mention or provide any link to code repositories related to the dataset construction, preprocessing, or generation. There is no indication that code for dataset creation or processing is publicly shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Experimental Setup, and throughout Sections 3.1 and 3.2",
          "reasoning": "The paper documents the dataset creation process by describing that the authors use the public THuman2.0 dataset (about 500 scans) for training and create a new benchmark by selecting from the CustomHumans dataset. They explain generation of training image pairs from 3D human scans with rendered views, UV maps, and masks used as conditions. The process is described in reasonable detail in the methodology and experimental setup."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-223-rubric-0",
    "token_usage": {
      "prompt_tokens": 14890,
      "completion_tokens": 147,
      "total_tokens": 15037
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 and Section 5.2, Experimental Evaluation and Datasets subsection",
          "Reasoning": "The paper evaluates the new STSL sampler on standard benchmarks including FFHQ (Flickr-Faces-HQ), ImageNet, and COCO datasets, which are known datasets consisting of human-generated images captured and curated by humans. The authors use these datasets to demonstrate performance in inverse problems and image editing tasks, confirming the presence of human-generated image data as the test dataset for their method. No evidence suggests the authors introduce a newly created dataset; they use existing benchmark datasets for evaluation only."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-223-rubric-1",
    "token_usage": {
      "prompt_tokens": 15690,
      "completion_tokens": 306,
      "total_tokens": 15996
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 5. Experiments and Supplementary Material (Appendix B.1)",
            "reasoning": "The paper discusses experimental results on several benchmarks and inverse problem datasets such as FFHQ, ImageNet, and COCO. The datasets are standard or prior works, but no new dataset collection is described, implying no explicit mention of annotation by non-expert or multiple annotators. The methodology and analysis suggest expertise-level experimental design and evaluation by the authors who are experts in the field."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper does not describe any annotation instructions or guidelines specific to manual annotations for a newly introduced dataset. The tasks involve evaluation metrics (LPIPS, PSNR, SSIM, CLIP Accuracy) computed automatically or by known tools, not manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "No mention of scoring rubrics or rating systems for human annotation processes is found in the paper regarding any new dataset. The evaluation metrics applied are standard computational metrics without rubrics for subjective scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper does not provide any annotation examples or instruction sets indicating human annotation examples for a new dataset. The focus is on applying the model to existing datasets with automated evaluation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-223-rubric-2",
    "token_usage": {
      "prompt_tokens": 16880,
      "completion_tokens": 299,
      "total_tokens": 17179
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance being conducted by a single human annotator or expert for the datasets used or introduced."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided that multiple non-expert humans were involved in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of AI models specifically for quality assurance of datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses algorithmic methods for inverse problems and image editing but does not describe automatic process-based quality assurance for dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new annotated datasets requiring quality assurance, nor does it mention any quality assurance process for dataset annotations or content. The datasets used are pre-existing benchmarks (e.g., FFHQ, ImageNet, COCO) without new annotated datasets introduced by the authors or related QA procedures documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-223-rubric-3",
    "token_usage": {
      "prompt_tokens": 16490,
      "completion_tokens": 338,
      "total_tokens": 16828
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any new dataset created entirely from scratch by the authors or human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the authors generating new datasets purely by AI or machine learning models in this paper."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state any data produced through human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of machine-translated data used or introduced."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used (FFHQ, ImageNet, COCO) are standard benchmarks and are not collated or aggregated by the authors as new datasets."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any new dataset derived from existing sources by modification or adaptation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. It uses standard pre-existing datasets as benchmarks without specifying any novel dataset creation."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-223-rubric-4",
    "token_usage": {
      "prompt_tokens": 17008,
      "completion_tokens": 251,
      "total_tokens": 17259
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 (Experiments), Tables 1 and 2, Figure 2",
          "reasoning": "The paper uses standard benchmark datasets FFHQ, ImageNet, and COCO purely for evaluation and benchmarking the proposed STSL method's performance on inverse problems and image editing tasks. The datasets are not introduced by the authors and are used for performance measurement only."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. The datasets referenced (FFHQ, ImageNet, COCO) are standard existing benchmarks used solely for evaluation without any novel dataset contributions."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-223-rubric-5",
    "token_usage": {
      "prompt_tokens": 17731,
      "completion_tokens": 581,
      "total_tokens": 18312
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Sections 1, 5.3, and throughout the paper",
          "reasoning": "The datasets introduced and used in the paper (e.g., FFHQ, ImageNet, COCO) are standard benchmark image datasets commonly associated with English-language annotations or descriptions when applicable. Image editing results and prompts are exclusively in English, for example: 'a dog', 'a cat', 'a tiger walking in the jungle'. The paper focuses on image inverse problems and text-guided image editing using English text prompts and descriptions. No mention or usage of other natural languages is indicated."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses algorithms, mathematical equations, and methods, it does not introduce a new dataset containing programming or structured code-related content. The methods and implementations are described theoretically and experimentally but no associated code dataset is introduced."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Sections 3 Methodology, 4 Theoretical Analysis, and throughout the paper",
          "reasoning": "The paper contains extensive mathematical notation including equations for the Ornstein-Uhlenbeck process, stochastic differential equations, Tweedie's formula, reverse diffusion processes, loss functions, and theoretical proofs. This mathematical content is part of the dataset description and methodology."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of datasets containing biological sequences or non-human communication in this paper."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or constructed languages are mentioned or used in the dataset or experiments."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages used are clearly specified or plainly English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain natural language (English prompts) alongside images and mathematical notation, so the dataset is not void of language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-223-rubric-6",
    "token_usage": {
      "prompt_tokens": 14949,
      "completion_tokens": 137,
      "total_tokens": 15086
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Not mentioned",
          "reasoning": "The paper does not mention or provide any links or references to code repositories related to the construction or preprocessing of datasets or the new dataset creation. The focus is on the method and use of existing standard benchmark datasets (FFHQ, ImageNet, COCO)."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Not mentioned",
          "reasoning": "The paper references use of standard benchmark datasets such as FFHQ, ImageNet, and COCO but does not describe any new datasets introduced by the authors, nor does it provide documentation of dataset creation or code related to dataset construction."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2233-rubric-0",
    "token_usage": {
      "prompt_tokens": 10272,
      "completion_tokens": 290,
      "total_tokens": 10562
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.3 (Dataset Utilization Strategy) and Section 4.4 (Reason-Edit for Better Evaluation)",
          "Reasoning": "The authors synthesize a small amount (476 paired samples) of editing data including original images, instructions, and synthetic target images for complex understanding and reasoning scenarios. The original images used are presumably human-generated photographs, as there is no indication they are synthetic. This dataset is newly introduced by the authors to stimulate SmartEdit's reasoning ability. Additionally, the evaluation dataset Reason-Edit consists of 219 image-text pairs collected by the authors, containing original images paired with instructions. Thus, for the image modality, the data originates from human sources."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.3 (Dataset Utilization Strategy) and Section 4.4 (Reason-Edit for Better Evaluation)",
          "Reasoning": "The paired instructions in the synthetic editing dataset and Reason-Edit evaluation dataset consist of natural language instructions designed for complex understanding and reasoning tasks. The instructions are human-generated as the paper outlines the collection and synthesis process for creating high-quality examples with complex instructions, implying manual design or human involvement in their creation. Therefore, for the text modality, the data is human-generated."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2233-rubric-1",
    "token_usage": {
      "prompt_tokens": 11072,
      "completion_tokens": 481,
      "total_tokens": 11553
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.4 and Section 5.1",
            "reasoning": "The Reason-Edit dataset is described as a collected evaluation dataset consisting of 219 image-text pairs for complex instruction-based image editing scenarios. The dataset involves manual annotation of ground truth labels for foreground areas, as indicated by manually annotated GT labels used in evaluation metrics (Section 5.1). Additionally, Section 5.1 mentions four workers hired to manually evaluate edited image results on Reason-Edit. Thus, the annotation was performed by human annotators, likely experts given the nature of the task and presence of multiple annotators (four workers). However, the paper provides no indication that multiple independent annotators annotated each item separately, nor mentions majority voting, so we select 'Single Human Expert' as the annotator type (these four evaluations were aggregated but not discussed as multiple independent expert labels for dataset creation). There is no statement that the dataset was labeled by crowdworkers or non-experts, nor that AI models or automatic processes were involved in label creation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.4",
            "reasoning": "The paper does not describe any annotation guideline or detailed instructions provided to annotators for Reason-Edit. There is no mention of instructions guiding annotators on the annotation process. The paper only states the dataset was collected and used for evaluation, with manual annotation of ground truth foreground labels. No explicit annotation guidelines or instructions are described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 5.1",
            "reasoning": "While evaluation criteria for manual evaluation of edited images involve checking whether the edited image aligns with instructions (Instruction-Alignment), the paper does not describe formal scoring rubrics provided in annotation guidelines for the Reason-Edit labels themselves. Rubrics appear defined for evaluation metrics, not for annotation. The dataset annotations themselves (ground truth foreground) do not mention rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.3 and Section 4.4",
            "reasoning": "The paper does not mention provision of annotation examples or sample labeled data for the Reason-Edit dataset annotation. While synthetic editing data examples and visual examples are provided for training and model insights, annotation examples for Reason-Edit data labeling are not described."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2233-rubric-2",
    "token_usage": {
      "prompt_tokens": 12262,
      "completion_tokens": 413,
      "total_tokens": 12675
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit description in the paper of quality assurance being conducted by a single human expert for the Reason-Edit dataset or the synthetic editing dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process involving multiple human experts for the Reason-Edit dataset or other new datasets introduced."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information indicating that a single non-expert performed quality assurance on the new datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 5.1, paragraph about Evaluation Metrics",
          "reasoning": "The Instruction-Alignment (Ins-align) metric is computed by manually hired four workers who evaluated edited images on Reason-Edit, averaging their results. This indicates the involvement of multiple human non-expert annotators in quality assurance of the edited image evaluation, and by implication, potentially the dataset quality evaluation as well."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of AI models being used for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated or algorithmic quality assurance process for the new datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process involving multiple human non-expert annotators is documented, so it is not the case that QA is absent or undocumented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2233-rubric-3",
    "token_usage": {
      "prompt_tokens": 11872,
      "completion_tokens": 633,
      "total_tokens": 12505
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.3 Dataset Utilization Strategy; Section 4.4 Reason-Edit for Better Evaluation",
          "reasoning": "The paper describes the collection of a new evaluation dataset named Reason-Edit, which contains 219 image-text pairs specifically collected to evaluate complex instruction-based image editing. This dataset is explicitly stated to be constructed by the authors, with no overlap with their synthetic training data. Additionally, the synthetic editing dataset of 476 paired data samples is produced via a data production pipeline created by the authors, involving human design of instructions and target images combined with existing images, indicating human creation of new paired data samples. Thus, the new datasets Reason-Edit and the synthetic editing dataset are original content created by human contributors from scratch for the purpose of this paper."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper mentions synthetic editing data, it does not state that these are generated entirely autonomously by AI or machine learning models without reference to existing data. Instead, the synthetic editing data are created via a pipeline involving edits to existing images with human-designed instructions and modifications. No claim is made about the dataset being generated entirely by models without any human input."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data being produced by human translation of content from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention data created by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.1 Training Datasets",
          "reasoning": "The training includes segmentation datasets such as COCOStuff, RefCOCO, GRefCOCO, and reasoning segmentation dataset from LISA, as well as editing datasets InstructPix2Pix and MagicBrush, and VQA dataset LLaVA-Instruct150k. These datasets are existing sources that the authors collect and aggregate for their training procedure without significant modification to the original data. Hence, these datasets are collated from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.3 Dataset Utilization Strategy",
          "reasoning": "The synthetic editing dataset of 476 paired data is created by modifying existing images using complex instructions involving object attributes and reasoning scenarios. This synthetic dataset is derived from existing images but includes modifications such as changes according to complex instructions. Therefore, it is derived data based on existing sources with adaptations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and generation of the new datasets introduced by the authors are documented and described in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2233-rubric-4",
    "token_usage": {
      "prompt_tokens": 12390,
      "completion_tokens": 292,
      "total_tokens": 12682
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.1, Section 5.4",
          "reasoning": "The paper describes synthesizing a small number of high-quality paired data with complex instructions as a supplement to conventional editing data and segmentations, and using this combined training set to fine-tune SmartEdit models (e.g., Section 4.3 and Section 5.4). The training is done with supervised learning methods to enhance the model's ability on complex instruction-based editing."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.4, Section 5.1, Section 5.2",
          "reasoning": "The paper constructs the Reason-Edit dataset with 219 image-text pairs specifically for evaluation of instruction-based image editing methods on complex understanding and reasoning scenarios, and uses it exclusively for benchmarking and performance measurement."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2233-rubric-5",
    "token_usage": {
      "prompt_tokens": 13113,
      "completion_tokens": 584,
      "total_tokens": 13697
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces the Reason-Edit dataset, which consists of image-text pairs using instructions and annotations in English only. There is no mention of any other human language in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset Reason-Edit contains only English textual instructions and annotations. There is no indication of exactly two human languages being used."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.4 and Section 5.1 discuss the Reason-Edit dataset as an evaluation dataset with 219 image-text pairs with complex instructions, all in English.",
          "reasoning": "The Reason-Edit dataset consists of 219 image-text pairs used for evaluation with instructions written in English. All textual content is exclusively in English, as indicated by the English instructions and annotations described in the paper."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains entries with a non-English language exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention including any programming or structured code-related content in the Reason-Edit or any other datasets introduced."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses some equations to define the model losses and architecture, the introduced datasets (Reason-Edit and synthetic editing data) themselves do not contain mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are images and textual instructions; there is no mention of biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset includes fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly describes the dataset's language (English); thus, the language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced in the paper contain natural language instructions (English), so cannot be considered to lack any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2233-rubric-6",
    "token_usage": {
      "prompt_tokens": 10331,
      "completion_tokens": 169,
      "total_tokens": 10500
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Front page and Abstract",
          "reasoning": "The paper provides a GitHub link (https://github.com/TencentARC/SmartEdit) on the front page, indicating that code related to their method, including potentially the dataset construction or synthetic data generation, is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.3 and Section 4.4",
          "reasoning": "The paper describes in Section 4.3 a detailed data production pipeline for synthesizing 476 paired data with complex instructions for training, including types of scenarios considered, and in Section 4.4 it documents the Reason-Edit evaluation dataset, including dataset size, scenario types, and their differentiation from training data. These descriptions provide documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2274-rubric-0",
    "token_usage": {
      "prompt_tokens": 10431,
      "completion_tokens": 138,
      "total_tokens": 10569
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Dataset Construction and Abstract",
          "Reasoning": "The SportsHHI dataset is a newly introduced dataset by the authors consisting of basketball and volleyball sports videos. These videos were carefully selected from existing MultiSports videos, which are human-recorded sports game videos at 720p resolution and 25FPS. The dataset includes annotated human bounding boxes and human-human interaction labels on keyframes extracted at 5FPS from these videos. Thus, the dataset is composed of natural videos captured by human-operated cameras, indicating human-generated origin and video modality."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2274-rubric-1",
    "token_usage": {
      "prompt_tokens": 11231,
      "completion_tokens": 310,
      "total_tokens": 11541
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1, 'Annotation' and 'Quality control' paragraphs",
            "reasoning": "The paper states that all annotators for interaction instance annotation are professional athletes or veteran amateurs, which indicates multiple human experts. It also mentions repeated annotation with disagreements resolved by meta-annotator, reinforcing the use of multiple experts in the annotation process."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1, 'Interaction classes definition' and 'Interaction instance formulation' paragraphs",
            "reasoning": "The paper describes the detailed definitions of interaction classes guided by professional athletes and iterative trial labeling, and the user interface for annotation specifying how annotators add instances. This implies detailed annotation instructions were provided to annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1, 'Interaction classes definition' and 'Quality control' paragraphs",
            "reasoning": "The interaction classes are carefully defined with high-level semantics and annotations were quality controlled by repeated annotation and meta-annotation checking disagreements, indicating that rubrics or criteria for annotation consistency and correctness were in place."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.1, Figure 2 (user interface) and Figure 3 (interaction classes hierarchy)",
            "reasoning": "The paper includes a figure showing interaction classes hierarchy and a user interface figure for annotation, indicating that concrete examples and visual references were provided to annotators to guide the annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2274-rubric-2",
    "token_usage": {
      "prompt_tokens": 12421,
      "completion_tokens": 310,
      "total_tokens": 12731
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance being conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset Construction, Quality control",
          "reasoning": "The paper states that all annotators are professional athletes or veteran amateurs, indicating subject matter expertise. Additionally, repeated annotation was performed with each video annotated by two different experts, and discrepancies were resolved by a meta-annotator. This describes a quality assurance process conducted by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators are described as professional athletes or veteran amateurs, i.e., experts, so multiple non-expert annotators did not conduct QA."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any AI model being used for quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or algorithmic quality assurance process for annotation validation is described in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes a quality control process involving multiple expert annotators and meta-annotator review, so QA is present and documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2274-rubric-3",
    "token_usage": {
      "prompt_tokens": 12031,
      "completion_tokens": 475,
      "total_tokens": 12506
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset Construction, Annotation",
          "reasoning": "The SportsHHI dataset is constructed by human annotators who manually labeled human-human interaction instances on keyframes extracted from basketball and volleyball videos. The bounding boxes and interaction annotations are created entirely through human effort, without automatic generation or adaptation. The annotators include professional athletes or veteran amateurs, and quality control is ensured by repeated annotation and meta-annotation, indicating original data creation from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any data being generated fully by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translating data from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe translating data via machine translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset Construction, Data preparation",
          "reasoning": "The dataset videos are selected from the existing MultiSports dataset. The authors curated and aggregated these videos to build SportsHHI. They selected 80 basketball and 80 volleyball videos from MultiSports, thus the source videos are pre-existing data collected and aggregated from an existing dataset."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset Construction, Annotation",
          "reasoning": "While the raw videos come from the MultiSports dataset, the SportsHHI dataset adds new interaction annotations (bounding boxes and interaction classes) that are not present in the source dataset. This involves adapting the existing videos with new high-level semantic annotations, thus making it derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation method are clearly specified and described in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2274-rubric-4",
    "token_usage": {
      "prompt_tokens": 12549,
      "completion_tokens": 548,
      "total_tokens": 13097
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the SportsHHI dataset for pre-training models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training models from randomly initialized parameters using SportsHHI; it focuses on fine-tuning or evaluation stages."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.1 Experimental Setup; Section 5.2 Ablation Study; Section 5.3 Quantitative Results",
          "reasoning": "The SportsHHI dataset is used to train and fine-tune interaction detection models in a supervised manner, as shown in the experimental setup and extensive ablation studies. The authors train models using SportsHHI to predict human-human interactions, fine-tuning pre-trained video backbones."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of using SportsHHI for reinforcement learning or RL-based post-training methods such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1 Experimental Setup; Section 5.3 Quantitative Results; Section 5.4 Error Analysis",
          "reasoning": "SportsHHI serves as a benchmark dataset for evaluating human-human interaction detection methods. The authors report recall, mean average precision, and perform detailed error analysis on model predictions using SportsHHI."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.2 Dataset Statistics and Characteristics; Section 5.4 Error Analysis",
          "reasoning": "The dataset is analyzed extensively for its characteristics, class distribution, spatial relationships, and error types in model predictions to aid understanding of the challenges posed by SportsHHI."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using SportsHHI as a knowledge base or for retrieval-augmented generation purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset has clear and multiple practical usages documented in the paper, excluding this option."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2274-rubric-5",
    "token_usage": {
      "prompt_tokens": 13272,
      "completion_tokens": 559,
      "total_tokens": 13831
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The SportsHHI dataset involves annotations and labels related to human-human interactions in sports videos but there is no mention of multiple human languages being represented or annotated in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not specify the presence of exactly two human languages in its content or annotations."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, e.g., Abstract, Sections 1 and 3",
          "reasoning": "The dataset annotations, interaction class labels, and all descriptions in the paper are provided in English only. There is no indication of other human languages being used in the dataset content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not contain entries with exactly one language other than English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of video frames with bounding box annotations and interaction labels; it does not contain programming or code snippets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Though the paper includes mathematical expressions to define notation for interactions and proposals (e.g., triplets and sets), the dataset entries themselves do not contain mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset exclusively contains human-human interaction annotations in sports videos. No biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention or evidence of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language in the dataset annotations is specified and clearly documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains language in the form of English labels and interaction class annotations."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2274-rubric-6",
    "token_usage": {
      "prompt_tokens": 10490,
      "completion_tokens": 178,
      "total_tokens": 10668
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section provides a link or reference to a code repository for data construction.",
          "reasoning": "The paper does not contain any mention or link to publicly available code for data collection, preprocessing, or dataset construction. No URLs, footnotes, or appendices indicate code availability for the dataset. Hence, code for dataset construction is not disclosed."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 \"Dataset Construction\" and Section 3.2 \"Dataset Statistics and Characteristics\".",
          "reasoning": "The paper provides detailed documentation describing the dataset construction process including data domain selection, interaction class definitions with expert guidance, annotation formulation and strategy, data preparation steps, annotation tools, and quality control mechanisms. It also provides statistical analysis and characteristics of the dataset. This reflects comprehensive documentation of the dataset creation."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2327-rubric-0",
    "token_usage": {
      "prompt_tokens": 12634,
      "completion_tokens": 230,
      "total_tokens": 12864
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4, Datasets & Evaluation Metrics; throughout the paper",
          "Reasoning": "The new dataset used in the paper builds on temporal LiDAR point cloud sequences, which are acquired via LiDAR sensors capturing real driving scenes. The data consists of consecutive frames of real point clouds representing the environment over time, recorded during vehicle movement. This data is human generated since it comes from real sensor recordings in real-world driving contexts."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Temporal Image Aggregation and Fusion and Section 4 Experiments (e.g., 4.1, 4.2)",
          "Reasoning": "Temporal images from cameras are utilized as part of the data for temporal multi-modal fusion. These images are recorded in real driving scenarios using vehicle-mounted cameras. Thus, the data modality is image, and it is human generated, as these images are captured by human-operated sensor systems in natural environments."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2327-rubric-1",
    "token_usage": {
      "prompt_tokens": 13434,
      "completion_tokens": 204,
      "total_tokens": 13638
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4. Experiments, Datasets & Evaluation Metrics",
            "reasoning": "The paper evaluates on SemanticKITTI and nuScenes datasets, which are publicly available and widely used datasets. The annotations for the point-wise labels and instance masks in these datasets are performed by multiple human experts originally, as these datasets are well-established benchmarks in autonomous driving point cloud segmentation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide or mention any detailed annotation instructions for labeling the new dataset. It only uses existing datasets (SemanticKITTI and nuScenes) without describing new annotation guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No scoring rubrics or detailed evaluation criteria for annotation are provided in the paper for the datasets."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No annotation examples or guideline examples are provided in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2327-rubric-2",
    "token_usage": {
      "prompt_tokens": 14624,
      "completion_tokens": 283,
      "total_tokens": 14907
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was conducted by a single human expert on any new dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No details are given about quality assurance involving multiple human experts for the datasets referenced or utilized."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any single non-expert human annotator performing quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss quality assurance by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses models and teachers for knowledge distillation, this is part of the model training process and not a quality assurance process on dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated verification or rule-based quality assurance of dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper only uses existing public datasets (SemanticKITTI and nuScenes) and does not introduce any new datasets. No quality assurance process on dataset annotations is described or applicable, as no new dataset was introduced by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2327-rubric-3",
    "token_usage": {
      "prompt_tokens": 14234,
      "completion_tokens": 444,
      "total_tokens": 14678
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the authors have created any new datasets entirely from scratch by human contributors. Instead, they use existing benchmark datasets SemanticKITTI and nuScenes for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset or data samples were generated by AI models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation of data from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation being used to generate the data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4. Experiments, Datasets & Evaluation Metrics",
          "reasoning": "The paper uses existing public datasets SemanticKITTI and nuScenes for training and evaluation. These datasets are well-established benchmarks that have been collected and published previously. The authors have not created new datasets but use these collated datasets from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.3 Static-Moving Switch Augmentation (SMSA)",
          "reasoning": "While the base data comes from existing datasets, the paper introduces the Static-Moving Switch Augmentation (SMSA) which manipulates temporal point clouds of objects to switch their motion states, effectively transforming and augmenting existing point cloud sequences. This constitutes derived data, as existing datasets are adapted via augmentation methods to produce modified data samples for training."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources and methods are specified in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2327-rubric-4",
    "token_usage": {
      "prompt_tokens": 14752,
      "completion_tokens": 440,
      "total_tokens": 15192
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using any new dataset exclusively for unsupervised or self-supervised pre-training."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset used for training models from scratch. Instead, it uses existing datasets for training and evaluation."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate using any introduced dataset for supervised fine-tuning of previously trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning based post-training or RLHF usage for any introduced dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses popular pre-existing datasets like SemanticKITTI and nuScenes for evaluation, which are not introduced by the authors."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper performs analysis and ablations on components of the proposed method but does not introduce a new dataset primarily for analysis purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset introduced serves as a knowledge base to augment models."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new datasets are introduced by the authors in this paper. All datasets used (SemanticKITTI and nuScenes) are existing public benchmarks, and the authors use them for training and evaluation of their model. Hence, no practical usage of new datasets is described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2327-rubric-5",
    "token_usage": {
      "prompt_tokens": 15475,
      "completion_tokens": 549,
      "total_tokens": 16024
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used (SemanticKITTI and nuScenes) are benchmark datasets for autonomous driving and contain annotations and sequences in English language only, as typically documented. No indication of multiple languages in the dataset entries is found in the paper."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset entries contain exactly two human languages. The paper only references English language usage."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4 Experiments, Datasets & Evaluation Metrics",
          "reasoning": "The datasets referenced for experiments, SemanticKITTI and nuScenes, are annotated and documented in English as per common conventions in autonomous driving datasets. The paper does not mention any other human language in dataset annotations."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset entries annotated or documented in non-English languages."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes methods and includes pseudocode-like formulas but does not introduce a new dataset that contains programming or code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical notation to describe their methods, this notation is part of the methodology description and not part of a new dataset introduced by the authors."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication systems are present in the proposed dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificially created languages are included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language characteristics of the dataset are explicitly understood as English; there is no language ambiguity."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language in annotation and documentation (English), so this label does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2327-rubric-6",
    "token_usage": {
      "prompt_tokens": 12693,
      "completion_tokens": 161,
      "total_tokens": 12854
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 4 (Experiments)",
          "reasoning": "The paper provides a code repository link for their method TASeg at https://github.com/LittlePey/TASeg, which appears to be for the model implementation rather than code for dataset construction. The datasets used (SemanticKITTI and nuScenes) are public benchmarks; no new datasets are introduced by the authors nor is any code for constructing new datasets provided in the paper."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "None",
          "reasoning": "The paper does not introduce any new datasets; it uses existing public datasets (SemanticKITTI and nuScenes). Therefore, there is no documentation or description of dataset creation or construction processes in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2336-rubric-0",
    "token_usage": {
      "prompt_tokens": 13674,
      "completion_tokens": 303,
      "total_tokens": 13977
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, 3.3, and Abstract",
          "Reasoning": "The TRINS dataset contains 39,153 text-rich images that were collected and filtered from LAION-5B, which is a web-scraped dataset. The images were selected using a combination of machine-assistance (CLIP-based filtering and classification) and human judgment for refinement. Thus the images are sourced from human-captured or human-created images available on the web, and human annotators were involved in the selection and filtering process, confirming a human-generated origin for the image modality."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.3 and 3.2",
          "Reasoning": "The dataset includes human-annotated captions and question-answer pairs. Captions were created fully by human annotators describing the images in detail. The visual question answering (VQA) data was generated semi-automatically: human-annotated captions and OCR results plus image tags served as inputs to large language models such as GPT-4 and LLaMA-2, which generated questions and answers. Hence, the text modality comprises both human-generated captions and model-generated question-answer pairs, reflecting a mixed origin. The paper explicitly states this via a semi-automatic annotation strategy employing both human annotation and large language model generation."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2336-rubric-1",
    "token_usage": {
      "prompt_tokens": 14474,
      "completion_tokens": 263,
      "total_tokens": 14737
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.2 and Section 3.3",
            "reasoning": "Section 3.2 states that all annotators were native English speakers with document annotation experience, hired via Upwork, indicating multiple human non-expert annotators. Section 3.3 specifies 20 native English speakers annotated TRINS-Cap captions and 10 annotators annotated TRINS-VQA test data, confirming multiple human non-experts involved."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 and Section 3.3",
            "reasoning": "Section 3.2 mentions annotators were provided with detailed annotation guidelines with multiple examples. Section 3.3 further confirms comprehensive annotation instructions were given to ensure detailed descriptions and text inclusion."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "None explicit",
            "reasoning": "The paper does not explicitly mention any scoring rubrics or formal evaluation criteria provided to annotators as part of the annotation process."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.2",
            "reasoning": "Section 3.2 explicitly states annotators were provided with detailed annotation guidelines including multiple examples, supporting the presence of annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2336-rubric-2",
    "token_usage": {
      "prompt_tokens": 15664,
      "completion_tokens": 460,
      "total_tokens": 16124
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that quality assurance was performed by a single human expert annotator. Annotators are described as having annotation experience but no mention of them being subject matter experts."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper states annotators have annotation experience but does not indicate that they are subject matter experts or that all annotators are experts. Hence, cannot claim multiple human experts conducted QA."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes multiple annotators involved in annotation and quality control processes, so QA was not performed by a single annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.2 Annotation Details and Section 3.3 TRINS-Cap: Text-Rich Image Captioning Annotation Process.",
          "reasoning": "The paper states that annotators hired via Upwork are native English speakers with experience but no mention of subject matter expertise, implying they are non-experts. Multiple annotators performed captioning and QA annotation, and multiple human evaluators reviewed annotations. Thus, multiple human non-expert annotators performed quality assurance."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.2 Annotation Details and Section 3.3 TRINS-VQA description.",
          "reasoning": "The paper describes using large language models like GPT-4 and LLaMA-2 to generate question-answer pairs from human-annotated captions, effectively using AI models to perform part of the annotation and quality assurance process by generating and verifying QA pairs."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 Annotation Details and Section 3.3 TRINS-Cap annotation process.",
          "reasoning": "An automated review process is described that uses OCR word matching and edit-distance based string matching as heuristic filters to accept or reject annotations before human evaluation, indicating use of automatic algorithmic verification for quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes multiple quality assurance procedures including human review, AI model involvement, and automated heuristic checks, so QA is present and documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2336-rubric-3",
    "token_usage": {
      "prompt_tokens": 15274,
      "completion_tokens": 612,
      "total_tokens": 15886
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.3 Annotation Details; Section 3 Text-Rich Image Instruction Dataset",
          "reasoning": "The TRINS-Cap dataset consists of human-annotated captions for text-rich images, where 20 native English-speaking annotators with document annotation experience were hired to create detailed image descriptions from scratch. The paper specifies that these captions are collected entirely from human annotators using a rigorous process including annotation guidelines, quality control, and manual review, demonstrating that this part of the dataset is original content created from scratch by humans."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.3 Annotation Details; Figure 1 Overview; Section 3 Text-Rich Image Instruction Dataset",
          "reasoning": "The TRINS-VQA dataset was generated using semi-automatic annotation by leveraging large language models like GPT-4 and LLaMA-2, which, given OCR outputs and human-written captions, generated question-answer pairs. This indicates that part of the dataset is newly generated by models rather than directly from human annotation, fulfilling the criterion for model-generated data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of human translation for the dataset creation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any application of machine translation in generating or processing the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Machine-Assisted Text-rich Image Selections",
          "reasoning": "The image source for TRINS was selected from the existing LAION-5B dataset through classifier filtering and clustering. This process is essentially selecting and aggregating images from an existing large corpus without significant modification to the images themselves, satisfying the collated category."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Machine-Assisted Text-rich Image Selections; Section 3.3 Annotation Details",
          "reasoning": "The dataset is derived from existing datasets such as LAION-5B image corpus, enhanced by applying classifiers and clustering to select text-rich images. In addition, OCR tools like Azure Read API and PaddleOCR extract textual information, and Recognize Anything Model generates tags used to assist annotators. These constitute transformations and adaptations applied on original data sources, thus the dataset is partially derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is clear documentation in the paper about data sources and methods of generation for all parts of the introduced dataset."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2336-rubric-4",
    "token_usage": {
      "prompt_tokens": 15792,
      "completion_tokens": 525,
      "total_tokens": 16317
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the TRINS dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of training models from scratch on the TRINS dataset; rather, pre-trained language models are fine-tuned."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 3 and 5",
          "reasoning": "The TRINS dataset is explicitly used for supervised fine-tuning of pretrained multimodal language models (e.g., LaRA) to enhance their ability to understand text-rich images. The paper details fine-tuning processes on TRINS-Cap and TRINS-VQA, demonstrating improved performance after fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any reinforcement learning based post-training methods using the TRINS dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5",
          "reasoning": "TRINS is used as a benchmark dataset to evaluate various models' performance on text-rich image captioning (TRINS-Cap), visual question answering (TRINS-VQA), and image generation (TRINS-Gen) tasks, demonstrating its role in evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 and Section 5",
          "reasoning": "The paper contains detailed statistical analyses and characteristics of the TRINS dataset such as caption lengths, question types, OCR word distributions, and comparisons to other datasets, serving analysis purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the TRINS dataset is used as a knowledge base for retrieval augmented generation or similar purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The TRINS dataset is clearly used in multiple practical ways including supervised fine-tuning, evaluation, and analysis as detailed within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2336-rubric-5",
    "token_usage": {
      "prompt_tokens": 16515,
      "completion_tokens": 657,
      "total_tokens": 17172
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3 (Text-Rich Image Instruction Dataset)",
          "reasoning": "The dataset TRINS is described as containing text-rich images annotated in English only. There is no mention of multiple languages in the data annotations or questions. The paper emphasizes that all annotators are native English speakers and detailed annotation instructions are provided in English, indicating monolingual English content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.2 (Annotation Details)",
          "reasoning": "The dataset is not described as containing exactly two languages. The annotations, questions, and answers are all constructed and reviewed in English only."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.2 (Annotation Details)",
          "reasoning": "The paper states that all annotators are native English speakers and the annotation instructions and reviews are conducted in English. The dataset consists of captions, questions, and answers in English. There is no indication that other languages are present in the annotations or questions."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "e.g., Section 3.2 (Annotation Details)",
          "reasoning": "The dataset is described as using only English for annotations and instructions. There is no mention of any non-English language being used monolingually."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Throughout the paper",
          "reasoning": "The dataset contains text-rich images with natural language captions, questions, and answers. There is no mention of programming or structured code content in the dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Throughout the paper",
          "reasoning": "No indication or mention that mathematical or formal logical expressions or symbolic representations are included in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Throughout the paper",
          "reasoning": "The dataset focuses on human language text embedded in images; no reference to biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Throughout the paper",
          "reasoning": "There is no mention of fictional or artificially constructed languages like Klingon or Esperanto in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Throughout the paper",
          "reasoning": "The language used in the dataset is explicitly stated to be English with detailed annotation procedures specifying English annotations; thus the language of dataset entries is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human language text, i.e., English captions, questions, and answers."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2336-rubric-6",
    "token_usage": {
      "prompt_tokens": 13733,
      "completion_tokens": 205,
      "total_tokens": 13938
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention or link to code repository found in the paper for the dataset construction.",
          "reasoning": "The paper describes the dataset collection and annotation pipeline in detail, including the semi-automatic process using CLIP and GPT-4 models and human annotation via Labelbox, but does not provide any link or mention of publicly releasing the code used for data collection, filtering, or annotations."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Text-Rich Image Instruction Dataset) and subsections 3.1 to 3.3",
          "reasoning": "The paper provides comprehensive documentation on the dataset creation process, including image selection from LAION-5B using CLIP-based classifiers, detailed annotation procedures with human annotators and heuristic filters, quality control measures, and statistics for TRINS-Cap, TRINS-VQA, and TRINS-Gen datasets. This thorough explanation supports reproducibility attempts despite the lack of code release."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2342-rubric-0",
    "token_usage": {
      "prompt_tokens": 13956,
      "completion_tokens": 128,
      "total_tokens": 14084
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Implement details",
          "Reasoning": "The new dataset introduced for training the plug-and-play model is the Laion2B-en dataset, which consists of 2.32 billion text-image pairs. These images are center-cropped to 512x512 pixels. Since Laion2B-en is an existing dataset curated from web data containing human-generated images and associated text prompts, and is used here as the data the authors used for training their model, it is considered human generated."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2342-rubric-1",
    "token_usage": {
      "prompt_tokens": 14756,
      "completion_tokens": 254,
      "total_tokens": 15010
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 3.4 and Section 4.1",
            "reasoning": "The paper describes training a model (denoted as \\bar{y}_\\theta) as a plug-and-play module on the Laion2B-en dataset using a U-net architecture, trained with AdamW and large-scale distributed training (Section 4.1). This model is used for initial singularity sampling and is integrated into existing diffusion models without further human annotation. This indicates the main annotation was performed via a trained AI model rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not provide detailed annotation instructions for annotators, as the process involves training a model rather than human annotations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics or manual evaluation criteria for annotation are described, since the process is model training."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No examples for manual annotation are provided; the model is trained on an existing large dataset (Laion2B-en) without human annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2342-rubric-2",
    "token_usage": {
      "prompt_tokens": 15946,
      "completion_tokens": 368,
      "total_tokens": 16314
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for dataset annotations or content validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper of quality assurance involving multiple human experts for dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA process performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA process performed by multiple non-expert humans."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of AI models being used for quality assurance or judgment of dataset content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper focuses on a plug-and-play module and theoretical analysis, but does not describe any automatic QA or verification process for dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a method trained on the Laion2B-en dataset but does not introduce any new datasets nor describe any quality assurance process for datasets. Thus, no QA process is applied or documented for datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2342-rubric-3",
    "token_usage": {
      "prompt_tokens": 15556,
      "completion_tokens": 445,
      "total_tokens": 16001
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new data that was created entirely from scratch by human contributors. Instead, it discusses theoretical analysis and a plug-and-play model for sampling in diffusion models."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper trains a new plug-and-play model (SingDiffusion), it is a model artifact rather than a dataset of content generated solely by AI without baseline data. The paper does not present new datasets consisting entirely of AI-generated data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data were produced by translating from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not involve data generated by machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Implement Details",
          "reasoning": "The paper's training data for the SingDiffusion model is based on existing datasets, specifically the Laion2B-en dataset, which contains 2.32 billion text-image pairs collected from publicly available sources. The authors use this existing dataset without significant modification to train their plug-and-play model."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the creation of a new dataset derived from existing data with additional modifications or transformations. The primary contribution is a modeling plug-in rather than derived datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the data origin (Laion2B-en dataset) used to train the model; thus, the data source is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2342-rubric-4",
    "token_usage": {
      "prompt_tokens": 16074,
      "completion_tokens": 406,
      "total_tokens": 16480
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 3.4",
          "reasoning": "The dataset Laion2B-en was used to train the plug-and-play module \\(\\bar{y}_\\theta\\) as described in Section 4.1 (Implementation details), where training is performed using image-text pairs. This module is integrated with pre-trained diffusion models to improve initial sampling without extra training of those models, effectively fine-tuning the approach on the dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "The authors employ the COCO dataset (Section 4.3) as a test set for quantitative evaluation of image generation quality (FID and CLIP scores) but this is a pre-existing dataset, not newly introduced. However, Laion2B-en is used for training only, not evaluation. No new dataset introduced for evaluation or benchmarking only."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2342-rubric-5",
    "token_usage": {
      "prompt_tokens": 16797,
      "completion_tokens": 543,
      "total_tokens": 17340
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the proposed dataset contains entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset includes exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 (Implementation Details), Section 4.2 (Average Brightness Issue)",
          "reasoning": "The dataset used for training the SingDiffusion model is the Laion2B-en dataset, which consists of image-text pairs where the text content is in English. The paper explicitly mentions training on this English dataset and uses only English prompts, indicating the dataset is monolingual in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as containing any language other than English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes algorithms and mathematical notation, the dataset entries themselves are images and English text, and there is no mention of datasets containing code or programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes mathematical expressions and theoretical analysis, but this refers to the method and not to a dataset containing mathematical or logical notation as content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence the dataset contains constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are clearly specified as English; thus there is no unknown language content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains English language text paired with images, so it includes language data and is not N/A."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2342-rubric-6",
    "token_usage": {
      "prompt_tokens": 14015,
      "completion_tokens": 234,
      "total_tokens": 14249
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or part of the paper provides a link or mention of code for dataset construction.",
          "reasoning": "The paper focuses on proposing a plug-and-play module (SingDiffusion) and its application to improve existing diffusion models. It mentions training a model on the Laion2B-en dataset but does not provide or reference any code repository or scripts for data processing or dataset construction. Hence, code relevant to dataset creation is not made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 (Implement details) and throughout the paper.",
          "reasoning": "Although the paper does not introduce a new dataset, it documents the use of the existing Laion2B-en dataset for training their model. The paper details the dataset source (Laion2B-en), preprocessing steps like center-cropping images to 512x512 resolution, and training details such as batch size, steps, and optimization methods. This documentation provides transparency on the data preparation process relevant to their method's reproducibility and is present primarily in Section 4.1 and its related discussion."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2364-rubric-0",
    "token_usage": {
      "prompt_tokens": 10832,
      "completion_tokens": 289,
      "total_tokens": 11121
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4 Experiments; Section 5 Discussion",
          "Reasoning": "The authors report adapting pretrained models on a variety of datasets including VOID, NYUv2, SceneNet, and ScanNet for indoor settings, and KITTI, Virtual KITTI (VKITTI), nuScenes, and Waymo for outdoor settings. The paper introduces a new dataset benchmarking setup and code release for test-time adaptation, indicating they introduce at least a new benchmarking setup using these existing datasets and models. The experimental section describes these datasets and pretrained models but does not mention creation of any entirely new image dataset collected or generated by the authors. Therefore, no new image dataset is introduced by the authors; the paper uses existing image datasets."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4 Experiments; Section 5 Discussion",
          "Reasoning": "Similarly, sparse depth maps derived from lidar or projected point clouds are used from the above datasets (e.g., KITTI, VOID) which are collected from sensors. The paper does not state that the authors collected a new dataset of sensor data or sparse depth maps. Instead, they leverage existing public datasets containing sparse depth with images. Hence, no new sensor/signal dataset is introduced by the authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2364-rubric-1",
    "token_usage": {
      "prompt_tokens": 11632,
      "completion_tokens": 262,
      "total_tokens": 11894
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3. Method Formulation and Section 4. Experiments",
            "reasoning": "The paper describes the creation of proxy embeddings and adaptation layers learned via training on source datasets and employed during test-time adaptation in target domains. The annotations correspond to supervised ground truth depth maps used during source model training on source datasets (VOID, KITTI) and the preparation of proxy mappings, which are learned automatically by neural network models and not annotated by humans."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not mention any specific manual annotation instructions for annotators regarding the introduced datasets; the data is derived from sensor measurements and synthetic data, and ground truth is obtained or simulated automatically."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics or structured evaluation criteria for human annotations are described in the paper as the datasets' ground truth is either sourced or generated through automated processes."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No example annotation guidelines or samples for manual annotation is presented since the ground truth and sparse depth maps come from existing datasets or automated generation rather than manual annotation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2364-rubric-2",
    "token_usage": {
      "prompt_tokens": 12822,
      "completion_tokens": 351,
      "total_tokens": 13173
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving a single human expert for dataset annotation or validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about quality assurance by multiple human experts for the datasets introduced or used."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention QA performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple non-expert annotators performed quality assurance on any dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance conducted by AI models acting as judges for dataset validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automatic or algorithmic quality assurance process is described for dataset annotations or content in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets, nor does it describe any quality assurance process for datasets. It uses existing datasets and focuses on methods and adaptation techniques without detailing annotation or QA procedures."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2364-rubric-3",
    "token_usage": {
      "prompt_tokens": 12432,
      "completion_tokens": 408,
      "total_tokens": 12840
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the authors created any new datasets solely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not claim generation of any datasets purely synthesized by AI or models without reliance on existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of new datasets produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that new datasets were generated by machine translation from other languages."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 Experiments and 5 Discussion",
          "reasoning": "The authors use multiple existing public datasets (VOID, NYUv2, SceneNet, ScanNet, KITTI, VKITTI, nuScenes, Waymo) to construct domain adaptation benchmarks and evaluation scenarios. They aggregate these datasets to test their method under various domain shift conditions. The paper mentions releasing a dataset benchmarking setup but does not describe creation of new raw data; thus, the datasets are collated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new dataset produced by modifying or adapting existing datasets; rather, evaluation setup uses existing datasets as provided."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents the data sources clearly; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2364-rubric-4",
    "token_usage": {
      "prompt_tokens": 12950,
      "completion_tokens": 235,
      "total_tokens": 13185
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper extensively uses and evaluates pre-existing datasets such as VOID, NYUv2, SceneNet, ScanNet, KITTI, VKITTI, nuScenes, and Waymo. However, the paper does not introduce any new datasets. It introduces a new method for test-time adaptation for depth completion but does not present or propose any new datasets. Therefore, there is no practical usage of any new dataset described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2364-rubric-5",
    "token_usage": {
      "prompt_tokens": 13673,
      "completion_tokens": 535,
      "total_tokens": 14208
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract, Sections 1, 3, 4",
          "reasoning": "The datasets referenced in the paper such as KITTI, VOID, NYUv2, SceneNet, and others are well-known datasets containing image and depth data, but all metadata, annotations, and documentation are in English. The paper itself uses English language consistently, and there is no mention or evidence of any other human language content in the newly introduced datasets or benchmarking setups. Therefore, the new dataset benchmarking setup introduced by the authors is monolingual in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes code implementation details and references to code repositories, these are not datasets introduced by the paper itself but rather artifacts supporting the method. The new dataset itself does not contain entries of programming or structured code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The mathematical notation is present in the paper text to describe the method but not in the dataset entries themselves. Thus, the dataset does not contain mathematical or logical symbolic representation entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the new datasets include biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not involve any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages present in the dataset are documented and English is the primary language of annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain natural language text related to annotations and documentation in English."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2364-rubric-6",
    "token_usage": {
      "prompt_tokens": 10891,
      "completion_tokens": 190,
      "total_tokens": 11081
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 5 (Last paragraph)",
          "reasoning": "The paper mentions that code will be released for models, dataset benchmarking setup, adaptation, and evaluation code to support research in test-time adaptation for depth completion; however, it does not provide any link or explicit statement that the code for constructing any new datasets is publicly available. Moreover, the paper primarily uses existing datasets and does not introduce any new dataset. Therefore, there is no code related to dataset construction provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "N/A",
          "reasoning": "The paper does not introduce any new datasets constructed by the authors. Instead, it uses existing public datasets such as VOID, NYUv2, SceneNet, ScanNet, KITTI, Virtual KITTI, nuScenes, and Waymo. Consequently, there is no documentation on dataset creation process within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2381-rubric-0",
    "token_usage": {
      "prompt_tokens": 13542,
      "completion_tokens": 235,
      "total_tokens": 13777
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and Section 4.2",
          "Reasoning": "The paper introduces a new dataset consisting of images from single-domain datasets (Pascal VOC, ADE20K, NYUv2) used for evaluating their method with improved image-text alignment via automated captioning. These images are human-generated and collected from existing human-captured datasets, but the new aspect is their use with generated image captions using BLIP-2, which modifies text conditioning but does not create new images. Thus, the image data modality is human-generated, not model generated."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and Section 4.2",
          "Reasoning": "The new dataset introduced includes text captions generated by the BLIP-2 image captioning model for each image in the single-domain datasets. These captions are automatically generated by a model (BLIP-2) and not manually produced by humans, therefore the text data modality is model generated."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2381-rubric-1",
    "token_usage": {
      "prompt_tokens": 14342,
      "completion_tokens": 239,
      "total_tokens": 14581
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 3.1, Section 4.2",
            "reasoning": "The paper uses BLIP-2, an automated image captioning model, to generate captions for every image in the new datasets introduced for single-domain tasks (Pascal VOC2012 segmentation, ADE20K segmentation, and NYUv2 depth estimation). There is no indication of human annotators labeling data; captions are generated automatically by BLIP-2 model."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "Since the annotation is performed by an AI Model (BLIP-2), no human annotators are involved and hence no instructions for annotation are discussed or necessary."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics are provided for annotation guidelines as the annotations are generated by an AI model automatically."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No examples of annotations or annotation guidelines are provided in the paper because BLIP-2 automatically generates captions without human annotation or guideline examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2381-rubric-2",
    "token_usage": {
      "prompt_tokens": 15532,
      "completion_tokens": 340,
      "total_tokens": 15872
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert on any new dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance processes involving multiple human experts for validating new dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any QA performed by single non-expert humans on new dataset annotations."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by multiple human non-experts on new dataset content."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses AI models for image captioning (BLIP-2) to generate captions, this is part of the method rather than a QA process to validate dataset annotations. The paper does not describe the use of AI models explicitly for quality assurance of new dataset content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the authors used an automated verification process or algorithmic rule checks as quality assurance on new datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; it only uses existing benchmark datasets (e.g., Pascal VOC, ADE20K, NYUv2, Watercolor2K, Comic2K, Cityscapes, Dark Zurich, Nighttime Driving). As such, no quality assurance process for new datasets is applicable or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2381-rubric-3",
    "token_usage": {
      "prompt_tokens": 15142,
      "completion_tokens": 480,
      "total_tokens": 15622
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creating any original datasets entirely from scratch by human contributors. Instead, it uses standard existing datasets such as Pascal VOC, ADE20K, NYUv2, Cityscapes, Watercolor2K, and Comic2K for training and evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets generated entirely by AI or machine learning models. While it uses automated captioning models (BLIP-2) to generate captions for images, the underlying image datasets themselves are not newly generated by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data has been produced through human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss or mention use of machine translation to generate data from other languages."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing datasets collected and aggregated by others (e.g., Pascal VOC, ADE20K, NYUv2). However, these are pre-existing datasets and not newly introduced by the authors, so this category does not apply to new datasets introduced by the paper."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets derived or adapted from existing datasets. Instead, it uses existing standard datasets and focuses on enhancing vision performance by leveraging automated captioning and diffusion model prompting strategies."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. All datasets used for training and evaluation are standard, publicly available datasets. The paper focuses on methods to improve model performance using automated captions and text prompts but does not produce or document any new dataset."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2381-rubric-4",
    "token_usage": {
      "prompt_tokens": 15660,
      "completion_tokens": 348,
      "total_tokens": 16008
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3 (Methods), Section 4 (Results)",
          "reasoning": "The paper describes training a task-specific head along with the diffusion backbone (which is pre-trained and frozen) using supervised learning on datasets such as ADE20K and NYUv2 for segmentation and depth estimation respectively, in both single-domain and cross-domain settings. Therefore, the proposed use of datasets (with captions) is for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 4 and 4.3 (Results - Cross-domain alignment)",
          "reasoning": "The datasets are used to benchmark and evaluate the performance of the diffusion-based models on semantic segmentation, depth estimation, and object detection tasks across domains, measuring metrics such as mIoU, AP, and RMSE."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 4.1, 4.2, 4.3 (Results and Ablations)",
          "reasoning": "The paper conducts systematic analyses of how different prompt strategies and caption modifications affect model performance and cross-attention maps, examining trends and sensitivities to text-image alignment and domain adaptation effects."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2381-rubric-5",
    "token_usage": {
      "prompt_tokens": 16383,
      "completion_tokens": 403,
      "total_tokens": 16786
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset entries containing more than two human languages; the focus is on English captions."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention datasets containing exactly two human languages per entry; captions and text prompts are exclusively English."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 'Text-Aligned Diffusion Perception (TADP)' and throughout Sections 4.2 and 4.3",
          "reasoning": "The authors use automated caption generation with BLIP-2 to produce English captions for images; all caption modifications and prompts are English text. The datasets introduced by the authors for prompting are thus monolingual English in nature."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English language data or captions are described in the paper for the new datasets introduced by the authors."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper contains code snippets in the methods, the dataset entries do not contain programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical expressions appear only in the exposition of methods but not as part of dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not contain biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or use of fictional or constructed languages in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of the dataset captions are explicitly English and described clearly."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets include captions/text prompts and therefore contain language entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2381-rubric-6",
    "token_usage": {
      "prompt_tokens": 13601,
      "completion_tokens": 161,
      "total_tokens": 13762
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 1, and footnote",
          "reasoning": "The paper includes a URL to the code repository in the abstract (github.com/damaggu/TADP), indicating the code is publicly available. This repository presumably contains code relevant to data processing and dataset construction involved in their method."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets; it uses existing datasets such as Pascal VOC, ADE20K, NYUv2, Watercolor2K, Comic2K, Cityscapes, Dark Zurich-val, and Nighttime Driving for evaluation. Since no new datasets are introduced or constructed by the authors, there is no documentation of dataset creation processes provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2411-rubric-0",
    "token_usage": {
      "prompt_tokens": 10746,
      "completion_tokens": 163,
      "total_tokens": 10909
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.2, 5.3, and Table 1",
          "Reasoning": "The paper introduces new post-hoc explanation methods evaluated on the ImageNet-Segmentation dataset [21] which, although originally pre-existing, the authors conduct new explanation experiments and generate heatmaps corresponding to image data to demonstrate their method's effectiveness. However, the paper does not explicitly state the introduction of any newly created dataset by the authors themselves. The datasets referenced (ImageNet-Segmentation, CIFAR-10, CIFAR-100, ImageNet) are pre-existing. Therefore, no new datasets with explicit data sources are introduced or created by the authors in this work."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2411-rubric-1",
    "token_usage": {
      "prompt_tokens": 11546,
      "completion_tokens": 273,
      "total_tokens": 11819
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.3 Aggregation Framework and Section 5 Experiments",
            "reasoning": "The paper introduces TokenTM, a post-hoc explanation method for Vision Transformers that computationally measures token transformations and attention weights to generate explanation heatmaps. The explanation maps (annotations) on datasets such as ImageNet-Segmentation are produced through the proposed automatic method, not by humans. There is no indication that humans annotated the explanation maps; instead, the method automatically generates them for evaluation purposes."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not describe any annotation guidelines, instructions, or detailed protocols provided to annotators for labeling the new dataset, nor does it mention any human-based annotation procedure requiring instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "There is no description or mention of scoring rubrics for annotation in the context of the new dataset; the generated explanations are from an automatic process, so rubrics for human annotation are not applicable."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not provide sample annotations or examples of annotations created by humans as guidance; all explanations are generated by the proposed automatic method without human annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2411-rubric-2",
    "token_usage": {
      "prompt_tokens": 12736,
      "completion_tokens": 259,
      "total_tokens": 12995
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for validating dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA performed by multiple human experts for dataset validation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information regarding QA performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of QA by multiple non-expert human annotators is present in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any use of AI models to perform quality assurance on datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any automatic verification or algorithmic rule-based quality assurance process applied to datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset, nor does it describe or apply any quality assurance processes related to datasets. Therefore, no QA process is documented or applied."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2411-rubric-3",
    "token_usage": {
      "prompt_tokens": 12346,
      "completion_tokens": 276,
      "total_tokens": 12622
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper utilizes existing datasets (e.g., ImageNet, CIFAR-10, CIFAR-100, ImageNet-Segmentation) for evaluation purposes but does not introduce or create any new datasets. There is no mention or documentation of collecting, curating, or creating new data. Hence, the data source or method of generation for any new dataset is not specified or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2411-rubric-4",
    "token_usage": {
      "prompt_tokens": 12864,
      "completion_tokens": 267,
      "total_tokens": 13131
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.2 and 5.3",
          "reasoning": "The authors use datasets such as ImageNet-Segmentation, CIFAR-10, CIFAR-100, and ImageNet exclusively for evaluating the performance of their explanation method TokenTM, measuring localization ability, impact on accuracy, and probability through segmentation and perturbation tests."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.4 (Ablation Study) and throughout experimental sections",
          "reasoning": "The datasets are used for analyzing the effectiveness of the proposed method and its components, including ablation studies and understanding layer aggregation depth effects, primarily for analyzing trends and characteristics rather than training or direct evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2411-rubric-5",
    "token_usage": {
      "prompt_tokens": 13587,
      "completion_tokens": 522,
      "total_tokens": 14109
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce a new dataset containing entries in more than two human languages. There is no mention of language diversity in any dataset introduced by the authors."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset introduced by the authors contains exactly two human languages. The paper does not discuss any bilingual dataset."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no new dataset introduced by the authors, and therefore no monolingual English dataset is proposed."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is introduced containing monolingual non-English content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical expressions and technical notation to describe the method, there is no proposed dataset consisting of code or programming languages."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains mathematical formulas to describe the proposed method, but does not introduce any datasets containing mathematical or formal logical expressions as data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data are introduced as new datasets."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any datasets containing constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets with unspecified or undocumented languages are introduced."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset, therefore there are no language entries to evaluate. The data used for experiments (e.g., ImageNet, CIFAR-10, CIFAR-100) are pre-existing datasets and not new to this paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2411-rubric-6",
    "token_usage": {
      "prompt_tokens": 10805,
      "completion_tokens": 144,
      "total_tokens": 10949
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not mention or provide any link or reference to code related to the construction or processing of any new datasets. The experiments use existing datasets such as ImageNet-Segmentation, CIFAR-10, CIFAR-100, and ImageNet. No new dataset is introduced by the authors, nor is any code for dataset construction released."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "",
          "reasoning": "Since the paper does not introduce any new dataset, it does not provide any documentation on dataset creation. The datasets used are pre-existing and referenced accordingly; thus, no dataset creation process is documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2454-rubric-0",
    "token_usage": {
      "prompt_tokens": 12443,
      "completion_tokens": 136,
      "total_tokens": 12579
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1, Section 4.1",
          "Reasoning": "The new dataset is constructed by the authors via multi-view image capture to reconstruct 3D meshes using grid-based NeRF. These multi-view images are collected to create initial 3D representations, indicating human involvement in capturing images through imaging devices. This is confirmed in Section 3.1 where multi-view images are input to grid-based NeRF, and experimental settings in Section 4.1 mention use of multi-view image data for 3D object reconstruction."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2454-rubric-1",
    "token_usage": {
      "prompt_tokens": 13243,
      "completion_tokens": 322,
      "total_tokens": 13565
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3, Methodology; Section 4, Experiments",
            "reasoning": "The paper presents a novel 3D adversarial example generation framework TT3D which requires complex understanding and design, indicating that the annotations (such as labeling or data generation of multi-view images to 3D mesh reconstructions) are produced by the research authors, who are experts in the field. There is no indication of crowd-sourcing or AI-based annotation processes."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not mention any detailed annotation instructions provided to annotators for the new dataset created or used. The dataset IM3D is referenced from prior work [28], so does not fall under new datasets introduced. The new data introduced concerns 3D reconstructions from multi-view images, but no annotation guidelines or instructions are detailed."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "No scoring rubrics or explicit evaluation criteria are mentioned for annotation processes or labeling in relation to the new dataset introduced in the paper. The experiments focus on attack success rates, but no rubrics in annotation guidelines are discussed."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not provide examples or detailed annotation guidelines with example annotations related to the new dataset introduced. Visual examples are provided of 3D adversarial objects and rendered images but these are results rather than annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2454-rubric-2",
    "token_usage": {
      "prompt_tokens": 14433,
      "completion_tokens": 405,
      "total_tokens": 14838
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for validating the datasets or annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information or description that multiple human experts performed quality assurance for any dataset introduced by the authors."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single non-expert human conducted quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of quality assurance by multiple non-expert humans is provided in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that an AI model was used as a judge or for validating dataset content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although automated methods like grid-based NeRF reconstruction and marching cubes are utilized for 3D mesh reconstruction, these describe data processing and not explicit quality assurance on dataset annotation or content. No explicit automated QA process for dataset validation is described."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document or describe any explicit quality assurance process applied to validate the dataset or annotations introduced by the authors. The datasets used are mainly the IM3D dataset (pre-existing) and the constructed 3D meshes from multi-view images, but no QA steps are discussed."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2454-rubric-3",
    "token_usage": {
      "prompt_tokens": 14043,
      "completion_tokens": 475,
      "total_tokens": 14518
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the authors created new 3D object data entirely from scratch by human contributors. Instead, they use existing multi-view images and datasets such as IM3D."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses grid-based NeRF models for reconstructing 3D meshes from multi-view images, the generated 3D meshes are reconstructions based on existing image data rather than entirely new data generated solely by models without any input data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data being produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of data generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 (Datasets)",
          "reasoning": "The experimental section describes the use of the IM3D dataset consisting of 3D objects from 100 ImageNet categories. This dataset is a collection of existing 3D objects rather than newly created data. The paper leverages these existing multi-view images for reconstruction."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.3 and 3.4 (Methodology), Section 4 (Experiments)",
          "reasoning": "The paper derives new 3D adversarial textured meshes by reconstructing from existing multi-view images and then performing adversarial fine-tuning in the parameter space of grid-based NeRF. This involves transforming and optimizing existing data to create adversarial examples, hence the data is derived with modifications applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies the sources and derivation process of the data, so the data origin is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2454-rubric-4",
    "token_usage": {
      "prompt_tokens": 14561,
      "completion_tokens": 273,
      "total_tokens": 14834
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Experiment Settings, Section 4.2 Attack performance in the Digital World, Section 4.3 Additional Results and Ablation Study, and Section 4.4 Physical Attack",
          "reasoning": "The IM3D dataset is introduced and used as input 3D objects to evaluate the proposed TT3D method. Specifically, 100 objects randomly sampled from IM3D are employed to assess attack success rates across numerous victim models and settings. The dataset serves as a benchmark to measure attack effectiveness, transferability, robustness and cross-task transferability of the generated adversarial 3D objects, as detailed in the experiments."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2454-rubric-5",
    "token_usage": {
      "prompt_tokens": 15284,
      "completion_tokens": 527,
      "total_tokens": 15811
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of multiple human languages in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets contain exactly two languages; all dataset references are related to image, 3D objects, or models without language content."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, e.g., Abstract, Introduction, Sections 3 and 4",
          "reasoning": "The datasets used, such as IM3D and ImageNet, include labels and textual content exclusively in English, with no mention of other languages. All example classes, labels, and annotations are provided in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset examples or references to datasets in non-English languages are present."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses algorithmic methods and neural networks, it does not introduce a new dataset containing programming or structured code as entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical and logical expressions are used for method formulation but not as dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not include biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content in the dataset is explicitly documented to be English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The introduced dataset (IM3D and related 3D objects with labels) clearly contain human language labels (English), so the dataset does contain language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2454-rubric-6",
    "token_usage": {
      "prompt_tokens": 12502,
      "completion_tokens": 163,
      "total_tokens": 12665
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention",
          "reasoning": "The paper does not contain any explicit statement, URL, or indication that the code related to dataset creation or the TT3D method is publicly available. There is no GitHub link or repository mentioned anywhere in the paper or supplementary information."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Experiment Settings - Datasets",
          "reasoning": "The paper documents the usage of the IM3D dataset with details on its scale (1K 3D objects from 100 ImageNet categories) and the selection of 100 objects from 30 categories for experiments. The process of dataset use is described sufficiently in Section 4.1 to understand the data underlying the experiments."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2551-rubric-0",
    "token_usage": {
      "prompt_tokens": 13168,
      "completion_tokens": 149,
      "total_tokens": 13317
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Experimental Setup, Datasets",
          "Reasoning": "The paper clearly states the use of various established image datasets such as CELEBA WILD, CUB-200-2011, 300W-LP, AFLW2000-3D, HORSE, and AFHQ, which are collections of real-world images (photographs). These datasets are mentioned as evaluation datasets but are not introduced as new datasets by the authors. The paper does not mention creation or introduction of any new datasets by the authors themselves. Instead, it uses these existing image datasets for training and evaluation."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2551-rubric-1",
    "token_usage": {
      "prompt_tokens": 13968,
      "completion_tokens": 300,
      "total_tokens": 14268
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 5.1, Experiments - Datasets; Section 5.3, Quantitative Results; Method Description in Sections 3 and 4",
            "reasoning": "The paper introduces a method that performs unsupervised 3D keypoint inference directly from category-specific image collections without manual annotations. The dataset usage is described as training and testing on available image collections with no human annotation involved in the 3D keypoints labeling process. Hence, the annotations (keypoints and 3D structure) result from the automatic method output rather than manual human labeling."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No section provides annotation instructions since no manual annotation is performed.",
            "reasoning": "The method does not involve human annotators labeling data; all analysis is done by the model trained on unannotated image collections, so there are no annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not applicable, as the annotation is not manual or human-labeled.",
            "reasoning": "Since there are no human annotators, no scoring rubrics or guidelines are provided for annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples are shown as no manual annotations are done.",
            "reasoning": "The paper does not present examples of manual annotations since the 3D keypoints are predicted automatically; thus, no annotation examples exist for the new dataset annotations."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2551-rubric-2",
    "token_usage": {
      "prompt_tokens": 15158,
      "completion_tokens": 305,
      "total_tokens": 15463
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on datasets introduced by the authors."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by multiple human experts on datasets introduced by the authors."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single non-expert human annotator on datasets introduced by the authors."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by multiple non-expert human annotators on datasets introduced by the authors."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used for quality assurance on datasets introduced by the authors."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper involves losses and automated processing during model training, there is no mention of automated quality assurance applied to datasets or annotations provided by the authors."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets by the authors, nor does it describe any quality assurance process performed on data or annotations. The datasets used are pre-existing datasets; therefore, no quality assurance related to new datasets is applicable or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2551-rubric-3",
    "token_usage": {
      "prompt_tokens": 14768,
      "completion_tokens": 425,
      "total_tokens": 15193
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not claim to have created new datasets from scratch by human contributors. The datasets used are existing public datasets such as CELEBA WILD, CUB-200-2011, 300W-LP, AFLW2000-3D, HORSE, and AFHQ."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset generated purely by models or machine learning. The training and evaluation are done on existing image collections."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset involved human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any dataset generated by machine translation methods."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.1 Experimental Setup - Datasets",
          "reasoning": "The paper uses existing publicly available datasets (CELEBA WILD, CUB-200-2011, 300W-LP, AFLW2000-3D, HORSE, AFHQ) collected by other researchers without significant modification. The authors gather and use these existing datasets to train and evaluate their method."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe creation of any new dataset derived from existing sources with adaptations or transformations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2551-rubric-4",
    "token_usage": {
      "prompt_tokens": 15286,
      "completion_tokens": 362,
      "total_tokens": 15648
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5.1 (Experimental Setup) and Section 5.3 (Quantitative Results)",
          "reasoning": "The paper reports training the proposed 3D keypoint inference model from scratch on category-specific image collections such as CELEBA WILD and 300W-LP datasets. The method is trained end-to-end without requiring pre-existing annotations or prior training on other data."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1 (Experimental Setup) and Section 5.3 (Quantitative Results)",
          "reasoning": "The datasets mentioned (e.g., AFLW2000-3D, CELEBA WILD) are used for evaluating and benchmarking the performance of the proposed method in terms of 2D and 3D keypoint detection accuracy, shown through normalized mean error metrics."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.2 (Qualitative Results), Section 6 (Ablation Study)",
          "reasoning": "The paper uses the datasets to analyze the 3D keypoint structures learned, visualizes qualitative results across different object categories, and performs ablation studies to understand the influence of different loss components and model parameters on keypoint quality."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2551-rubric-5",
    "token_usage": {
      "prompt_tokens": 16009,
      "completion_tokens": 551,
      "total_tokens": 16560
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are collections of images from various categories (faces, animals, birds, horses), which primarily contain visual data without textual language content. The paper does not state that the dataset entries contain English language textual content; rather, the datasets are image-based without any language annotations or text entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that the datasets include text or entries in any specific non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are image collections used for 3D keypoint inference and do not contain programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical notations and formulas, these are part of the methodological description and not contained in the dataset entries. The datasets are image collections without embedded mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain images of biological entities (e.g., animals, birds, human faces), but no biological sequences or non-human communication data such as DNA or animal signals are explicitly included as dataset entries."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any fictional or artificially created language data."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The linguistic content of the dataset entries is either non-existent (images only) or not language-based, so this category does not apply."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The datasets introduced consist solely of image data without any language or textual entries. The datasets mainly contain collections of images from specific object categories used for computer vision tasks, thus do not contain any language entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2551-rubric-6",
    "token_usage": {
      "prompt_tokens": 13227,
      "completion_tokens": 148,
      "total_tokens": 13375
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "N/A",
          "reasoning": "The paper does not mention any publicly available code repository or link associated with the dataset construction or code for reproducing the dataset. There is no explicit statement about releasing the code."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "N/A",
          "reasoning": "The paper does not introduce any new dataset collected by the authors. Instead, it uses existing public datasets such as CELEBA WILD, CUB-200-2011, 300W-LP, AFLW2000-3D, HORSE, and AFHQ for evaluation. There is no description or documentation on dataset creation or construction by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-257-rubric-0",
    "token_usage": {
      "prompt_tokens": 13565,
      "completion_tokens": 153,
      "total_tokens": 13718
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1, Dataset and Section 4 (Experiments)",
          "Reasoning": "The paper evaluates the proposed boosted implicit neural representation models primarily on multiple known video datasets such as Bunny, UVG, and DAVIS validation dataset. However, these datasets are pre-existing and publicly available. The paper does not explicitly state that it introduces a new dataset created by the authors. Instead, all experimental datasets mentioned are standard benchmarks referenced in Sections 4.1 and Experiments. Therefore, no new dataset is introduced by the authors in the paper. Consequently, there are no new data sources from newly introduced datasets to list."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-257-rubric-1",
    "token_usage": {
      "prompt_tokens": 14365,
      "completion_tokens": 219,
      "total_tokens": 14584
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Experimental Setup, Dataset",
            "reasoning": "The experiments and data processing involve overfitting models to specific video datasets such as Bunny, UVG, and DAVIS. There is no mention of manual annotation or subjective human labeling. The video data itself is existing benchmark video datasets, not newly annotated data for new datasets. Instead, the paper focuses on training neural networks on these videos, which is an automatic process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not describe any annotation instructions provided to human annotators for these datasets, as there is no human annotation involved."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not mention any scoring rubrics or evaluation guidelines provided for annotation tasks."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples or guidelines are discussed or provided since no manual annotation process is involved."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-257-rubric-2",
    "token_usage": {
      "prompt_tokens": 15555,
      "completion_tokens": 287,
      "total_tokens": 15842
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert for any datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance conducted by multiple human experts on any dataset introduced by the authors in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance by a single non-expert human annotator for dataset validation."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance process involving multiple non-expert human annotators for datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance by an AI model as a judge for dataset validation is not discussed in the paper."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated verification or rule-based quality assurance process applied to the datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. The experimental evaluation is conducted on existing benchmark datasets: Bunny [19], UVG [29], and DAVIS validation [35]. Since no new datasets are introduced, no quality assurance process is applicable or described."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-257-rubric-3",
    "token_usage": {
      "prompt_tokens": 15165,
      "completion_tokens": 255,
      "total_tokens": 15420
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 (Experimental Setup - Dataset)",
          "reasoning": "The paper uses existing datasets such as Bunny, UVG, and DAVIS validation datasets for evaluation. These datasets are standard benchmarks in video processing and are not newly created by the authors but collected from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-257-rubric-4",
    "token_usage": {
      "prompt_tokens": 15683,
      "completion_tokens": 456,
      "total_tokens": 16139
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new dataset being used exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper focuses on using existing datasets for overfitting and fine-tuning implicit neural representations, not training models from scratch on new datasets introduced by the authors."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any new dataset introduced by the authors being used to fine-tune pre-trained models with supervised learning methods."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any use of reinforcement learning post-training methods or datasets relevant to RL."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets mentioned (Bunny, UVG, DAVIS) are pre-existing benchmarks used for evaluation. No new dataset introduced by the authors is used for evaluation exclusively."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No newly introduced dataset is described as being used primarily for analyzing trends, patterns, or characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any new dataset introduced serves as a knowledge base to augment models."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. It uses several existing datasets (e.g., Bunny, UVG, DAVIS) for evaluation and experiments, but no practical usage of a novel dataset described by the authors is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-257-rubric-5",
    "token_usage": {
      "prompt_tokens": 16406,
      "completion_tokens": 503,
      "total_tokens": 16909
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets introduced or used contain two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets referenced (e.g., Bunny, UVG, DAVIS) are video datasets and do not contain textual English content or linguistic data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention datasets with entries in a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses algorithms, models, and training code, it does not introduce a new dataset composed of code or programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notations and equations are used for method description, but no new dataset entries containing mathematical or logical notation are introduced."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication datasets are introduced in the paper."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are included in any dataset introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly documents the datasets used; no unknown language entries are reported."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The datasets introduced and evaluated (video datasets such as Bunny, UVG, DAVIS validation) consist of video frames without any human or programming languages as dataset entries. They are visual data without language content."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-257-rubric-6",
    "token_usage": {
      "prompt_tokens": 13624,
      "completion_tokens": 158,
      "total_tokens": 13782
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 4 (Experimental Setup)",
          "reasoning": "The paper provides a GitHub link for the code related to the proposed boosting framework and methods. However, it does not introduce any new datasets constructed by the authors themselves; rather, it evaluates on existing datasets such as Bunny, UVG, and DAVIS. Therefore, there is no code for dataset construction provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 4.1 (Experimental Setup)",
          "reasoning": "The paper uses publicly available datasets (Bunny, UVG, DAVIS validation) for evaluation; these datasets are pre-existing and well-known. No new dataset creation process is introduced or documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2573-rubric-0",
    "token_usage": {
      "prompt_tokens": 14267,
      "completion_tokens": 225,
      "total_tokens": 14492
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Prompt Suite",
          "Reasoning": "The paper describes a prompt suite of around 100 human-curated text prompts designed specifically for each of the 16 evaluation dimensions, which are used as test cases for text-to-video model evaluation. These prompts were carefully designed and cleaned by humans (including manual cleaning and use of large language models for labeling). This constitutes a new textual dataset introduced by the authors in this paper."
        },
        {
          "Modality": "video",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.3 Human Preference Annotation and Section 3.2 Prompt Suite, Section 4 Experiments",
          "Reasoning": "The authors generate a large number of videos from multiple video generation models using their prompt suite. These generated videos are used as data for human preference annotation and evaluation benchmarking. These are synthetic videos generated by multiple text-to-video models for benchmarking, thus are model generated and constitute a new video dataset introduced in the paper."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2573-rubric-1",
    "token_usage": {
      "prompt_tokens": 15067,
      "completion_tokens": 291,
      "total_tokens": 15358
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.3",
            "reasoning": "Section 3.3 specifically describes that human preference annotations were collected by multiple human annotators who were instructed and trained to perform pairwise comparisons of generated videos according to each evaluation dimension. The description mentions training human annotators and multiple quality assurance protocols, indicating a group of crowd annotators (non-expert) rather than single or expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3",
            "reasoning": "Section 3.3 describes that annotators were given specific instructions to focus only on the particular evaluation dimension when making preference judgments, and that instructions were carefully prepared and annotators were trained to understand the dimension definitions."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.3",
            "reasoning": "Section 3.3 details that annotators were asked to select a preferred video in pairwise comparisons according to the dimension of interest, effectively providing a rubric or scoring guideline for preference judgments."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figure 4 and Section 3.3",
            "reasoning": "Figure 4 shows the annotation interface with example prompts and choices, illustrating examples presented to annotators. Section 3.3 mentions preparing instructions and training annotators, implying example-based guidance to ensure quality annotations."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2573-rubric-2",
    "token_usage": {
      "prompt_tokens": 16257,
      "completion_tokens": 348,
      "total_tokens": 16605
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert annotator for the dataset or annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention of multiple human experts performing quality assurance. While human annotators were used for preference annotations, the expertise level is not indicated."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of quality assurance conducted by a single human non-expert annotator is found in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.3 Human Preference Annotation",
          "reasoning": "The paper describes a large-scale human preference annotation process where multiple human annotators provide pairwise preferences on generated videos focused on each evaluation dimension. The annotators were trained and given instructions, but there is no indication they are domain experts, suggesting they are non-expert human annotators performing quality assurance through multiple annotations."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the evaluation methods use AI models to judge video quality dimensions, these are evaluation metrics and not described as part of the quality assurance process validating the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification of code or annotations is described as part of the quality assurance of the human preference annotation dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Human preference annotations underwent multiple quality assurance protocols including instruction, training, pre-labeling trials, and post-labeling checks, indicating QA processes were applied and documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2573-rubric-3",
    "token_usage": {
      "prompt_tokens": 15867,
      "completion_tokens": 539,
      "total_tokens": 16406
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.2 Prompt Suite",
          "reasoning": "The paper states that the authors carefully designed prompt suites consisting of around 100 text prompts per evaluation dimension and 800 human-curated prompts across categories, manually cleaned and prepared for the benchmark. This indicates the prompts are original content created entirely from scratch by human contributors to serve as test cases for evaluation."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.3 Human Preference Annotation; Section 4.1 Per-Dimension Evaluation",
          "reasoning": "The paper describes generating videos by running four video generation models (LaVie, ModelScope, VideoCrafter, CogVideo) using the designed prompt suites, resulting in a new dataset of generated videos used for evaluation and human preference annotation. These generated videos are original content created entirely by models without deriving from existing video data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication in the paper of any data being produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any machine translation processes to generate data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Per-Dimension Evaluation - Empirical Max and WebVid-Avg baselines",
          "reasoning": "The benchmark uses retrieved videos from the WebVid-10M dataset matched to prompts as baselines (Empirical Max and WebVid-Avg), which are existing videos collected and aggregated from an existing video dataset without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Empirical Min baseline; Section 3.1 Evaluation Dimension Suite",
          "reasoning": "For Empirical Min baseline, the authors generate specific degraded or modified videos (e.g., concatenated frames from real videos or Gaussian noise clips) derived from existing video sources to approximate minimum achievable scores. Also, some evaluation metrics transform existing features or captions to derive evaluation scores."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation methods are clearly specified and documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2573-rubric-4",
    "token_usage": {
      "prompt_tokens": 16385,
      "completion_tokens": 358,
      "total_tokens": 16743
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 3.1, 3.2, 4.1, 4.2",
          "reasoning": "The paper presents VBench, a comprehensive benchmark suite specifically designed for evaluating video generative models. It introduces 16 fine-grained evaluation dimensions, prompt suites, and evaluation methodologies used to assess the quality and condition consistency of videos generated by different models. The dataset includes human preference annotations to validate alignment with human perception. The datasets (prompts and human annotations) are employed exclusively for performance measurement and benchmarking, as detailed in Sections 3.1 (Evaluation Dimension Suite), 3.2 (Prompt Suite), and 4.1-4.2 (Evaluation results and human alignment validation)."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5",
          "reasoning": "The dataset, including the multi-dimensional evaluation results and annotated videos, is used to analyze trends, strengths, and weaknesses of existing video generation models across various evaluation dimensions and content categories. Section 5 discusses insights and observations drawn from the benchmark analyses, indicating usage of the dataset for analyzing patterns and characteristics rather than for training or evaluation alone."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2573-rubric-5",
    "token_usage": {
      "prompt_tokens": 17108,
      "completion_tokens": 571,
      "total_tokens": 17679
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's dataset, including the prompt suites and human annotations, is described only in the context of English language text prompts. There is no indication or evidence of inclusion of more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or evidence in the paper of the dataset containing exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Sections 3.2 (Prompt Suite), 3.3 (Human Preference Annotation), and throughout the paper",
          "reasoning": "The prompt suite consists of around 100 English text prompts designed for each evaluation dimension and content category. The human preference annotation tasks are performed based on these English prompts. There is no mention of other natural languages, implying the dataset entries are solely in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of English prompts and related annotations only, with no indication of any non-English single-language content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the evaluation methods involve various computational models and metrics, the dataset itself (prompts, annotations, generated videos) does not contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions or symbolic representations are contained within the dataset entries as described."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes video prompts and generated videos but does not contain biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or artificially created languages being part of the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of dataset entries is explicitly English as stated in the prompt suite and annotation descriptions."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains natural language text prompts (English), thus it is not without language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2573-rubric-6",
    "token_usage": {
      "prompt_tokens": 14326,
      "completion_tokens": 223,
      "total_tokens": 14549
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 3, and Conclusion",
          "reasoning": "The paper states multiple times that they will open-source VBench including all prompts, evaluation methods, generated videos, and human preference annotations (Abstract and Conclusion). It explicitly mentions open-sourcing VBench's evaluation dimension suite, evaluation method suite, prompt suite, generated videos, and human preference annotations in Section 3 and Conclusion, indicating that the code related to dataset creation and evaluation will be publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and 3.1 to 3.3",
          "reasoning": "The paper provides a detailed and comprehensive description of the dataset creation process, including the design of the evaluation dimensions (Section 3.1), the prompt suites per dimension and per content category (Section 3.2), and the human preference annotation protocol (Section 3.3). The creation and rationale behind each component of the dataset and annotations are well documented in the main text and supplementary material references, thus fulfilling the documentation criteria."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2593-rubric-0",
    "token_usage": {
      "prompt_tokens": 9908,
      "completion_tokens": 322,
      "total_tokens": 10230
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Datasets",
          "Reasoning": "The paper describes experiments conducted on several large-scale scenes (Rubble, Building, Campus, Residence, Sci-Art) from existing datasets (Mill-19 and UrbanScene3D). No indications suggest these datasets are introduced by the authors; they are pre-existing and used for evaluation, thus no new dataset with image modality was introduced."
        },
        {
          "Modality": "other",
          "Human Generated": false,
          "Model Generated": false,
          "Unknown Origin": true,
          "Reference": "Throughout the paper, especially Section 4 (Method) and 5 (Experiments)",
          "Reasoning": "The authors introduce VastGaussian, a method for large-scale scene reconstruction and rendering based on 3D Gaussian splatting. They use pre-existing datasets of images and point clouds for training and evaluation. The 'dataset' used for training consists of a sparse point cloud and training images with known camera positions derived from Structure-from-Motion applied to those images. The point cloud and camera parameters are generated by SfM algorithms from images. However, the paper does not introduce any new dataset of images or point clouds collected by the authors, nor does it specify introducing a novel dataset capturing new data modalities. The 3D Gaussians and cells used in VastGaussian are algorithmic constructs generated by their method, but these are not datasets per se but internal model representations. Therefore, there is no explicit new data source introducing a new dataset from the authors to be listed."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2593-rubric-1",
    "token_usage": {
      "prompt_tokens": 10708,
      "completion_tokens": 277,
      "total_tokens": 10985
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 and 5.1",
            "reasoning": "The paper describes an automatic progressive data partitioning strategy for large scene reconstruction, which partitions scenes into multiple cells and assigns training views and point clouds to cells using visibility and spatial criteria. This partitioning is done algorithmically (progressive data partitioning with airspace-aware visibility calculation). Further optimization is performed using GPUs in parallel. Thus, the annotation or labeling of data (point clouds and images assigned to cells) is performed via an automatic, deterministic process rather than by human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not mentioned",
            "reasoning": "The paper does not provide any specific instructions or guidelines for human annotators because the data assignment and partitioning is done automatically. There is no mention of instructions given to any annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not mentioned",
            "reasoning": "The paper does not mention any scoring rubrics or evaluation guidelines given for annotation or data labeling, as the partitioning and assignment is algorithmic."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not mentioned",
            "reasoning": "No annotation examples or labeling examples are provided; the process is automatic and based on visibility computations and geometric partitioning rather than human labeling."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2593-rubric-2",
    "token_usage": {
      "prompt_tokens": 11898,
      "completion_tokens": 267,
      "total_tokens": 12165
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by multiple human experts for the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that multiple non-expert annotators performed quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by AI models on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of automated verification or algorithmic quality assurance to validate dataset content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces the UrbanScene3D and Mill-19 datasets but does not describe any quality assurance procedures conducted on these datasets nor mention any validation or curation process. Thus, there is no documented quality assurance process for the new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2593-rubric-3",
    "token_usage": {
      "prompt_tokens": 11508,
      "completion_tokens": 555,
      "total_tokens": 12063
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 5.1 (Datasets), Abstract",
          "reasoning": "The paper uses several large-scale scene datasets including Mill-19 and UrbanScene3D datasets. These datasets contain thousands of high-resolution images taken from various real-world scenes, which are original content collected by human contributors. Although these datasets are pre-existing (cited as [26] and [44]), the paper does not mention introducing any new datasets created by the authors themselves. Therefore, no new data is created entirely from scratch by the authors. However, the paper describes the method of initializing and partitioning these datasets, but this is data processing rather than creation of new data. Hence, this category does not apply for any new dataset created by the authors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any new dataset entirely by AI or machine learning models without referencing existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are existing large-scale scene datasets collected independently by other works and cited appropriately. The paper does not aggregate or collect new data from diverse existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 (Progressive Data Partitioning), Section 5.1 (Datasets)",
          "reasoning": "Though the datasets themselves are not created by the authors, the paper presents a novel partitioning strategy that divides existing large-scene datasets into multiple cells for independent optimization and training. This involves modification and transformation of existing datasets into a partitioned format adapted for their method. The paper also applies preprocessing steps such as downsampling images for training. Thus, the datasets used in experiments are derived from existing data with specific modifications and adaptations to enable the proposed VastGaussian method."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly indicates usage of existing datasets and the modifications applied to them; thus the data source is specified."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2593-rubric-4",
    "token_usage": {
      "prompt_tokens": 12026,
      "completion_tokens": 281,
      "total_tokens": 12307
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1 Experimental Setup, Section 5.2 Result Analysis",
          "reasoning": "The datasets from Mill-19 [44] and UrbanScene3D [26] are used as benchmarks to evaluate the performance of the proposed VastGaussian method in terms of reconstruction quality, rendering speed, and memory consumption. The datasets serve as standard testbeds for comparative analysis rather than for training new models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.2 Result Analysis, Section 5.3 Ablation Study",
          "reasoning": "The datasets are used for analyzing the effects of various components of the model such as data partition strategies, visibility calculation, and appearance modeling. Ablation studies and qualitative comparisons to understand method behavior employ these datasets."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2593-rubric-5",
    "token_usage": {
      "prompt_tokens": 12749,
      "completion_tokens": 515,
      "total_tokens": 13264
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced is image-based from large-scale scenes but there is no mention of multiple human languages represented in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as containing exactly two human languages; only image data is discussed without any textual or language content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of visual data (images and 3D point clouds) without linguistic content in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English language content is mentioned as part of the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While methods and implementations are described, the dataset itself does not contain programming or code-related entries; it contains images and 3D geometry data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of images and 3D point clouds; mathematical formulas in the paper describe methods but are not data entries of the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not involve biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are present in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset's content is clearly described with no ambiguity about language presence; it largely contains images without linguistic content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new dataset consists of large-scale scene images and associated 3D point clouds used for reconstruction and rendering. It does not contain any language data, either textual or spoken."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2593-rubric-6",
    "token_usage": {
      "prompt_tokens": 9967,
      "completion_tokens": 175,
      "total_tokens": 10142
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or link in the paper",
          "reasoning": "The paper describes the method VastGaussian and references a project page (https://vastgaussian.github.io) for the method, but does not mention or provide code related to dataset construction or dataset creation scripts. The code referenced appears to be related to their method, not the construction or collection of datasets. Therefore, there is no explicit code availability for dataset construction."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section in the paper",
          "reasoning": "The paper uses existing large-scale datasets (Mill-19 and UrbanScene3D) for evaluation and does not introduce any new datasets. It does not describe any dataset creation or data collection process for a new dataset. Consequently, there is no documentation about dataset construction in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2623-rubric-0",
    "token_usage": {
      "prompt_tokens": 14364,
      "completion_tokens": 172,
      "total_tokens": 14536
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.3, Pre-training; Section 4, Experiments",
          "Reasoning": "The paper states that VideoMAC is pre-trained on YouTube-VOS 2018 (YT18), a publicly available video dataset. YT18 consists of human-recorded videos, thus the data is a video modality and originates from human-generated sources (human recording). This is the only new dataset introduced by the authors for pre-training their method as explicitly mentioned, distinguishing it from other pre-existing datasets like Kinetics-400 which are used in prior works. The paper specifically highlights pre-training on YT18 for 100 epochs to reduce training time, confirming the dataset as the new introduced data source for VideoMAC."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2623-rubric-1",
    "token_usage": {
      "prompt_tokens": 15164,
      "completion_tokens": 272,
      "total_tokens": 15436
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3 Pre-training",
            "reasoning": "The dataset used for pre-training the model is YouTube-VOS (YT18), which is an existing annotated video dataset used for video object segmentation. The paper does not describe any new manual annotation process on this dataset but uses it as a source data for self-supervised pre-training. Thus, the data annotation related to this dataset was done previously and is not part of the contributions of this paper. Therefore, annotations here are treated as an automatic data source rather than manual annotation by humans or models performed by the authors."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not provide or describe any annotation instructions relevant to the use or creation of the YouTube-VOS dataset. It simply uses the dataset for pre-training, so no new instructions are included."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There is no mention of scoring rubrics or guidelines for annotation quality or evaluation in the paper regarding this dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not provide annotation examples related to the dataset; it uses the dataset as is for pre-training."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2623-rubric-2",
    "token_usage": {
      "prompt_tokens": 16354,
      "completion_tokens": 294,
      "total_tokens": 16648
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert on dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts performed quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no details about quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model was used to perform quality assurance on any dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses automated methods for training, it does not describe any automated quality assurance processes for the datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses pre-existing datasets such as YouTube-VOS (YT18), DAVIS 2017, and JHMDB for training and downstream evaluation. It does not introduce new datasets, nor does it describe any quality assurance processes applied to dataset annotations or content. Therefore, no quality assurance process is documented for new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2623-rubric-3",
    "token_usage": {
      "prompt_tokens": 15964,
      "completion_tokens": 437,
      "total_tokens": 16401
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe creation of new datasets from scratch by humans. The datasets used are existing public datasets for pre-training and downstream evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper focuses on training self-supervised models using existing video datasets. It does not report generation of novel data samples produced entirely by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of data being human-translated from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of data being machine-translated from other languages."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.3 Implementation; Section 4.1 Comparison with Previous methods",
          "reasoning": "The datasets used for pre-training and evaluation, such as YouTube-VOS (YT18), DAVIS17, VIP, JHMDB, Kinetics-400, and ImageNet1K, are public existing datasets. The authors use these datasets as input data for training and validation without creating new datasets. Thus, the data is collated from existing sources without significant modifications or new data collection."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the authors apply masking strategies and transformations during training, these are part of model input processing but do not constitute creation of derived datasets with significant modifications or adaptations as new data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the datasets used for pre-training and evaluation; thus, the data origin is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2623-rubric-4",
    "token_usage": {
      "prompt_tokens": 16482,
      "completion_tokens": 458,
      "total_tokens": 16940
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.3 Pre-training",
          "reasoning": "The paper describes pre-training the VideoMAC model on the YouTube-VOS [68] dataset using their proposed masked video autoencoder approach, indicating that the dataset is used exclusively for pre-training in a self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention training models from randomly initialized parameters using the dataset; instead, it focuses on pre-training with self-supervised masked video modeling."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses fine-tuning models on downstream tasks, the dataset introduced by the authors is used only for pre-training rather than supervised fine-tuning. Downstream fine-tuning uses different datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning post-training methods such as RLHF using the dataset in the paper."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use the proposed dataset solely for evaluation or benchmarking; instead, it is primarily used for pre-training."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used primarily for analysis of trends, patterns, or characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for retrieval-augmented generation or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes practical usage of the dataset for pre-training, so N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2623-rubric-5",
    "token_usage": {
      "prompt_tokens": 17205,
      "completion_tokens": 529,
      "total_tokens": 17734
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Entire paper, especially Abstract, Introduction, and Experiments sections",
          "reasoning": "The paper exclusively uses English for all textual content including dataset descriptions, experimental evaluations, and explanations. The datasets used (YouTube-VOS, DAVIS17, VIP, JHMDB, ImageNet1K) are standard video and image datasets and are not described as containing multiple human languages. The paper's textual content is entirely in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any datasets containing programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes mathematical expressions and formulas for loss functions and other model components, these do not constitute a dataset of mathematical or logical notation. The datasets described are video/image datasets without symbolic or formal logic data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or usage of biological sequences or non-human communication datasets is made in the paper."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any fictional or artificially constructed languages in its datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of the datasets (video/image data) are not specified as unknown; textual content is in English."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The datasets primarily consist of visual data (video frames and images) without intrinsic language content. The paper focuses on visual representation learning using video and image datasets, which do not contain language data in the entries themselves."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2623-rubric-6",
    "token_usage": {
      "prompt_tokens": 14423,
      "completion_tokens": 202,
      "total_tokens": 14625
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Page 1 (Abstract) and Implementation Section 3.3",
          "reasoning": "The paper provides a GitHub link at the beginning (https://github.com/NUST-Machine-Intelligence-Laboratory/VideoMAC) indicating that the code related to the work is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 Approach (including 3.2 Video Masked Autoencoder with ConvNets and 3.3 Implementation) and Section 4 Experiments",
          "reasoning": "The paper thoroughly documents the dataset usage and creation process, including pre-training on YouTube-VOS 2018 (YT18) dataset, details about the masking strategy, frame selection, and training procedures. It explains the choice of datasets for pre-training, discusses frame masking ratios, and presents evaluation on downstream tasks referencing standard datasets, thus providing sufficient documentation of the data-related methods used in the work."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2629-rubric-0",
    "token_usage": {
      "prompt_tokens": 11768,
      "completion_tokens": 187,
      "total_tokens": 11955
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.4 Dataset",
          "Reasoning": "The authors introduce a new dataset by training on the large-scale CO3Dv2 dataset, which consists of posed multi-view images of real-world objects. These images are captured with human involvement, i.e., real-world photos with known camera poses. The dataset categories and their multiview images are explicitly mentioned as real-world multi-view images used for training the model."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.4 Dataset",
          "Reasoning": "Text captions for the CO3Dv2 images are generated by the BLIP-2 model, which is an AI model generating text automatically from images. Therefore, the text captions are model generated, not human authored."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2629-rubric-1",
    "token_usage": {
      "prompt_tokens": 12568,
      "completion_tokens": 271,
      "total_tokens": 12839
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.4 Dataset",
            "reasoning": "The dataset used is CO3Dv2 [31], which is a publicly available large-scale multi-view real-world dataset, and captions are generated automatically with the BLIP-2 model [20]. There is no indication in the paper that humans manually annotated or curated this dataset specifically for this work; rather, the dataset is used as-is, with automatic captioning applied."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.4 Dataset",
            "reasoning": "The paper does not provide or mention any manual annotation instructions given to human annotators for the CO3Dv2 dataset used; captions are generated automatically, and the dataset is a pre-existing real-world dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.4 Dataset",
            "reasoning": "There is no mention or description of any scoring rubrics or evaluation guidelines used for annotations in the dataset; the use of annotations is automatic via caption generation and dataset usage."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.4 Dataset",
            "reasoning": "No examples of manual annotation are provided, nor are annotation guideline examples discussed, since captions are automatically generated and the dataset is pre-existing."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2629-rubric-2",
    "token_usage": {
      "prompt_tokens": 13758,
      "completion_tokens": 352,
      "total_tokens": 14110
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human expert performing quality assurance on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple human experts conducting QA on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any single human non-expert validating or assuring data quality."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of multiple non-expert human annotators performing QA."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No AI-based model is mentioned as performing quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic validation process specifically for quality assurance of the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The dataset used for training (CO3Dv2) is a pre-existing multi-view dataset. The paper does not describe any quality assurance process applied by the authors on this dataset. There is no mention of additional annotation, validation, or QA steps performed or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2629-rubric-3",
    "token_usage": {
      "prompt_tokens": 13368,
      "completion_tokens": 428,
      "total_tokens": 13796
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any new datasets created entirely from scratch by human contributors. The datasets used are existing real-world datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets generated purely by AI or machine learning models without reference to existing data. The focus is on using pretrained models to generate images, but these are outputs of the method, not new datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication the dataset was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication the dataset was generated by machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.4 (Dataset)",
          "reasoning": "The authors use the CO3Dv2 dataset, which is a large-scale real-world multi-view image dataset. Since CO3Dv2 is a publicly existing dataset collected by prior works, their use constitutes the collation or aggregation of existing sources without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.4 (Dataset)",
          "reasoning": "The authors generate text captions for the images using the BLIP-2 model, which adds a layer of derived data (captions) based on the existing CO3D images. This shows some transformation or adaptation of the original dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources are specified and documented, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2629-rubric-4",
    "token_usage": {
      "prompt_tokens": 13886,
      "completion_tokens": 231,
      "total_tokens": 14117
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.4 Implementation Details, Dataset and Training paragraphs",
          "reasoning": "The CO3Dv2 dataset is used to fine-tune a pretrained latent-diffusion text-to-image model's U-Net component, with the VAE encoder and decoder frozen. This fine-tuning is supervised using the diffusion denoising objective on multi-view images from CO3Dv2, enabling the model to generate 3D-consistent images."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2629-rubric-5",
    "token_usage": {
      "prompt_tokens": 14609,
      "completion_tokens": 415,
      "total_tokens": 15024
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.4 Dataset, Abstract",
          "reasoning": "The dataset used for training is CO3Dv2, composed of multi-view images of objects, for which text captions are generated using the BLIP-2 model as stated in Section 3.4. These captions serve as textual input for the model training. Since BLIP-2 is an English-language captioning model and the paper does not mention captions in any other language, the dataset's linguistic content is effectively monolingual English text accompanying the image data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes images with English captions; thus, it contains language and is not language-free."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2629-rubric-6",
    "token_usage": {
      "prompt_tokens": 11827,
      "completion_tokens": 155,
      "total_tokens": 11982
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper",
          "reasoning": "The paper does not provide any link or indication that the code related to dataset construction is publicly available. The paper primarily focuses on the method and training on existing datasets rather than creating a new dataset or providing code for dataset creation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No explicit mention in the paper",
          "reasoning": "The paper uses pre-existing datasets (e.g., CO3Dv2) and does not introduce or document a new dataset creation process within the text. There is no description of how a new dataset was constructed, only that the method was trained on CO3Dv2. Therefore, no dataset creation documentation is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2648-rubric-0",
    "token_usage": {
      "prompt_tokens": 12719,
      "completion_tokens": 98,
      "total_tokens": 12817
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Pre-training datasets",
          "Reasoning": "The paper explicitly states that the pre-training is performed on three public CT scan datasets: BTCV, TCIA Covid19, and LUNA, totaling about 1.6k CT scans. These datasets are pre-existing public datasets and not introduced by the authors, thus not considered here."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2648-rubric-1",
    "token_usage": {
      "prompt_tokens": 13519,
      "completion_tokens": 221,
      "total_tokens": 13740
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2, Section 3.3",
            "reasoning": "The annotation process is based on automatically cropping base volumes and random sub-volumes from 3D medical images and generating position labels by calculating overlap proportions between these crops, all performed by algorithmic procedures without human involvement."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described in the paper",
            "reasoning": "The paper does not mention any detailed annotation instructions provided to annotators because the labeling process is automated and does not involve human annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described in the paper",
            "reasoning": "Since the label generation is automated and based on spatial overlap proportions, no scoring rubrics or evaluation criteria for human annotators are provided or necessary."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described in the paper",
            "reasoning": "There are no example annotations provided because the labeling is performed by deterministic overlap calculation between crops without human annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2648-rubric-2",
    "token_usage": {
      "prompt_tokens": 14709,
      "completion_tokens": 295,
      "total_tokens": 15004
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any evidence or description that quality assurance (QA) of dataset annotations or content was performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that multiple human experts performed quality assurance for the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that a single human non-expert conducted quality assurance of the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple non-expert human annotators conducted quality assurance of the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of AI models to perform quality assurance or judging the dataset annotation quality."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic/rule-based quality assurance processes performed on dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses publicly available datasets for pre-training and downstream tasks without any description of quality assurance processes applied to dataset annotation or validation. No QA steps or procedures are documented, thus quality assurance is considered as not applied or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2648-rubric-3",
    "token_usage": {
      "prompt_tokens": 14319,
      "completion_tokens": 416,
      "total_tokens": 14735
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets that are created entirely from scratch by human contributors. Instead, it utilizes existing public medical imaging datasets for pre-training and downstream evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any new datasets are generated entirely by AI or machine learning models without reference to pre-existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention translation of data from another language by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation of data from other languages is reported or used."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets - Pre-training datasets",
          "reasoning": "The pre-training data consists of publicly available datasets BTCV, TCIA Covid19, and LUNA, which are existing sources aggregated together for pre-training. The paper states these are used as is for pre-training, indicating collating existing datasets without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the SSL methods operate on the data with various cropping and transformations for training, these do not constitute newly derived datasets released by the authors. The paper does not state creation of any new datasets derived from existing ones."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the datasets used for pre-training and downstream tasks. Therefore, the data origin is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2648-rubric-4",
    "token_usage": {
      "prompt_tokens": 14837,
      "completion_tokens": 601,
      "total_tokens": 15438
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 4.1 (Datasets) - Pre-training datasets; Section 3.1 (Overall Framework); throughout experimental sections",
          "reasoning": "The paper describes using three public datasets (BTCV, TCIA Covid19, and LUNA) as pre-training datasets in an unsupervised self-supervised learning framework VoCo. The method leverages contextual position priors for volume contrastive learning pre-training, which is done prior to downstream tasks. The emphasis on self-supervised pre-training using these datasets is a core contribution."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.3 (Experiments on downstream tasks); Table 1, 2, 5",
          "reasoning": "The paper reports results of training models from scratch (randomly initialized) as baselines for comparison against models pretrained on their datasets. For example, in Table 1, 'From Scratch' rows show supervised baseline performance, meaning these datasets are also used for supervised training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.3 (Experiments on downstream tasks)",
          "reasoning": "The pre-trained models are fine-tuned on downstream datasets for evaluation of segmentation and classification tasks. The use of pre-trained models followed by supervised fine-tuning is explicitly described and evaluated."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of the introduced datasets with reinforcement learning post-training techniques such as RLHF or similar methods."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The introduced datasets are primarily used for pre-training and fine-tuning. No dataset is introduced solely for evaluation or benchmarking. All evaluation datasets mentioned are pre-existing public datasets."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are not primarily used for analysis of trends or characteristics. The paper focuses on learning representations and downstream task performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not described or used as knowledge bases to augment models, such as retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates practical usage of the proposed datasets, mainly for pre-training and fine-tuning, hence 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2648-rubric-5",
    "token_usage": {
      "prompt_tokens": 15560,
      "completion_tokens": 487,
      "total_tokens": 16047
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset entries containing more than two human languages. The datasets used are medical imaging datasets without language annotations or content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains exactly two human languages; the data are medical images, not language corpora."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 (Datasets) and Abstract",
          "reasoning": "The datasets are standard 3D medical imaging datasets with English-language documentation and annotations (implied by the paper being in English, datasets named in English, and references), and no other languages are indicated. The dataset entries themselves (images and labels) do not contain multilingual content, and any metadata or labels are in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the use of a dataset with a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although code is mentioned in the form of pre-training code and framework, the dataset itself does not contain code or programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain mathematical or logical notation; such notation only appears in the paper's description and methods sections."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the datasets are medical images of human anatomy, the dataset entries themselves do not contain biological sequences or non-human communication data like DNA sequences or animal signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate inclusion of any fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset is documented and known to be English for annotations and metadata."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The primary dataset consists of 3D CT or MRI medical images. These images do not contain any natural language text within their entries (the images themselves are volumetric pixel data). Therefore, the dataset entries primarily do not contain any language content, only imaging data."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2648-rubric-6",
    "token_usage": {
      "prompt_tokens": 12778,
      "completion_tokens": 195,
      "total_tokens": 12973
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 4.1 Pre-training datasets",
          "reasoning": "The paper states that code for the proposed method 'will be available' on GitHub, but it does not mention any code related to dataset construction, collection, or preprocessing being publicly released or linked. The pre-training datasets used are existing public datasets (BTCV, TCIA Covid19, LUNA), and no new datasets or corresponding code for dataset creation are introduced by the authors."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Pre-training datasets",
          "reasoning": "The paper clearly documents the datasets used for pre-training and downstream tasks, specifying their sources, sizes, and how they are used in experiments. These datasets are publicly available and well-cited. However, the paper does not introduce any new dataset constructed by the authors, so no dataset creation process beyond citing existing datasets is described."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2659-rubric-0",
    "token_usage": {
      "prompt_tokens": 12447,
      "completion_tokens": 122,
      "total_tokens": 12569
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 (Setup), Datasets",
          "Reasoning": "The paper explicitly states using the UIEBD and LSUI datasets for training and evaluation, but these are pre-existing datasets. The paper does not introduce any new dataset. Thus, there is no new dataset introduced by the authors. The input data to train and evaluate the proposed method, WF-Diff, are human-captured underwater images with corresponding ground-truth images, indicating human-generated image data."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2659-rubric-1",
    "token_usage": {
      "prompt_tokens": 13247,
      "completion_tokens": 214,
      "total_tokens": 13461
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 (Datasets)",
            "reasoning": "The paper states that for training and evaluation, the authors utilize two existing datasets: UIEBD and LSUI, but does not mention introducing any new dataset. These datasets are pre-existing benchmark datasets commonly used in the UIE task. There is no indication that any new dataset was introduced or annotated by humans or models in this work."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "The paper does not mention any new dataset annotation procedure or instructions provided to annotators since no new dataset was introduced."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "Since no new dataset was introduced and annotated, rubrics for annotation were not provided."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "No examples for data annotation are discussed or included for any new dataset, as no new dataset was introduced."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2659-rubric-2",
    "token_usage": {
      "prompt_tokens": 14437,
      "completion_tokens": 297,
      "total_tokens": 14734
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on any new dataset introduced by the authors."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by multiple human experts on any new dataset; no such information or procedures are provided."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about quality assurance done by a single non-expert annotator for the datasets mentioned."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance conducted by multiple non-expert annotators for the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper involves AI models for image enhancement, it does not describe using AI models for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automatic process or algorithmic verification used as quality assurance for dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses existing datasets (UIEBD, LSUI, and U45) for training and evaluation but does not introduce any new datasets. Thus, no quality assurance process is described or needed for new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2659-rubric-3",
    "token_usage": {
      "prompt_tokens": 14047,
      "completion_tokens": 469,
      "total_tokens": 14516
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any newly created dataset by the authors. Instead, they utilize existing datasets such as UIEBD [28], LSUI [18], and U45 [20]. Hence, no original data created entirely from scratch by human contributors is introduced."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the authors generated a completely new dataset by AI or machine learning models without source data. Their work focuses on underwater image enhancement methods rather than generating new data from models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss translating any data from other languages using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or evidence of machine translation used to generate any dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The datasets used for training and evaluation (UIEBD [28], LSUI [18], and U45 [20]) are existing real-world underwater image datasets. The authors utilize these pre-existing datasets without describing the creation of new datasets; thus, the data can be considered as collated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit description of modifications, transformations, or adaptations applied by the authors to existing datasets to produce a new derived dataset. The paper focuses on method development rather than creating or adapting datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the sources of the datasets used (UIEBD, LSUI, U45), so the origin of data is specified. Hence, N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2659-rubric-4",
    "token_usage": {
      "prompt_tokens": 14565,
      "completion_tokens": 314,
      "total_tokens": 14879
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 (Setup)",
          "reasoning": "The authors train their proposed WF-Diff model from scratch on underwater image datasets UIEBD and LSUI, as stated in Section 4.1. This training uses paired training data to learn the mapping from degraded images to enhanced images, indicating training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 (Results and Comparisons)",
          "reasoning": "The paper uses existing datasets UIEBD, LSUI, and U45 for evaluation and benchmarking the performance of the proposed WF-Diff method in Section 4.2. However, no new dataset is introduced specifically for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. It only uses existing public underwater image datasets (UIEBD, LSUI, U45) for training and evaluation. Therefore, no new datasets with documented utility are described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2659-rubric-5",
    "token_usage": {
      "prompt_tokens": 15288,
      "completion_tokens": 510,
      "total_tokens": 15798
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Setup - Datasets",
          "reasoning": "The paper uses real-world underwater image datasets UIEBD and LSUI for training and testing, which are image datasets and the paper is written entirely in English. No mention is made of linguistic or multilingual content within the dataset. The paper text and dataset descriptions are in English, implying the dataset entries are monolingual English in context of description and metadata."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is image-based underwater images and their enhanced versions, there is no indication the dataset contains code or programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains many mathematical expressions explaining their methods, there is no new dataset introduced containing mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced consists of images and does not include any biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any constructed or artificial languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is specified (image datasets associated with English descriptions), so it is not unknown."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The dataset consists of images of underwater scenes and their enhanced versions. Images do not contain language per se, so no human language is contained in the dataset entries themselves."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2659-rubric-6",
    "token_usage": {
      "prompt_tokens": 12506,
      "completion_tokens": 185,
      "total_tokens": 12691
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section mentions code availability for dataset construction.",
          "reasoning": "The paper states that code is available for their method at https://github.com/zhihefang/WF-Diff; however, this repository refers to code for the proposed image enhancement method, not for dataset creation or construction. The datasets used (UIEBD, LSUI, U45) are pre-existing real-world underwater image datasets obtained from other sources, and no code or scripts for their construction are provided in this paper."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section describes dataset creation process in the paper.",
          "reasoning": "The paper uses publicly available datasets (UIEBD, LSUI, U45) and does not introduce any new datasets, nor does it document any dataset creation or collection process. Hence, no documentation about dataset construction is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2672-rubric-0",
    "token_usage": {
      "prompt_tokens": 13393,
      "completion_tokens": 130,
      "total_tokens": 13523
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5 Results (Datasets) and Abstract",
          "Reasoning": "The paper benchmarks their method on FFHQ and AFHQv2 Cats datasets at 512\u00d7512 resolution, which are collections of human and animal face images captured from real-world data, i.e., human-generated images. These datasets are standard benchmarks in the literature but are not introduced as new datasets by the authors in this paper, thus not qualifying. No new datasets are introduced by the authors in the paper, only existing datasets are used."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2672-rubric-1",
    "token_usage": {
      "prompt_tokens": 14193,
      "completion_tokens": 206,
      "total_tokens": 14399
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 5, Datasets",
            "reasoning": "The paper uses publicly available datasets FFHQ and AFHQv2 Cats for training and evaluation; it does not introduce any new dataset. The training data consists of existing image collections, and annotations in terms of 3D geometry or labels are not manually performed but inferred automatically. No novel data collection or manual annotation process for a new dataset is discussed."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No new dataset with annotation guidelines or instructions is introduced or described in the paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No new dataset means no scoring rubrics or evaluation rubrics related to dataset annotation are provided."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples or labeling examples are presented, as no new dataset with annotations is introduced."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2672-rubric-2",
    "token_usage": {
      "prompt_tokens": 15383,
      "completion_tokens": 259,
      "total_tokens": 15642
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no evidence of quality assurance by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model used for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification or algorithmic quality assurance applied to the dataset content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses standard pre-existing datasets (FFHQ and AFHQ) and does not introduce any new dataset. Therefore, no quality assurance process related to new dataset annotation or content validation is described or applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2672-rubric-3",
    "token_usage": {
      "prompt_tokens": 14993,
      "completion_tokens": 432,
      "total_tokens": 15425
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe creation of any new dataset by human collection from scratch. The authors use existing datasets (FFHQ and AFHQ) for their experiments."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the method generates images and 3D shapes, these are outputs of the proposed model rather than new datasets introduced by the authors. The paper does not present a novel dataset generated entirely by AI or ML models as a benchmark or training corpus."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of datasets generated by human translations from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No parts of the datasets or data are produced by machine translation according to the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used (FFHQ, AFHQ) are pre-existing standard datasets and are not newly collated by the authors. The paper does not mention any new collections of data aggregated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes a method and results based on existing datasets but does not introduce any derived datasets (e.g., transformed, augmented versions) as new datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. The data used for training and evaluation are publicly available datasets (FFHQ, AFHQ), with no indication of any newly created dataset. Therefore, the origin of new data is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2672-rubric-4",
    "token_usage": {
      "prompt_tokens": 15511,
      "completion_tokens": 429,
      "total_tokens": 15940
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe use of any newly introduced dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing datasets FFHQ and AFHQv2 Cats for training their 3D GAN model from scratch, these are not newly introduced datasets by the authors."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of fine-tuning pre-trained models with any new dataset introduced by the authors."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not reference any reinforcement learning based post-training techniques using datasets introduced in the paper."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset introduced by the authors is used exclusively for evaluation or benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset newly introduced by the paper is used primarily for analyzing trends or characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use any newly created dataset as a knowledge base for retrieval or model augmentation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. It uses existing datasets FFHQ and AFHQv2 Cats for training and evaluation. Therefore, no practical usage of any newly proposed dataset is described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2672-rubric-5",
    "token_usage": {
      "prompt_tokens": 16234,
      "completion_tokens": 472,
      "total_tokens": 16706
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets mentioned contain images of faces and cats, taken from FFHQ and AFHQv2 Cats, which are collections of 2D images and do not include multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used or generated are image datasets without any linguistic content in two languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5 (Results), Datasets subsection",
          "reasoning": "The paper uses standard datasets such as FFHQ and AFHQv2 Cats, which consist of images with English annotations or metadata. The paper text and descriptions are in English, and no other language data content is indicated for the datasets. Therefore, the dataset content is effectively monolingual English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets exclusively containing non-English language data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the method and paper describe code and algorithms, there is no proposed dataset that contains code or programming language data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains mathematical formulas and descriptions as part of its method and background, but does not propose a dataset containing mathematical or logical notation as content entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are composed of 2D image collections of humans and cats, with no biological sequence or non-human communication data entries."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets containing constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages involved in the dataset are known (English), as per dataset descriptions and usage."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The proposed datasets themselves do not contain linguistic content as dataset entries; they consist of images and associated camera parameters and do not contain natural language content per se. Thus, linguistic classification is not applicable to the dataset entries themselves, only to the annotations which are in English."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2672-rubric-6",
    "token_usage": {
      "prompt_tokens": 13452,
      "completion_tokens": 174,
      "total_tokens": 13626
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper",
          "reasoning": "The provided excerpt from the paper does not contain any explicit reference, link, or statement about the availability of the dataset construction code or any related code repositories. There is no indication in the abstract, methods, results, discussion, or appendix that the code used for dataset construction or preprocessing is made publicly available."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No dataset creation documentation found in the paper",
          "reasoning": "The paper uses standard existing datasets FFHQ and AFHQ for benchmarks and no new datasets are introduced by the authors. Therefore, there is no description or documentation regarding dataset creation or construction in the paper. The focus is instead on methodology and model training from existing datasets, with usage described but no new dataset construction process documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2693-rubric-0",
    "token_usage": {
      "prompt_tokens": 16446,
      "completion_tokens": 458,
      "total_tokens": 16904
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4, 5, and Evaluation Setup (Section 5)",
          "Reasoning": "The paper introduces a novel compositional image retrieval framework which is trained on paired sketch-photo datasets such as Sketchy [58], ShoeV2, and ChairV2. These datasets contain photos (images) that are human-generated natural images captured by cameras. The new dataset introduced implicitly is the possibly augmented combined dataset for training their model on fine-grained sketch-photo pairs (e.g., ShoeV2 and ChairV2 datasets and their splits used in the paper). These photos are human-generated, not model-generated, and not from an unknown or web-crawled source as per typical image datasets used in sketch-image retrieval research."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4, 5, Evaluation Setup (Section 5), and Abstract",
          "Reasoning": "The authors train their SBCIR framework on sketches paired with photos. Sketches are freehand human-drawn sketches collected from humans (Section 1, Abstract, and related works mention freehand sketches). They use human-generated sketch datasets (e.g., Sketchy, ShoeV2, ChairV2), thus the modality 'image' (sketch images) is considered human generated. These sketches are part of the new training setup introduced in the paper, enabling fine-grained compositional retrieval, implying their human origin. The paper does not mention generation or simulation of sketches by any model."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4, Training Setup, Evaluation Setup, and Abstract",
          "Reasoning": "The paper explicitly states that the framework is trained without requiring annotated fine-grained textual descriptions. During training, the authors generate short phrases as 'neutral text' using a lightweight GPT model (Section 4). This neutral text is used as a regularizer to preserve grammaticality and maintain compositionality constraints. Since the text is generated by a model and not human annotated or sourced, the text modality data introduced is model generated."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2693-rubric-1",
    "token_usage": {
      "prompt_tokens": 17246,
      "completion_tokens": 221,
      "total_tokens": 17467
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 5 Evaluation Setup and related methodological descriptions",
            "reasoning": "The paper mentions manual collection of fine-grained captions for test sets of ShoeV2, ChairV2, and Sketchy datasets, indicating human experts crafted annotations specifically for the new datasets. No indication of crowdsourcing or non-expert involvement is stated."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention found in the paper",
            "reasoning": "The paper does not provide or discuss detailed annotation instructions for annotators involved in caption collection or annotation tasks."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention found in the paper",
            "reasoning": "There is no discussion or presentation of scoring rubrics or guidelines for annotators about how to rate or judge annotations in the new dataset creation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention found in the paper",
            "reasoning": "The paper does not provide example annotations or examples as part of annotation guidelines for the new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-2693-rubric-2",
    "token_usage": {
      "prompt_tokens": 18436,
      "completion_tokens": 320,
      "total_tokens": 18756
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about quality assurance being conducted by a single human expert for any dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process involving multiple human experts for the datasets used or introduced."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance conducted by a single non-expert human annotator for dataset annotations or content."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance performed by multiple non-expert human annotators in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any AI model was used as a judge for quality assurance of introduced datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or rule-based algorithmic quality assurance process for dataset annotations is reported in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper references existing datasets such as ShoeV2, ChairV2, Sketchy, FS-COCO, SketchyCOCO, and ImageNet-R, but does not introduce a new dataset. For the datasets discussed, the paper does not describe any quality assurance process for dataset annotations or content. Therefore, no quality assurance process is documented for any new dataset introduced by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2693-rubric-3",
    "token_usage": {
      "prompt_tokens": 18046,
      "completion_tokens": 645,
      "total_tokens": 18691
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 5 Evaluation Setup and Results; Section 4 Sketch-Based Composed Image Retrieval",
          "reasoning": "The paper introduces fine-grained textual captions manually collected for test set images of ShoeV2, ChairV2, and Sketchy. These captions are original textual descriptions created by human annotators to enable evaluation of fine-grained composed queries of sketch and text. The paper clearly states manual collection of these fine-grained captions (Section 5 Evaluation Setup) for testing purposes, indicating new human-created data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the method involves generating pseudo-word tokens and uses GPT to create neutral text regularisers, these are intermediate representations or auxiliary data generated by models rather than datasets introduced by the authors. The paper does not present a new dataset that is generated entirely by AI models as primary evaluation data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data created by translating from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation or translation process on data is discussed or indicated as part of dataset creation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5 Evaluation Setup; Section 2 Related Works",
          "reasoning": "The datasets used in evaluation primarily rely on existing datasets like ShoesV2, ChairV2, Sketchy, FS-COCO, and SketchyCOCO, which are publicly available. The paper mentions collecting fine-grained captions for test set images from these datasets, implying that the base image and sketch data are collated from existing sources. The authors build upon these datasets by manual annotation but the underlying image and sketch data are collated from existing datasets."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 5.1 Performance Analysis; Section 4 Sketch-Based Composed Image Retrieval",
          "reasoning": "The authors derive pseudo-word tokens representing sketches by converting their visual features via a learned visual-to-word converter network trained on existing sketch-photo pairs. Similarly, neutral text regularisers are generated via GPT, which are model outputs but used as derived data to regularize compositionality. Hence, some of the data used for training (e.g., these pseudo-word tokens and neutral texts) are derived from existing data or existing models with transformations applied. This shows data derivation involving adaptations and transformations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin of data, referencing existing datasets for sketches, photos, and newly manually collected fine-grained captions for test images, thus the data sources are specified."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2693-rubric-4",
    "token_usage": {
      "prompt_tokens": 18564,
      "completion_tokens": 350,
      "total_tokens": 18914
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4: Sketch-Based Composed Image Retrieval; Section 5.1: Performance Analysis",
          "reasoning": "The authors train the visual-to-word converter network and components of their model on datasets containing paired sketches and photos, using triplet loss and reconstruction loss, to fine-tune the model for effective fine-grained sketch+text composed image retrieval. This supervised fine-tuning is essential to learn the compositional embedding without requiring paired text descriptions."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1: Performance Analysis; Section 5.2: Downstream Tasks",
          "reasoning": "The authors use new fine-grained composed queries created from available datasets for evaluation, benchmarking the retrieval performance of their method against baselines, including accuracy metrics (Acc.@k) and FID scores for generation, thus applying the datasets for evaluation and performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.3: Ablation on Design",
          "reasoning": "The paper analyzes the role and impact of different components, losses, and the pseudo-word token representation on performance, which involves analysis of the dataset utility to understand model behavior and effectiveness."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2693-rubric-5",
    "token_usage": {
      "prompt_tokens": 19287,
      "completion_tokens": 547,
      "total_tokens": 19834
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention or introduce any dataset containing entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or statement in the paper that the proposed dataset contains exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5.1, Evaluation Setup, and throughout the paper",
          "reasoning": "The paper mentions collecting fine-grained English textual captions for test images on ShoeV2, ChairV2, and Sketchy datasets. All textual data described and used in queries and evaluation are in English. No other languages are indicated as part of the new dataset(s) introduced."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only refers to English textual content for captions and does not mention any non-English language dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses code/network architectures and loss functions, it does not present any dataset entries containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the paper uses mathematical notation to explain methods (e.g., compositionality relationships), these are annotations for method explanation and not content of a dataset introduced."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of biological sequences or non-human communication data in any dataset introduced by the authors."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificially created languages are included in the datasets introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages present in the datasets introduced are explicitly stated as English; there is no ambiguity or unspecified language."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced include English textual captions; hence, they contain language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-2693-rubric-6",
    "token_usage": {
      "prompt_tokens": 16505,
      "completion_tokens": 186,
      "total_tokens": 16691
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or link in the paper",
          "reasoning": "The paper does not mention publicly releasing the code or provide any links to code repositories related to dataset construction or processing. The authors describe using existing datasets (e.g., ShoeV2, ChairV2, FS-COCO, Sketchy, SketchyCOCO) and manually collected fine-grained captions but do not specify any code availability for dataset creation or annotation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No dedicated section or detailed description",
          "reasoning": "The paper uses several existing datasets and mentions manual collection of fine-grained captions for test images of certain datasets, but it provides no detailed documentation or process description of how these new caption datasets or any new dataset were constructed. There is no methodological explanation or procedural detail about dataset creation, collection criteria, or annotation protocols."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-293-rubric-0",
    "token_usage": {
      "prompt_tokens": 11459,
      "completion_tokens": 133,
      "total_tokens": 11592
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets and Evaluation",
          "Reasoning": "The new dataset introduced by the authors is the COCO-Stuff dataset used for training, which contains 118k densely annotated training images. The dataset is described as having images with annotated categories, and since COCO-Stuff is a publicly available dataset collected via human processes (images collected and annotated by humans), it is human-generated image data. The paper states that they train their model on COCO-Stuff dataset, indicating a new dataset introduced for training."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-293-rubric-1",
    "token_usage": {
      "prompt_tokens": 12259,
      "completion_tokens": 216,
      "total_tokens": 12475
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Datasets and Evaluation",
            "reasoning": "The paper uses the COCO-Stuff dataset for training, which is a publicly available dataset with human annotations performed prior to this work. The authors do not mention introducing any new dataset in the paper; rather, they train and evaluate on existing datasets such as COCO-Stuff, ADE20K, PASCAL VOC, PASCAL-Context, and MESS. The paper does not indicate any novel dataset annotation collection."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "Since the paper does not introduce a new dataset annotation, no annotation instructions are provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation rubrics are described or provided associated with any new dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples are described or provided associated with any new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-293-rubric-2",
    "token_usage": {
      "prompt_tokens": 13449,
      "completion_tokens": 312,
      "total_tokens": 13761
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for dataset annotation or validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by multiple human experts for dataset annotation or validation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human non-expert for dataset annotation or validation."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by multiple human non-experts for dataset annotation or validation."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA process performed by an AI model as a judge for dataset validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any dataset quality assurance process based on automated verification or algorithmic/rule-based techniques."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses existing datasets such as COCO-Stuff, ADE20K, PASCAL VOC, and PASCAL Context for training and evaluation; it does not introduce any new datasets nor document any quality assurance process for annotation or dataset content. Therefore, no QA process is applied or described for new datasets, as none are introduced by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-293-rubric-3",
    "token_usage": {
      "prompt_tokens": 13059,
      "completion_tokens": 313,
      "total_tokens": 13372
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The paper states that the model is trained on the COCO-Stuff dataset (Section 4.1), which is a pre-existing dataset, and evaluated on standard benchmarks such as ADE20K, PASCAL VOC, and PASCAL-Context, as well as a multi-domain MESS benchmark. The authors did not introduce any new dataset but used existing datasets without significant modification, indicating the data is collated from existing sources for training and evaluation."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources are specified clearly as existing public datasets used for training and evaluation."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-293-rubric-4",
    "token_usage": {
      "prompt_tokens": 13577,
      "completion_tokens": 360,
      "total_tokens": 13937
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 4.2",
          "reasoning": "The authors train their model on COCO-Stuff dataset and fine-tune the encoders of CLIP in a supervised manner for open-vocabulary semantic segmentation, as detailed in Section 4.1 (Datasets and Evaluation) and Section 4.2 (Implementation Details). This supervised fine-tuning enables adaptation of the pre-trained CLIP model to the segmentation task."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 4.3",
          "reasoning": "The datasets (e.g., ADE20K, PASCAL VOC, PASCAL-Context, MESS benchmark) are employed for evaluation and benchmarking the model's performance on open-vocabulary semantic segmentation tasks, as described in Section 4.1 and demonstrated in the experimental results in Section 4.3."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.4",
          "reasoning": "The paper uses the datasets to analyze the effects of different components and methods in the model (e.g., fine-tuning methods, cost vs feature aggregation), as shown in ablation studies and analyses in Section 4.4."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-293-rubric-5",
    "token_usage": {
      "prompt_tokens": 14300,
      "completion_tokens": 570,
      "total_tokens": 14870
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's described datasets and tasks involve class labels given as textual descriptions in English; there is no mention of datasets containing more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets contain exactly two human languages; the datasets referenced and used for training and evaluation utilize English text labels exclusively."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets and Evaluation",
          "reasoning": "The datasets used for training and evaluation, such as COCO-Stuff, ADE20K, PASCAL VOC, and PASCAL-Context, provide class labels and textual descriptions in English only. The paper consistently references datasets with English textual annotations as the input text."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or evidence is provided indicating use of any datasets with non-English languages only."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though programming code is discussed in the methodological descriptions, the datasets themselves contain images and textual class labels, not code or code-language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical expressions are used in the methodology to explain the cost computation and aggregation but the datasets themselves contain images and textual labels, not mathematical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets comprise natural images and classes relevant to open-vocabulary semantic segmentation, and do not include biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets or tasks involving fictional or artificially created languages are described."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages used in the datasets are clearly specified as English textual descriptions; there is no uncertainty regarding language content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain human language text labels (English), so the language coverage is applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-293-rubric-6",
    "token_usage": {
      "prompt_tokens": 11518,
      "completion_tokens": 150,
      "total_tokens": 11668
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "N/A",
          "reasoning": "The paper does not mention or include any statement or link to publicly available code related to dataset construction or preprocessing. The focus is mainly on model architecture, training, and evaluation using existing datasets."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "N/A",
          "reasoning": "The paper does not introduce any new dataset, nor does it document any dataset creation process. The datasets used for training and evaluation (e.g., COCO-Stuff, ADE20K, PASCAL VOC, PASCAL-Context, MESS) are all pre-existing and sourced from prior works; hence, no dataset creation or construction process is described."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-295-rubric-0",
    "token_usage": {
      "prompt_tokens": 10999,
      "completion_tokens": 172,
      "total_tokens": 11171
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2 Benchmark Establishment",
          "Reasoning": "The BalanceCC benchmark dataset is introduced as a new dataset for evaluation, consisting of 100 diverse open-license videos collected by the authors. The videos are real and human-generated as they are open-license video clips suitable for legal modification, not indicated to be algorithmically generated or simulated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2 Benchmark Establishment",
          "Reasoning": "The dataset includes detailed scene descriptions, categorical annotations, and other metadata manually refined by human annotators after initial generation by GPT-4V(ision), making the finalized textual annotations and labels human-generated data."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-295-rubric-1",
    "token_usage": {
      "prompt_tokens": 11799,
      "completion_tokens": 305,
      "total_tokens": 12104
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.2 Benchmark Establishment",
            "reasoning": "The BalanceCC benchmark dataset is introduced as a new dataset and involves manual human annotation for video attributes such as camera movement, object movement, and categorical content, with detailed scene descriptions refined by human annotators, indicating multiple human experts performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.2 Benchmark Establishment, Supplementary Materials",
            "reasoning": "The paper states that the GPT-4V(ision)-assisted process is refined by human annotators and refers to the supplementary materials for details on the prompting pipeline, specific instructions, and principles of labeling, which implies detailed annotation instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4.2 Benchmark Establishment, Supplementary Materials",
            "reasoning": "Annotations include complexity scores (rated 1 to 3), motion rated 1 to 3 scales, categories, and a 'Fantasy Level' for target prompts indicating scoring rubrics are used to annotate videos systematically, as supported by the description in Section 4.2 and supplementary materials."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 4.2 Benchmark Establishment, Supplementary Materials",
            "reasoning": "The paper mentions that the supplementary materials provide illustrative examples related to annotation and labeling principles, which confirms examples were provided in the annotation guidelines for the BalanceCC dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-295-rubric-2",
    "token_usage": {
      "prompt_tokens": 12989,
      "completion_tokens": 350,
      "total_tokens": 13339
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance being performed by a single human expert for the BalanceCC dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance on the BalanceCC dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 4.2 Benchmark Establishment",
          "reasoning": "The paper states that GPT-4V(ision) was used to provide initial video descriptions and complexity scores, which were then refined by human annotators. However, it does not specify that these annotators are experts. The presence of multiple human annotators refining the descriptions and labels suggests QA by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 4.2 Benchmark Establishment",
          "reasoning": "The paper explicitly mentions using the GPT-4V(ision) AI model to generate video descriptions, complexity scores, and target prompts as part of the process of establishing the BalanceCC benchmark, thus indicating AI model involvement in quality assurance."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention of automated verification or rule-based QA processes applied to the dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a QA process involving both an AI model and human annotators; therefore, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-295-rubric-3",
    "token_usage": {
      "prompt_tokens": 12599,
      "completion_tokens": 459,
      "total_tokens": 13058
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The BalanceCC benchmark dataset was curated by the authors collecting 100 open-license videos suitable for editing. Additionally, they manually annotated each video for camera movement, object movement, and categorical content, and refined GPT-4V generated scene descriptions and prompts with human annotations. This indicates original content creation by humans through curation and annotation."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The authors used the GPT-4V(ision) model to assist in generating video descriptions, complexity scores, and target editing prompts. These outputs are original content generated by AI, creating new descriptive and instructional data for the benchmark."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translation from another language by humans in the dataset construction or any other dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of machine translations being used for dataset generation or processing is provided."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The dataset consists of 100 videos collected from open-license sources without indication of significant modification to the original video content, representing a collection from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The authors derived additional data such as detailed video descriptions, complexity scores, motion annotations, and target editing prompts through processing and human refinement based on existing videos, constituting adapted or transformed data from original sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the data sources and methods of generation for the BalanceCC benchmark dataset."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-295-rubric-4",
    "token_usage": {
      "prompt_tokens": 13117,
      "completion_tokens": 414,
      "total_tokens": 13531
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the BalanceCC dataset for training any models from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fine-tuning or supervised training using the BalanceCC dataset is mentioned."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report the use of BalanceCC for reinforcement learning based methods or RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (BalanceCC Benchmark) and Section 5.3 (State-of-the-Art Comparisons)",
          "reasoning": "BalanceCC is explicitly introduced as a benchmark dataset designed for comprehensive evaluation of generative video editing methods, with detailed attributes and target prompts to facilitate performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (BalanceCC Benchmark)",
          "reasoning": "The paper uses the dataset's attributes such as scene complexity, motion, camera movement, and fantasy level to analyze trends and characteristics in video editing challenges and to guide evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "BalanceCC is actively used for evaluation and analysis as clearly described in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-295-rubric-5",
    "token_usage": {
      "prompt_tokens": 13840,
      "completion_tokens": 585,
      "total_tokens": 14425
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The BalanceCC benchmark dataset contains video data and associated text prompts and annotations, all described in English. There is no indication that the dataset contains entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset descriptions, prompts, and annotations are all in English; there is no mention of a bilingual dataset with exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.2 Benchmark Establishment and Section 4.3 Statistics",
          "reasoning": "The BalanceCC dataset includes descriptions, scene complexity scores, category labels, and editing prompts generated by GPT-4V(ision) and refined by human annotators, all presented and documented in English. No other human languages are mentioned in the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There are no indications in the paper that the BalanceCC dataset contains entries in only one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains formulae and describes a deep learning framework including algorithms and network structures, the BalanceCC dataset itself does not contain entries of programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper includes mathematical equations to explain the model and training steps, the BalanceCC dataset introduced does not include entries with mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the BalanceCC dataset includes biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no reference to entries in fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are described explicitly in English, so the language is specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries consist of videos and associated text prompts and annotations; these texts are in English, so the dataset contains language entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-295-rubric-6",
    "token_usage": {
      "prompt_tokens": 11058,
      "completion_tokens": 240,
      "total_tokens": 11298
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4 (BalanceCC Benchmark) and Implementation Details in Section 5.1",
          "reasoning": "The paper describes the creation of the BalanceCC benchmark dataset including selection of 100 open-license videos, annotation process with GPT-4V(ision) and human annotators, and detailed labeling for video categories and attributes, but does not mention releasing code or scripts for data collection, preprocessing, or prompt generation publicly. There is no link or explicit statement about dataset code availability."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 (BalanceCC Benchmark) and 4.2 Benchmark Establishment",
          "reasoning": "The paper provides a detailed description of the dataset creation process including video collection criteria (open-license, non-stigmatizing content), annotation steps with a GPT-4V(ision) assistant, human refinement of annotations, labeling categories (scene complexity, motion, fantasy level), and generation of target prompts, thereby documenting the process transparently."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-298-rubric-0",
    "token_usage": {
      "prompt_tokens": 13073,
      "completion_tokens": 124,
      "total_tokens": 13197
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Datasets and Performance Matrices; Section 5 Experiments",
          "Reasoning": "The authors train their proposed CFAT model using the DIV2K and Flickr2K datasets, which contain images captured or created by humans. These datasets are widely used and standard in super-resolution tasks, and the paper does not indicate that the authors created new datasets. The paper does not introduce any new dataset; instead, it uses these publicly available datasets for training and evaluation."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-298-rubric-1",
    "token_usage": {
      "prompt_tokens": 13873,
      "completion_tokens": 248,
      "total_tokens": 14121
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 5.1 Datasets and Performance Matrices",
            "reasoning": "The paper states that the proposed model is trained using existing datasets (DIV2K and Flicker2K) and evaluated on standard benchmarks (Set5, Set14, BSD100, Urban100, Manga109), which are all pre-existing datasets. The paper does not introduce any new dataset or describe performing new annotation efforts by humans or models. Thus, no new dataset with human or AI annotation is introduced, implying that data annotation is not performed in this work but existing datasets are utilized."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "Since no new dataset is introduced or annotated in the paper, there are no instructions for annotation provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "Since no new dataset is introduced or annotated in the paper, no scoring rubrics or annotation standards are provided."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "Since no new dataset is introduced or annotated in the paper, the paper does not provide annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-298-rubric-2",
    "token_usage": {
      "prompt_tokens": 15063,
      "completion_tokens": 291,
      "total_tokens": 15354
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for any dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by multiple human experts for any dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that a single non-expert performed quality assurance on the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe involvement of multiple human non-experts for dataset quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any AI model was utilized to perform quality assurance on datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about automated verification or rule-based quality assurance methods applied to datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses existing public benchmark datasets for training and evaluation (e.g., DIV2K, Flickr2K, Set5, Set14, BSD100, Urban100, Manga109) and does not introduce any new datasets. Consequently, there is no mention or documentation of any quality assurance process applied to new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-298-rubric-3",
    "token_usage": {
      "prompt_tokens": 14673,
      "completion_tokens": 294,
      "total_tokens": 14967
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.1 Datasets and Performance Matrices",
          "reasoning": "The paper states that training data is based on widely used existing datasets, DIV2K and Flickr2K, and evaluation is performed on existing standard SR benchmark datasets such as Set5, Set14, BSD100, Urban100, and Manga109. These datasets are publicly available image datasets collected from existing sources, and there is no indication of significant modification. Thus, the data used is collated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-298-rubric-4",
    "token_usage": {
      "prompt_tokens": 15191,
      "completion_tokens": 306,
      "total_tokens": 15497
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5.2 Experimental Settings",
          "reasoning": "The paper describes training the proposed CFAT model from scratch on the DIV2K and Flickr2K datasets using L1 loss with the Adam optimizer and detailed training settings, indicating the datasets are used for model training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1 Datasets and Performance Matrices, Section 5.4 Comparisons with State-of-the-art Methods",
          "reasoning": "Standard benchmark datasets such as Set5, Set14, BSD100, Urban100, and Manga109 are used exclusively for evaluation to measure the performance of the proposed model compared to state-of-the-art methods."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.3 Ablation Study",
          "reasoning": "Various ablation studies analyze the impact of different hyperparameters, window sizes, shift sizes, and module components on model performance, demonstrating use of datasets for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-298-rubric-5",
    "token_usage": {
      "prompt_tokens": 15914,
      "completion_tokens": 558,
      "total_tokens": 16472
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only discusses image super-resolution datasets and models with no indication of multiple human languages in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of datasets containing exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets (DIV2K, Flickr2K, Set5, Set14, BSD100, Urban100, Manga109) are image datasets and do not contain textual linguistic content in English or other languages; thus, this label does not apply."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset described involves any single non-English human language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper mentions using Python and PyTorch frameworks, no dataset containing entries of programming code is introduced; the focus is on image data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical formulas and symbolic notations are used throughout the paper to describe the model and attention mechanisms, but these are part of the paper content and not the dataset entries themselves."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data are part of the proposed datasets."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are involved in any dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages or types of content in provided datasets are clearly described and involve image data, so this label does not apply."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The proposed datasets (DIV2K, Flickr2K, Set5, Set14, BSD100, Urban100, Manga109) are image datasets without any linguistic content. They consist of images, possibly with associated metadata, but no human language text entries are present within the datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-298-rubric-6",
    "token_usage": {
      "prompt_tokens": 13132,
      "completion_tokens": 263,
      "total_tokens": 13395
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 5.2 (Experimental Settings)",
          "reasoning": "The abstract states 'Find the code Here,' indicating that the code is publicly available. Additionally, Section 5.2 provides detailed experimental settings that imply the availability of code used for replication. Although the exact repository link is not provided in the text excerpt, the mention in the abstract suggests the authors provide accessible code for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 5.1 (Datasets and Performance Matrices), Section 5.2 (Experimental Settings), and Section 5.3 (Ablation Study)",
          "reasoning": "The paper documents the datasets used for training (DIV2K and Flickr2K) and evaluation (Set5, Set14, BSD100, Urban100, Manga109) in Section 5.1. The data augmentation and preprocessing details, including patch cropping and geometric transformations, are described in Section 5.2. Additionally, the ablation studies provide detailed methodology and parameter settings influencing dataset usage and model training in Section 5.3. However, the paper does not introduce any new datasets; it uses existing benchmark datasets, and the creation of new datasets is not described."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-353-rubric-0",
    "token_usage": {
      "prompt_tokens": 12345,
      "completion_tokens": 161,
      "total_tokens": 12506
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2 (Construction of the Retrieval Database) and Section 3 (Methodology)",
          "Reasoning": "The new dataset consists of pairs of textual descriptions of safety-critical scenarios and corresponding Scenic code snippets. These pairs include adversarial behaviors, road geometry, and relative spawn positions used to generate scenarios in the CARLA simulator. The descriptions and code snippets are manually collected initially and then expanded using LLM (large language model)-based few-shot learning, thus they are model generated rather than human generated raw data. This dataset is specifically constructed by the authors for retrieving and assembling scenario code from text descriptions, serving as the new data source introduced by the paper."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-353-rubric-1",
    "token_usage": {
      "prompt_tokens": 13145,
      "completion_tokens": 281,
      "total_tokens": 13426
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 3.2 Construction of the Retrieval Database",
            "reasoning": "The snippet collection process is initially manual on Scenic repository examples; then, LLMs are used for few-shot learning to generate more diverse snippets iteratively. This indicates an annotation process using large language models for generating and labeling the data snippets."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Construction of the Retrieval Database",
            "reasoning": "The paper describes an iterative process that uses LLMs with few-shot learning and demonstrations to convert original text descriptions into sub-descriptions and Scenic code snippets, indicating that instructions for decomposition and annotation are provided as part of the procedure."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2 Construction of the Retrieval Database",
            "reasoning": "There is no explicit mention in the paper of scoring rubrics or specific scoring guidelines applied to the annotation or snippet generation process."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.2 Construction of the Retrieval Database and Figure 2",
            "reasoning": "The paper includes several few-shot examples of adversarial scenarios decomposed into sub-descriptions and their corresponding Scenic code snippets, e.g., Figure 2 shows selected text-simulation cases, supporting the presence of annotation examples in the guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-353-rubric-2",
    "token_usage": {
      "prompt_tokens": 14335,
      "completion_tokens": 342,
      "total_tokens": 14677
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process conducted by a single human expert for the dataset created."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of quality assurance being performed by multiple human experts on the generated scenarios or retrieval database."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance carried out by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance by multiple non-expert humans for the dataset."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 4.2 Safety-Critical Scenarios Generation; Section 4 Experiment",
          "reasoning": "The paper describes using a surrogate ego vehicle trained with SAC (and other RL algorithms) as a test model to select and evaluate adversarial safety-critical scenarios generated by ChatScene. This indicates that quality assurance of the generated scenarios is performed by AI models as judges by testing scenarios against multiple ego vehicles to validate their safety-critical nature."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although automated generation and simulation under Scenic and CARLA occur, the paper does not document an automated verification or rule-based quality assurance process for the dataset content itself."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is documented, specifically involving AI models evaluating the safety-critical nature of generated scenarios; therefore, 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-353-rubric-3",
    "token_usage": {
      "prompt_tokens": 13945,
      "completion_tokens": 522,
      "total_tokens": 14467
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.2 Construction of the Retrieval Database",
          "reasoning": "The paper details that the initial Scenic code snippets and their corresponding descriptions were manually collected and decomposed by the authors from the Scenic repository, indicating original human curation and creation. This collection forms the basis of their retrieval database and includes diverse adversarial behaviors and geometric configurations, which were then expanded using LLMs. This shows that at least part of the dataset consists of data created entirely by human contributors."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.2 Construction of the Retrieval Database",
          "reasoning": "The authors describe using large language models with few-shot learning to generate additional Scenic code snippets and descriptions based on an initial human-curated dataset. These generated snippets extend the dataset beyond the original human-created data, indicating that some new data is generated entirely by AI models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by translating content from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate using machine translation to produce any data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.2 Construction of the Retrieval Database",
          "reasoning": "The authors collect existing Scenic code examples from the Scenic repository and decompose them into description-snippet pairs. This process involves aggregation of existing data without significant modification, indicating collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 Construction of the Retrieval Database",
          "reasoning": "The paper describes that starting from collected Scenic snippets, the authors use LLMs to generate additional diverse snippets, effectively adapting and transforming existing examples to expand the database. Moreover, the overall scenarios are generated by assembling snippets and sampling parameter ranges within Scenic, indicating derivation and adaptation of existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data generation and source methods are adequately documented and described in Section 3.2."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-353-rubric-4",
    "token_usage": {
      "prompt_tokens": 14463,
      "completion_tokens": 384,
      "total_tokens": 14847
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.3 (Adversarial Training on Safety-Critical Scenarios) and Table 3",
          "reasoning": "The paper explicitly states that the dataset created by ChatScene is used to fine-tune pre-trained ego vehicle models via supervised adversarial training, showing improved collision rates and overall scores compared to baselines."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": true,
          "reference": "Section 4.3 (Adversarial Training on Safety-Critical Scenarios) and Section 4.1 (Setup)",
          "reasoning": "The ego vehicles controlled by reinforcement learning algorithms (e.g., SAC) are fine-tuned using the scenarios generated by the dataset, demonstrating use in RL-based post-training methods."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 (Safety-Critical Scenarios Generation) and Table 1, Section 4.3 and Tables 2, 3",
          "reasoning": "The dataset is used extensively for evaluation and benchmarking by testing ego vehicles trained with various RL methods on the generated scenarios, measuring collision rates and overall driving performance."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": true,
          "reference": "Section 3.2 (Construction of the Retrieval Database)",
          "reasoning": "The paper describes the dataset as a comprehensive retrieval database of Scenic code snippets paired with scenario descriptions, which augments the generation process via retrieval-augmented code synthesis; thus, serving as a knowledge base."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-353-rubric-5",
    "token_usage": {
      "prompt_tokens": 15186,
      "completion_tokens": 574,
      "total_tokens": 15760
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the dataset containing entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the dataset containing entries with exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, especially Section 3.2 and examples in Figures and Tables",
          "reasoning": "The new dataset consists of scenario descriptions and code snippet descriptions provided entirely in English. The textual scenario descriptions and retrieval database descriptions in the paper are all in English, indicating the dataset contents are monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any dataset entries contain only non-English languages."
        },
        "Code / Programming Language": {
          "is_applicable": true,
          "reference": "Section 3.2 (Construction of the Retrieval Database) and related Scenic code examples throughout Section 3 and the paper.",
          "reasoning": "The proposed dataset contains entries with programming or structured code-related content in the Scenic programming language (e.g., code snippets describing adversarial behaviors, geometry, and spawn positions). This is a key component of the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the Scenic code contains some logical constructs (like interrupts, behavior definitions, and ranges), no specific mathematical or formal logical expressions or symbolic notations are emphasized or described as part of the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain any biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is specified clearly as English and Scenic code, so this label does not apply."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language in the form of English text and Scenic code, so it is not without any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-353-rubric-6",
    "token_usage": {
      "prompt_tokens": 12404,
      "completion_tokens": 194,
      "total_tokens": 12598
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 3.2 Construction of the Retrieval Database",
          "reasoning": "The abstract explicitly provides a GitHub link to the code repository (https://github.com/javyduck/ChatScene), indicating the code for generating the safety-critical scenarios is publicly available. Section 3.2 details the methodology for constructing the retrieval database and the use of Scenic code snippets, suggesting code related to dataset/scenario generation is included."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 Methodology and Section 4 Experiment",
          "reasoning": "The paper provides detailed descriptions of the dataset creation process in Section 3, explaining how scenarios are decomposed, encoded, retrieved, assembled, and simulated. Section 4 further elaborates on experiment setups and evaluations of the generated scenarios, indicating comprehensive documentation of how the datasets/scenarios are generated, selected, and used for training and testing."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-389-rubric-0",
    "token_usage": {
      "prompt_tokens": 9765,
      "completion_tokens": 172,
      "total_tokens": 9937
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Initial Label Generation, Section 4.1 Datasets",
          "Reasoning": "The authors introduce the Commonsense Prototype-based Detector (CPD) for unsupervised 3D object detection using LiDAR point cloud data captured from real-world scenes. The datasets used, including the Waymo Open Dataset (WOD), PandaSet, and KITTI, provide LiDAR scans which are human-recorded sensor data from vehicles driving in urban environments. The new dataset constructed within CPD involves concatenating multiple consecutive LiDAR frames with motion artifact removal and clustering to generate initial pseudo-labels, indicating the primary modality is signal/sensor data collected via LiDAR sensors operated by humans during data capture."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-389-rubric-1",
    "token_usage": {
      "prompt_tokens": 10565,
      "completion_tokens": 233,
      "total_tokens": 10798
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Sections 3.1 and 3.2",
            "reasoning": "The pseudo-labels and annotations are generated through unsupervised methods involving automatic multi-frame clustering (MFC), scoring (CSS), and prototype-based refinement (CBR) without human annotation involvement as explicitly described in Sections 3.1 and 3.2."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "The paper does not describe any manual annotation process or provide instructions for human annotators since all labels are generated automatically via unsupervised methods."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "There is no mention of rubrics or scoring guidelines provided for human annotation; the scoring methods (CSS) are algorithmic and not human-judged rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "No annotation examples or guideline samples for human annotators are provided because the annotation is fully automatic and unsupervised."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-389-rubric-2",
    "token_usage": {
      "prompt_tokens": 11755,
      "completion_tokens": 378,
      "total_tokens": 12133
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the dataset introduced or used."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on the dataset; no such QA process is described."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance by a single non-expert human annotator is described or indicated."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information suggesting multiple non-expert annotators performed quality assurance on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that an AI model was used as a judge to perform quality assurance for the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 (CProto-constrained Box Regularization for Label Refinement); Section 3.1 (Initial Label Generation)",
          "reasoning": "The paper describes the use of algorithmic and rule-based techniques for quality assurance, such as the unsupervised Completeness and Size Similarity (CSS) scoring to select high-quality pseudo-labels for building the Commonsense Prototype set and refining pseudo-labels. This involves automatic calculation of distance score, Multi-Level Occupancy score, and Size Similarity score to assess label quality without human intervention. Thus, the QA process is effectively an automatic process involving automated verification of pseudo-label quality via algorithmic metrics."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents and applies automatic quality assurance methods for pseudo-label quality assessment and refinement, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-389-rubric-3",
    "token_usage": {
      "prompt_tokens": 11365,
      "completion_tokens": 482,
      "total_tokens": 11847
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that any data was created entirely from scratch by human contributors. Instead, it utilizes existing datasets such as the Waymo Open Dataset, PandaSet, and KITTI dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the method generates pseudo-labels using models and clustering algorithms, these labels are derived or refined from existing LiDAR scan data rather than being entirely new data generated from models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data being generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The datasets used, such as WOD, PandaSet, and KITTI, are collections of LiDAR scans aggregated from driving scenes. The paper uses these existing datasets as data sources without claiming significant modification, implying that the data is collated."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The paper generates initial pseudo-labels through Multi-Frame Clustering and then refines these labels using the proposed Commonsense Prototype-based methods. These pseudo-labels and refined annotations are based on existing LiDAR scans from the original datasets but involve modifications, transformations, and adaptations (e.g., clustering, scoring, box regularization). Hence, the data is derived from existing sources with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies the origin of the input data and the method of pseudo-label generation and refinement. Therefore, this category is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-389-rubric-4",
    "token_usage": {
      "prompt_tokens": 11883,
      "completion_tokens": 479,
      "total_tokens": 12362
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.3, Section 4.2, Section 4.3",
          "reasoning": "The CPD method uses pseudo-labels generated and refined from the dataset without human annotations to train the 3D object detection network. Section 3.3 describes the self-training method with CSS-weighted detection loss and geometry contrast loss for training the detector. Section 4.2 details training on the datasets without human labels, and Section 4.3 reports detection results after training, demonstrating supervised fine-tuning on the generated pseudo-labels."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, Section 4.3, Section 4.5",
          "reasoning": "The datasets (WOD, PandaSet, KITTI) are used to evaluate and benchmark the proposed CPD method's performance against state-of-the-art unsupervised and supervised methods. Quantitative comparisons in Section 4.3 and pseudo-label evaluation in Section 4.5 demonstrate usage for performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 1, Section 4.5",
          "reasoning": "The authors analyze characteristics of the data such as completeness and size distributions (Fig. 2) and use unsupervised scoring (CSS) to evaluate pseudo-label quality. Section 4.5 also contains detailed analysis of pseudo-label quality including recall, precision, error distributions, and IoU statistics, primarily to understand and improve detection performance."
        },
        "Knowledge Base": {
          "is_applicable": true,
          "reference": "Section 3.2, Section 3.3",
          "reasoning": "The Commonsense Prototype (CProto) constructed from high-quality pseudo-labels serves as a knowledge base representing accurate geometry and size priors. This knowledge base is used to refine pseudo-labels (CBR) and augment training (CST) by providing geometric constraints and improved features."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-389-rubric-5",
    "token_usage": {
      "prompt_tokens": 12606,
      "completion_tokens": 618,
      "total_tokens": 13224
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the proposed dataset contains entries with more than two human languages. The dataset descriptions focus on 3D point cloud data for object detection in urban environments, not linguistic content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset contains exactly two human languages. The data consists of sensor point cloud data for 3D objects and associated annotations/pseudo-labels, not bilingual textual data."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Paper sections describing dataset and experimental settings, e.g., Abstract, Sections 1, 3, and 4",
          "reasoning": "Although the dataset itself contains 3D point cloud data, all documentation, comments, and label descriptions are in English. The dataset does not include multilingual human language content but English is used in all textual parts present."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper and dataset documentation are provided solely in English; no non-English language is mentioned."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of 3D point cloud data and associated pseudo-labels for object detection. Code or programming language content is not described as part of the dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical equations and formulas describing the methods (e.g., scoring functions), these are part of the methodological description and not part of the dataset's entries themselves."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is related to 3D object detection from LiDAR scans for autonomous driving and does not contain biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include fictional or artificially constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages involved in the dataset are clear and indicated as English for documentation; there is no ambiguity or missing specification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset does contain language in the form of English textual labels, comments, and descriptions, so it is not language-free."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-389-rubric-6",
    "token_usage": {
      "prompt_tokens": 9824,
      "completion_tokens": 201,
      "total_tokens": 10025
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 5 Conclusion",
          "reasoning": "The paper mentions that the code for their CPD method will be available at a GitHub link, but this refers to the detection method code rather than code for constructing any new datasets. The paper does not introduce any new datasets created by the authors themselves, and does not provide code for dataset construction. The datasets used (WOD, PandaSet, KITTI) are existing datasets. Thus, no code related to new dataset construction is available."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "N/A",
          "reasoning": "The paper does not introduce any new datasets constructed by the authors. Instead, it uses existing public datasets (Waymo Open Dataset, PandaSet, KITTI) and documents the use of these datasets in Section 4.1. Since no new datasets were constructed or introduced by the authors, there is no documentation about dataset creation in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-411-rubric-0",
    "token_usage": {
      "prompt_tokens": 14058,
      "completion_tokens": 170,
      "total_tokens": 14228
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and Table 2",
          "Reasoning": "The paper introduces the use of SynLiDAR as a source domain dataset for experiments (Table 2). SynLiDAR is a synthetic LiDAR dataset, thus generated by simulation/modeling techniques, not human generated recordings. However, the paper does not indicate that the authors introduce a new dataset named SynLiDAR; it is cited from [49]. Therefore, SynLiDAR is not a new dataset introduced by the authors and is excluded. No other new dataset introduced by the authors is found in the paper. The datasets referenced are pre-existing benchmarks. Thus, based strictly on the paper, no new datasets are introduced by the authors."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-411-rubric-1",
    "token_usage": {
      "prompt_tokens": 14858,
      "completion_tokens": 176,
      "total_tokens": 15034
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1 (Spherical Projection)",
            "reasoning": "The paper describes projecting raw 3D point clouds into 2D images using a spherical projection formula, which is a deterministic and automatic transformation, not involving human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No instructions related to this automatic projection process are described in the paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics for annotation are applicable because this is an automatic data processing step, not an annotation task."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples are provided for this projection as it is an automated process, not human labeling."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-411-rubric-2",
    "token_usage": {
      "prompt_tokens": 16048,
      "completion_tokens": 303,
      "total_tokens": 16351
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance of dataset annotations was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of multiple human experts performing quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance performed by a single human non-expert annotator on the dataset annotations."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description or evidence is provided about multiple human non-expert annotators conducting quality assurance for the dataset annotations."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of AI models for quality assurance in the context of dataset annotation or validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of any automated verification or algorithmic/rule-based quality assurance process applied to dataset annotations within the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset nor describe any quality assurance process validating dataset annotations. The datasets used are existing benchmarks (e.g., SemanticKITTI, SynLiDAR, nuScenes) and no QA process is described for them in this work."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-411-rubric-3",
    "token_usage": {
      "prompt_tokens": 15658,
      "completion_tokens": 396,
      "total_tokens": 16054
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe creation of any new dataset entirely from scratch by human contributors; instead, it uses existing datasets for experiments."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any new data entirely by AI or machine learning models; it focuses on domain adaptation methods applied on existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation to generate data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe collecting or aggregating data from multiple existing sources to build a new dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly describe creating any derived datasets by modifying, adapting, or transforming existing datasets. It uses existing standard datasets but does not introduce new datasets derived from them."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets, nor does it specify the origin or method of generation of any novel dataset. The datasets used (e.g., SynLiDAR, SemanticKITTI, nuScenes) are pre-existing benchmarks used for evaluation. Therefore, this metric is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-411-rubric-4",
    "token_usage": {
      "prompt_tokens": 16176,
      "completion_tokens": 209,
      "total_tokens": 16385
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses pre-existing datasets such as SynLiDAR, SemanticKITTI, and nuScenes for training, evaluation, and analysis. There is no introduction or creation of a new dataset by the authors in this work. Hence, no practical usage of a newly proposed dataset is described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-411-rubric-5",
    "token_usage": {
      "prompt_tokens": 16899,
      "completion_tokens": 471,
      "total_tokens": 17370
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Entire paper, e.g., Abstract and Sections 1-5",
          "reasoning": "The datasets introduced and used in the paper (SynLiDAR, SemanticKITTI, nuScenes) are point cloud datasets for semantic segmentation. The annotations and related descriptions are in English only as the paper and dataset documentation are in English. No other human languages are indicated in the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes code and formulas as part of methodology explanation, but the datasets themselves do not contain code or programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notation is present in the paper for model explanation, but the datasets' entries themselves do not contain such notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data is involved in the datasets."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or constructed languages are involved in the dataset entries."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of dataset entries are fully specified as English annotation labels for semantic segmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain language in the form of semantic class annotations in English."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-411-rubric-6",
    "token_usage": {
      "prompt_tokens": 14117,
      "completion_tokens": 131,
      "total_tokens": 14248
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section in the paper provides a link or mention of the code for dataset construction.",
          "reasoning": "The paper discusses domain adaptation methods and evaluates on pre-existing datasets such as SynLiDAR, SemKITTI, and nuScenes, but does not introduce any new datasets nor mention releasing or describing code related to dataset construction."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section in the paper details dataset creation or construction procedures.",
          "reasoning": "The paper uses existing benchmark datasets and does not document any dataset creation process, thus no documentation is provided for any new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-412-rubric-0",
    "token_usage": {
      "prompt_tokens": 13728,
      "completion_tokens": 118,
      "total_tokens": 13846
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, Abstract",
          "Reasoning": "The paper introduces a new task setting and training methods, and evaluates on three public datasets, but does not introduce a new dataset. The paper uses images from multiple datasets for their experiments, but no new dataset is introduced by the authors themselves. The paper discusses images from multiple medical centers as unlabeled and labeled data but all data are from existing datasets. Thus, no new dataset is introduced in the paper."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-412-rubric-1",
    "token_usage": {
      "prompt_tokens": 14528,
      "completion_tokens": 222,
      "total_tokens": 14750
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3 and experimental settings",
            "reasoning": "The paper describes segmentation tasks based on publicly available datasets with expert annotations collected from clinical centers, as typical in medical imaging research. The annotations are considered to be done by domain experts since these are medical image segmentation labels provided by data centers."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly discussed in the paper",
            "reasoning": "The paper does not mention providing any specific annotation instructions or protocols to the annotators for the new datasets, only uses existing public datasets with available labels."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly discussed in the paper",
            "reasoning": "There is no mention of scoring rubrics or standardized evaluation guidelines provided to annotators for the segmentation labeling process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly discussed in the paper",
            "reasoning": "No annotation examples or detailed examples for annotation guidelines are described for the new datasets; the paper assumes pre-existing expert annotations from data providers."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-412-rubric-2",
    "token_usage": {
      "prompt_tokens": 15718,
      "completion_tokens": 399,
      "total_tokens": 16117
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that dataset annotations were validated or quality assured by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper references public datasets (e.g., Prostate dataset, Fundus dataset, M&Ms dataset) but does not describe any quality assurance process involving multiple human experts for the new or combined data used."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of quality assurance conducted by a single human non-expert on the data in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by multiple non-expert humans."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the method involves generating pseudo labels via a teacher model for unlabeled data, this is part of the proposed machine learning method, not a quality assurance process for dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention of automated quality assurance or algorithmic verification processes applied to dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses existing public datasets but does not describe any new dataset creation or specific quality assurance procedures applied to dataset annotations or content. No quality assurance process for dataset annotation quality is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-412-rubric-3",
    "token_usage": {
      "prompt_tokens": 15328,
      "completion_tokens": 488,
      "total_tokens": 15816
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the creation of any new dataset through manual human annotation initiated by the authors. Instead, it utilizes publicly available medical image datasets for experiments."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper generates intermediate samples via Unified Copy-Paste (UCP) and uses style transitions through Training Process aware Random Amplitude MixUp (TP-RAM), these are derived or augmented data based on existing images, not entirely new original data generated solely by AI without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset generated by translating content from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data produced by machine translation is described in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), and Introduction",
          "reasoning": "The paper utilizes three public datasets: Fundus dataset, Prostate dataset, and M&Ms dataset, each with labeled and unlabeled subsets collected from multiple medical centers. These datasets are aggregated from existing sources without modifications. The authors only aggregate and organize these datasets for their problem setting."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 (Intermediate Samples Generation by UCP), Section 3.3 (Style Transition in Intermediate Samples)",
          "reasoning": "The paper creates intermediate domain samples by combining and mixing existing labeled and unlabeled images using Unified Copy-Paste (UCP) and applies style transfer via TP-RAM. These processes modify existing data through augmentation and synthesis techniques, producing derived datasets adapted from pre-existing images."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly states the datasets used and the data processing methods applied; therefore, the data origin is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-412-rubric-4",
    "token_usage": {
      "prompt_tokens": 15846,
      "completion_tokens": 517,
      "total_tokens": 16363
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using any new dataset exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention that the new dataset is used to train models starting from randomly initialized parameters without prior training."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3 (Methodology), Sections 4.1-4.3 (Experiments and Results)",
          "reasoning": "The paper uses the labeled portion of datasets (MiDSS setting with limited labeled data from one domain) to supervise the fine-tuning of pre-trained segmentation models. For example, in the experimental setup, 20 or 40 labeled samples are used for supervised training and fine-tuning as part of the semi-supervised framework."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning based post-training or RLHF techniques."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 4.2 and 4.3 (Results and Comparisons)",
          "reasoning": "The new datasets introduced for the MiDSS scenario are used extensively to evaluate and benchmark the proposed method's segmentation performance across multiple domains."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.4 (Ablation Study)",
          "reasoning": "The paper conducts ablation studies on the newly constructed intermediate domains to analyze the impact of their proposed components and understand the characteristics leading to improved performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as a knowledge base for retrieval-augmented generation or similar augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The new datasets introduced are actively used for both supervised fine-tuning and evaluation as well as analysis, so N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-412-rubric-5",
    "token_usage": {
      "prompt_tokens": 16569,
      "completion_tokens": 662,
      "total_tokens": 17231
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes medical image datasets from multiple domains or centers but does not mention any multiple human languages used in these datasets. The content focuses on image data with annotations, without linguistic text in multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset entries contain exactly two human languages. The datasets described are medical imaging data without bilingual text content."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "General description throughout the paper including Abstract and Method sections",
          "reasoning": "The paper is written in English and medical image datasets generally include English annotations and labels. There are no mentions of other human languages or multilingual textual annotations in the dataset. The datasets consist mainly of images and their corresponding masks/segmentations, with any textual labels presumably in English as no alternative language is mentioned."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not described as using a single non-English language instead of English. The paper is written in English and does not indicate the data uses a different language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset itself does not contain programming or code-related content. The paper describes methods and algorithms using code, but the dataset consists of medical images and annotations, not code or structured programming languages."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes formulas and definitions in the methodology, the dataset itself does not contain mathematical or logical expressions as entries. The dataset entries are images from medical scanners and their labels."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of medical images (e.g., prostate MRI, cardiac MR images, fundus images) and their segmentation masks, but does not contain biological sequences or forms of biological communication like DNA or animal signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that datasets include constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is known and can be inferred as English since annotations and labels are presumably in English (this is standard for medical datasets described in English papers)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of medical images and related annotations or segmentation masks, which can be considered a form of linguistic labels (English). Therefore, the dataset is not language-free."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-412-rubric-6",
    "token_usage": {
      "prompt_tokens": 13787,
      "completion_tokens": 162,
      "total_tokens": 13949
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract",
          "reasoning": "The paper provides a repository link https://github.com/MQinghe/MiDSS for the code of their method, but there is no indication or mention that code related to dataset construction, data collection, or preprocessing for creating new datasets is publicly available. The datasets mentioned appear to be public datasets, and no new dataset creation or collection code is specified or linked."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "None",
          "reasoning": "The paper uses three existing public datasets (Fundus, Prostate, M&Ms) but does not introduce any new datasets constructed by the authors. Since no new dataset is introduced, there is no documentation provided for dataset creation or construction within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-415-rubric-0",
    "token_usage": {
      "prompt_tokens": 12803,
      "completion_tokens": 120,
      "total_tokens": 12923
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Implementation Details; Section 4.2 Ablation Study; Section 4.3 Benchmark Evaluation (TNL2K)",
          "Reasoning": "The TNL2K dataset is mentioned as a training and evaluation benchmark specifically designed for natural language-guided tracking, comprising diverse videos including natural, animation, infrared, and virtual game videos, which indicates human-captured or human-created video data as its modality is video and creation involves humans."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-415-rubric-1",
    "token_usage": {
      "prompt_tokens": 13603,
      "completion_tokens": 275,
      "total_tokens": 13878
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1 Implementation Details and Section 4 Experiment",
            "reasoning": "The paper discusses training and evaluation on the new dataset TNL2K, which it introduces as a benchmark designed for natural language-guided tracking. The dataset and evaluation are presented within the scope of this paper's experiments, and the annotation process is assumed to be conducted by experts given the nature of the dataset and the tracking task."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit instructions are provided in the paper regarding annotation guidelines for the TNL2K dataset.",
            "reasoning": "The paper does not describe or mention any annotation instruction documents or guidelines for human annotators during the dataset creation or evaluation phases."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention of annotation rubrics or scoring criteria in the paper's description of TNL2K.",
            "reasoning": "No details on rubrics or scoring metrics specifically for annotation are presented; evaluations are quantitative model performance metrics rather than annotation scoring rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of annotation guidelines or annotated samples are provided in the paper.",
            "reasoning": "The paper provides qualitative examples of tracking results, but not explicit examples of annotation instructions or guidelines for the dataset creation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-415-rubric-2",
    "token_usage": {
      "prompt_tokens": 14793,
      "completion_tokens": 300,
      "total_tokens": 15093
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert on dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of dataset annotation or quality assurance performed by multiple human experts in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no evidence of dataset annotation or quality assurance being conducted by a single non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that multiple non-expert annotators were involved in dataset quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model was used as a judge for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated or algorithmic quality assurance process for dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets, nor does it describe any quality assurance process related to dataset annotations or content. All datasets mentioned (TNL2K, OTB-Lang, LaSOT, RefCOCOg) are pre-existing benchmarks. Therefore, no quality assurance process is described or performed for new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-415-rubric-3",
    "token_usage": {
      "prompt_tokens": 14403,
      "completion_tokens": 524,
      "total_tokens": 14927
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation Details; Abstract; Introduction",
          "reasoning": "The paper states in the Implementation Details that the training dataset comprises TNL2K, OTB-Lang, LaSOT, and RefCOCOg datasets. While these datasets are pre-existing, the new dataset introduced is TNL2K, which was released by Wang et al. [30] prior to this work. No new dataset is explicitly introduced by the authors themselves in this paper. Therefore, no indication exists that the authors have created original data entirely from scratch by human contributors. Consequently, since the paper does not introduce any new datasets created from scratch by humans, this label does not apply to new data created by the authors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any new dataset data content purely from AI or machine learning models without referencing existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of dataset translation from other languages by humans is found in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of dataset translation from other languages by machine translation is found in the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used and referenced are existing datasets; no evidence is provided for aggregation or collation of data from multiple sources into a new dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate creation of any derived dataset based on existing datasets with modifications or adaptations. The data usage is on existing datasets only."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; it only uses and evaluates on existing datasets such as TNL2K, OTB-Lang, LaSOT, and RefCOCOg. There is no documentation or specification of new dataset creation or methods for generating new datasets. Thus, this category applies as the paper does not specify any new dataset origin."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-415-rubric-4",
    "token_usage": {
      "prompt_tokens": 14921,
      "completion_tokens": 340,
      "total_tokens": 15261
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation Details",
          "reasoning": "The paper describes training their model on a combination of datasets including TNL2K, OTB-Lang, LaSOT, and RefCOCOg, using supervised learning with losses such as L1, GIoU, and cross-entropy. This indicates these datasets are used to fine-tune their model rather than training from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3 Experimental Results (Tables 3 and 4)",
          "reasoning": "The paper extensively evaluates the proposed approach on existing datasets: TNL2K, OTB-Lang, LaSOT, and RefCOCOg, serving as benchmarks to measure their model's performance. These datasets are not proposed by the authors but are used for evaluation purposes."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new datasets are introduced by the authors in this paper. The paper utilizes several existing datasets for training and evaluation, but does not present any novel dataset. Therefore, the task utility metric cannot apply to new datasets in this paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-415-rubric-5",
    "token_usage": {
      "prompt_tokens": 15644,
      "completion_tokens": 656,
      "total_tokens": 16300
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses datasets used for evaluation such as TNL2K, OTB-Lang, LaSOT, and RefCOCOg, but does not state that the new dataset introduced contains more than two human languages. There is no mention of multilingual dataset composition."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the new dataset introduced contains exactly two human languages. The paper does not specify bilingual content."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation Details; Results and Dataset Descriptions in Section 4",
          "reasoning": "The paper's new dataset introduced for training and evaluation primarily involves language descriptions used for natural language tracking. All linguistic descriptions mentioned are in English, such as 'white bird on the left'. The datasets used (TNL2K, OTB-Lang, LaSOT, RefCOCOg) are English-language datasets or use English annotations, and no other language is referenced. The paper uses English as the single language for all language-based content in the introduced dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset with a single non-English language; only English is used."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described and introduced consist of images, videos, and language annotations in English. The paper does not mention any dataset entries containing programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical expressions and formulas describing the model and methods, these do not form part of the dataset entries presented as data for training or evaluation. Thus, the new dataset does not contain mathematical or logical notation entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset and task focus on visual object tracking guided by natural language in videos, and do not include biological sequences or non-human communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of fictional or artificial languages is present in the dataset entries."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is explicitly described and clearly identified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets include natural language annotations, so the dataset entries do include language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-415-rubric-6",
    "token_usage": {
      "prompt_tokens": 12862,
      "completion_tokens": 196,
      "total_tokens": 13058
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "The paper does not mention any code repository or links for dataset construction or creation code.",
          "reasoning": "While the paper provides code for the proposed tracking method (available at https://github.com/twotwo2/QueryNLT), it does not introduce any new datasets itself, nor does it provide code related to dataset collection or construction. The datasets used (TNL2K, OTB-Lang, LaSOT, RefCOCOg) are all pre-existing datasets. Hence, there is no code related to new dataset construction in the paper."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No sections or descriptions in the paper discuss the creation or construction process of a new dataset.",
          "reasoning": "The paper does not introduce any new datasets nor does it describe any process of dataset creation or collection. It evaluates on existing datasets only. Therefore, it lacks documentation about dataset construction."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-416-rubric-0",
    "token_usage": {
      "prompt_tokens": 13889,
      "completion_tokens": 256,
      "total_tokens": 14145
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 1 Introduction, Section 4 Experiments",
          "Reasoning": "The paper introduces a new framework CG-STVG aimed at spatio-temporal video grounding evaluated on benchmark datasets HCSTVG and VidSTG, which consist of videos. The videos are human recorded, as these are standard human-centric video datasets. The authors do not introduce a new dataset but propose a new method evaluated on existing benchmarks only. Hence, no new dataset is introduced."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 1 Introduction, Section 3.1 Multimodal Encoder",
          "Reasoning": "Textual queries for grounding are human-generated natural language queries describing target objects in videos, as standard in the STVG task. The paper does not introduce new text datasets but uses existing query annotations from benchmarks HCSTVG-v1/-v2 and VidSTG."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-416-rubric-1",
    "token_usage": {
      "prompt_tokens": 14689,
      "completion_tokens": 283,
      "total_tokens": 14972
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4 and throughout the paper",
            "reasoning": "The paper introduces CG-STVG framework and evaluates on three datasets: HCSTVG-v1, HCSTVG-v2, and VidSTG. These datasets are pre-existing benchmarks for spatio-temporal video grounding, as indicated by citations such as [31] for HCSTVG and [42] for VidSTG. The paper does not report the introduction, collection, or manual annotation of any new dataset. Instead, it uses these existing datasets for evaluation purposes. Therefore, no new dataset annotation was conducted by humans or models in this paper. The evaluation metrics and experimental results are automatically computed from model predictions against provided ground truth."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "The paper does not describe any new dataset creation or annotation instructions; it only refers to existing datasets. Hence, no instructions related to newly annotated data are provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "No new annotation task or scoring rubric for annotations on new data is introduced or mentioned in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "Since no new dataset or annotation process is introduced, no examples related to annotation guidelines are included."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-416-rubric-2",
    "token_usage": {
      "prompt_tokens": 15879,
      "completion_tokens": 279,
      "total_tokens": 16158
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process conducted by a single human expert for any dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that dataset annotations were validated or quality assured by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information that a single non-expert performed quality assurance on dataset annotations."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance performed by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of an AI model being used as a judge for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe automated verification or algorithmic/rule-based quality assurance for dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets and does not describe any quality assurance processes for datasets. It only uses existing benchmark datasets (HCSTVG-v1/-v2 and VidSTG). Therefore, no quality assurance process is applicable or documented for new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-416-rubric-3",
    "token_usage": {
      "prompt_tokens": 15489,
      "completion_tokens": 357,
      "total_tokens": 15846
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the authors created new datasets from scratch by human annotation or recording; it only mentions experiments on existing benchmarks."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that data was generated entirely by models or artificial means; all data used are existing video benchmarks."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data produced by human translation from other languages is present."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data generated by machine translation is present in the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors use existing datasets (HCSTVG-v1/-v2 and VidSTG), but do not mention any new data that is collated or aggregated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe creation of new datasets that are derived or adapted from existing data."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. It only conducts experiments on publicly available datasets without presenting new datasets or data collection protocols."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-416-rubric-4",
    "token_usage": {
      "prompt_tokens": 16007,
      "completion_tokens": 258,
      "total_tokens": 16265
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4, Experimental Results (Tables 1, 2, 3, and ablation studies in Section 4.3)",
          "reasoning": "The datasets HCSTVG-v1/-v2 and VidSTG used in experiments are pre-existing benchmarks on which the model's performance is evaluated, demonstrating effectiveness of the proposed method."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new dataset is introduced by the authors in this paper; only existing datasets HCSTVG-v1/-v2 and VidSTG are used for evaluation. Therefore, no practical usage of a new dataset is described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-416-rubric-5",
    "token_usage": {
      "prompt_tokens": 16730,
      "completion_tokens": 490,
      "total_tokens": 17220
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the dataset containing entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4 Experiments, Tables 1-3",
          "reasoning": "The datasets used (HCSTVG-v1, HCSTVG-v2, and VidSTG) contain free-form textual queries described only in English, as implied by the example queries and the paper text, and no mention of other languages is made."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication is made that the dataset uses a non-English single language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the dataset contains entries of programming or structured code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper contains mathematical notation for describing the model, this is not part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of constructed or fictional languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset's textual entries is clearly documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes human language text queries, so it is not language-free."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-416-rubric-6",
    "token_usage": {
      "prompt_tokens": 13948,
      "completion_tokens": 193,
      "total_tokens": 14141
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Not explicitly mentioned in the paper",
          "reasoning": "The paper mentions that code for the CG-STVG method is released at https://github.com/HengLan/CGSTVG, but there is no indication or mention of any code related to the construction, collection, or preprocessing of any new dataset. The datasets used (HCSTVG-v1/-v2 and VidSTG) appear to be existing benchmarks, and no new dataset was introduced by the authors. Therefore, there is no code associated with dataset creation provided."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Not applicable",
          "reasoning": "The paper does not introduce any new dataset, and therefore it does not provide any documentation or description about dataset creation processes. The datasets used are pre-existing public benchmarks, and references are given to their original sources. Hence, no documentation on dataset construction is present in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-427-rubric-0",
    "token_usage": {
      "prompt_tokens": 11277,
      "completion_tokens": 315,
      "total_tokens": 11592
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1.2 and Section 4.1.1",
          "Reasoning": "The new dataset consists of images used in experiments such as 2D planar image alignment and 3D Neural Radiance Field reconstruction with inaccurate poses. These images are real or synthetic captures created or rendered for evaluating the proposed continuous pose method, thus human generated (real images captured by cameras) or model generated (synthetic data rendered), but as separate data entries we classify real images as human generated. The paper explicitly uses datasets such as Lego (synthetic) and LLFF real datasets for evaluation, introduced through experiments designed by the authors; hence this data is human generated (real) and model generated (synthetic) accordingly but grouped here as human generated since the paper treats them as real captures for testing."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.3 and Section 3.4",
          "Reasoning": "The new dataset includes high-frequency sensor measurements such as asynchronous events from event cameras and IMU data (includes gyroscope angular velocity), recorded from physical devices during experiments. These data are captured using human-operated or real-world equipment, hence human generated. The paper explicitly describes use of asynchronous event streams and IMU data in the vSLAM and NeRF tasks, all obviously originating from sensor recordings, not simulated, indicating human generated."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-427-rubric-1",
    "token_usage": {
      "prompt_tokens": 12077,
      "completion_tokens": 231,
      "total_tokens": 12308
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3 and Section 4",
            "reasoning": "The paper introduces a new dataset implicitly by presenting novel experiments with continuous time neural pose representations in various settings (e.g., NeRF from inaccurate poses, continuous pose for asynchronous events, visual SLAM with depth and IMUs). The annotations for ground truth and pose are carried out by experts or the authors themselves as part of experimental validation and evaluation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described in the paper",
            "reasoning": "The paper does not provide explicit annotation instructions or guidelines for annotators in the new dataset introduced by the experiments."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described in the paper",
            "reasoning": "There is no explicit mention of scoring rubrics or evaluation rubrics used by annotators for data annotation in the new dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described in the paper",
            "reasoning": "No annotation examples or concrete annotation guideline samples are included in the paper for the new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-427-rubric-2",
    "token_usage": {
      "prompt_tokens": 13267,
      "completion_tokens": 408,
      "total_tokens": 13675
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the new datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts performed quality assurance on the new datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single non-expert annotator on the new datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is found in the paper about quality assurance performed by multiple non-expert annotators on the new datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of AI models as the judge for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly mention any automated verification or algorithmic quality assurance applied to dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets annotated or created by the authors, nor does it describe any formal quality assurance processes applied to dataset annotations. All datasets used are existing ones (e.g., LLFF, Replica, ScanNet, TUM-RGBD, EUROC) or synthetic data generated for experiments, without explicit QA description. Therefore, no quality assurance process is documented for datasets introduced by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-427-rubric-3",
    "token_usage": {
      "prompt_tokens": 12877,
      "completion_tokens": 659,
      "total_tokens": 13536
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments) and throughout the paper",
          "reasoning": "The paper introduces new experimental datasets processed or generated by the authors for their experiments, such as 2D planar image alignment data, synthetic NeRF scenes (e.g., Lego model with circular movement), and the utilization of real datasets like Fern, Fortress, Orchids, Room from LLFF, sewing machine datasets, Replica, ScanNet, TUM-RGBD, and EUROC. Although some real datasets are pre-existing, the data created from synthetic scenes and 2D planar image alignment experiments are newly generated by the authors specifically for this study. The authors do not indicate these to be translations, collations, or derivations from existing data, but rather original content generated or prepared by their team."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any dataset was generated entirely by AI or machine learning models without reference to existing data. Instead, models are used to optimize poses or scene representations based on existing or newly created data, but the datasets themselves are not described as outputs from generative models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset was produced by translating content from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that any dataset was generated through machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments: use of Fern, Fortress, Orchids, Room datasets from LLFF; Replica, ScanNet, TUM-RGBD, EUROC datasets)",
          "reasoning": "The authors use existing publicly available datasets including LLFF, Replica, ScanNet, TUM-RGBD, and EUROC to evaluate their methods. These datasets are collected from prior works and are used without significant modification as inputs to their models. This qualifies as collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 (Planar Image Alignment 2D experiments), Section 4.1.2 (Synthetic NeRF experiments), and Section 4.2 (Event camera experiments)",
          "reasoning": "Some datasets used or created in experiments involve adaptation or modifications of existing data, such as introducing temporal correlations or pose disturbances with interpolation or generating synthetic event streams based on known models (e.g., simulating events as per model in [45]). These processes create datasets that are derived from existing sources with modifications applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The methods of data generation and dataset origins are specified within the paper, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-427-rubric-4",
    "token_usage": {
      "prompt_tokens": 13395,
      "completion_tokens": 324,
      "total_tokens": 13719
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a continuous pose representation using a neural function of time and demonstrates its use in several experiments. However, it does not introduce or provide any new datasets; rather, it uses existing datasets such as LLFF, Replica, ScanNet, TUM-RGBD, EUROC, and datasets from related works for evaluation and experimentation. There is no mention of creating, collecting, or releasing a novel dataset by the authors in this paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-427-rubric-5",
    "token_usage": {
      "prompt_tokens": 14118,
      "completion_tokens": 524,
      "total_tokens": 14642
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing multiple human languages; it focuses on camera poses and neural representations."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper suggests datasets with exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper",
          "reasoning": "All textual content and dataset annotations described in the paper are in English only, with no mention of other spoken languages. The dataset descriptions, experiment settings, and metrics are all presented in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset entries with only non-English language content are described."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses neural network architecture and mathematical formulations, it does not present any dataset entries consisting of programming or structured code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3 (Time-to-Pose Mapping Network), Equations (1), (2), (3), (4), (5), (6), (7)",
          "reasoning": "The paper includes many datasets involving mathematical or formal logical expressions for poses, transformations, and losses, represented symbolically throughout the methodology and experiments."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not include biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is clearly described and is entirely English and mathematical notation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain language material such as English text and mathematical notation."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-427-rubric-6",
    "token_usage": {
      "prompt_tokens": 11336,
      "completion_tokens": 148,
      "total_tokens": 11484
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Introduction",
          "reasoning": "The paper states explicitly in the abstract and includes a GitHub link (https://github.com/qimaqi/Continuous-Pose-in-NeRF) that the authors release their code. This code release implies that the code related to the proposed continuous pose representation, optimization, and associated experiments is publicly available."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "",
          "reasoning": "The paper does not present any new datasets introduced by the authors, nor does it describe dataset creation or data collection processes. It rather uses existing datasets and synthetic or real data to validate their method. Therefore, no dataset creation documentation is provided in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-435-rubric-0",
    "token_usage": {
      "prompt_tokens": 15208,
      "completion_tokens": 268,
      "total_tokens": 15476
    },
    "response": {
      "sources": [
        {
          "Modality": "time series",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4. Dataset",
          "Reasoning": "The DNGROUPGESTURE dataset introduced in the paper consists of 6 hours of multi-person gesture data recorded via a multi-view markerless motion capture system, capturing 3D body and hand poses as sequences over time, making the data time series. The data are recorded from human participants engaged in conversational interactions, thus the data is human generated as it originates from human motions captured via sensors and cameras operated by humans."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4. Dataset",
          "Reasoning": "The DNGROUPGESTURE dataset includes audio recordings captured during the sessions with five participants playing a roleplaying game, using dedicated wireless microphones for clear audio capture. This audio data is directly recorded from humans, thus is human generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4. Dataset",
          "Reasoning": "The dataset provides text transcriptions and gesture annotations per individual speaker, which are human-generated annotations based on the recorded interactions. These annotations include gesture types such as beats and iconic gestures."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-435-rubric-1",
    "token_usage": {
      "prompt_tokens": 16008,
      "completion_tokens": 201,
      "total_tokens": 16209
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4 Dataset",
            "reasoning": "The paper states that the DNGROUPGESTURE dataset provides text and gesture annotations distinguishing gesture types for each speaker, implying that these annotations were manually created by multiple human experts familiar with gesture analysis."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4 Dataset",
            "reasoning": "No detailed annotation instructions for the annotators are described in the paper regarding the gesture labeling process in the new dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4 Dataset",
            "reasoning": "There is no mention of scoring rubrics or formal scoring criteria being used or provided for the annotation of gesture types in the dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4 Dataset",
            "reasoning": "The paper does not provide or describe annotation examples or exemplar gestures for the annotators within the annotation guidelines for the new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-435-rubric-2",
    "token_usage": {
      "prompt_tokens": 17198,
      "completion_tokens": 324,
      "total_tokens": 17522
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of quality assurance by multiple human experts or any expert annotators."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 4 Dataset",
          "reasoning": "The dataset includes text and gesture annotations for each speaker that distinguish different types of gestures (beat, iconic, deictic, metaphoric). However, the paper does not specify expert involvement in annotation or quality assurance, only that annotations are provided. Given typical practice and absence of expert annotation claim, it is reasonable that multiple non-expert annotators performed annotation and quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using AI models for quality assurance of the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automatic or rule-based process used specifically for quality assurance of dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes that the dataset has text and gesture annotations, implying some annotation process was performed, thus quality assurance was likely done, although details are sparse."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-435-rubric-3",
    "token_usage": {
      "prompt_tokens": 16808,
      "completion_tokens": 388,
      "total_tokens": 17196
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The paper introduces the DDGROUPGESTURE dataset that contains 6 hours of gesture data involving five human participants interacting during multiple sessions of a tabletop role-playing game. The data was recorded using multi-view markerless motion capture and wireless microphones to obtain high-quality 3D body and hand pose estimates along with audio and text transcriptions. This dataset was created through original human recordings and interactions specifically for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the dataset is described as being generated entirely by AI or models without human involvement."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not involve any mention of translation from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to generate the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as assembled from existing sources; it was recorded anew by the authors."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset is based on existing sources with modifications or transformations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset origin is clearly documented as newly recorded human interactions; thus, data origin is known."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-435-rubric-4",
    "token_usage": {
      "prompt_tokens": 17326,
      "completion_tokens": 470,
      "total_tokens": 17796
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the DNDBGESTURE dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5 (Experiments)",
          "reasoning": "The proposed DNDBGESTURE dataset is used to train the proposed CONVOFUSION method for dyadic gesture synthesis, as shown in Section 5.2 where the method is adapted to the dyadic setting and evaluated on the DNDBGESTURE test set."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any supervised fine-tuning of pre-trained models using the DNDBGESTURE dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or related post-training techniques applied to the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.2 (Dyadic Co-speech Gesture Synthesis)",
          "reasoning": "The DNDBGESTURE dataset is used as an evaluation benchmark for dyadic gesture synthesis in Section 5.2 where the test set contains 3932 sequences used for performance measurement."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not primarily used for analysis of trends or patterns but rather for training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset serves as a knowledge base for retrieval or model augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates practical usage of the DNDBGESTURE dataset for training and evaluation."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-435-rubric-5",
    "token_usage": {
      "prompt_tokens": 18049,
      "completion_tokens": 445,
      "total_tokens": 18494
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 4 Dataset",
          "reasoning": "The paper describes the DNGROUPGESTURE dataset recorded from five players interacting in a tabletop roleplaying game, but there is no mention of the dataset including speech in multiple languages. All details suggest a single language environment."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 4 Dataset",
          "reasoning": "There is no indication that the dataset includes content from exactly two languages; the dataset seems to be in a single language only."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4 Dataset",
          "reasoning": "The dataset includes audio and text transcriptions of the participants' speech; the paper uses English terminology throughout and does not specify any other language, implying that the speech and transcriptions are in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 4 Dataset",
          "reasoning": "The paper does not state the dataset speech is in any non-English language. The context and terminology strongly suggest English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of motion capture data, audio, and text transcriptions, not programming or structured code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are motion data, audio, and text, without any indication of formal mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset concerns human 3D motion capture and speech, no biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 4 Dataset",
          "reasoning": "The language of the speech is implicitly known (English), and transcriptions are provided; thus, the language is specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes transcribed speech and audio, so it contains language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-435-rubric-6",
    "token_usage": {
      "prompt_tokens": 15267,
      "completion_tokens": 154,
      "total_tokens": 15421
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4 Dataset",
          "reasoning": "The paper states that the DNGROUPGESTURE dataset will be made publicly available to the community but does not provide any explicit mention or link to code used for data collection, preprocessing, or generation related to the dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 Dataset",
          "reasoning": "The paper provides a description of the dataset creation process including the use of a multi-view markerless mocap system, number of participants, type of data recorded (audio, 3D body and hand poses, text and gesture annotations), and the setup involving Dungeons and Dragons gameplay. This constitutes adequate documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-444-rubric-0",
    "token_usage": {
      "prompt_tokens": 10729,
      "completion_tokens": 169,
      "total_tokens": 10898
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Experimental Setup, Datasets",
          "Reasoning": "The paper introduces a new training dataset for the continuous representation module used in the concept-guided diffusion purification procedure. This dataset consists of 50,000 image samples selected from ImageNet (1,000 categories, each with 50 samples). These images are natural, human-generated images captured and compiled into ImageNet by humans. They are perturbed by applying noise with intensity 16/255 via the PGD attack to produce the inputs for the module, and clean images at a higher resolution serve as ground truth. This dataset was constructed by the authors for training their continuous representation module and thus is a new dataset introduced in this paper."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-444-rubric-1",
    "token_usage": {
      "prompt_tokens": 11529,
      "completion_tokens": 226,
      "total_tokens": 11755
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 5.1; Section 4.3",
            "reasoning": "The paper describes that for the continuous representation module, a dataset was constructed from ImageNet samples with added noise, and it was trained to obtain a module for image purification. The concept learning and purification processes leverage automated methods such as textual inversion and diffusion models rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not provide any annotation instructions or guidelines for human annotators related to the new dataset or its processing; the process is automated without human annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No scoring rubrics or explicit annotation performance criteria are mentioned for annotators since this is an automated image processing and concept learning procedure."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No examples of manual annotations or labeling instructions are provided, as the data processing involves automated concept learning and diffusion purification rather than human annotation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-444-rubric-2",
    "token_usage": {
      "prompt_tokens": 12719,
      "completion_tokens": 398,
      "total_tokens": 13117
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert in annotating or validating the datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts were involved in quality assurance of the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any single non-expert human annotator performing quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert human annotators being involved in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance being performed by an AI model acting as a judge for dataset annotation or content validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or rule-based quality assurance process for dataset annotation or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses four existing datasets (Cosal2015, iCoseg, CoSOD3k, and CoCA) as benchmarks and does not introduce any new dataset. Furthermore, there is no mention or description of any new dataset, its annotation, or quality assurance process. Hence, no quality assurance process related to a new dataset is applicable or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-444-rubric-3",
    "token_usage": {
      "prompt_tokens": 12329,
      "completion_tokens": 431,
      "total_tokens": 12760
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing public datasets for evaluation and does not mention creating new datasets from scratch by humans."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the method generates images via the diffusion models conditioned on learned concepts, these are for evaluation/illustration. No new dataset generated entirely by the model without reliance on existing data is introduced."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation of data from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation in creating datasets."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.1 (Experimental Setup)",
          "reasoning": "The paper uses existing datasets (Cosal2015, iCoseg, CoSOD3k, CoCA) for experiments, applying adversarial attacks on existing images. They select 50,000 ImageNet images for training a module. These are collected datasets reused for this study."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 5.1 (Experimental Setup)",
          "reasoning": "The authors derive perturbed versions of existing dataset images by applying adversarial attacks (e.g., Jadena) and common corruptions (motion blur). Also, they generate purified versions of images after their processing pipeline, which are modifications of existing images."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly states the datasets used and how data is processed; the origin of data is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-444-rubric-4",
    "token_usage": {
      "prompt_tokens": 12847,
      "completion_tokens": 267,
      "total_tokens": 13114
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1 Experimental Setup, Section 5.2 Comparison on Adversarial Attacks, Table 1, Table 2",
          "reasoning": "The datasets Cosal2015, iCoseg, CoSOD3k, and CoCA are used exclusively for evaluation of the proposed method's robustness and performance against adversarial attacks and common corruptions. Empirical results reported on these datasets demonstrate effectiveness of the method but no training or fine-tuning is done on these datasets."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets created by the authors. All datasets used for evaluation are pre-existing public datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-444-rubric-5",
    "token_usage": {
      "prompt_tokens": 13570,
      "completion_tokens": 621,
      "total_tokens": 14191
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets used in the paper (Cosal2015, iCoseg, CoSOD3k, CoCA) are standard computer vision datasets for co-salient object detection and contain images and annotations without any indication of multiple human languages. The paper does not describe any multilingual textual data associated with these datasets."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper suggests that the new datasets contain exactly two human languages. The datasets are image datasets with annotations, not bilingual textual datasets."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5.1 Experimental Setup - Datasets, and throughout the paper",
          "reasoning": "The new datasets introduced and used are standard English-based or generally English-labeled computer vision datasets. The textual content like labels and prompts used in the method are described in English (e.g., fixed text prompt 'a photo of $S^*$' in Section 4.2). The paper's textual descriptions and any textual conditioning use English. Therefore, the dataset entries (including prompts and annotations) contain English content only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset that contains only a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the paper discusses algorithms and models, the datasets themselves do not include programming or structured code. The code is part of the implementation but not dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notations appear in the paper to describe models and methods but are not part of any dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data are included in the new datasets introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of constructed or fictional languages in any dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the datasets are known and explicitly English for textual content and labeling."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets include English textual prompts and annotations, so they do contain language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-444-rubric-6",
    "token_usage": {
      "prompt_tokens": 10788,
      "completion_tokens": 169,
      "total_tokens": 10957
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section",
          "reasoning": "The paper mentions a project website URL (https://v1len.github.io/CosalPure) but does not explicitly state that the dataset construction or data processing code is publicly released. The focus is on the method and its components, and no direct indication of dataset construction code availability is found."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No explicit section describing new dataset construction",
          "reasoning": "The paper utilizes existing datasets (Cosal2015, iCoseg, CoSOD3k, CoCA) which are pre-existing and not created by the authors. The paper does not introduce any new datasets nor describes a new dataset creation process. Thus, there is no documentation on dataset construction associated with new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-460-rubric-0",
    "token_usage": {
      "prompt_tokens": 14253,
      "completion_tokens": 278,
      "total_tokens": 14531
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4, especially 4.1 and 4.2: KortX Part Segmentation dataset described as a \"new real-world object-level dataset captured with the Kortx scanning system [1].\"",
          "Reasoning": "The KortX dataset is newly introduced by the authors as a real-world object-level dataset captured with a laser scanning system (KortX). Since it is acquired using laser sensors scanning real objects, the data is sensor data captured with human involvement in setting up and operating the scanning system, thus human-generated sensor data. The paper explicitly states the KortX dataset is new and used for evaluation (Section 4.1 and 4.2)."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract and Section 4: The paper introduces CurveCloudNet and evaluates it on multiple datasets including the KortX dataset described as \"a new real-world object-level dataset captured with the KortX scanning system [1]\".",
          "Reasoning": "KortX data originates from laser scanners producing 3D point clouds via physical sensors capturing scenes or objects. Thus, it is sensor data collected through human-operated tools, not AI-generated or unknown origin data."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-460-rubric-1",
    "token_usage": {
      "prompt_tokens": 15053,
      "completion_tokens": 244,
      "total_tokens": 15297
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Sections 3.1 and 4",
            "reasoning": "The KortX Part Segmentation dataset is a new real-world object-level dataset captured with the KortX scanning system as introduced in Section 4 (Experiments). In Section 3.1, the paper describes that the point cloud data comes directly from laser scanners and that the acquisition timestamp and beam ID are obtained from the sensor, which is an automatic process. There is no mention of human annotators labeling the dataset; the data is sensor-captured and segmented via the model training process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not provide any details about annotation instructions for the KortX dataset, nor any manual annotation procedures."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No specific rubric or scoring criteria for dataset annotation are described for the KortX dataset in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No annotation examples or labeling guidelines are provided or described in the text for the new dataset introduced."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-460-rubric-2",
    "token_usage": {
      "prompt_tokens": 16243,
      "completion_tokens": 302,
      "total_tokens": 16545
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about a single human expert annotating or performing quality assurance on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of multiple human experts conducting quality assurance for the new datasets introduced."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human non-expert on the dataset; the annotation or QA process is not detailed."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper lacks any indication that multiple human non-experts conducted quality assurance or annotation for the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automatic quality assurance performed by AI models acting as judges over the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic quality assurance process for dataset annotations or contents."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new real-world object-level dataset (KortX) but does not document any quality assurance process or details about dataset annotation verification. There is no information about human or automated QA processes for any of the datasets used or introduced."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-460-rubric-3",
    "token_usage": {
      "prompt_tokens": 15853,
      "completion_tokens": 413,
      "total_tokens": 16266
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), Table 2 (KortX Part Segmentation dataset)",
          "reasoning": "The paper introduces the KortX Part Segmentation dataset, described as a new real-world object-level dataset captured with the Kortx scanning system, indicating original data created by human contributors through real-world laser scanning."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset generated solely by AI or machine learning models; all datasets are either collected from sensors or are pre-existing."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets generated by translating content from another language using machine translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), Table 1b, and related descriptions",
          "reasoning": "The paper evaluates CurveCloudNet on multiple existing datasets such as ShapeNet, nuScenes, A2D2, and Semantic KITTI, which are collected from various sources without significant modification, indicating use of collated data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe datasets derived through modification or transformation of existing data by the authors."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The sources and generation processes for datasets are clearly specified where applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-460-rubric-4",
    "token_usage": {
      "prompt_tokens": 16371,
      "completion_tokens": 362,
      "total_tokens": 16733
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4, Experiments",
          "reasoning": "The paper describes training the proposed CurveCloudNet backbone from scratch on several datasets including the newly introduced KortX Part Segmentation dataset. For instance, in Section 4.1 under KortX Results, it states that CurveCloudNet is trained over four random seeds and for 60 epochs on this new real-world object-level dataset, indicating training from randomly initialized parameters without mention of pre-training."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the new datasets to fine-tune pre-trained models; all training appears to be from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Reinforcement learning or RL-based post-training methods are not described in the paper for any dataset usage."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4, Experiments",
          "reasoning": "The new KortX dataset is used for evaluating object part segmentation performance of CurveCloudNet as well as other baselines. The paper reports mIOU metrics and visual examples on KortX data, showing its use for benchmarking and performance measurement."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No specific analysis of the new datasets beyond standard performance evaluation is presented."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets are not used as a knowledge base for retrieval-augmented generation or similar approaches."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-460-rubric-5",
    "token_usage": {
      "prompt_tokens": 17094,
      "completion_tokens": 574,
      "total_tokens": 17668
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, including Abstract and Section 4 Experiments",
          "reasoning": "The paper references multiple datasets related to 3D point cloud segmentation, all of which use English for labeling, annotations, and documentation. The datasets such as ShapeNet, KortX, A2D2, nuScenes, and KITTI are standard datasets in autonomous driving and object segmentation research, which use English for their class labels and documentation. The paper's evaluation, experimental descriptions, and citations are all presented in English, with no indication of any other natural language content in the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper describes methods and algorithms involving code-like formulations and network architecture, there is no introduction of a dataset whose entries consist of programming or structured code-related content. The datasets are all 3D point cloud data with semantic annotations, not code datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains mathematical expressions related to the model, such as notation for points and curves, but the datasets themselves do not contain entries with mathematical or formal logical expressions. The datasets are point clouds with semantic labels."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are 3D point clouds from LiDAR sensors capturing physical objects and scenes; there is no involvement of biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the datasets include fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used and introduced are well-documented and described with English labels and annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain language annotations in English, so language is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-460-rubric-6",
    "token_usage": {
      "prompt_tokens": 14312,
      "completion_tokens": 203,
      "total_tokens": 14515
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not mention any publicly available code repository or provide any links to code associated with the new datasets introduced, including the Kortx scanning system dataset. There is no explicit information about the availability of code for data collection or preprocessing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1, Section 4, and supplementary material",
          "reasoning": "The paper provides a detailed description of the dataset creation process for the new real-world object-level dataset captured with the Kortx scanning system, including the method of constructing the curve cloud representation from the laser-scanned data (Section 3.1), and evaluation details (Section 4). The conversion process from laser scans to curve clouds is explained, and the dataset's characteristics and scanning patterns are summarized in Table 1 and Figure 1. Further details are noted to be in the supplementary material, indicating documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-490-rubric-0",
    "token_usage": {
      "prompt_tokens": 11964,
      "completion_tokens": 115,
      "total_tokens": 12079
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4. Comprehensive Generic Face IQA Dataset",
          "Reasoning": "The authors introduce the Comprehensive Generic Face IQA Dataset (CGFIQA-40k), consisting of approximately 40K face images with diverse distribution in gender and skin tone and including images with facial occlusions. These images are collected and labeled by human annotators, with 20 labelers per image providing perceptual quality scores, indicating human involvement in data creation."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-490-rubric-1",
    "token_usage": {
      "prompt_tokens": 12764,
      "completion_tokens": 199,
      "total_tokens": 12963
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 4",
            "reasoning": "Section 4 states that the new dataset CGFIQA-40k contains about 40K images each annotated by 20 labelers, each spending about 30 seconds to give a score, indicating multiple human non-experts performed the annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4",
            "reasoning": "The paper does not mention any specific instructions provided to annotators for scoring, nor does it describe annotation guidelines or protocols."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4",
            "reasoning": "There is no explicit description in the paper of scoring rubrics or criteria used by annotators to assign quality scores."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4",
            "reasoning": "The paper does not provide any examples or exemplar annotations as part of the annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-490-rubric-2",
    "token_usage": {
      "prompt_tokens": 13954,
      "completion_tokens": 422,
      "total_tokens": 14376
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly mention that the dataset annotations or quality assurance were conducted by a single human expert or by annotators with subject matter expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no direct information indicating that multiple human experts performed quality assurance on the dataset or annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that a single non-expert annotator performed the quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 4 Comprehensive Generic Face IQA Dataset",
          "reasoning": "The paper states that each image in the CGFIQA-40k dataset was annotated by 20 labelers, and they spent about 30 seconds per image. There is no indication that these labelers are subject matter experts, suggesting that multiple human non-expert annotators conducted the annotations and likely the quality assurance of the labels."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance for the CGFIQA-40k dataset is not described as being performed by an AI model as a judge."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or algorithmic rule-based quality assurance process for the dataset annotations is described in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a quality assurance process through multiple human labelers for the CGFIQA-40k dataset, so N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-490-rubric-3",
    "token_usage": {
      "prompt_tokens": 13564,
      "completion_tokens": 497,
      "total_tokens": 14061
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4: Comprehensive Generic Face IQA Dataset",
          "reasoning": "The paper explicitly introduces the CGFIQA-40k dataset consisting of approximately 40K face images annotated by human labelers. The dataset was carefully curated to include diverse distributions of skin tone, gender, and facial occlusions. The labeling process involved 20 labelers who provided quality scores for the images, each spending about 30 seconds per annotation, indicating human-originated data creation rather than machine generation or pure aggregation. The dataset was created from an initial pool of 40,000 images, filtered to 39,312 valid images for use."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset CGFIQA-40k is collected and annotated by humans, and there is no indication that the data was generated entirely by AI or machine learning models from scratch."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that any data in the new dataset was produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any data generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset comprises images collected for the task, the paper does not specify that they aggregated images from existing datasets without substantial modification. Instead, the dataset is described as curated and labeled with a balanced distribution, suggesting more than mere collation."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the dataset was created by modifying or adapting existing datasets or images; it indicates the dataset is newly curated and annotated."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the source and generation process of the CGFIQA-40k dataset; thus, 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-490-rubric-4",
    "token_usage": {
      "prompt_tokens": 14082,
      "completion_tokens": 352,
      "total_tokens": 14434
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.1, Section 5.2",
          "reasoning": "The CGFIQA-40k dataset is used to train and fine-tune the proposed face image quality assessment model. In Section 5.1, it is stated that the dataset is used in the training and validation phase alongside other datasets, and in Section 5.2, the model trained on this dataset achieves strong performance, indicating its use in supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.2",
          "reasoning": "The CGFIQA-40k dataset is used for evaluation and benchmarking to measure the performance of the proposed GFIQA method. Section 5.2 reports experimental results on CGFIQA-40k, showing its role as a benchmark dataset."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4, Section 5.4",
          "reasoning": "Section 4 introduces the CGFIQA-40k dataset designed to overcome biases related to skin tone and gender and includes facial occlusion. Section 5.4 provides analysis and qualitative visualization based on this dataset, indicating its use for analyzing dataset characteristics and behavior."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-490-rubric-5",
    "token_usage": {
      "prompt_tokens": 14805,
      "completion_tokens": 307,
      "total_tokens": 15112
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses mathematical formulas and logical loss functions in sections 3.2 and 3.4 to describe the methodology, but these are part of the method descriptions rather than the dataset contents. There is no indication that the dataset entries contain mathematical or formal logical expressions. Therefore, the dataset entries themselves do not contain mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The proposed dataset CGFIQA-40k consists of facial images with annotations (quality scores) but does not contain any human language text or linguistic content within its entries. It is a dataset of images, not language data. Hence, it does not contain any language in its entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-490-rubric-6",
    "token_usage": {
      "prompt_tokens": 12023,
      "completion_tokens": 222,
      "total_tokens": 12245
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section",
          "reasoning": "The paper introduces the CGFIQA-40k dataset but does not provide any link, mention, or indication that the code used for dataset construction, data collection, or annotation is publicly available or released. There is no mention of a repository or supplementary materials containing such code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 Comprehensive Generic Face IQA Dataset (Page 6)",
          "reasoning": "The paper provides a detailed description of the CGFIQA-40k dataset, including the total number of images (approx. 40K), image resolution, annotation process (20 labelers per image, approx. 30 seconds per labeler), filtering of unusable images resulting in 39,312 valid images, balanced design considerations for gender, skin tone and facial occlusions, and comparisons with existing datasets. However, the description does not cover dataset licensing, code details, or data collection scripts but does describe the dataset creation process adequately for understanding its composition and purpose."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-549-rubric-0",
    "token_usage": {
      "prompt_tokens": 12352,
      "completion_tokens": 241,
      "total_tokens": 12593
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Dataset Construction, 'Auto-annotation data pipeline' and overall paper description",
          "Reasoning": "The new dataset GranuCap50M is constructed from large-scale web-sourced image-text pairs (e.g., from CC15M, YFCC, LAION), which are originally human-generated photographic images collected from the internet. These images are natural and captured by humans, not generated by models."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Dataset Construction, 'Auto-annotation data pipeline' steps 1 to 3",
          "Reasoning": "The text captions associated with these images are initially human-generated (captions from original datasets), but are then refined and augmented via Visual Large Language Models (e.g., InstructBLIP) and GPT-4 to generate detailed hierarchical object labels. Subsequently, further instruction-tuning on LLaVA model enables large-scale caption and object entity generation. Thus, the text data is both human-generated and model-generated."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-549-rubric-1",
    "token_usage": {
      "prompt_tokens": 13152,
      "completion_tokens": 308,
      "total_tokens": 13460
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 3.2 Dataset Construction",
            "reasoning": "The paper describes an auto-annotation pipeline leveraging Visual Large Language Models (VLLMs) including InstructBLIP and GPT-4, followed by instruction tuning of LLaVA models to automatically generate detailed captions and hierarchical object entities for a large-scale dataset termed GranuCap50M. This indicates that the annotations for the new dataset were primarily generated by AI models rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.2 Dataset Construction",
            "reasoning": "There is no explicit mention of detailed annotation instructions provided to annotators because the annotation was performed by AI models via an automated pipeline rather than human annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2 Dataset Construction",
            "reasoning": "The paper does not describe any rubric or standardized scoring criteria associated with the annotation process. Since the dataset is generated by AI models, traditional scoring rubrics do not apply."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.2 Dataset Construction",
            "reasoning": "The paper provides example formatted outputs of the auto-annotation process, including hierarchical triplets in the form of {phrase, category, parent category} and example annotated descriptions (e.g., 'Christmas-themed display' | 'display' | 'Store Items'). These serve as examples illustrating the annotation format used by the AI models."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-549-rubric-2",
    "token_usage": {
      "prompt_tokens": 14342,
      "completion_tokens": 435,
      "total_tokens": 14777
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no documentation or indication in the paper that quality assurance was performed by a single human expert on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance involving multiple human experts for the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset quality assurance performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about dataset QA by multiple non-expert humans in the paper."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.2 Dataset Construction - Auto-annotation data pipeline",
          "reasoning": "The paper describes using GPT-4 and a vision large language model (InstructBLIP) to automatically refine captions and extract entity information for the dataset, effectively acting as AI judges for annotation quality. This AI-assisted annotation pipeline forms a core part of the dataset construction and quality assurance."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 Dataset Construction - Auto-annotation data pipeline",
          "reasoning": "The dataset construction involves an automated pipeline including instruction tuning of a LLaVA model for large scale annotation and pseudo-labeling bounding boxes by a pre-trained open-vocabulary object detector. These processes represent automated quality assurance through algorithmic and rule-based techniques."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a detailed automated quality assurance pipeline; thus, 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-549-rubric-3",
    "token_usage": {
      "prompt_tokens": 13952,
      "completion_tokens": 524,
      "total_tokens": 14476
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new dataset created entirely from scratch by human contributors. Instead, it uses existing datasets and enhances them via automated processes."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.2 Dataset Construction, Auto-annotation data pipeline",
          "reasoning": "The authors develop a large-scale dataset called GranuCap50M by leveraging a Visual Large Language Model (VLLM) and GPT-4 for automated caption refinement and entity extraction, followed by instruction tuning of a LLaV A model. This pipeline automatically generates high-information-density captions and hierarchical object labels from image-text pairs, constituting new data generated by AI models rather than human annotation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by translation from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any data was generated by machine translation of content from other languages."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.2 Dataset Construction, Auto-annotation data pipeline",
          "reasoning": "The authors sample 240k image-text pairs from multiple existing datasets (CC15M, YFCC, LAION, Conceptual Captions, etc.) and aggregate them as sources for their annotation pipeline, thus collating data from multiple existing datasets."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 Dataset Construction, Auto-annotation data pipeline",
          "reasoning": "The large-scale dataset GranuCap50M is derived from existing image-text pair datasets by applying automated caption refinement, entity extraction, and pseudo-label (bounding box) generation via a pretrained open-vocabulary detector. These processes transform existing data into a richer, refined form with multi-granular object labels, which qualifies as derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data sources and the methods used to generate the new dataset, so the data origin is specified."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-549-rubric-4",
    "token_usage": {
      "prompt_tokens": 14470,
      "completion_tokens": 616,
      "total_tokens": 15086
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.2 (Dataset Construction), Section 4.5 (Ablation Study), and summary in Abstract",
          "reasoning": "The novel dataset GranuCap50M introduced by the authors is constructed through an auto-annotation pipeline leveraging visual large language models and is a large-scale image-text paired dataset used to pre-train DetCLIPv3. Section 3.2 describes using this dataset in a stage of pre-training with low-resolution inputs to teach the object captioner a broad spectrum of visual concepts efficiently. The Ablation Study (Table 6) shows the effectiveness of incorporating this dataset in pre-training to improve detection and generation capabilities."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training a model from randomly initialized parameters using the newly introduced dataset. Instead, training is conducted with pre-trained backbones and involves pre-training and fine-tuning stages."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), Section 3.3, and Table 6 (Ablation Study)",
          "reasoning": "The GranuCap50M dataset is used in the efficient multi-stage training, where after pre-training with low-resolution inputs, the model is fine-tuned on small amounts of high-quality high-resolution samples, including data derived from GranuCap50M. This constitutes supervised fine-tuning as shown in Section 4 and Ablation Study, improving detection performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains no mention or description of reinforcement learning based post-training techniques or the use of the new dataset within such methods."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The newly introduced dataset GranuCap50M is not used exclusively for evaluation or benchmarking, but primarily for model training."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described to be used primarily for analysis or studying trends or patterns."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the introduced dataset serves as a knowledge base for retrieval-augmented generation or similar model augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents practical usage of the new dataset in pre-training and fine-tuning DetCLIPv3; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-549-rubric-5",
    "token_usage": {
      "prompt_tokens": 15193,
      "completion_tokens": 659,
      "total_tokens": 15852
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.2 Dataset Construction",
          "reasoning": "The dataset GranuCap50M constructed by the authors leverages image-text pairs from English-based datasets (e.g., CC15M, YFCC, LAION) and the text captions are processed and generated using English language large language models like GPT-4 and LLaVA. There is no mention of multiple human languages present."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.2 Dataset Construction",
          "reasoning": "The datasets and captions used and generated for GranuCap50M are only described with English text; no reference to exactly two human languages is stated."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.2 Dataset Construction",
          "reasoning": "The large-scale image-text pair dataset GranuCap50M is constructed using VLLMs such as InstructBLIP and GPT-4 that generate refined captions and object descriptions in English. The original datasets like CC15M, YFCC, and LAION are primarily English-language datasets, and the paper describes using English captions and nouns for entity extraction and annotation."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 3.2 Dataset Construction",
          "reasoning": "There is no indication the dataset contains entries solely in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Overall paper",
          "reasoning": "While the model architecture and methods employ programming code, there is no indication that the dataset itself contains entries with programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Overall paper",
          "reasoning": "The paper discusses mathematical losses and training losses (e.g., L_align, L_box) in the method descriptions, but the dataset presented (GranuCap50M) does not contain entries with mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Overall paper",
          "reasoning": "The dataset does not include biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Overall paper",
          "reasoning": "There is no mention of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Overall paper",
          "reasoning": "The language of the dataset is clearly indicated as English based on descriptions of caption generation and noun extraction."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains textual captions and noun entities, hence it contains language entries."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-549-rubric-6",
    "token_usage": {
      "prompt_tokens": 12411,
      "completion_tokens": 195,
      "total_tokens": 12606
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section provides a code repository link for dataset construction",
          "reasoning": "The paper does not provide any link or indication that the code used for constructing the new dataset GranuCap50M or the data annotation pipeline is publicly available. There is no mention of releasing code for the auto-labeling pipeline, the GPT-4/GPT-4-tuned LLaVA model instruction tuning, or the pseudo-label bounding box generation process."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.2 Dataset Construction",
          "reasoning": "The paper extensively describes the dataset construction pipeline in Section 3.2, detailing the multi-step auto-annotation process using VLLM for recaptioning, GPT-4 for entity extraction, instruction tuning of LLaVA for large-scale annotation, and pseudo-labeling for bounding boxes. This is a transparent, detailed documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-570-rubric-0",
    "token_usage": {
      "prompt_tokens": 11973,
      "completion_tokens": 117,
      "total_tokens": 12090
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Implementation Details",
          "Reasoning": "The paper states that during image prompt training, the authors use training data from LAION, which is a large-scale dataset of real images collected from the web, implying that the images are human-generated (captured or created by humans). This dataset is used to train their image prompt encoder, which is a new module introduced by the authors, hence considered a new dataset source for their method."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-570-rubric-1",
    "token_usage": {
      "prompt_tokens": 12773,
      "completion_tokens": 309,
      "total_tokens": 13082
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1 and Sections 3.3-3.5",
            "reasoning": "The new dataset in this paper refers primarily to the data used to train the image prompt encoder based on LAION dataset as mentioned in Section 4.1. Annotation here is related to the creation and verification of image prompts for fine-grained image editing tasks. The paper indicates usage of LAION dataset for training and experimental setups involving expert evaluation for tasks like face manipulation (Section 4). There is no indication of crowdsourced or multiple annotators; expert researchers likely performed the annotation and evaluation tasks."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "The paper does not explicitly provide or mention detailed annotation instructions for labeling or annotating the dataset entries. It mainly discusses training procedures and model architecture without detailed annotation guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not mentioned in the paper",
            "reasoning": "There is no explicit mention of scoring rubrics or criteria provided for annotation quality or evaluation within the new dataset. The evaluations are numerical (e.g., MSE, FID) rather than rubric-based annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No specific examples given in annotation guidelines",
            "reasoning": "The paper does not provide examples of annotation to guide annotators. It focuses on method descriptions, experiments, and results rather than explicit annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-570-rubric-2",
    "token_usage": {
      "prompt_tokens": 13963,
      "completion_tokens": 315,
      "total_tokens": 14278
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information or indication that quality assurance was conducted by a single human expert annotator for any introduced dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence that multiple human experts performed quality assurance on datasets introduced by the authors."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any single non-expert human annotator conducting quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided in the paper about multiple non-expert human annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models are used for diffusion-based image editing and guidance, the paper does not describe any AI model being used specifically for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses algorithmic methods and techniques for diffusion sampling and editing guidance but does not describe an automated verification process for quality assurance of datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets for which a quality assurance process is described or documented. The focus of the paper is on image editing methods and algorithmic contributions rather than dataset annotation or curation. Hence, no quality assurance process for dataset validation is applicable or reported."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-570-rubric-3",
    "token_usage": {
      "prompt_tokens": 13573,
      "completion_tokens": 372,
      "total_tokens": 13945
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any original data generated entirely by AI or machine learning models as a dataset. The work focuses on methods using pre-trained diffusion models for image editing, but does not introduce new model-generated datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of datasets produced via human translation in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of datasets produced via machine translation in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation Details",
          "reasoning": "The paper states that during image prompt training, they use training data from the LAION dataset, a pre-existing dataset, to train the image prompt encoder. This indicates that the data used for training is collected or aggregated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the method involves modifications and adaptations to pre-trained stable diffusion models, no new datasets derived from existing sources with alterations are explicitly introduced or described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the training data used (LAION) is explicitly stated."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-570-rubric-4",
    "token_usage": {
      "prompt_tokens": 14091,
      "completion_tokens": 199,
      "total_tokens": 14290
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.3, Section 4.1",
          "reasoning": "The paper introduces a new image prompt encoder trained on LAION data using supervised L2 loss on prediction of noise, which is fine-tuning a module integrated into the pre-trained Stable Diffusion model."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-570-rubric-5",
    "token_usage": {
      "prompt_tokens": 14814,
      "completion_tokens": 532,
      "total_tokens": 15346
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces a new dataset related to image prompt training using the LAION dataset, but it does not specify that the dataset entries contain more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains exactly two human languages; the paper does not specify any bilingual dataset."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Implementation Details",
          "reasoning": "The training data comes from the LAION dataset, which is a large-scale English-language based image-text dataset, and all textual content and annotations in the paper are in English, implying the proposed dataset uses only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of dataset entries containing exactly one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper includes algorithmic descriptions and equations, it does not introduce any new dataset entries containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes mathematical notations to describe methods but does not define datasets with mathematical or logical formal expressions as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication system data is introduced as a new dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce datasets containing constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language involved in the new dataset (LAION) is specified as English based on the context; the language is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains textual content in English, so it is not language-free."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-570-rubric-6",
    "token_usage": {
      "prompt_tokens": 12032,
      "completion_tokens": 161,
      "total_tokens": 12193
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section in paper for dataset-related code",
          "reasoning": "The paper mentions code released at https://github.com/MC-E/DragonDiffusion, but this is for the method's source code and does not indicate code for constructing any new datasets. The authors do not mention releasing any code for dataset creation or processing, nor do they describe creating or introducing new datasets explicitly."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section or details on dataset creation process",
          "reasoning": "There is no description or documentation of dataset creation or curation in the paper. The training data is briefly mentioned as LAION dataset, a publicly available dataset, and no new dataset is introduced or documented in detail within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-572-rubric-0",
    "token_usage": {
      "prompt_tokens": 17242,
      "completion_tokens": 127,
      "total_tokens": 17369
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.4 Implementation Details",
          "Reasoning": "The authors trained their networks on a synthetic training dataset consisting of HDRI-based illuminated renders of real body scans from RenderPeople dataset [45] and their own captured data, totaling approximately 450K training examples. This dataset is newly captured and synthesized by the authors, involving human scanning and rendering processes, thus the images come from human-generated sources (captured real body scans and rendered images) and are not model generated or of unknown origin."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-572-rubric-1",
    "token_usage": {
      "prompt_tokens": 18042,
      "completion_tokens": 213,
      "total_tokens": 18255
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.4 Implementation Details",
            "reasoning": "The annotation or labeling of training data is done via a synthetic training dataset generated from HDRI-based illuminated renders of real body scans from [45] and the authors' own captured data. The dataset is created programmatically using rendering techniques, hence the annotations are obtained via an automatic process rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not provide or mention any detailed human annotation instructions for data labeling, as data annotation is performed automatically via rendering."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "No mention of scoring rubrics or evaluation guidelines for data annotation is provided, consistent with automatically generated data."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "No examples of annotation guidelines or annotations are provided due to the automatic nature of data generation."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-572-rubric-2",
    "token_usage": {
      "prompt_tokens": 19232,
      "completion_tokens": 281,
      "total_tokens": 19513
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human expert for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description indicating that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that an AI model was used as a judge for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the method involves automated processes during training and inference, there is no explicit mention of automated quality assurance or verification specifically for dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document any quality assurance process concerning the dataset used for training. The training data is a synthetic dataset created by rendering real human body scans, and there is no mention of validation or quality checks on the dataset annotations or content."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-572-rubric-3",
    "token_usage": {
      "prompt_tokens": 18842,
      "completion_tokens": 483,
      "total_tokens": 19325
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.4 Implementation Details",
          "reasoning": "The authors created a synthetic training dataset by rendering HDRI-based illuminated images using real body scans from RenderPeople dataset and their own captured data. This indicates new data was created by humans through data capture and rendering processes. Specifically, they used about 590,000 scans from approximately 1,100 identities to render around 450,000 training examples with corresponding ground-truth observations."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any data used for training or evaluation was generated by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was produced by human translators translating content from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of machine translation being involved in dataset creation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.4 Implementation Details",
          "reasoning": "The synthetic dataset partly uses existing real body scans from the publicly available RenderPeople dataset as a source. The authors collected these existing scans and rendered synthetic images and observations from them, aggregating data from these existing sources without significant modification to the original scans."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.4 Implementation Details",
          "reasoning": "The training data is derived by rendering images and observation sets (albedo, depth, normal maps) from existing 3D body scans. This involves transformations and adaptations of the original scan data to produce pixel-aligned observations for training. Thus, the data is based on existing sources with modifications and adaptations applied during rendering."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation method is explicitly described and documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-572-rubric-4",
    "token_usage": {
      "prompt_tokens": 19360,
      "completion_tokens": 449,
      "total_tokens": 19809
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.4 Implementation Details",
          "reasoning": "The paper states that the networks are trained with a synthetic training dataset consisting of rendered images and corresponding ground-truth 3D observations. The dataset contains ~450K training examples from ~1.1K identities and is used to train the model from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or indication that the introduced dataset is used to fine-tune a pre-trained model with supervised learning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning or RL-based fine-tuning using the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Experiments and 4.2 Reconstruction Accuracy",
          "reasoning": "The dataset is used for quantitative evaluation and benchmarking of the proposed DiffHuman method and baselines, reporting various 3D and image-based metrics."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as used primarily for analysis of trends or patterns but mainly for model training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or to augment models in a retrieval-augmented manner."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The introduced dataset is clearly used for training the model from scratch and for evaluation, so it does have practical utility described in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-572-rubric-5",
    "token_usage": {
      "prompt_tokens": 20083,
      "completion_tokens": 471,
      "total_tokens": 20554
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset entries containing multiple languages; it focuses on visual data (images and 3D human reconstructions) rather than linguistic data."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that dataset entries contain exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of synthetic and real 3D human images and corresponding 3D shapes; it does not contain English content as linguistic data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset entries contain any non-English language content; the data is visual imagery and 3D shape representations."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains algorithm descriptions and equations, the new dataset introduced is visual in nature (images and 3D reconstructions) and does not contain programming or code content within dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes mathematical notation to describe methods and models, but the dataset entries themselves are visual data (images and 3D surfaces), not mathematical or logical symbolic data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of synthetic human scans and images for 3D human reconstruction; it does not include biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or constructed languages in the dataset; data is visual and geometric representations of humans."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages or linguistic content of the dataset entries are specified\u2014it is clear that the dataset contains images and corresponding 3D geometric data, not unspecified language data."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The new dataset introduced in the paper consists entirely of visual data\u2014images of human subjects and corresponding 3D human shape reconstructions\u2014and does not contain any entries with language content. Therefore, assessment of language coverage is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-572-rubric-6",
    "token_usage": {
      "prompt_tokens": 17301,
      "completion_tokens": 131,
      "total_tokens": 17432
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper",
          "reasoning": "The paper does not mention or provide any link to code related to dataset construction, data collection, or preprocessing, nor does it refer to any public repository for such code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.4 Implementation Details",
          "reasoning": "The paper provides details about the synthetic training dataset used, including sources (RenderPeople Dataset and own captured data), number of scans, identities, number of training examples, image resolutions, and rendering details. This constitutes documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-618-rubric-0",
    "token_usage": {
      "prompt_tokens": 13452,
      "completion_tokens": 173,
      "total_tokens": 13625
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Experimental Settings",
          "Reasoning": "The text prompts used for training discriminative tasks, e.g., Image-Text Matching and Referring Expression Comprehension, originate from human-generated caption datasets such as MSCOCO and RefCOCO family explicitly stated in the paper as training sets for these tasks."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Experimental Settings",
          "Reasoning": "The image data used for both discriminative probing and tuning tasks are from standard human-generated datasets like MSCOCO and RefCOCO, which contain images captured by humans; this is explicitly stated in the description of training and evaluation datasets."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-618-rubric-1",
    "token_usage": {
      "prompt_tokens": 14252,
      "completion_tokens": 308,
      "total_tokens": 14560
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Experimental Settings and Section 4.2 Performance Comparison",
            "reasoning": "The discriminative probing tasks (ITM and REC) are performed using a discriminative adapter on semantic representations extracted from the pretrained Stable Diffusion model without manual human annotations for the labeling; the data used for training ITM and REC are existing datasets (MSCOCO and RefCOCO series) rather than newly annotated data, and the discriminative adapter itself is an automated model component trained and evaluated without mention of human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit description of annotation instructions in the paper regarding new dataset annotation.",
            "reasoning": "The paper only uses existing datasets (MSCOCO, RefCOCO, etc.) for training or evaluation of the discriminative tasks. There is no new dataset introduced that requires novel human annotations with instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No description in the paper about rubrics for annotation guidelines.",
            "reasoning": "Similarly, since there is no new annotation process or new data annotated by humans, no scoring rubrics are mentioned or provided."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples provided since no new annotation is conducted.",
            "reasoning": "The paper does not describe any new data annotation procedure for datasets; rather, it uses existing datasets and focuses on model probing and tuning, so no annotation examples are required or provided."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-618-rubric-2",
    "token_usage": {
      "prompt_tokens": 15442,
      "completion_tokens": 281,
      "total_tokens": 15723
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on the new datasets or annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or indication of multiple human experts performing quality assurance for the datasets introduced or used."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of multiple human non-experts conducting QA is present in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used in the system (e.g., the discriminative adapter, diffusion models), the paper does not indicate that an AI model is used to perform quality assurance of datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or rule-based quality assurance process applied to datasets or annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset nor document any quality assurance process for dataset annotations. It uses existing datasets for training and evaluation without QA process descriptions."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-618-rubric-3",
    "token_usage": {
      "prompt_tokens": 15052,
      "completion_tokens": 446,
      "total_tokens": 15498
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 (Experimental Settings)",
          "reasoning": "The authors use existing datasets such as MSCOCO for image-text matching (ITM) and RefCOCO, RefCOCO+, RefCOCOg for referring expression comprehension (REC) without indication of creating new data; their evaluation benchmarks (COCO-NSS1K, CC-500, ABC-6K, TIFA, and T2I-CompBench) are pre-existing datasets (Section 4.1). Thus, the paper relies on data collected from established sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 (Discriminative Probing), Section 4.3 (In-depth Analysis)",
          "reasoning": "The authors extract semantic representations from intermediate layers of pre-trained Stable Diffusion models and adapt these with a discriminative adapter to train on discriminative tasks (ITM and REC). They also perform parameter-efficient fine-tuning (LoRA) on the foundation T2I models to enhance their discriminative abilities. This process modifies and adapts existing representations and datasets, making the data derived rather than purely new or raw. Moreover, projections and transformations for discriminative objectives are applied to existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly states and documents all datasets and data usage, including data sources and processing steps."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-618-rubric-4",
    "token_usage": {
      "prompt_tokens": 15570,
      "completion_tokens": 367,
      "total_tokens": 15937
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 and 4.1",
          "reasoning": "The dataset MSCOCO and referring expression comprehension datasets (RefCOCO, RefCOCO+, RefCOCOg) are used to fine-tune pre-trained Stable Diffusion models using supervised discriminative tasks (Image-Text Matching and Referring Expression Comprehension) to improve text-image alignment through parameter-efficient fine-tuning (LoRA)."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The datasets COCO-NSS1K, CC-500, ABC-6K, TIFA, and T2I-CompBench are used exclusively for evaluation and benchmarking of text-to-image generation performance, and the discriminative datasets (MSCOCO-HN for ITM, RefCOCO series for REC) are used to evaluate discriminative abilities of the models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The authors conduct in-depth analysis using the datasets to assess the impact of tuning objectives, self-correction, and feature block selection on both generative and discriminative performances, thus using datasets to analyze trends and characteristics rather than just training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-618-rubric-5",
    "token_usage": {
      "prompt_tokens": 16293,
      "completion_tokens": 406,
      "total_tokens": 16699
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any newly introduced dataset containing more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any newly introduced dataset containing exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental Settings",
          "reasoning": "The authors use several well-known datasets for training and evaluation (MSCOCO, RefCOCO, RefCOCO+, RefCOCOg, COCO-NSS1K, CC-500, ABC-6K, TIFA, T2I-CompBench), all of which contain English textual prompts and annotations. There is no indication of other human languages being involved in the proposed datasets or adaptations."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any dataset in a single non-English language being introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced or used in the paper do not contain entries with programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains equations and formal notations describing the method, the datasets themselves do not contain mathematical or logical expression entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data is mentioned in the datasets."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are mentioned in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of the dataset entries are clearly documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain English text prompts and annotations, thus they do contain language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-618-rubric-6",
    "token_usage": {
      "prompt_tokens": 13511,
      "completion_tokens": 195,
      "total_tokens": 13706
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract, Introduction",
          "reasoning": "The paper mentions that code is available at https://dpt-t2i.github.io/, but this link points to the code for the proposed method and experiments related to text-to-image generation improvements and discriminative probing/tuning tasks, not to code for dataset construction. The paper uses existing public datasets such as MSCOCO, RefCOCO, RefCOCO+, and RefCOCOg for training, and several benchmark datasets for evaluation, but does not introduce any new datasets constructed by the authors. Therefore, no code related to dataset creation is provided or described."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "N/A",
          "reasoning": "The paper does not describe the creation process of any new datasets; all datasets mentioned are existing public datasets used for training or evaluation. Therefore, there is no documentation about dataset construction or creation process in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-640-rubric-0",
    "token_usage": {
      "prompt_tokens": 13292,
      "completion_tokens": 224,
      "total_tokens": 13516
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2: Domain-Specific Foundation Model; Section 5.1: Datasets and Evaluation Metrics",
          "Reasoning": "The paper introduces usage of large-scale remote sensing and medical image datasets via domain-specific foundation models (e.g., the large-scale remote sensing foundation model [35] and MedSAM [23]) which are trained on millions of human-generated remote sensing and medical images. These images originate from real-world acquisitions and are not artificially generated or synthesized. The paper uses these domain-specific foundation models to provide domain knowledge but does not introduce any entirely new raw datasets itself; the datasets mentioned for evaluation (e.g., MLRSNet, PatternNet, BTMRI) are pre-existing. Therefore, no new dataset introduced by the authors is explicitly described in the paper. However, the foundation models use image data captured by humans and their sensors. Hence, modality is image, origin is human generated, confirmed in Sections 4.2 and 5.1."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-640-rubric-1",
    "token_usage": {
      "prompt_tokens": 14092,
      "completion_tokens": 257,
      "total_tokens": 14349
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 5.1 (Datasets and Evaluation Metrics)",
            "reasoning": "The paper uses publicly available datasets for evaluation without manual annotation described; the new dataset use in experiments aligns with automatic sampling of few-shot training samples for evaluation, indicating no manual annotation by humans or experts was described."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No specific section detailing annotation instructions; Section 5.1 describes datasets only by reference to existing datasets, no new annotation process details.",
            "reasoning": "The paper does not describe explicit instructions given to annotators for creating or labeling the new data; datasets are pre-existing and reused rather than newly annotated."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No rubric or scoring guideline section present in the paper for dataset annotation.",
            "reasoning": "No mention of rubrics or scoring criteria guiding annotations for the datasets suggests absence of such rubric documentation in this work."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of annotations or labeling guidelines provided in the paper Appendix or main text.",
            "reasoning": "The paper does not provide any annotation examples, as it reuses publicly available benchmark datasets and focuses on modeling techniques and evaluations."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-640-rubric-2",
    "token_usage": {
      "prompt_tokens": 15282,
      "completion_tokens": 316,
      "total_tokens": 15598
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information or indication that quality assurance of new datasets was performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper of quality assurance involving multiple human expert annotators for any new dataset introduced by the authors."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance carried out by a single human non-expert annotator on new datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about quality assurance conducted by multiple human non-expert annotators for datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance process using AI model as a judge on datasets is described in the paper."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated verification or algorithmic/rule-based quality assurance for datasets in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets by the authors themselves; it only uses existing large-scale remote sensing and medical image datasets, as well as large-scale domain-specific foundation models. There is no documentation or description of a quality assurance process for any new datasets introduced by the authors. Thus, quality assurance is considered not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-640-rubric-3",
    "token_usage": {
      "prompt_tokens": 14892,
      "completion_tokens": 396,
      "total_tokens": 15288
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creating any new data entirely from scratch by human contributors. The datasets evaluated are pre-existing domain-specific datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report generating new data purely by AI or machine learning models without reference to existing data. It focuses on adapting models using existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using machine translation systems to create or translate datasets."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses multiple existing remote sensing and medical image datasets sourced from prior works; however, these datasets were not collated or aggregated by the authors themselves as new datasets."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify creating derived datasets by modifying or transforming existing data. The focus is on model adaptation using existing datasets and domain-specific foundation models."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new datasets are introduced by the authors in this paper; only existing domain-specific datasets and foundation models are utilized. The method adapts models on these existing data sources without reporting any new dataset creation or curation."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-640-rubric-4",
    "token_usage": {
      "prompt_tokens": 15410,
      "completion_tokens": 506,
      "total_tokens": 15916
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new datasets introduced for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any new datasets introduced by the authors are used for training models from scratch starting with randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets are not described as used for supervised fine-tuning of pre-trained models. Instead, the paper focuses on prompt learning methods applied on existing large-scale foundation models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention reinforcement learning based post-training techniques such as RLHF using any new datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1",
          "reasoning": "Eight remote sensing datasets (MLRSNet, PatternNet, RSSCN7, AID, RSICD, UCM, WHURS19, NWPU) and three medical datasets (BTMRI, CCBTM, CHMNIST) are used extensively in experiments exclusively for evaluation, benchmarking, and performance measurement of the proposed method. However, these are pre-existing datasets, not newly introduced by the authors. The paper does not introduce any new datasets of its own."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets are introduced by the authors for analysis purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of new datasets introduced to serve as a knowledge base for model augmentation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. All datasets used for evaluation and experiments are pre-existing domain-specific datasets. Therefore, no practical usage of newly proposed datasets is described."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-640-rubric-5",
    "token_usage": {
      "prompt_tokens": 16133,
      "completion_tokens": 411,
      "total_tokens": 16544
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the introduced domain-specific datasets contain entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication the introduced datasets contain exactly two human languages; no bilingual data is referenced."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5.1 Datasets and Evaluation Metrics",
          "reasoning": "The datasets introduced and used for evaluation are remote sensing and medical image datasets with content and labels described in English only, such as category names referenced in English prompts ('a photo of category'). There is no mention of other human languages in the dataset entries or labels."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no evidence that any dataset entries are labeled or described in a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical formulas and code concepts in method descriptions, the datasets themselves do not contain programming or code entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although mathematical notation appears in the methodology, the datasets themselves do not contain mathematical or logical symbolic expressions as data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data are part of the datasets introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or constructed languages in the dataset entries."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is specified as English, so the language is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain language (English) in class labels and prompts, so they are not without language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-640-rubric-6",
    "token_usage": {
      "prompt_tokens": 13351,
      "completion_tokens": 183,
      "total_tokens": 13534
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention related to dataset code availability in the paper",
          "reasoning": "The paper references several existing large-scale domain-specific foundation models and datasets (e.g., remote sensing and medical imaging datasets) used for evaluation, but does not state that the authors introduced any new dataset nor that they released code for dataset construction or preprocessing. The only code link provided (https://github.com/caoql98/DPLQ) pertains to the implementation of their method, not dataset construction."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No specific description or documentation about dataset creation process in the paper",
          "reasoning": "The paper describes usage of multiple existing public datasets in remote sensing and medical imaging fields but does not introduce any novel datasets or provide documentation or methodology for dataset creation. Hence, there is no documentation regarding dataset construction process."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-696-rubric-0",
    "token_usage": {
      "prompt_tokens": 10909,
      "completion_tokens": 196,
      "total_tokens": 11105
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1 (Abstract), Section 3 (Dataset Process), Table 1, and throughout the paper",
          "Reasoning": "The paper introduces the Extreme Pose Face High-Quality Dataset (EFHQ) as a new dataset composed of up to 450k high-quality images of faces. These images are extracted frames from videos originally captured in the VFHQ and CelebV-HQ datasets, which are themselves collections of real video footage. The images are photographic in nature, representing still frames from human-recorded videos, thus their modality is 'image'. The collection process involves human-in-the-loop annotation and manual review, and the images originate from human recordings via cameras, not AI or procedural generation. The paper explicitly states that EFHQ is sampled and curated from these publicly available video datasets and cleaned with a processing pipeline, confirming human-generated origin."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-696-rubric-1",
    "token_usage": {
      "prompt_tokens": 11709,
      "completion_tokens": 259,
      "total_tokens": 11968
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1",
            "reasoning": "The paper states a manual review process was conducted to ensure high-quality labels using a graphical user interface tool, indicating involvement of human annotators reviewing pose binning annotations and facial landmarks. The mention of a large dataset and multiple reviewers implies multiple human experts were involved in the annotation review step."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "The paper describes a manual review process using a developed GUI tool to focus on verifying binning annotations and subsampling based on pose angle and image quality for quality validation, which implies that instructions or guidelines were provided to annotators to perform these focused reviews accurately."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no explicit mention of formal scoring rubrics or grading schemes supplied to annotators for the attribute annotations or review steps; the process focuses on manual review and filtering rather than rubric-based scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not mention providing explicit examples or sample annotations as part of annotation guidelines or review procedures."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-696-rubric-2",
    "token_usage": {
      "prompt_tokens": 12899,
      "completion_tokens": 532,
      "total_tokens": 13431
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human annotator who is an expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that multiple human experts conducted the quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Attributes Preparation and Annotation Review",
          "reasoning": "The paper states in Section 3.1 and Figure 2 that after automated attribute extraction and ensemble pose estimation, the authors manually reviewed results using a graphical user interface to verify pose binning annotations. Given the dataset size (~450k images), multiple human annotators (non-experts) likely participated in this manual review process to ensure annotation quality, but the paper does not indicate these annotators were domain experts."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.1 Attributes Preparation",
          "reasoning": "The dataset processing pipeline applies multiple automated methods including SynergyNet (3DMM-based), DirectMHP (landmark-free pose estimator), FacePoseNet (improved with data augmentation), as well as RetinaFace and HyperIQA for boxes, landmarks, and quality scores. These AI models collectively perform automated attribute extraction and initial pose binning, representing quality assurance through AI models."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 Attributes Preparation",
          "reasoning": "The paper details the use of an automated dataset processing pipeline including Hungarian matching algorithms for matching annotations, majority voting ensemble for pose binning labels, and filters for landmark quality and image selection. These algorithmic and rule-based techniques constitute an automatic quality assurance process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes multiple quality assurance steps involving automated and human-in-the-loop review methods, so it is not the case that no quality assurance was applied or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-696-rubric-3",
    "token_usage": {
      "prompt_tokens": 12509,
      "completion_tokens": 470,
      "total_tokens": 12979
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The EFHQ dataset is not newly captured or created from scratch by human contributors, but rather derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset EFHQ is not generated by AI or machine learning models; it consists of real images from existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translation of data between languages in the creation of the dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation being involved in the dataset creation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3, Dataset Process, especially 3.1 and 3.2",
          "reasoning": "The EFHQ dataset is created by collecting (curating and sampling) frames from two publicly available datasets, VFHQ and CelebV-HQ, without significant creation of new data but by aggregating existing high-resolution face videos. The data is collected from existing sources (videos in public datasets) and organized into a new curated dataset."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The authors process the collected data with multiple steps including attribute extraction (bounding boxes, landmarks, pose estimation, image quality scoring), majority voting to finalize pose bins, manual review and filtering, pose-based sampling, image quality filtering, and constructing paired images for various tasks. Thus, the final EFHQ dataset is based on existing data but modified, filtered, and adapted significantly via a novel and meticulous data processing pipeline."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation is well specified and documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-696-rubric-4",
    "token_usage": {
      "prompt_tokens": 13027,
      "completion_tokens": 365,
      "total_tokens": 13392
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.2, 4.3",
          "reasoning": "The EFHQ dataset is used to train generative models such as StyleGAN2-ADA and EG3D from scratch, as well as to fine-tune diffusion-based models. Training details and evaluation protocols in Sections 4.1, 4.2, and 4.3 describe usage of EFHQ combined with other datasets to improve model generalization on extreme pose images."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.2, 6.2",
          "reasoning": "EFHQ is used to fine-tune or augment training of face reenactment methods and face recognition models, improving performance on extreme pose data as described in Sections 5.2 and 6.2 where models trained or fine-tuned on EFHQ show improved metrics."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 6.1, 6.2",
          "reasoning": "The EFHQ dataset is used as a challenging benchmark for face verification under extreme pose conditions, with curated balanced positive/negative pairs and evaluation metrics presented in Sections 6.1 and 6.2 to reveal gaps in state-of-the-art models."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-696-rubric-5",
    "token_usage": {
      "prompt_tokens": 13750,
      "completion_tokens": 481,
      "total_tokens": 14231
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The EFHQ dataset is composed of facial images without any mention of linguistic content in multiple human languages. The paper does not describe the presence of multiple human languages within the dataset images or associated metadata."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the EFHQ dataset contains exactly two human languages. The dataset is made of facial images, and no language pairing is described."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the EFHQ dataset entries are exclusively in English or contain English textual content. The dataset primarily consists of facial images without language annotations or texts."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the EFHQ dataset contains entries with a single non-English language. The dataset consists of face images, not language-labeled data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset itself is composed of images and does not include programming or structured code content. The paper mentions methods and models in code but not that the dataset includes code entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains facial images and associated annotations (e.g., landmarks, poses), but no mathematical or formal logical expressions or symbolic language as data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human facial images, not biological sequences like DNA or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include fictional or constructed languages but human face images without associated constructed language content."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset is not specified as unknown because the dataset does not focus on language data; linguistic content is not applicable."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The EFHQ dataset consists purely of facial images with associated non-linguistic metadata (e.g., bounding boxes, landmarks, pose labels) but does not include any linguistic entries, making it appropriate to classify as N/A."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-696-rubric-6",
    "token_usage": {
      "prompt_tokens": 10968,
      "completion_tokens": 188,
      "total_tokens": 11156
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit code repository mentioned in the paper",
          "reasoning": "The paper mentions that the final dataset will be publicly available as metadata files to comply with original datasets' licenses and pretrained models will be released, but there is no explicit mention or link to the source code or code repository used for the data processing pipeline, attribute extraction, or dataset creation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 (Dataset Process) and 3.1 (Attributes Preparation)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including the data sources (VFHQ and CelebV-HQ), attribute extraction methods with multiple pose estimators, manual annotation review, dataset sampling strategies, and sub-datasets tailored for specific tasks. Figures and tables further illustrate the pipeline and pose distributions, indicating transparent and comprehensive documentation of the dataset construction."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-708-rubric-0",
    "token_usage": {
      "prompt_tokens": 12275,
      "completion_tokens": 149,
      "total_tokens": 12424
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets and Evaluation Metrics",
          "Reasoning": "The paper introduces evaluation on two 2D medical image segmentation benchmark tasks using public datasets collected from different medical centers, but does not mention introducing any new datasets. The datasets mentioned (joint optic disc and cup segmentation, and polyp segmentation) are existing public datasets used to evaluate the method, not newly created or introduced by the authors. Therefore, the paper does not present a new dataset, only uses existing public datasets. Hence no new dataset sources are introduced, and no data origin beyond human generated medical images from public datasets is presented."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-708-rubric-1",
    "token_usage": {
      "prompt_tokens": 13075,
      "completion_tokens": 234,
      "total_tokens": 13309
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Sections 4.1, 4.2",
            "reasoning": "The paper describes using publicly available medical image segmentation datasets that are well-known and presumably annotated by multiple human experts. The datasets introduced (OD/OC segmentation and polyp segmentation) are composed of images from multiple medical centers, which indicates expert annotation due to the clinical nature of data."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not explicitly mention any annotation instructions provided to annotators for the new datasets. It uses public datasets likely with their own instructions, but no new annotation instructions are detailed."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "There is no indication of scoring rubrics or criteria provided to annotators for labeling in the new datasets within the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not present any annotation examples or guidelines for annotators regarding the new datasets; it references existing public datasets without detailing examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-708-rubric-2",
    "token_usage": {
      "prompt_tokens": 14265,
      "completion_tokens": 274,
      "total_tokens": 14539
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process involving a single human expert for the new datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by multiple human experts for the datasets used."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention of QA performed by a single non-expert annotator on any new dataset introduced."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance conducted by multiple non-expert annotators on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No AI model is reported as a judge or means to perform quality assurance for the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification process or rule-based algorithm is described as QA for dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses publicly available, pre-existing medical image datasets collected from different medical centers, and does not introduce any new datasets. It does not describe any quality assurance process applied or conducted for these datasets within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-708-rubric-3",
    "token_usage": {
      "prompt_tokens": 13875,
      "completion_tokens": 367,
      "total_tokens": 14242
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creating any new datasets manually from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not present any dataset generated purely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using machine translation to generate or transform any dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses existing public datasets collected from different medical centers, but these datasets are referenced as pretrained source or test data, not newly collated by the authors."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets and Evaluation Metrics",
          "reasoning": "The authors use several existing public datasets for the medical image segmentation tasks. They crop regions of interest, resize, and normalize images following previous works. This constitutes a derived dataset since the data is based on existing public datasets with modifications applied for their experiments."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origins are clearly specified and documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-708-rubric-4",
    "token_usage": {
      "prompt_tokens": 14393,
      "completion_tokens": 257,
      "total_tokens": 14650
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.3",
          "reasoning": "The paper uses multiple public medical imaging datasets from different domains exclusively as benchmark test datasets to evaluate the proposed method's performance on continual test-time adaptation tasks. These datasets are employed for comparative evaluation against other methods, such as in Tables 1, 2, and 4, highlighting their use for evaluation and benchmarking purposes."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; it only uses existing public medical image segmentation datasets for evaluation. Hence, there is no practical usage of a newly proposed dataset to assess."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-708-rubric-5",
    "token_usage": {
      "prompt_tokens": 15116,
      "completion_tokens": 593,
      "total_tokens": 15709
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not mention that the new datasets introduced contain entries with more than two human languages. The datasets are medical image datasets consisting of images and segmentation labels without linguistic annotations."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets introduced are medical imaging datasets without any indication of containing exactly two human languages. They are composed of image data and segmentation labels."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not specify any human language content within the datasets; therefore, no monolingual English content is present in the data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no evidence from the paper that the datasets contain exactly one non-English language content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "While the paper contains code (pseudo-code) describing the method, the new datasets themselves consist of medical images and labels; they do not contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The new datasets are medical image datasets. Mathematical and logical notations appear in the paper in describing the method, but not as part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Although the datasets are of biological tissues (medical images), the dataset entries are images and annotations, not biological sequences or communication systems such as DNA or animal signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets do not contain fictional or artificial languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The languages present in the dataset are not applicable since the datasets consist of images without linguistic content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The proposed datasets contain medical image data and corresponding segmentation labels. There is no presence of entries with any human language or linguistic content; therefore, this dataset is categorized as not containing any language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-708-rubric-6",
    "token_usage": {
      "prompt_tokens": 12334,
      "completion_tokens": 184,
      "total_tokens": 12518
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 4.1",
          "reasoning": "The paper provides a GitHub link (https://github.com/Chen-Ziyang/VPTTA) for the code and weights of pre-trained source models related to their method. However, this code pertains to the method implementation, not to the construction or creation of any new datasets. The datasets used are all publicly available medical image datasets collected from different centers, and the paper does not provide code for constructing new datasets."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 4.1",
          "reasoning": "The paper uses several existing public datasets for evaluation, which are named and cited in Section 4.1. There is no mention of any new dataset being created or constructed by the authors. Thus, there is no documentation on dataset creation process provided by the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-712-rubric-0",
    "token_usage": {
      "prompt_tokens": 11953,
      "completion_tokens": 210,
      "total_tokens": 12163
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.3, Pseudo Edge Label Calculation; Abstract; Section 4.3 Ablation Studies",
          "Reasoning": "The paper introduces pseudo edge labels computed from point-wise instance labels using k-Nearest Neighbors (KNN) based on point cloud data from 3D scenes. These 3D point clouds are acquired from real-world scanned environments (indoor/outdoor) such as ScanNetV2, S3DIS, ScanNet200, and STPLS3D, which are human-captured sensor data (e.g., RGB-D or LiDAR scans). The pseudo edge labels are generated algorithmically by the authors to supervise the edge prediction module, but the base 3D point clouds are human captured sensor data. Hence, the signal/sensor modality is human generated in origin, with the edge labels generated by the authors' algorithmic method on this base data."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-712-rubric-1",
    "token_usage": {
      "prompt_tokens": 12753,
      "completion_tokens": 198,
      "total_tokens": 12951
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3, Pseudo Edge Label Calculation paragraph",
            "reasoning": "The paper states that pseudo edge labels are computed automatically using a traditional k-Nearest Neighbors (KNN) classification algorithm based on KD-tree data structure, using dataset-provided point-wise instance labels. This indicates a deterministic automatic process rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No detailed annotation instructions for this automatic pseudo edge label generation process are described in the paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No scoring rubrics or evaluation metrics related to annotation guidelines for the pseudo edge labels generation are provided or discussed."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not provide annotation examples or sample pseudo edge labels in the methodology or appendix."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-712-rubric-2",
    "token_usage": {
      "prompt_tokens": 13943,
      "completion_tokens": 324,
      "total_tokens": 14267
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for any dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or mention that multiple human experts were involved in quality assurance of dataset annotations in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any QA by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information suggesting multiple non-expert annotators conducted quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not cite use of AI models as judges for quality assurance of datasets."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3 Edge-Aware 3DIS Framework",
          "reasoning": "The paper describes using a k-Nearest Neighbors (KNN) algorithm based on KD-tree data structures to generate pseudo edge labels automatically from the dataset's existing instance labels. The process involves algorithmic generation of point-wise pseudo edge labels without human intervention, indicating an automated verification or labeling process as quality assurance for edge annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "While the paper uses existing datasets, it does not document any lack of QA; rather, it uses an automatic process to generate pseudo edge labels, which is a form of QA for that annotation."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-712-rubric-3",
    "token_usage": {
      "prompt_tokens": 13553,
      "completion_tokens": 401,
      "total_tokens": 13954
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.3 Edge-Aware 3DIS Framework and Ablation Studies",
          "reasoning": "The paper describes generating point-wise pseudo edge labels by applying k-nearest neighbor (KNN) classification based on existing dataset point-wise instance labels, effectively aggregating and processing existing data rather than creating wholly new data. The authors precompute these pseudo edge labels for every 3D scene in existing datasets ScanNetV2, S3DIS, ScanNet200, and STPLS3D, which are well-known and pre-existing datasets. Thus, the edge label data can be considered collated from existing datasets with automatic processing."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.3 Edge-Aware 3DIS Framework",
          "reasoning": "The pseudo edge labels are generated by modifying the existing instance labels through a processing step using KNN neighborhood analysis to derive boundary points, which transforms original labels to new edge labels. This is a derivation of the existing data with additional computations (thresholding neighborhood differences). Therefore, the data is derived from existing sources with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-712-rubric-4",
    "token_usage": {
      "prompt_tokens": 14071,
      "completion_tokens": 242,
      "total_tokens": 14313
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper extensively uses and evaluates their method on several existing benchmark datasets: ScanNetV2, ScanNet200, S3DIS, and STPLS3D. However, these datasets are all pre-existing datasets and not newly introduced by the authors. The paper does not introduce any new dataset. Therefore, there is no new dataset introduced by the authors whose usage can be assessed. Hence, no practical usage of any newly proposed dataset is described or demonstrated in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-712-rubric-5",
    "token_usage": {
      "prompt_tokens": 14794,
      "completion_tokens": 523,
      "total_tokens": 15317
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.2 Intelligent Semantic Prior; Section 3.1 Overview",
          "reasoning": "The semantic guidance network uses text embeddings from CLIP trained on English text labels describing object categories (e.g., 'Cabinet', 'Bookshelf'). The dataset entries, including instance category labels and textual descriptions, are in English only. The dataset itself contains point-cloud data accompanied by English text labels for training and evaluation."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper describes the method in algorithmic and network terms, there is no indication that the dataset entries contain programming code or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notation is present in the description of losses and methods, but these are part of model formulation, not dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used (e.g., ScanNetV2, S3DIS, etc.) contain 3D point cloud scenes of indoor or outdoor environments, no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or artificially created languages in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of dataset labels and text embeddings is clearly English; no ambiguity or unspecified language content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets include human language in the form of English category labels used as text embeddings in the semantic guidance network."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-712-rubric-6",
    "token_usage": {
      "prompt_tokens": 12012,
      "completion_tokens": 137,
      "total_tokens": 12149
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention found in the paper",
          "reasoning": "The paper discusses methods and evaluation on existing large-scale datasets (ScanNetV2, ScanNet200, S3DIS, STPLS3D) but does not introduce any new datasets or describe any code related to dataset construction or preprocessing being released."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section or description of dataset creation or construction",
          "reasoning": "The paper only uses existing public datasets and does not describe any process of collecting, constructing, or curating new datasets; thus, no documentation on dataset creation is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-74-rubric-0",
    "token_usage": {
      "prompt_tokens": 12794,
      "completion_tokens": 159,
      "total_tokens": 12953
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4. Experimental setup, Datasets; multiple dataset names are mentioned like ADE20K, COCO-Stuff-10K, Cityscapes, Pascal Context",
          "Reasoning": "The paper reports experiments conducted on multiple well-known datasets, such as ADE20K, COCO-Stuff-10K, Cityscapes, and Pascal Context. These datasets consist of images manually collected and annotated by humans, as is standard practice in computer vision semantic segmentation datasets. However, these datasets are existing pre-existing datasets, not newly introduced by the authors. The paper does not state any new dataset introduced by the authors themselves, only evaluates on existing datasets."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-74-rubric-1",
    "token_usage": {
      "prompt_tokens": 13594,
      "completion_tokens": 375,
      "total_tokens": 13969
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4 Datasets",
            "reasoning": "The paper discusses the ADE20K dataset as the main dataset, along with Pascal Context, Cityscapes, and COCO-Stuff-10K datasets. These are standard, pre-existing datasets and not newly introduced in the paper."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "The paper does not indicate any annotation instructions because no new dataset annotation was conducted by the authors."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "No mention of scoring rubrics related to annotation tasks is given since the datasets are pre-existing."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "Since no new annotation or annotation guidelines were established, no examples are provided."
          }
        },
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3 Adaptive Local-then-Global Merging",
            "reasoning": "The paper introduces a token merging method (ALGM) applied to pre-existing datasets. No new data annotations were performed; instead, token merging is done algorithmically during model processing."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "This is a computational method rather than human annotation, so no instructions apply."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "No scoring rubrics apply for an automatic token merging process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "No annotation examples are provided, as this is a model processing technique."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-74-rubric-2",
    "token_usage": {
      "prompt_tokens": 14784,
      "completion_tokens": 298,
      "total_tokens": 15082
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by multiple human experts on dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single non-expert human annotator on dataset annotations or content."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by multiple non-expert human annotators on dataset annotations or content."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that an AI model was used as a judge for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper of automatic or algorithmic quality assurance of dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper solely focuses on proposing a token merging method and evaluating it on established datasets; it does not introduce any new datasets, nor does it describe any quality assurance processes for dataset annotations. Therefore, no quality assurance process related to dataset annotations is applied or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-74-rubric-3",
    "token_usage": {
      "prompt_tokens": 14394,
      "completion_tokens": 377,
      "total_tokens": 14771
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset created by human contributors from scratch; it uses existing datasets for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not present datasets generated de novo by AI or machine learning models; rather, it focuses on a method applied to existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication or mention of datasets produced by human translation is present in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use or introduction of machine-translated datasets."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any newly collated datasets; it uses established benchmark datasets like ADE20K, Pascal Context, Cityscapes, and COCO-Stuff-10K."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the authors derived new datasets by modifying or transforming existing datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset and does not specify any new data sources; all used datasets are pre-existing and well-known. Therefore, the data source or method of generation for new datasets is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-74-rubric-4",
    "token_usage": {
      "prompt_tokens": 14912,
      "completion_tokens": 202,
      "total_tokens": 15114
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper exclusively uses standard established datasets (ADE20K, Pascal Context, Cityscapes, COCO-Stuff-10K) for evaluating the proposed token merging method. No new dataset is introduced by the authors in this work, hence none of the usage categories apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-74-rubric-5",
    "token_usage": {
      "prompt_tokens": 15635,
      "completion_tokens": 629,
      "total_tokens": 16264
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses semantic segmentation datasets such as ADE20K, COCO-Stuff, Cityscapes, and Pascal-Context. These are image datasets without explicit linguistic content. There is no indication that the dataset contains text in English or any language. The paper focuses on image tokens and vision transformers, not on language data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Throughout the paper, especially Section 3 (Method) and Section 4 (Experimental Setup)",
          "reasoning": "The paper involves technical descriptions of algorithms and models using mathematical notation, but it does not introduce a new dataset containing programming or code entries. The code mentioned is software implementation, but no code dataset is introduced."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Section 3.1 (Preliminaries), Section 3.2 (Token similarity analysis), Section 3.3 (Adaptive Local-then-Global Merging)",
          "reasoning": "While the paper includes mathematical expressions and formal notations to describe methods (e.g., token sets, cosine similarity, transformers), this is part of methodological exposition, not a new dataset containing such notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper focuses on semantic segmentation of images using vision transformers and does not mention any biological or non-human communication system datasets."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or use of fictional or artificially created languages is found in the paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are well-known public datasets of images for semantic segmentation tasks, their nature and data contents are known and documented."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces no new datasets containing textual or linguistic content. The datasets used are well-known image datasets used for semantic segmentation. The focus is on images and image tokens processed by vision transformers, thus datasets entries do not contain language. Therefore, the linguistic coverage metric is not applicable to the datasets introduced (in fact, no new dataset is introduced)."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-74-rubric-6",
    "token_usage": {
      "prompt_tokens": 12853,
      "completion_tokens": 178,
      "total_tokens": 13031
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section in the paper mentions code availability for dataset creation or preprocessing.",
          "reasoning": "The paper mentions dataset names that are commonly used benchmarks (ADE20K, Pascal Context, Cityscapes, COCO-Stuff-10K), but does not describe creating any new datasets nor provide code related to dataset construction. The only code link given is for the method (https://tue-mps.github.io/ALGM) related to their token merging approach, not the dataset."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "There is no section or mention describing dataset creation or collection process.",
          "reasoning": "The datasets mentioned are standard, pre-existing datasets widely used in the field, and no new dataset was introduced by the authors. Therefore, no documentation of dataset construction is provided in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-755-rubric-0",
    "token_usage": {
      "prompt_tokens": 14026,
      "completion_tokens": 145,
      "total_tokens": 14171
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets and Metrics; Abstract; Introduction",
          "Reasoning": "The paper evaluates the AdaTAD method on four datasets: ActivityNet-1.3, THUMOS14, EPIC-Kitchens 100, and Ego4D-MQ. These datasets are mentioned as existing third-person and egocentric video collections (e.g., web-collected or from egocentric cameras). The authors do not introduce any new dataset; rather, they evaluate on these public datasets. No new dataset is introduced by the authors in the paper according to the content."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-755-rubric-1",
    "token_usage": {
      "prompt_tokens": 14826,
      "completion_tokens": 283,
      "total_tokens": 15109
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1 Datasets and Metrics",
            "reasoning": "The paper introduces new experimental evaluations on existing datasets (ActivityNet-1.3, THUMOS14, EPIC-Kitchens 100, Ego4D-MQ) but does not describe a newly created dataset by the authors. The annotations used for training and evaluation come from these well-known public datasets, which have been labeled by presumably expert annotators (as is standard for these widely used datasets). Since the paper uses these established datasets without indicating new annotation efforts, no new annotation subject or guidelines are presented by authors. However, in the paper's context, they evaluate on four TAD datasets, but none is newly introduced in this paper."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No new dataset is introduced in the paper, hence no annotation instructions are provided in the paper themselves."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No new dataset with annotation rubrics is introduced; thus, no rubrics are described or evaluated for annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "Since no new dataset annotations are presented or collected here, no examples are provided in the annotation guidelines. The paper uses pre-existing datasets without elaboration on annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-755-rubric-2",
    "token_usage": {
      "prompt_tokens": 16016,
      "completion_tokens": 386,
      "total_tokens": 16402
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about dataset annotation being validated or quality assured by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance of dataset annotations by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that dataset quality assurance was performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that multiple non-expert annotators were involved in dataset quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report that AI models were used to perform quality assurance on the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification or rule-based QA processes applied to datasets introduced by the authors."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets; rather, it uses and evaluates on pre-existing datasets (ActivityNet-1.3, THUMOS14, Epic-Kitchens 100, Ego4D-MQ). There is no documentation of any quality assurance process for dataset annotations applied or discussed by the authors. Therefore, QA process is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-755-rubric-3",
    "token_usage": {
      "prompt_tokens": 15626,
      "completion_tokens": 373,
      "total_tokens": 15999
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new datasets that were created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated by AI or machine learning models as original new datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of any machine-translated dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used (ActivityNet-1.3, THUMOS14, EPIC-Kitchens 100, Ego4D-MQ) are publicly available and collected by prior works; the paper uses them but does not create a new dataset by collection."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the authors transformed or adapted existing datasets to derive a new dataset; they only use existing datasets for evaluation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets. All datasets used are established and pre-existing. Thus, no information on data origin for new datasets applies."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-755-rubric-4",
    "token_usage": {
      "prompt_tokens": 16144,
      "completion_tokens": 215,
      "total_tokens": 16359
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper evaluates the method on multiple existing standard datasets such as ActivityNet-1.3, THUMOS14, EPIC-Kitchens 100, and Ego4D-MQ, but does not introduce or provide any new dataset. Therefore, no new dataset is introduced by the authors, and hence no task utility categorization applies."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-755-rubric-5",
    "token_usage": {
      "prompt_tokens": 16867,
      "completion_tokens": 553,
      "total_tokens": 17420
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not indicate that any new dataset introduced contains entries with more than two human languages. The datasets used (e.g., ActivityNet-1.3, THUMOS14, EPIC-Kitchens 100) are video datasets without mention of linguistic content or multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the proposed dataset contains exactly two human languages. The paper focuses on video datasets for temporal action detection without linguistic annotations involving multiple languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The new datasets introduced or used are video action detection datasets; the paper does not describe them as containing only English content or language annotations."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not mention any datasets with content exclusively in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Although the paper includes code examples and descriptions of algorithms and training methods, the paper does not propose datasets that contain programming or structured code content as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3.3, Equation 1 and 2; Figure 3.",
          "reasoning": "The paper includes mathematical notation describing the Temporal-Informative Adapter architecture with equations and symbolic formulas that represent model components. These expressions are part of the dataset description of the method rather than dataset entries, however, since these describe the adapter design within the proposed method associated with the dataset, this property is applicable."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No datasets involving biological sequences or non-human communication systems are introduced or described in the paper."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No constructed or fictional languages are present or described within the datasets introduced in the paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper specifically mentions all datasets used and their nature; no dataset has unspecified or undocumented language content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The datasets introduced or used in this paper consist of video data for temporal action detection, which do not contain language content per se. The linguistic attributes are not associated with dataset entries but rather with video content. Therefore, the entries do not contain any human language text data as part of the dataset format."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-755-rubric-6",
    "token_usage": {
      "prompt_tokens": 14085,
      "completion_tokens": 234,
      "total_tokens": 14319
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit code repository link or description for dataset construction is provided in the paper",
          "reasoning": "The paper provides a link to the code repository for the AdaTAD model (https://github.com/sming256/AdaTAD) but it is related to the model implementation and training rather than dataset construction. There is no mention of new datasets being constructed or code for dataset creation being released. The datasets used are publicly available benchmarks (ActivityNet-1.3, THUMOS14, EPIC-Kitchens 100, Ego4D-MQ) and no new dataset is introduced in this paper."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No specific section or description about dataset creation process; only usage of existing datasets is described in Section 4.1",
          "reasoning": "The paper uses existing standard datasets for evaluation (ActivityNet-1.3, THUMOS14, EPIC-Kitchens 100, Ego4D-MQ) and does not introduce a new dataset. Therefore, there is no documentation about dataset creation or construction process within the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-773-rubric-0",
    "token_usage": {
      "prompt_tokens": 11724,
      "completion_tokens": 139,
      "total_tokens": 11863
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Experiment settings, Dataset",
          "Reasoning": "The authors introduce training of their EpiDiff model on the LVIS subset of the Objaverse, which comprises over 40,000 3D models rendered into 2D images, sampled uniformly across multiple views. These datasets are composed of images of objects, captured or rendered from 3D models curated by humans. The paper explicitly states this training dataset for their model, indicating image data modality and human-generated origin as these are scanned or modeled assets rendered into images for training purposes."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-773-rubric-1",
    "token_usage": {
      "prompt_tokens": 12524,
      "completion_tokens": 293,
      "total_tokens": 12817
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Dataset",
            "reasoning": "The new dataset used for training is the LVIS subset of the Objaverse dataset, described as over 40,000 3D models with camera layouts and rendered multiview images. The generation of data (rendered images and camera parameters) is conducted through an automated camera setup and rendering process, not by manual annotation or human labeling. Thus, the annotation or data generation process is an automatic process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1 Dataset",
            "reasoning": "The paper describes the dataset generation or preparation from the LVIS subset of Objaverse with camera sampling and rendering, but it does not provide any instructions for annotators, as this process is automated rendering rather than a manual annotation task."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1 Dataset",
            "reasoning": "No scoring rubrics or evaluation rubrics or annotation rubrics are described for the data creation in the new dataset, as the data is automatically rendered and not labeled or annotated by humans."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.1 Dataset",
            "reasoning": "There are no annotation examples discussed or shown. The dataset generation is automated rendering based on 3D models and camera parameters; thus, annotation examples are not applicable."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-773-rubric-2",
    "token_usage": {
      "prompt_tokens": 13714,
      "completion_tokens": 294,
      "total_tokens": 14008
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert annotator on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving multiple human experts for the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that a single non-expert human performed quality checks on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by multiple non-expert humans on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that an AI model was used for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper of automatic verification or algorithmic quality assurance applied to the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses existing datasets (e.g., LVIS subset of Objaverse and Google Scanned Object dataset) for training and evaluation. There is no description of new datasets introduced by the authors or any quality assurance process related to dataset annotations or content validation. Therefore, no quality assurance process is applied or documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-773-rubric-3",
    "token_usage": {
      "prompt_tokens": 13324,
      "completion_tokens": 414,
      "total_tokens": 13738
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any new datasets were manually created or annotated from scratch by human contributors. The datasets used are existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes models generating new multiview images and reconstructions, but this refers to model outputs, not datasets. The paper does not introduce new datasets generated entirely by AI or ML models without connection to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of any dataset created by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any datasets produced via machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Experiment settings - Dataset",
          "reasoning": "The authors use existing public datasets such as the LVIS subset of Objaverse and the Google Scanned Object dataset as the data sources for training and evaluation. These datasets are collected and aggregated from existing sources without indication of significant modification. Thus, the data used is collated from existing datasets."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the authors have created new datasets by modifying or adapting existing datasets. The training and evaluation rely on existing datasets without described derivations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies the data sources clearly, thus this category is not applicable."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-773-rubric-4",
    "token_usage": {
      "prompt_tokens": 13842,
      "completion_tokens": 423,
      "total_tokens": 14265
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 (Experiment settings)",
          "reasoning": "The authors train their model, EpiDiff, on the LVIS subset of the Objaverse dataset, which consists of over 40,000 3D models. They train for 30,000 steps using multiple GPUs starting with a pretrained backbone model Zero123, but the newly introduced dataset consisting of multiview images of 3D models is used for supervised training of their 3D modeling module (epipolar attention block) from scratch, adapting the multiview conditional diffusion model."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3 (Method), Section 4.1 (Experiment settings)",
          "reasoning": "The EpiDiff model is trained by fine-tuning only the parameters of the novel epipolar-constrained attention (ECA) blocks while freezing the pretrained base diffusion model (Zero123). This process is a form of supervised fine-tuning on the new dataset of multiview images extracted from the Objaverse dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 (Experiment settings) and Tables 1-3",
          "reasoning": "The Google Scanned Object dataset (GSO) is used extensively for evaluation of the model's multiview synthesis quality, as well as for testing 3D reconstruction quality. Metrics such as PSNR, SSIM, LPIPS, Chamfer Distance, and Volume IoU are reported on this dataset, indicating its use exclusively for performance measurement."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-773-rubric-5",
    "token_usage": {
      "prompt_tokens": 14565,
      "completion_tokens": 553,
      "total_tokens": 15118
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset containing entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced is not described as containing exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Experiment settings - Dataset",
          "reasoning": "The introduced training dataset is the LVIS subset of Objaverse, which consists of 3D models and rendered images described and referenced in English. The paper's text, dataset descriptions, and references are all in English, implying the dataset content and metadata are English-based."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication exists that the dataset contains entries in any non-English single language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses algorithms, methods, and network structures in programming contexts, no dataset entries themselves contain programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3 Method and subsection 3.1 and 3.2",
          "reasoning": "The paper includes explicit mathematical expressions to describe diffusion models, attention mechanisms, and optimization objectives. The dataset samples include camera parameters and transformations represented mathematically as rotations and translations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves 3D models and rendered images, with no inclusion of biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain entries in fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset entries is clearly specified as English or mathematical expressions, with no uncertainty or ambiguity."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains linguistic content in English and mathematical notation, so it does contain language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-773-rubric-6",
    "token_usage": {
      "prompt_tokens": 11783,
      "completion_tokens": 220,
      "total_tokens": 12003
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit location",
          "reasoning": "The paper does not provide any explicit information or link to code repositories related to the construction, collection, or preprocessing of the dataset used for training or evaluation. The project page is mentioned only for the model (huanngzh.github.io/EpiDiff/), but no mention of dataset construction code availability is made."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Experiment settings - Dataset",
          "reasoning": "The paper documents the dataset used for training and evaluation: it specifies that the model is trained on the LVIS subset of Objaverse containing over 40,000 3D models, describes the camera layout for view sampling (96 views per object across six concentric circles with specified elevations and azimuth sampling), and details the views used during training (random 16 views) and evaluation (fixed and varied elevation/azimuth) on the Google Scanned Object dataset. This shows that the dataset creation and sampling process is described in detail for reproducibility."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-851-rubric-0",
    "token_usage": {
      "prompt_tokens": 14568,
      "completion_tokens": 121,
      "total_tokens": 14689
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Experimental Setup - Datasets",
          "Reasoning": "The paper uses the PASCAL-Context and NYUD-v2 datasets as benchmarks for evaluation, which are established datasets with images and annotations for multiple vision tasks. However, the paper does not introduce any new dataset; instead, it uses these existing datasets for evaluation. There is no mention of introducing any new dataset by the authors, so there is no new data source introduced in the paper."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-851-rubric-1",
    "token_usage": {
      "prompt_tokens": 15368,
      "completion_tokens": 219,
      "total_tokens": 15587
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Experimental Setup, Datasets",
            "reasoning": "The paper uses existing benchmark datasets PASCAL-Context and NYUD-v2; samples are partitioned randomly among clients to create HC-FMTL scenarios. Annotation is not described as manual or by humans but uses these pre-annotated benchmark datasets for clients' private local datasets."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not mentioned",
            "reasoning": "The paper does not mention providing instructions for any new manual annotation process for the dataset. The datasets used are existing and their annotations come with the dataset itself."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not mentioned",
            "reasoning": "No mention of scoring rubrics or guidelines relevant to annotators for the dataset is described in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not mentioned",
            "reasoning": "No annotation examples or guidelines for annotators are provided in the paper, as it uses existing datasets with known annotations."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-851-rubric-2",
    "token_usage": {
      "prompt_tokens": 16558,
      "completion_tokens": 304,
      "total_tokens": 16862
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance of the datasets was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper about multiple human expert annotators performing quality assurance on the datasets used."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state or imply that quality assurance was conducted by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any involvement of multiple non-expert human annotators in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that quality assurance on the datasets was performed by AI models as judges."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic quality assurance process for dataset annotation or content validation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The datasets used (PASCAL-Context and NYUD-v2) are established benchmark datasets and the paper does not describe any new datasets nor any quality assurance procedures performed or documented by the authors on these datasets. Therefore, no quality assurance process is applied or documented in this paper regarding datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-851-rubric-3",
    "token_usage": {
      "prompt_tokens": 16168,
      "completion_tokens": 347,
      "total_tokens": 16515
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any new dataset created from scratch by human contributors is introduced by the authors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of any dataset generated entirely by models or AI without reference to existing data in the paper."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced via human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data generation through machine translation is specified in the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used for experiments are existing benchmark datasets (PASCAL-Context and NYUD-v2) and not newly collected or aggregated by the authors."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the authors create new derived datasets by modifying or adapting existing datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce or specify any new datasets. The experimental evaluation uses publicly available benchmark datasets without any description of new data creation or adaptation."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-851-rubric-4",
    "token_usage": {
      "prompt_tokens": 16686,
      "completion_tokens": 439,
      "total_tokens": 17125
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using any newly introduced dataset exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention training models from randomly initialized parameters on any newly introduced dataset; instead, it uses established benchmark datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of using a new dataset introduced by the authors for fine-tuning pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any use of reinforcement learning post-training techniques on newly introduced datasets."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses two existing benchmark datasets, PASCAL-Context and NYUD-v2, for evaluation but does not introduce any new datasets for evaluation purposes."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets are introduced in the paper for analyzing trends, patterns, or characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using any new dataset as a knowledge base for model augmentation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset; all datasets used (PASCAL-Context and NYUD-v2) are pre-existing benchmarks. Therefore, no practical usage of a new dataset introduced by the authors is described or demonstrated."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-851-rubric-5",
    "token_usage": {
      "prompt_tokens": 17409,
      "completion_tokens": 540,
      "total_tokens": 17949
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used (PASCAL-Context and NYUD-v2) are standard computer vision datasets involving image data and are not described to contain more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention in the paper that the datasets contain exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental Setup, Datasets",
          "reasoning": "The datasets (PASCAL-Context and NYUD-v2) are standard computer vision datasets commonly annotated in English for tasks such as semantic segmentation, and the paper is written in English; thus the dataset annotations and labels are English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication that datasets have annotations exclusively in a non-English human language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though code snippets or algorithm pseudocode are present in the paper describing methodology and algorithms, these do not constitute part of a dataset introduced by the authors."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While mathematical notations are used in the paper for theoretical analysis, these are not part of any dataset introduced; the datasets themselves do not contain mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or animal communication data are part of the datasets introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or constructed languages in the datasets used or introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of datasets are well known and documented (English annotations)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets consist of images with human language annotations (English labels), so language is present."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-851-rubric-6",
    "token_usage": {
      "prompt_tokens": 14627,
      "completion_tokens": 209,
      "total_tokens": 14836
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract, Section 4.1 Implementation",
          "reasoning": "The paper provides a code link at https://github.com/innovator-zero/FedHCA2 for their FedHCA2 framework; however, this code is related to the method implementation and federated learning framework, not explicitly for dataset construction, collection, or preprocessing scripts. The paper uses publicly available benchmark datasets (PASCAL-Context and NYUD-v2) and does not mention new datasets constructed by the authors or provide code for dataset creation."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 4.1 Datasets and Implementation",
          "reasoning": "The paper uses two existing benchmark datasets, PASCAL-Context and NYUD-v2, for their experiments. They describe how they configure and partition these datasets for evaluation but do not document any new dataset creation or new dataset construction process. No documentation on dataset creation is present as no new dataset is introduced by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-859-rubric-0",
    "token_usage": {
      "prompt_tokens": 14404,
      "completion_tokens": 169,
      "total_tokens": 14573
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1, 4.1",
          "Reasoning": "The new dataset corresponds to the few-shot object detection training data comprising query images and few-shot support images with bounding box annotations. These images are from the PASCAL VOC and MSCOCO datasets, which are human-generated images captured and annotated by humans, used for training and evaluating the proposed method. The paper follows existing benchmark splits and protocols, but the few-shot sets (query and support images with annotations) used for training the model are presented as new training setups rather than pre-existing datasets introduced by others. Thus, the data modality is 'image', and its origin is human-generated as these are real-world images with manual annotations."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-859-rubric-1",
    "token_usage": {
      "prompt_tokens": 15204,
      "completion_tokens": 310,
      "total_tokens": 15514
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.3 The Training Framework",
            "reasoning": "The paper describes that support images are cropped around target objects with bounding box annotations and that the model training involves pre-training and fine-tuning steps using annotated base and novel class datasets. This implies that human experts provided bounding box annotations for both base and novel classes in the newly introduced training datasets used for few-shot learning."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Few-Shot Proposal Classification",
            "reasoning": "The paper explicitly states that language instructions are carefully designed to prompt the LLM to classify each proposal, including instructions that provide the mapping between categories and visual prototypes as part of the input. This indicates that detailed annotation instructions are provided and used in the few-shot classification process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention of scoring rubrics in the paper",
            "reasoning": "The paper does not describe any scoring rubrics or annotation guidelines for scoring detections or annotations. The evaluation uses standard detection metrics (e.g., AP, AP50), but no specific rubric guidelines for annotators are detailed in the paper content."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention of annotation examples in the paper",
            "reasoning": "The paper does not provide or describe annotation examples or sample annotations in the guidelines for the novel dataset or few-shot annotations. There is no appendix or section with annotation examples mentioned."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-859-rubric-2",
    "token_usage": {
      "prompt_tokens": 16394,
      "completion_tokens": 332,
      "total_tokens": 16726
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by multiple human experts for the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about any quality assurance conducted by a single non-expert for the dataset annotations or content."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance performed by multiple non-expert annotators for the dataset annotations or content."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models (e.g., LLMs) are used in the method for proposal classification, the paper does not indicate that an AI model was used as a judge for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification process or rule-based QA applied to the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses standard benchmark datasets (MSCOCO and PASCAL VOC) following published splits and protocols. It does not introduce nor describe any new dataset created or annotated by the authors, nor any quality assurance process related to dataset annotations. Therefore, no quality assurance process is applied or documented for new datasets in this paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-859-rubric-3",
    "token_usage": {
      "prompt_tokens": 16004,
      "completion_tokens": 412,
      "total_tokens": 16416
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the authors collected or created any new dataset from scratch by human contributors. Instead, it uses existing FSOD benchmark datasets (MSCOCO and PASCAL VOC) for evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the dataset used in the paper is generated entirely by AI or machine learning models according to the paper content. The paper focuses on model development and evaluation rather than data generation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data being translated from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset that is machine-translated from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "Section 4.1",
          "reasoning": "The paper uses existing datasets (MSCOCO and PASCAL VOC) as benchmarks but does not introduce any new dataset that is simply collated from existing sources without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the authors created any new dataset derived by modifying or adapting existing datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. It only evaluates the proposed model on existing FSOD benchmark datasets. Therefore, the data source for the benchmark datasets is not specified as new or created by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-859-rubric-4",
    "token_usage": {
      "prompt_tokens": 16522,
      "completion_tokens": 455,
      "total_tokens": 16977
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe introducing any new dataset exclusively for pre-training. Instead, it uses existing large-scale pre-trained foundation models like DINOv2 and Vicuna."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets are introduced for training models from scratch. The authors use existing FSOD benchmarks for evaluation and training stages."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes fine-tuning on existing datasets (MSCOCO and PASCAL VOC) but does not introduce a new dataset for supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning or RLHF-based post-training using new datasets is described in the paper."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used for evaluation, MSCOCO and PASCAL VOC, are widely used existing benchmarks. There are no new datasets introduced specifically for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does perform analysis on foundation models and components, but using existing datasets; no new dataset is introduced for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset serves as a knowledge base to augment models. Existing datasets and pre-trained models are used."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. All datasets used (MSCOCO, PASCAL VOC) are existing benchmarks. Hence, no new dataset utility can be assessed."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-859-rubric-5",
    "token_usage": {
      "prompt_tokens": 17245,
      "completion_tokens": 584,
      "total_tokens": 17829
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes datasets used for few-shot object detection, specifically MSCOCO and PASCAL VOC. Both datasets consist of images annotated with English labels for object categories. There is no mention of entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset containing exactly two human languages. The datasets used are standard object detection datasets with English category labels."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The new dataset introduced by the authors is a few-shot object detection dataset based on the MSCOCO and PASCAL VOC datasets, which have category labels in English only. The paper describes the dataset split into base and novel classes but does not mention any other languages. The textual content related to dataset labels and instructions for LLMs are entirely in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets and data labels discussed in the paper are in English; no non-English language is indicated."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses programming models and algorithms, it does not introduce any dataset entries containing programming language or code snippets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes mathematical notation and formulas describing the problem and models, but the proposed new datasets themselves do not contain entries with mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The proposed datasets are standard image detection datasets and do not include biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of constructed or fictional languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is documented and known (English)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain language (English labels and annotations), so this label does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-859-rubric-6",
    "token_usage": {
      "prompt_tokens": 14463,
      "completion_tokens": 140,
      "total_tokens": 14603
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section",
          "reasoning": "The paper does not provide any information or links regarding the code for constructing any datasets. It uses existing public datasets (MSCOCO and PASCAL VOC) following established protocols, but does not release or describe new dataset construction or related code."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No section",
          "reasoning": "The paper does not introduce any new datasets nor does it describe any dataset creation process. It uses two widely used existing benchmarks (MSCOCO and PASCAL VOC) and follows their evaluation protocols, but no new dataset creation or detailed documentation thereof is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-883-rubric-0",
    "token_usage": {
      "prompt_tokens": 11514,
      "completion_tokens": 258,
      "total_tokens": 11772
    },
    "response": {
      "sources": [
        {
          "Modality": "graph",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.1.1 (Datasets) and throughout the paper",
          "Reasoning": "The new dataset(s) introduced or used by the paper correspond to benchmark datasets of neural architectures represented as directed acyclic graphs with operations as nodes. These datasets are collections of neural architecture graphs generated in the context of neural architecture search benchmarks (e.g., NAS-Bench-101, NAS-Bench-201, NAS-Bench-301, NAS-Bench-ASR, NAS-Bench-Graph). They are algorithmically generated or enumerated architectures, not human-created data like images or audio recordings. The paper uses these benchmarks directly to evaluate the proposed method and does not introduce a new dataset besides these standard benchmarks. However, since the instruction requests only new datasets introduced by the authors, and the paper does not explicitly state the introduction of any new dataset, strictly per instruction, no new dataset is introduced by the authors. Therefore, there are no new data sources introduced. Still, as per the paper, their input modeling takes neural architectures represented as graphs, which are algorithmically constructed data structures. Since no new dataset is introduced, the list is empty."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-883-rubric-1",
    "token_usage": {
      "prompt_tokens": 12314,
      "completion_tokens": 215,
      "total_tokens": 12529
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2 and 4.1.1 Datasets",
            "reasoning": "The paper describes representing neural architectures as directed acyclic graphs derived from benchmark datasets, with no explicit mention of human annotators performing manual annotations. The process of construction appears automatic or based on existing dataset labels and graph representations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No mention of annotation instructions or guidelines for human annotators is provided in the paper for the introduced datasets or their representations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not describe any rubrics or scoring guidelines related to annotation procedures, as annotation appears to be automatic data preparation rather than human labeling."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples are provided or discussed in the paper, as the datasets are standard benchmarks used for model evaluation rather than human annotation tasks."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-883-rubric-2",
    "token_usage": {
      "prompt_tokens": 13504,
      "completion_tokens": 322,
      "total_tokens": 13826
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for any introduced dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on the datasets introduced in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of quality assurance by a single non-expert human annotator is found in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA process involving multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The QA of dataset annotations or content is not described as being done by any AI model in the paper."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses several benchmark datasets, it does not describe any automatic verification or algorithmic quality assurance process applied to the datasets introduced."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces the FLOWERFormer model and evaluates it on several existing benchmark datasets (NAS-Bench-101, NAS-Bench-201, NAS-Bench-301, NAS-Bench-ASR, NAS-Bench-Graph) but does not introduce any new dataset itself, nor does it describe any quality assurance process related to new datasets. Therefore, no quality assurance processes are applied or documented regarding new datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-883-rubric-3",
    "token_usage": {
      "prompt_tokens": 13114,
      "completion_tokens": 399,
      "total_tokens": 13513
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the creation of any new dataset generated entirely from scratch by human contributors. Instead, it uses existing benchmark datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset generated solely by AI or machine learning models without referencing existing data. It focuses on neural architecture encoding models rather than generating new data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of datasets produced through human translation of content from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any datasets produced by machine translation from other languages."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1.1 Datasets",
          "reasoning": "The paper utilizes existing benchmark datasets such as NAS-Bench-101, NAS-Bench-201, NAS-Bench-301, NAS-Bench-ASR, and NAS-Bench-Graph for performance evaluation. These datasets are collected from prior works and aggregated for benchmarking without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe creating new datasets derived from existing ones with modifications or adaptations made by the authors."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources and dataset origins are clearly documented and referenced; hence this category does not apply."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-883-rubric-4",
    "token_usage": {
      "prompt_tokens": 13632,
      "completion_tokens": 358,
      "total_tokens": 13990
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 4 (Experiments)",
          "reasoning": "The paper utilizes existing benchmark datasets (e.g., NAS-Bench-101, NAS-Bench-201, NAS-Bench-301, NAS-Bench-ASR, NAS-Bench-Graph) for evaluating the proposed FLOWERFormer model's performance prediction accuracy. These datasets are employed for performance benchmarking and evaluation of neural architecture encoding methods, not for training from scratch or fine-tuning."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No new dataset is introduced by the authors in the paper. All datasets used are existing benchmark datasets for neural architecture performance prediction, used solely for evaluation and benchmarking."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-883-rubric-5",
    "token_usage": {
      "prompt_tokens": 14355,
      "completion_tokens": 602,
      "total_tokens": 14957
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce or describe any dataset containing more than two human languages. The focus is on neural architecture graphs for performance prediction, not human language data."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset with exactly two human languages is introduced or described in the paper. The datasets discussed (NAS-Bench variants) relate to neural architectures and do not involve human language bilingual content."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1.1 (Datasets); general paper context",
          "reasoning": "The paper and datasets use English for descriptions, names, and documentation. The paper itself is in English and the datasets are related to neural architectures with no mention of any other language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets introduced in the paper contain only non-English language content. The paper and datasets use English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced contain neural architecture graphs composed of nodes and edges with features representing operations (e.g., convolution, pooling), but the datasets themselves are graph-structured data rather than source code or programming scripts."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical notation to describe models and methods, the datasets introduced are neural architecture graph benchmarks consisting of operation nodes and connectivity and do not contain explicit mathematical or logical symbolic notation entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not contain biological sequences, animal signals, or other forms of non-human communication. They focus on neural network architectures."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional language data is part of the datasets introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content in the datasets is clearly documented and consists of operation labels and graph structures, all in English context."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain language in the form of English operation labels and descriptions, so it is not the case that they contain no language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-883-rubric-6",
    "token_usage": {
      "prompt_tokens": 11573,
      "completion_tokens": 215,
      "total_tokens": 11788
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or statement about code release for dataset construction is provided in the paper.",
          "reasoning": "The paper mentions that the authors' code is available at a GitHub repository, but this appears to be for their model (FlowerFormer) rather than for constructing any new dataset. The paper exclusively uses existing benchmark datasets (e.g., NAS-Bench-101, NAS-Bench-201, NAS-Bench-301, NAS-Bench-ASR, NAS-Bench-Graph) without introducing any new datasets constructed by the authors. Therefore, there is no code associated with constructing new datasets provided or referenced."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No explicit documentation of new dataset creation is provided in the paper.",
          "reasoning": "The paper does not introduce any new datasets\u2014it only uses and evaluates on previously established benchmark datasets whose creation details are not discussed. There is no description or documentation in the paper about dataset construction processes by the authors, hence no documentation on new dataset creation."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-961-rubric-0",
    "token_usage": {
      "prompt_tokens": 11950,
      "completion_tokens": 97,
      "total_tokens": 12047
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Implementation Details",
          "Reasoning": "The authors introduced new multi-view video datasets for their experiments, specifically 2 identities from HAvatar comprising multi-view 4K resolution videos, which after preprocessing are used at 2K resolution. These videos are captured using multiple cameras distributed about 120 degrees in front, involving human recording."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-961-rubric-1",
    "token_usage": {
      "prompt_tokens": 12750,
      "completion_tokens": 276,
      "total_tokens": 13026
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 5.1: 'In the experiment we use 12 sets of data, 10 of which are from NeRSemble [28], and the other 2 are multi-view video data from HAvatar [71].",
            "reasoning": "The two datasets from HAvatar are pre-existing datasets, so only the 10 sets from NeRSemble are considered pre-existing. No explicit new dataset is introduced in the paper, but the method is applied to these datasets. There is no mention of a newly introduced dataset collected and annotated by humans or automatic methods."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No specific section or appendix describing annotation instructions for a new dataset.",
            "reasoning": "The paper does not mention any instructions given to annotators related to data annotation for a newly introduced dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No specific section or appendix describing annotation rubrics for a new dataset.",
            "reasoning": "The paper does not mention any scoring or evaluation rubrics used by annotators for a newly introduced dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No appendix or section shows annotation examples for a new dataset.",
            "reasoning": "No examples of annotation or data labeling are described for any new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-961-rubric-2",
    "token_usage": {
      "prompt_tokens": 13940,
      "completion_tokens": 310,
      "total_tokens": 14250
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any single human expert performing quality assurance of dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper of multiple human experts performing quality assurance on dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single human non-expert conducted quality assurance for the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human non-experts performing quality assurance on the dataset or its annotations."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report using any AI model as a judge or for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper mentions automated processes such as background removal and 3D landmark extraction (Section 3 and Section 5.1), these are described as preprocessing steps rather than explicit quality assurance or verification of annotations or dataset content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces the Gaussian Head Avatar dataset using multi-view video sequences, but does not describe any quality assurance process applied to validate annotations or dataset content. No explicit QA process, human or automated, is documented for the datasets used or introduced."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-961-rubric-3",
    "token_usage": {
      "prompt_tokens": 13550,
      "completion_tokens": 458,
      "total_tokens": 14008
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 5.1 Implementation Details",
          "reasoning": "The paper uses 12 sets of data captured from multi-view video recordings of human subjects, 10 from NeRSemble and 2 from HAvatar datasets, which are original real-world multi-view videos of human heads collected by humans. These video datasets are not described as pre-existing datasets created by other researchers but are original collections of multi-view videos including 2K to 4K resolution frames captured from multiple synchronized cameras. The paper further preprocesses these videos by background removal and landmark extraction, indicating the raw captured data is human-collected and original."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset generated entirely by AI or machine learning models as new data within this work."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translating data from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data generated by machine translation systems is reported."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses NeRSemble and HAvatar data, these are pre-existing datasets or data captured by the authors. The paper does not describe aggregating existing datasets without modification as the new dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper preprocesses data by removing backgrounds and estimating 3D landmarks and expression coefficients, these transformations are part of data preparation, not creating new datasets based on existing sources as such."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes the origin and collection method of the datasets used."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-961-rubric-4",
    "token_usage": {
      "prompt_tokens": 14068,
      "completion_tokens": 271,
      "total_tokens": 14339
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2 Training",
          "reasoning": "The paper describes training the Gaussian Head Avatar model from randomly initialized parameters including neutral Gaussians, dynamic generator MLPs, and a super-resolution network, on multi-view RGB videos under supervised losses."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.2 Results and Comparisons",
          "reasoning": "The collected video datasets from NeRSemble and HAvatar are used as training and evaluation splits to quantitatively and qualitatively benchmark the proposed method against state-of-the-art baselines."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.3 Ablation Study",
          "reasoning": "The datasets are employed for ablation studies analyzing the impact of initialization strategies and deformation modeling approaches to understand model performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-961-rubric-5",
    "token_usage": {
      "prompt_tokens": 14791,
      "completion_tokens": 475,
      "total_tokens": 15266
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset presented involves multi-view RGB videos of human heads and appears to be from a single language environment, no indication of more than two human languages in the data."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that exactly two human languages are involved in the dataset content."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5.1 Implementation Details",
          "reasoning": "The dataset consists of video and images annotated and processed using English-based models and terminology. The paper is written entirely in English, and all expressions, annotations, and processing descriptions are in English. The 3D landmarks and expression coefficients derive from models (e.g., Basel Face Model) typically using English descriptors. The dataset itself is facial images/videos from individuals but with associated annotations described in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not have entries in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include code or programming language entries; it contains video and image data along with geometric and deformation labels."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 4.1 Avatar Representation and Section 4.3 Geometry-guided Initialization",
          "reasoning": "The dataset includes mathematical notations related to 3D Gaussian splatting, MLP transformations, SDF representation, and differential mesh extraction, which describe the geometric and deformation models applied to the data representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human head images and associated geometric and expression parameters, but no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no use of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language and annotation descriptions of the dataset are clearly specified as English-based."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language annotations and metadata in English, so entries do contain language."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-961-rubric-6",
    "token_usage": {
      "prompt_tokens": 12009,
      "completion_tokens": 204,
      "total_tokens": 12213
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper",
          "reasoning": "The paper does not provide any explicit mention or link to publicly available code repositories related to the dataset creation, data collection, preprocessing, or generation. Although there is a project page link mentioned in the abstract, it does not specify code availability for dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3, 4.3, 5.1",
          "reasoning": "The paper includes detailed documentation of the dataset creation and preprocessing processes. Section 3 (Overview) and Section 4.3 (Geometry-guided Initialization) describe the data preprocessing steps including background removal, facial landmark extraction, 3D landmark and expression coefficient estimation. Section 5.1 (Implementation Details) further describes the datasets used, their sources, data formats, camera setups, and specific preprocessing procedures such as background removal and landmark extraction, demonstrating good documentation of the dataset preparation and construction."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-967-rubric-0",
    "token_usage": {
      "prompt_tokens": 11042,
      "completion_tokens": 98,
      "total_tokens": 11140
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets and Metrics - DynVideo Dataset",
          "Reasoning": "The DynVideo dataset is introduced by the authors as a new collection of videos capturing a character performing various movements recorded with a mobile phone camera. This dataset consists of videos with human performances captured directly by humans operating the recording devices, hence it is human-generated video data."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-967-rubric-1",
    "token_usage": {
      "prompt_tokens": 11842,
      "completion_tokens": 249,
      "total_tokens": 12091
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Datasets and Metrics",
            "reasoning": "The paper explicitly mentions that the DynVideo dataset is a new dataset collected by the authors to capture dynamic human appearance from monocular videos using a mobile phone, and SMPL parameter sequences are obtained by their proposed method. The annotations (SMPL parameters) are generated by the proposed optimization method, implying an automatic process rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper does not provide any description of annotation instructions for human annotators regarding the new DynVideo dataset; moreover, since the annotation is automatic (SMPL parameters from their method), instructions for manual annotation do not apply."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "No scoring rubrics or evaluation criteria for manual annotation are described in relation to the DynVideo dataset annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "No example annotations or sample annotation instances are provided or described for the DynVideo dataset annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-967-rubric-2",
    "token_usage": {
      "prompt_tokens": 13032,
      "completion_tokens": 297,
      "total_tokens": 13329
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on the new dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple experts performed quality assurance for the new dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance by a single non-expert on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that multiple non-expert annotators conducted quality assurance for the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe or suggest that quality assurance was performed by an AI model on the new dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any automated verification or algorithmic quality assurance procedures applied to the new dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new dataset (DynVideo) collected by the authors but does not describe any quality assurance process or validation procedure applied to the dataset's annotations or content. There is no mention of human or automated QA, nor any details on dataset curation or verification. Hence, no quality assurance process is documented."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-967-rubric-3",
    "token_usage": {
      "prompt_tokens": 12642,
      "completion_tokens": 413,
      "total_tokens": 13055
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets and Metrics",
          "reasoning": "The authors introduce the DynVideo Dataset (Section 4.1), which they collected themselves by recording videos with a mobile phone of a character performing various movements. This dataset is original content created by human contributors from scratch, not adapted or translated from existing data. It includes new monocular video recordings along with corresponding SMPL parameter sequences obtained by the authors' method."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any dataset is generated entirely by AI or machine learning models without referencing or transforming existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication the paper's datasets were produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence of datasets generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes usage of existing datasets like People-Snapshot and NeuMan for evaluation, but does not introduce these as new datasets. The new DynVideo dataset is newly collected rather than aggregated from pre-existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is described as based on existing sources with modifications or adapted from existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source for the new DynVideo dataset is explicitly documented as collected by the authors."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-967-rubric-4",
    "token_usage": {
      "prompt_tokens": 13160,
      "completion_tokens": 228,
      "total_tokens": 13388
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, Section 4.3",
          "reasoning": "The authors introduce a new dataset called DynVideo dataset for evaluating reconstruction quality with an emphasis on dynamic appearances. This dataset is used alongside public datasets for benchmarking and performance measurement of their avatar modeling method, as detailed in the quantitative evaluation results reported in Tables 1, 2, and 3, and qualitative comparisons in Section 4.3."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-967-rubric-5",
    "token_usage": {
      "prompt_tokens": 13883,
      "completion_tokens": 329,
      "total_tokens": 14212
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, e.g., Abstract, Introduction, and Section 4.1 Datasets and Metrics",
          "reasoning": "The paper and the introduced dataset (DynVideo) are entirely documented and described in English, with no mention or inclusion of any other human language. The dataset captures videos of human subjects recorded with mobile phones but does not involve any linguistic content in other languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3.1 Animatable 3D Gaussians, Section 3.2 Dynamic 3D Gaussian Property Estimation",
          "reasoning": "The dataset description and modeling rely extensively on formal mathematical notation and symbolic expressions, including functions, mappings, parameters (e.g., pose \u03b8, shape b), and equations defining skinning and rendering processes used to represent and animate humans."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-967-rubric-6",
    "token_usage": {
      "prompt_tokens": 11101,
      "completion_tokens": 136,
      "total_tokens": 11237
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Footnote in the paper",
          "reasoning": "The paper explicitly states in the abstract that the code and dataset are available at https://github.com/aipixel/GaussianAvatar, indicating that the code related to dataset collection and processing is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Datasets and Metrics",
          "reasoning": "The paper provides a detailed description of the newly introduced DynVideo dataset in Section 4.1, including its collection procedure, content, and associated SMPL parameter sequences, demonstrating transparency and documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-981-rubric-0",
    "token_usage": {
      "prompt_tokens": 11345,
      "completion_tokens": 240,
      "total_tokens": 11585
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.3, 'Expression-dependent Modification Learning'",
          "Reasoning": "The new training dataset is generated by the authors using a large-scale 3DMM-based 3D GAN teacher model to synthesize paired original and edited volumetric avatars with varying geometry and texture modifications under sampled shape, expression, and pose parameters. This synthetic data is used for supervised training distillation of their expression-aware modification generative model. Thus, the image data for training consists of synthetic renderings created by generative models, not captured from human sources."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.3, 'Texture Distillation'",
          "Reasoning": "The authors use real human-generated images from open-sourced makeup datasets as reference images to perform makeup transfer algorithms for texture editing in the distillation process. These makeup dataset images, containing complex makeup styles, are created by humans and serve as inputs for texture editing tools, indicating human-generated image data involved in the new training procedure."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-981-rubric-1",
    "token_usage": {
      "prompt_tokens": 12145,
      "completion_tokens": 279,
      "total_tokens": 12424
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3 Expression-dependent Modification Learning",
            "reasoning": "The paper describes that training data for the expression-aware modification generator is synthesized by leveraging a large-scale 3DMM-based 3D GAN and 2D face texture editing algorithms automatically. This involves sampling latent codes and 3DMM parameters from distributions and generating edited avatars and images via automatic generative models and editing tools, constituting an automatic annotation process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No mention in the paper",
            "reasoning": "The paper does not provide any instructions oriented towards human annotators for performing data annotation, as training data is generated automatically by sampled parameters and models, not by manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention in the paper",
            "reasoning": "No scoring rubrics or evaluation schemes for annotation quality are described, since annotation is generated automatically without human raters."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.3 Expression-dependent Modification Learning",
            "reasoning": "Examples of generated paired data are described, including original and edited avatars or images synthesized via the teacher 3DMM-based GAN and 2D face editing algorithms like SBA and MTA, demonstrating the process of automatic paired editing data creation for training."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-981-rubric-2",
    "token_usage": {
      "prompt_tokens": 13335,
      "completion_tokens": 285,
      "total_tokens": 13620
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication or mention of multiple human experts performing quality assurance on the datasets is provided in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that a single non-expert performed quality assurance for the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA process involving multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence suggests that AI models were used as judges for quality assurance of the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses automated processes for training (e.g., distillation from a teacher model), it does not describe an automated quality assurance process applied to datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any explicit quality assurance process applied to the new datasets or paired editing data used in training. Training data is synthetically generated by a teacher model and 2D editing tools without mention of validation or QA procedures."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-981-rubric-3",
    "token_usage": {
      "prompt_tokens": 12945,
      "completion_tokens": 436,
      "total_tokens": 13381
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any new datasets were created entirely from scratch by human contributors. Instead, their approach uses existing datasets and models for distillation and training."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.3 Expression-dependent Modification Learning",
          "reasoning": "The paper describes generating synthetic training data through a distillation scheme that leverages a pre-trained 3DMM-based 3D GAN (Section 3.3). This method synthesizes paired avatars with different geometry and texture modifications to create training data, indicating the new dataset is generated by AI models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data created via human translation from another language is present in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that any data was created by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the new datasets are simply a compilation or collection of data from existing sources without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.3 Expression-dependent Modification Learning",
          "reasoning": "The new datasets for training the modification generator are derived by applying modifications generated by a teacher 3DMM GAN and 2D face texture editing tools to existing avatars. This constitutes derived data based on existing sources with transformations and adaptations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly details the generation process of the new data used for training, so the data source is specified."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-981-rubric-4",
    "token_usage": {
      "prompt_tokens": 13463,
      "completion_tokens": 275,
      "total_tokens": 13738
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The authors describe training their expression-aware modification generator model from scratch using a distillation scheme that leverages synthetic paired editing data generated by a teacher 3DMM-based 3D GAN and 2D face editing tools, indicating training from scratch with generated datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "User studies and quantitative metrics such as identity similarity are performed to evaluate their editing results, using datasets generated from different avatar representations and editing methods."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.4",
          "reasoning": "Analysis of native editing capabilities of existing avatar models and comparison to baselines is performed using gathered avatar data to understand limitations and justify the new dataset's use."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-981-rubric-5",
    "token_usage": {
      "prompt_tokens": 14186,
      "completion_tokens": 492,
      "total_tokens": 14678
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Whole paper, including abstract and methodology sections",
          "reasoning": "The new dataset entries introduced or used for training are described using English in the entire paper. The paper references English-language datasets such as the open-sourced makeup dataset and tools, and the entire presentation of dataset construction and experiments is in English. There is no indication of any other human language used in dataset labels or content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses latent spaces, generative models, and some implementation details, it does not introduce a new dataset containing programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes some mathematical equations to describe their models and methods, but these are not part of the proposed new dataset itself."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset relates to human facial avatars and does not include biological sequences or non-human communication forms."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or use of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages used in the dataset are specified as English, and no ambiguity is present."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain human language (English) in annotations, descriptions, or related textual data."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-981-rubric-6",
    "token_usage": {
      "prompt_tokens": 11404,
      "completion_tokens": 217,
      "total_tokens": 11621
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Not specified in the paper",
          "reasoning": "The paper does not provide any explicit link, URL, or reference to a publicly available code repository related to the dataset construction or data generation process. Although the project page is mentioned (https://zju3dv.github.io/geneavatar/), there is no clear indication that the dataset construction code is available there or elsewhere."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.3 Expression-dependent Modification Learning, and throughout Section 3 Method",
          "reasoning": "The paper thoroughly documents the dataset construction process for training the modification generator, including the use of a large-scale 3DMM-based 3D GAN teacher model to synthesize original and edited avatars, sampling of 3DMM parameters and latent codes, and employing 2D face editing tools for texture modification to generate paired data. The process is described clearly in Section 3.3 and supported by technical details for reproducing the paired training data for geometry and texture modifications."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-984-rubric-0",
    "token_usage": {
      "prompt_tokens": 14710,
      "completion_tokens": 117,
      "total_tokens": 14827
    },
    "response": {
      "sources": [
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset",
          "Reasoning": "The paper introduces the ArtiFace dataset [55] as a new dataset containing 160 artistic portraits covering diverse artwork styles from Renaissance to Comics, with each face annotated with 68 landmarks. The paper treats 300W and CariFace as existing datasets, while ArtiFace is described with its size and detailed description of styles, indicating it is newly introduced or used distinctively for their experiments."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-984-rubric-1",
    "token_usage": {
      "prompt_tokens": 15510,
      "completion_tokens": 241,
      "total_tokens": 15751
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1 Dataset",
            "reasoning": "The paper introduces the CariFace and ArtiFace datasets explicitly as newly introduced datasets used in experiments. These datasets are described as containing facial images annotated with 68 landmarks, which implies human annotation was performed. Given the complexity of landmark annotation and that these are specifically created datasets, the annotators are likely human experts familiar with facial landmarks."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1 Dataset",
            "reasoning": "The paper does not provide any explicit mention of annotation guidelines, instructions given to annotators, or training procedures for landmark annotation for the new datasets."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1 Dataset",
            "reasoning": "There is no information or mention of any scoring rubrics or evaluation criteria provided to annotators for landmark labeling in the newly introduced datasets."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.1 Dataset",
            "reasoning": "No examples of annotated landmarks or sample annotation instances or guidelines examples are provided in the paper for the new datasets."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-984-rubric-2",
    "token_usage": {
      "prompt_tokens": 16700,
      "completion_tokens": 316,
      "total_tokens": 17016
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance of the new dataset annotations was performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of multiple human experts performing quality assurance or annotation validation for the new datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information presented about any single human non-expert performing quality assurance or annotation validation."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving multiple human non-expert annotators or validators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication exists that AI models were used as judges for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide details on any automated verification or algorithmic rule-based technique used for quality assurance of dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces the ArtiFace dataset as a new dataset or at least uses it extensively, but it does not describe any quality assurance processes for the dataset annotations. There is no documentation or description of who performed the annotation or how the annotation quality was validated. Therefore, we must conclude that no quality assurance process is applied or documented for the new dataset annotation quality in this paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-984-rubric-3",
    "token_usage": {
      "prompt_tokens": 16310,
      "completion_tokens": 426,
      "total_tokens": 16736
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper states that the CariFace dataset consists of caricatures collected from the Internet, where faces have been annotated with landmarks. The ArtiFace dataset contains artistic portraits annotated with landmarks. These annotated datasets imply human effort in labeling the data, indicating new data created by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any new data solely by AI or models without reference or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by translating content through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no reference to machine translation producing any dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The CariFace dataset is described as being created by searching and selecting caricatures from the Internet, which is an aggregation of existing images. Similarly, 300W dataset is a compilation of several known face datasets. This points to the use of collated data collected from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper utilizes pseudo landmarks generated through warping to improve landmarking models, this is a training strategy rather than a new dataset derived by modifying existing data. No specific mention of derived dataset creation is given."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset origins and characteristics are specified clearly in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-984-rubric-4",
    "token_usage": {
      "prompt_tokens": 16828,
      "completion_tokens": 271,
      "total_tokens": 17099
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2 and Algorithm 1",
          "reasoning": "The paper describes training the face landmarker model from scratch using labeled real human face datasets (e.g., 300W) and unlabeled stylized face datasets (CariFace and ArtiFace) under an alternating optimization framework guided by conditional face warping."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 4.3",
          "reasoning": "The introduced stylized face datasets (CariFace and ArtiFace) are used to evaluate the generalization and performance of the trained face landmarker on stylized and unseen face domains, measuring Normalized Mean Error (NME) against ground truth landmarks."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "CVPR-2024-984-rubric-5",
    "token_usage": {
      "prompt_tokens": 17551,
      "completion_tokens": 537,
      "total_tokens": 18088
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset (Pages 8-9), Supplementary material references, and throughout the paper",
          "reasoning": "The newly introduced datasets (CariFace and ArtiFace) and the 300W dataset all contain facial images primarily labeled with landmarks, and the annotation, method descriptions, and all textual parts in the paper are presented in English. There is no mention of any non-English textual data or multilingual content within these datasets. The label information is numeric landmark coordinates, and any associated text such as labels and annotations appear in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the paper presents loss functions and algorithms, these are method descriptions, not datasets containing code entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical expressions appear within the paper text to describe methods, but the datasets themselves do not contain mathematical or formal logical expressions as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of human facial images and landmark annotations, with no non-human communication or biological sequences."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificially constructed languages are involved in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language or labeling schema is clearly described and documented; there is no unknown or unspecified language content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain entries related to human face images and landmark annotations (numerical labels), which are associated with human language annotations in English; thus, they are not language-free datasets."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-984-rubric-6",
    "token_usage": {
      "prompt_tokens": 14769,
      "completion_tokens": 163,
      "total_tokens": 14932
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or location in the paper mentions dataset code release.",
          "reasoning": "The paper does not provide any indication or link to code for dataset construction or preprocessing. The datasets mentioned (300W, CariFace, ArtiFace) are existing datasets or collected datasets but no code for creation is referenced or linked."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper documents the datasets used, including descriptions of the 300W, CariFace, and ArtiFace datasets along with their characteristics, sample counts, annotation details, and usage in experiments. However, the documentation appears to be on dataset usage rather than dataset construction, and no detailed process for dataset creation is provided."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-996-rubric-0",
    "token_usage": {
      "prompt_tokens": 13203,
      "completion_tokens": 325,
      "total_tokens": 13528
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (Text descriptions), Section 4.1",
          "Reasoning": "The text descriptions used in the new dataset are high-level language prompts and instructions specifying human actions with spatial relations in scenes. These texts are human-generated as they follow a natural language template from Sr3D and are used as input instructions in the dataset."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (Scene representations), Section 4.2 and 4.3",
          "Reasoning": "The dataset includes 3D scenes represented as point clouds captured from real 3D scans. These point clouds contain positions and normal information of each point and are obtained via human-involved scanning or capturing processes. Additionally, volumetric sensors (environment, target, trajectory sensors) constructed around the target object and human are derived from these sensor data to represent the scenes in an object-centric manner."
        },
        {
          "Modality": "time series",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (Motion representations), Section 5.2",
          "Reasoning": "The human motion data in the dataset is represented as sequences of SMPL-X body meshes over time (i.e., dynamic sequences of human poses and global orientations). These motion sequences are human-captured or generated from recorded real human motions as per the HUMANISE dataset, which this paper uses and fine-tunes on for motion generation training."
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-996-rubric-1",
    "token_usage": {
      "prompt_tokens": 14003,
      "completion_tokens": 633,
      "total_tokens": 14636
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 5.2, Section 4.4, and Section 3",
            "reasoning": "The HUMANISE dataset is described as a benchmark dataset with 16.5k motion sequences and 543 scenes, originally from prior works and used here for training and evaluation. It appears to be provided from prior datasets (some motions from AMASS dataset augmented with labels) and is not introduced as newly annotated data in this paper. The new dataset aspects are not explicitly described as human-annotated but based on existing datasets (HUMANISE, AMASS) and synthetic processing. Additionally, the paper involves motion data from AMASS and the HUMANISE dataset, which is prior work. No indication of multiple annotators or crowdsourcing is present for the dataset itself."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit annotation guidelines, instructions, or dataset creation instructions are detailed in the paper for the HUMANISE dataset or the motion data.",
            "reasoning": "The paper does not provide any detailed instructions about annotation or dataset creation for HUMANISE or any new dataset; it uses existing datasets and focuses on method development."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention in Sections 3, 4, or 5 of annotation scoring rubrics for the dataset.",
            "reasoning": "No scoring rubrics or evaluation criteria for annotation are described; metrics relate to generated motion evaluation rather than annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of annotations or annotation guidelines provided in the paper (no Appendix B mentioned).",
            "reasoning": "There are no sample annotations or example labelings presented in the paper for dataset annotation tasks."
          }
        },
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 4.1 (Language grounding of objects in 3D scenes) and Figure 3",
            "reasoning": "The localization of target objects is performed using ChatGPT, a large language model, with prompts constructed from scene graph textual descriptions and text instructions. This is an annotation-like process that produces object localization labels via an AI model (LLM), not by humans."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.1 describes the prompt engineering process with two-stage prompting for ChatGPT.",
            "reasoning": "Detailed prompt design and procedure for guiding the LLM to localize objects from text and scene graph is explained, effectively acting as instructions for the LLM to perform annotation (localization)."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit rubrics or scoring systems described for the LLM localization annotation process.",
            "reasoning": "The paper does not define rubrics to score or guide the LLM output beyond prompt design."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figure 3 and Section 4.1 provide example prompts and answers used in the two-stage process for the LLM localization.",
            "reasoning": "Explicit few-shot examples and question-answer format samples are provided and discussed for prompt construction in localizing target objects using ChatGPT."
          }
        }
      ]
    }
  },
  {
    "id": "CVPR-2024-996-rubric-2",
    "token_usage": {
      "prompt_tokens": 15193,
      "completion_tokens": 298,
      "total_tokens": 15491
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human expert for dataset annotation or validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any involvement of multiple human experts performing quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert human annotator conducted quality assurance for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any QA process involving multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the method uses AI models to generate and evaluate motions, the paper does not mention using AI models as judges specifically for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification processes or algorithmic rule-based techniques for quality assurance of dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new method for motion generation and leverages existing datasets (HUMANISE and PROX) for experiments, but it does not introduce any new datasets. Therefore, there is no quality assurance process described or performed for any new dataset."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-996-rubric-3",
    "token_usage": {
      "prompt_tokens": 14803,
      "completion_tokens": 472,
      "total_tokens": 15275
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any newly created dataset content was entirely created from scratch by human contributors. Instead, it uses existing datasets for training and evaluation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper generates human motion data using diffusion models, but these generated motions are outputs of the method, not new datasets introduced by the authors. The paper does not claim to release any newly generated datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translating data from other languages by humans to create new data."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to generate data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.2 Evaluation on HUMANISE dataset",
          "reasoning": "The paper uses the HUMANISE dataset, which consists of 16.5k motion sequences in 543 scenes for training and 3.1k motion sequences in 100 scenes for testing. The paper does not claim to create a new dataset but uses this existing dataset for evaluation and fine-tuning. The HUMANISE dataset itself is based on existing motion capture data and 3D scanned scenes aggregated together. Thus, the data used is collated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the authors transformed or adapted existing datasets to create new derivatives. The paper describes usage and evaluation on existing datasets, including pretraining on AMASS and fine-tuning on HUMANISE."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and nature of datasets used (HUMANISE and AMASS) are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-996-rubric-4",
    "token_usage": {
      "prompt_tokens": 15321,
      "completion_tokens": 290,
      "total_tokens": 15611
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.4 Implementation details; Section 5.2 Generating human motion from text and scene",
          "reasoning": "The HUMANISE dataset, which is a known dataset and not introduced by the authors, is used for fine-tuning the model after pre-training on the AMASS dataset. The paper discusses fine-tuning on HUMANISE to improve motion quality, indicating the dataset's role in supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new dataset. The datasets referenced and used are pre-existing ones, such as HUMANISE and PROX. The authors use these existing datasets for training and evaluation but do not propose or describe a new dataset. Therefore, no dataset introduced by the authors has practical usage described in the paper."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-996-rubric-5",
    "token_usage": {
      "prompt_tokens": 16044,
      "completion_tokens": 646,
      "total_tokens": 16690
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3 (Problem setup and preliminaries), Section 4.1 (Language grounding of objects in 3D scenes), and general text throughout the paper",
          "reasoning": "The paper explicitly describes the textual inputs and instructions used in the datasets as English language text (e.g., textual descriptions following templates in Sr3D [1] which are English phrases), and all prompts, questions, and instructions for ChatGPT and other LLMs are in English. There is no mention of any other natural language used in the dataset descriptions or experimental inputs. Therefore, the proposed new dataset (HUMANISE dataset with text annotations) contains entries only with English language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the method involves algorithmic model descriptions and usage of diffusion models, the proposed dataset itself does not contain programming or code-related content as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 3 (Problem setup and preliminaries), Section 4 (Method), Equations (1), (2), (3), (4)",
          "reasoning": "The dataset includes structured annotations such as 3D scene graphs, positions, orientations, and motion representations encoded via mathematical formalisms (e.g., 3D point clouds, SMPL-X parameters), and the paper documents these with formal equations. Thus, the dataset contains mathematical and logical notation for representing motion sequences, scene graphs, and spatial relations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on human motion capture and scene interaction, involving human semantic language and 3D motion data but does not include biological sequences such as DNA or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or use of constructed or fictional languages in the dataset or associated with the textual inputs."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages used in the dataset entries are clearly documented and described as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains textual language data in English as part of the annotations and inputs."
        }
      }
    }
  },
  {
    "id": "CVPR-2024-996-rubric-6",
    "token_usage": {
      "prompt_tokens": 13262,
      "completion_tokens": 166,
      "total_tokens": 13428
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 4.4 Implementation details",
          "reasoning": "The paper mentions that the code will be available at a link, but no explicit link or repository is provided within the paper text. There is no detailed mention or statement confirming that code related to dataset construction or preparation is publicly released. Thus, the code for data collection or preprocessing is not clearly stated as publicly available."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No explicit section or detailed description",
          "reasoning": "The paper uses existing datasets such as HUMANISE and PROX for experiments and does not introduce any new dataset constructed by the authors. There is no documentation about dataset creation or construction process provided in the paper. Therefore, there is no documentation related to new dataset creation."
        }
      }
    }
  }
]